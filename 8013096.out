--------------------- Slurm Task Prolog ------------------------
Job ID: 8013096
Job name: pong_supervised_gnn
Host: cn-402
Date: Wed Jul 10 19:19:46 CEST 2024
User: rishabh.bhatia
Slurm account: rleap
Slurm partition: rleap_gpu_24gb
Work dir: 
------------------
Node usage:
JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
8008578 rleap_gpu hiking.s blai.bon  R   23:53:27      1 cn-402
8012560 rleap_gpu exp5_del markus.f  R    1:24:28      1 cn-402
8013096 rleap_gpu pong_sup rishabh.  R       0:01      1 cn-402
------------------
Show launch script with:
sacct -B -j 
------------------
--------------------- Slurm Task Prolog ------------------------
wandb: Currently logged in as: rishabh-bhatia (rwth-ml). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.1
wandb: Run data is saved locally in /work/rleap1/rishabh.bhatia/Master-Thesis-GNN/wandb/run-20240710_191953-zh4pvtyg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-mountain-189
wandb: ‚≠êÔ∏è View project at https://wandb.ai/rwth-ml/gnn_atari_pong
wandb: üöÄ View run at https://wandb.ai/rwth-ml/gnn_atari_pong/runs/zh4pvtyg
wandb: Currently logged in as: rishabh-bhatia (rwth-ml). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: rishabh-bhatia (rwth-ml). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: rishabh-bhatia (rwth-ml). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: rishabh-bhatia (rwth-ml). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: rishabh-bhatia (rwth-ml). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: rishabh-bhatia (rwth-ml). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...
wandb: - Waiting for wandb.init()...
wandb: \ Waiting for wandb.init()...
wandb: \ Waiting for wandb.init()...
wandb: | Waiting for wandb.init()...
wandb: | Waiting for wandb.init()...
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.1
wandb: Run data is saved locally in /work/rleap1/rishabh.bhatia/Master-Thesis-GNN/wandb/run-20240710_192003-fqepfvjh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sponge-192
wandb: ‚≠êÔ∏è View project at https://wandb.ai/rwth-ml/gnn_atari_pong
wandb: üöÄ View run at https://wandb.ai/rwth-ml/gnn_atari_pong/runs/fqepfvjh
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.[0m
  logger.warn(
wandb: / Waiting for wandb.init()...
wandb: / Waiting for wandb.init()...
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.1
wandb: Run data is saved locally in /work/rleap1/rishabh.bhatia/Master-Thesis-GNN/wandb/run-20240710_192003-qzh6bw9r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-meadow-191
wandb: ‚≠êÔ∏è View project at https://wandb.ai/rwth-ml/gnn_atari_pong
wandb: üöÄ View run at https://wandb.ai/rwth-ml/gnn_atari_pong/runs/qzh6bw9r
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.1
wandb: Run data is saved locally in /work/rleap1/rishabh.bhatia/Master-Thesis-GNN/wandb/run-20240710_192003-q2ssnmu8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comfy-eon-190
wandb: ‚≠êÔ∏è View project at https://wandb.ai/rwth-ml/gnn_atari_pong
wandb: üöÄ View run at https://wandb.ai/rwth-ml/gnn_atari_pong/runs/q2ssnmu8
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.[0m
  logger.warn(
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.[0m
  logger.warn(
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.1
wandb: Run data is saved locally in /work/rleap1/rishabh.bhatia/Master-Thesis-GNN/wandb/run-20240710_192004-snm02dsm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-butterfly-193
wandb: ‚≠êÔ∏è View project at https://wandb.ai/rwth-ml/gnn_atari_pong
wandb: üöÄ View run at https://wandb.ai/rwth-ml/gnn_atari_pong/runs/snm02dsm
wandb: Currently logged in as: rishabh-bhatia (rwth-ml). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: rishabh-bhatia (rwth-ml). Use `wandb login --relogin` to force relogin
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.[0m
  logger.warn(
wandb: - Waiting for wandb.init()...
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.1
wandb: Run data is saved locally in /work/rleap1/rishabh.bhatia/Master-Thesis-GNN/wandb/run-20240710_192003-451eaqzm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dauntless-butterfly-194
wandb: ‚≠êÔ∏è View project at https://wandb.ai/rwth-ml/gnn_atari_pong
wandb: üöÄ View run at https://wandb.ai/rwth-ml/gnn_atari_pong/runs/451eaqzm
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.[0m
  logger.warn(
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.1
wandb: Run data is saved locally in /work/rleap1/rishabh.bhatia/Master-Thesis-GNN/wandb/run-20240710_192008-4u3p6wsp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zany-dawn-196
wandb: ‚≠êÔ∏è View project at https://wandb.ai/rwth-ml/gnn_atari_pong
wandb: üöÄ View run at https://wandb.ai/rwth-ml/gnn_atari_pong/runs/4u3p6wsp
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.[0m
  logger.warn(
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.1
wandb: Run data is saved locally in /work/rleap1/rishabh.bhatia/Master-Thesis-GNN/wandb/run-20240710_192008-obd2j4t9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-snowflake-197
wandb: ‚≠êÔ∏è View project at https://wandb.ai/rwth-ml/gnn_atari_pong
wandb: üöÄ View run at https://wandb.ai/rwth-ml/gnn_atari_pong/runs/obd2j4t9
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.[0m
  logger.warn(
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.1
wandb: Run data is saved locally in /work/rleap1/rishabh.bhatia/Master-Thesis-GNN/wandb/run-20240710_192003-swmm8r7y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smart-dream-195
wandb: ‚≠êÔ∏è View project at https://wandb.ai/rwth-ml/gnn_atari_pong
wandb: üöÄ View run at https://wandb.ai/rwth-ml/gnn_atari_pong/runs/swmm8r7y
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.[0m
  logger.warn(
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f4712a6de40> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f47121203d0>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 738      |
|    ep_rew_mean     | -63      |
| time/              |          |
|    fps             | 827      |
|    iterations      | 1        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 727         |
|    ep_rew_mean          | -63.9       |
| time/                   |             |
|    fps                  | 200         |
|    iterations           | 2           |
|    time_elapsed         | 163         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.008650014 |
|    clip_fraction        | 0.0718      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | -0.0232     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.967       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.000732   |
|    value_loss           | 2.61        |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=35.80 +/- 3.40
Episode length: 3000.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 3e+03     |
|    mean_reward          | 35.8      |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 0.0087387 |
|    clip_fraction        | 0.0702    |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.09     |
|    explained_variance   | 0.726     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.37      |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.00403  |
|    value_loss           | 3.53      |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 795      |
|    ep_rew_mean     | -60.6    |
| time/              |          |
|    fps             | 134      |
|    iterations      | 3        |
|    time_elapsed    | 364      |
|    total_timesteps | 49152    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 868         |
|    ep_rew_mean          | -56         |
| time/                   |             |
|    fps                  | 131         |
|    iterations           | 4           |
|    time_elapsed         | 497         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.009039925 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.8         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00674    |
|    value_loss           | 4.94        |
-----------------------------------------
Num timesteps: 80000
Best mean reward: -inf - Last mean reward per episode: -47.66
Saving new best model at 86988 timesteps
Saving new best model to ./logs/Pong-GNN-training/best_model.zip
Eval num_timesteps=80000, episode_reward=42.10 +/- 7.71
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 42.1        |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.009800492 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.42        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00877    |
|    value_loss           | 5.39        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 944      |
|    ep_rew_mean     | -51.7    |
| time/              |          |
|    fps             | 120      |
|    iterations      | 5        |
|    time_elapsed    | 682      |
|    total_timesteps | 81920    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | -45.1       |
| time/                   |             |
|    fps                  | 122         |
|    iterations           | 6           |
|    time_elapsed         | 805         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.011625949 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.685       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.42        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0055     |
|    value_loss           | 5.1         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.12e+03    |
|    ep_rew_mean          | -36.5       |
| time/                   |             |
|    fps                  | 123         |
|    iterations           | 7           |
|    time_elapsed         | 928         |
|    total_timesteps      | 114688      |
| train/                  |             |
|    approx_kl            | 0.014889757 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.699       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.94        |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0023     |
|    value_loss           | 3.43        |
-----------------------------------------
Eval num_timesteps=120000, episode_reward=91.20 +/- 20.07
Episode length: 2656.00 +/- 431.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 2.66e+03  |
|    mean_reward          | 91.2      |
| time/                   |           |
|    total_timesteps      | 120000    |
| train/                  |           |
|    approx_kl            | 0.0132805 |
|    clip_fraction        | 0.166     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.06     |
|    explained_variance   | 0.662     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.24      |
|    n_updates            | 70        |
|    policy_gradient_loss | -1.39e-05 |
|    value_loss           | 3.15      |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.21e+03 |
|    ep_rew_mean     | -28.5    |
| time/              |          |
|    fps             | 118      |
|    iterations      | 8        |
|    time_elapsed    | 1109     |
|    total_timesteps | 131072   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.3e+03     |
|    ep_rew_mean          | -21.7       |
| time/                   |             |
|    fps                  | 118         |
|    iterations           | 9           |
|    time_elapsed         | 1240        |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.011809381 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.717       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.75        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00337    |
|    value_loss           | 2.68        |
-----------------------------------------
Num timesteps: 160000
Best mean reward: -47.66 - Last mean reward per episode: 5.32
Saving new best model at 193120 timesteps
Saving new best model to ./logs/Pong-GNN-training/best_model.zip
Eval num_timesteps=160000, episode_reward=59.40 +/- 17.36
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 59.4        |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.013179647 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.821       |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.000122    |
|    value_loss           | 2.43        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.4e+03  |
|    ep_rew_mean     | -13.1    |
| time/              |          |
|    fps             | 114      |
|    iterations      | 10       |
|    time_elapsed    | 1425     |
|    total_timesteps | 163840   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.54e+03    |
|    ep_rew_mean          | -2.23       |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 11          |
|    time_elapsed         | 1555        |
|    total_timesteps      | 180224      |
| train/                  |             |
|    approx_kl            | 0.012472589 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.05        |
|    n_updates            | 100         |
|    policy_gradient_loss | 0.001       |
|    value_loss           | 2.03        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.67e+03    |
|    ep_rew_mean          | 8.65        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 12          |
|    time_elapsed         | 1681        |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.013804957 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.83        |
|    n_updates            | 110         |
|    policy_gradient_loss | 0.00245     |
|    value_loss           | 1.8         |
-----------------------------------------
Eval num_timesteps=200000, episode_reward=61.40 +/- 30.59
Episode length: 3000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3e+03        |
|    mean_reward          | 61.4         |
| time/                   |              |
|    total_timesteps      | 200000       |
| train/                  |              |
|    approx_kl            | 0.0140576875 |
|    clip_fraction        | 0.176        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.04        |
|    explained_variance   | 0.732        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.319        |
|    n_updates            | 120          |
|    policy_gradient_loss | 0.000925     |
|    value_loss           | 1.66         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.77e+03 |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 13       |
|    time_elapsed    | 1866     |
|    total_timesteps | 212992   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.88e+03    |
|    ep_rew_mean          | 27.5        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 14          |
|    time_elapsed         | 1997        |
|    total_timesteps      | 229376      |
| train/                  |             |
|    approx_kl            | 0.014482273 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.672       |
|    n_updates            | 130         |
|    policy_gradient_loss | 0.00128     |
|    value_loss           | 1.51        |
-----------------------------------------
Num timesteps: 240000
Best mean reward: 5.32 - Last mean reward per episode: 56.29
Saving new best model at 303695 timesteps
Saving new best model to ./logs/Pong-GNN-training/best_model.zip
Eval num_timesteps=240000, episode_reward=47.00 +/- 14.30
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 47          |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.012492142 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.763       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.302       |
|    n_updates            | 140         |
|    policy_gradient_loss | 0.000465    |
|    value_loss           | 1.47        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | 35.5     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 15       |
|    time_elapsed    | 2180     |
|    total_timesteps | 245760   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.07e+03    |
|    ep_rew_mean          | 45.7        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 16          |
|    time_elapsed         | 2311        |
|    total_timesteps      | 262144      |
| train/                  |             |
|    approx_kl            | 0.012442539 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.7         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.389       |
|    n_updates            | 150         |
|    policy_gradient_loss | 0.00235     |
|    value_loss           | 1.66        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.15e+03    |
|    ep_rew_mean          | 52.4        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 17          |
|    time_elapsed         | 2437        |
|    total_timesteps      | 278528      |
| train/                  |             |
|    approx_kl            | 0.012538707 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.657       |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00142    |
|    value_loss           | 1.54        |
-----------------------------------------
Eval num_timesteps=280000, episode_reward=43.90 +/- 7.33
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 43.9        |
| time/                   |             |
|    total_timesteps      | 280000      |
| train/                  |             |
|    approx_kl            | 0.014627841 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.747       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.639       |
|    n_updates            | 170         |
|    policy_gradient_loss | 0.00287     |
|    value_loss           | 1.83        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.23e+03 |
|    ep_rew_mean     | 58.7     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 18       |
|    time_elapsed    | 2623     |
|    total_timesteps | 294912   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.31e+03    |
|    ep_rew_mean          | 68.9        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 19          |
|    time_elapsed         | 2754        |
|    total_timesteps      | 311296      |
| train/                  |             |
|    approx_kl            | 0.013141506 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.629       |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.000692   |
|    value_loss           | 1.36        |
-----------------------------------------
Num timesteps: 320000
Best mean reward: 56.29 - Last mean reward per episode: 70.64
Saving new best model at 413577 timesteps
Saving new best model to ./logs/Pong-GNN-training/best_model.zip
Eval num_timesteps=320000, episode_reward=48.90 +/- 23.56
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 48.9        |
| time/                   |             |
|    total_timesteps      | 320000      |
| train/                  |             |
|    approx_kl            | 0.011478149 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.411       |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.000651   |
|    value_loss           | 1.75        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.34e+03 |
|    ep_rew_mean     | 70.5     |
| time/              |          |
|    fps             | 111      |
|    iterations      | 20       |
|    time_elapsed    | 2936     |
|    total_timesteps | 327680   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.36e+03     |
|    ep_rew_mean          | 72.6         |
| time/                   |              |
|    fps                  | 112          |
|    iterations           | 21           |
|    time_elapsed         | 3067         |
|    total_timesteps      | 344064       |
| train/                  |              |
|    approx_kl            | 0.0127163455 |
|    clip_fraction        | 0.143        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.02        |
|    explained_variance   | 0.755        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.307        |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.0017      |
|    value_loss           | 1.83         |
------------------------------------------
Eval num_timesteps=360000, episode_reward=52.50 +/- 30.71
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 52.5        |
| time/                   |             |
|    total_timesteps      | 360000      |
| train/                  |             |
|    approx_kl            | 0.011192925 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.423       |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00115    |
|    value_loss           | 1.54        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.39e+03 |
|    ep_rew_mean     | 74.1     |
| time/              |          |
|    fps             | 110      |
|    iterations      | 22       |
|    time_elapsed    | 3253     |
|    total_timesteps | 360448   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.4e+03    |
|    ep_rew_mean          | 75.2       |
| time/                   |            |
|    fps                  | 111        |
|    iterations           | 23         |
|    time_elapsed         | 3384       |
|    total_timesteps      | 376832     |
| train/                  |            |
|    approx_kl            | 0.01231358 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.04      |
|    explained_variance   | 0.766      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.44       |
|    n_updates            | 220        |
|    policy_gradient_loss | 0.000413   |
|    value_loss           | 1.65       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.36e+03    |
|    ep_rew_mean          | 74.6        |
| time/                   |             |
|    fps                  | 111         |
|    iterations           | 24          |
|    time_elapsed         | 3515        |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.012577966 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.78        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.38        |
|    n_updates            | 230         |
|    policy_gradient_loss | 0.00161     |
|    value_loss           | 2.19        |
-----------------------------------------
Num timesteps: 400000
Best mean reward: 70.64 - Last mean reward per episode: 66.44
Eval num_timesteps=400000, episode_reward=34.20 +/- 23.30
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 34.2        |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.014650315 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.876       |
|    n_updates            | 240         |
|    policy_gradient_loss | 0.000696    |
|    value_loss           | 2.33        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.34e+03 |
|    ep_rew_mean     | 72.9     |
| time/              |          |
|    fps             | 110      |
|    iterations      | 25       |
|    time_elapsed    | 3700     |
|    total_timesteps | 409600   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.33e+03     |
|    ep_rew_mean          | 72.4         |
| time/                   |              |
|    fps                  | 111          |
|    iterations           | 26           |
|    time_elapsed         | 3831         |
|    total_timesteps      | 425984       |
| train/                  |              |
|    approx_kl            | 0.0125472015 |
|    clip_fraction        | 0.146        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0.768        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.05         |
|    n_updates            | 250          |
|    policy_gradient_loss | 0.000127     |
|    value_loss           | 1.97         |
------------------------------------------
Eval num_timesteps=440000, episode_reward=47.50 +/- 14.98
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 47.5        |
| time/                   |             |
|    total_timesteps      | 440000      |
| train/                  |             |
|    approx_kl            | 0.013189371 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.775       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.465       |
|    n_updates            | 260         |
|    policy_gradient_loss | 0.00084     |
|    value_loss           | 2.18        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.3e+03  |
|    ep_rew_mean     | 71       |
| time/              |          |
|    fps             | 110      |
|    iterations      | 27       |
|    time_elapsed    | 4014     |
|    total_timesteps | 442368   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.29e+03    |
|    ep_rew_mean          | 70.4        |
| time/                   |             |
|    fps                  | 110         |
|    iterations           | 28          |
|    time_elapsed         | 4145        |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.013365014 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.598       |
|    n_updates            | 270         |
|    policy_gradient_loss | 0.000517    |
|    value_loss           | 2.07        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.27e+03    |
|    ep_rew_mean          | 68.9        |
| time/                   |             |
|    fps                  | 111         |
|    iterations           | 29          |
|    time_elapsed         | 4271        |
|    total_timesteps      | 475136      |
| train/                  |             |
|    approx_kl            | 0.014611945 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.575       |
|    n_updates            | 280         |
|    policy_gradient_loss | 0.000974    |
|    value_loss           | 2           |
-----------------------------------------
Num timesteps: 480000
Best mean reward: 70.64 - Last mean reward per episode: 64.19
Eval num_timesteps=480000, episode_reward=72.80 +/- 31.62
Episode length: 2628.00 +/- 744.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.63e+03    |
|    mean_reward          | 72.8        |
| time/                   |             |
|    total_timesteps      | 480000      |
| train/                  |             |
|    approx_kl            | 0.016013734 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.741       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.29        |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00191    |
|    value_loss           | 1.89        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.27e+03 |
|    ep_rew_mean     | 66.6     |
| time/              |          |
|    fps             | 110      |
|    iterations      | 30       |
|    time_elapsed    | 4451     |
|    total_timesteps | 491520   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.26e+03    |
|    ep_rew_mean          | 67.4        |
| time/                   |             |
|    fps                  | 110         |
|    iterations           | 31          |
|    time_elapsed         | 4581        |
|    total_timesteps      | 507904      |
| train/                  |             |
|    approx_kl            | 0.015359576 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.775       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.495       |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.000481   |
|    value_loss           | 2.15        |
-----------------------------------------
Eval num_timesteps=520000, episode_reward=41.30 +/- 9.33
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 41.3        |
| time/                   |             |
|    total_timesteps      | 520000      |
| train/                  |             |
|    approx_kl            | 0.016425889 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.235       |
|    n_updates            | 310         |
|    policy_gradient_loss | 0.00247     |
|    value_loss           | 1.79        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.27e+03 |
|    ep_rew_mean     | 67.9     |
| time/              |          |
|    fps             | 109      |
|    iterations      | 32       |
|    time_elapsed    | 4769     |
|    total_timesteps | 524288   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.27e+03    |
|    ep_rew_mean          | 66.1        |
| time/                   |             |
|    fps                  | 110         |
|    iterations           | 33          |
|    time_elapsed         | 4900        |
|    total_timesteps      | 540672      |
| train/                  |             |
|    approx_kl            | 0.016965307 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.77        |
|    n_updates            | 320         |
|    policy_gradient_loss | 0.00401     |
|    value_loss           | 2.06        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.29e+03    |
|    ep_rew_mean          | 62.8        |
| time/                   |             |
|    fps                  | 110         |
|    iterations           | 34          |
|    time_elapsed         | 5031        |
|    total_timesteps      | 557056      |
| train/                  |             |
|    approx_kl            | 0.015553951 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.751       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.374       |
|    n_updates            | 330         |
|    policy_gradient_loss | 0.00257     |
|    value_loss           | 1.92        |
-----------------------------------------
Num timesteps: 560000
Best mean reward: 70.64 - Last mean reward per episode: 58.88
Eval num_timesteps=560000, episode_reward=61.20 +/- 28.54
Episode length: 2699.00 +/- 602.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.7e+03     |
|    mean_reward          | 61.2        |
| time/                   |             |
|    total_timesteps      | 560000      |
| train/                  |             |
|    approx_kl            | 0.015414266 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.669       |
|    n_updates            | 340         |
|    policy_gradient_loss | 0.0046      |
|    value_loss           | 2.73        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.27e+03 |
|    ep_rew_mean     | 61.9     |
| time/              |          |
|    fps             | 110      |
|    iterations      | 35       |
|    time_elapsed    | 5207     |
|    total_timesteps | 573440   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.24e+03  |
|    ep_rew_mean          | 61.2      |
| time/                   |           |
|    fps                  | 110       |
|    iterations           | 36        |
|    time_elapsed         | 5338      |
|    total_timesteps      | 589824    |
| train/                  |           |
|    approx_kl            | 0.0161867 |
|    clip_fraction        | 0.192     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.03     |
|    explained_variance   | 0.701     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.358     |
|    n_updates            | 350       |
|    policy_gradient_loss | 0.003     |
|    value_loss           | 2.26      |
---------------------------------------
Eval num_timesteps=600000, episode_reward=36.40 +/- 3.29
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 36.4        |
| time/                   |             |
|    total_timesteps      | 600000      |
| train/                  |             |
|    approx_kl            | 0.014604416 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.05        |
|    n_updates            | 360         |
|    policy_gradient_loss | 0.00159     |
|    value_loss           | 2.04        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.25e+03 |
|    ep_rew_mean     | 62       |
| time/              |          |
|    fps             | 109      |
|    iterations      | 37       |
|    time_elapsed    | 5525     |
|    total_timesteps | 606208   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.27e+03    |
|    ep_rew_mean          | 60.9        |
| time/                   |             |
|    fps                  | 110         |
|    iterations           | 38          |
|    time_elapsed         | 5655        |
|    total_timesteps      | 622592      |
| train/                  |             |
|    approx_kl            | 0.014501795 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.794       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.75        |
|    n_updates            | 370         |
|    policy_gradient_loss | 0.00335     |
|    value_loss           | 1.75        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.27e+03    |
|    ep_rew_mean          | 61.5        |
| time/                   |             |
|    fps                  | 110         |
|    iterations           | 39          |
|    time_elapsed         | 5786        |
|    total_timesteps      | 638976      |
| train/                  |             |
|    approx_kl            | 0.015206015 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.757       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.3         |
|    n_updates            | 380         |
|    policy_gradient_loss | 0.00191     |
|    value_loss           | 2.41        |
-----------------------------------------
Num timesteps: 640000
Best mean reward: 70.64 - Last mean reward per episode: 56.69
Eval num_timesteps=640000, episode_reward=80.90 +/- 29.89
Episode length: 2545.40 +/- 565.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.55e+03    |
|    mean_reward          | 80.9        |
| time/                   |             |
|    total_timesteps      | 640000      |
| train/                  |             |
|    approx_kl            | 0.016085995 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.79        |
|    n_updates            | 390         |
|    policy_gradient_loss | 0.00526     |
|    value_loss           | 2.47        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.27e+03 |
|    ep_rew_mean     | 58.4     |
| time/              |          |
|    fps             | 109      |
|    iterations      | 40       |
|    time_elapsed    | 5959     |
|    total_timesteps | 655360   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.29e+03    |
|    ep_rew_mean          | 58.6        |
| time/                   |             |
|    fps                  | 110         |
|    iterations           | 41          |
|    time_elapsed         | 6091        |
|    total_timesteps      | 671744      |
| train/                  |             |
|    approx_kl            | 0.014097142 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.769       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.81        |
|    n_updates            | 400         |
|    policy_gradient_loss | 0.00387     |
|    value_loss           | 2.36        |
-----------------------------------------
Eval num_timesteps=680000, episode_reward=46.60 +/- 13.51
Episode length: 3000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 3e+03      |
|    mean_reward          | 46.6       |
| time/                   |            |
|    total_timesteps      | 680000     |
| train/                  |            |
|    approx_kl            | 0.01511378 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.755      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.427      |
|    n_updates            | 410        |
|    policy_gradient_loss | 0.00255    |
|    value_loss           | 2.3        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.28e+03 |
|    ep_rew_mean     | 57.1     |
| time/              |          |
|    fps             | 109      |
|    iterations      | 42       |
|    time_elapsed    | 6275     |
|    total_timesteps | 688128   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.29e+03    |
|    ep_rew_mean          | 57.9        |
| time/                   |             |
|    fps                  | 109         |
|    iterations           | 43          |
|    time_elapsed         | 6406        |
|    total_timesteps      | 704512      |
| train/                  |             |
|    approx_kl            | 0.016729346 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.925       |
|    n_updates            | 420         |
|    policy_gradient_loss | 0.00412     |
|    value_loss           | 2.1         |
-----------------------------------------
Num timesteps: 720000
Best mean reward: 70.64 - Last mean reward per episode: 58.08
Eval num_timesteps=720000, episode_reward=60.50 +/- 23.07
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 60.5        |
| time/                   |             |
|    total_timesteps      | 720000      |
| train/                  |             |
|    approx_kl            | 0.016925944 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.46        |
|    n_updates            | 430         |
|    policy_gradient_loss | 0.00483     |
|    value_loss           | 2.07        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.28e+03 |
|    ep_rew_mean     | 59       |
| time/              |          |
|    fps             | 109      |
|    iterations      | 44       |
|    time_elapsed    | 6588     |
|    total_timesteps | 720896   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.27e+03     |
|    ep_rew_mean          | 56.4         |
| time/                   |              |
|    fps                  | 109          |
|    iterations           | 45           |
|    time_elapsed         | 6719         |
|    total_timesteps      | 737280       |
| train/                  |              |
|    approx_kl            | 0.0155098755 |
|    clip_fraction        | 0.19         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0.786        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.88         |
|    n_updates            | 440          |
|    policy_gradient_loss | 0.00507      |
|    value_loss           | 1.89         |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.23e+03   |
|    ep_rew_mean          | 57.8       |
| time/                   |            |
|    fps                  | 110        |
|    iterations           | 46         |
|    time_elapsed         | 6846       |
|    total_timesteps      | 753664     |
| train/                  |            |
|    approx_kl            | 0.01776404 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.789      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.857      |
|    n_updates            | 450        |
|    policy_gradient_loss | 0.000849   |
|    value_loss           | 2.37       |
----------------------------------------
Eval num_timesteps=760000, episode_reward=32.80 +/- 12.71
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 32.8        |
| time/                   |             |
|    total_timesteps      | 760000      |
| train/                  |             |
|    approx_kl            | 0.015225172 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.24        |
|    n_updates            | 460         |
|    policy_gradient_loss | 0.00234     |
|    value_loss           | 2.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.2e+03  |
|    ep_rew_mean     | 59.4     |
| time/              |          |
|    fps             | 109      |
|    iterations      | 47       |
|    time_elapsed    | 7030     |
|    total_timesteps | 770048   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.2e+03     |
|    ep_rew_mean          | 61.6        |
| time/                   |             |
|    fps                  | 109         |
|    iterations           | 48          |
|    time_elapsed         | 7161        |
|    total_timesteps      | 786432      |
| train/                  |             |
|    approx_kl            | 0.015872486 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.844       |
|    n_updates            | 470         |
|    policy_gradient_loss | 0.00384     |
|    value_loss           | 2.49        |
-----------------------------------------
Num timesteps: 800000
Best mean reward: 70.64 - Last mean reward per episode: 60.20
Eval num_timesteps=800000, episode_reward=47.30 +/- 10.05
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 47.3        |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.013574224 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.803       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.31        |
|    n_updates            | 480         |
|    policy_gradient_loss | 0.00366     |
|    value_loss           | 1.46        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.21e+03 |
|    ep_rew_mean     | 62.7     |
| time/              |          |
|    fps             | 109      |
|    iterations      | 49       |
|    time_elapsed    | 7343     |
|    total_timesteps | 802816   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.2e+03     |
|    ep_rew_mean          | 62.9        |
| time/                   |             |
|    fps                  | 109         |
|    iterations           | 50          |
|    time_elapsed         | 7474        |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.014969628 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.763       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.526       |
|    n_updates            | 490         |
|    policy_gradient_loss | 0.00437     |
|    value_loss           | 1.89        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.23e+03     |
|    ep_rew_mean          | 59.7         |
| time/                   |              |
|    fps                  | 109          |
|    iterations           | 51           |
|    time_elapsed         | 7600         |
|    total_timesteps      | 835584       |
| train/                  |              |
|    approx_kl            | 0.0133659225 |
|    clip_fraction        | 0.17         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0.725        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.813        |
|    n_updates            | 500          |
|    policy_gradient_loss | 0.000623     |
|    value_loss           | 2.45         |
------------------------------------------
Eval num_timesteps=840000, episode_reward=30.10 +/- 12.60
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 30.1        |
| time/                   |             |
|    total_timesteps      | 840000      |
| train/                  |             |
|    approx_kl            | 0.013759714 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.774       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.05        |
|    n_updates            | 510         |
|    policy_gradient_loss | 0.00136     |
|    value_loss           | 2.81        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.24e+03 |
|    ep_rew_mean     | 59.2     |
| time/              |          |
|    fps             | 109      |
|    iterations      | 52       |
|    time_elapsed    | 7786     |
|    total_timesteps | 851968   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22e+03    |
|    ep_rew_mean          | 60.5        |
| time/                   |             |
|    fps                  | 109         |
|    iterations           | 53          |
|    time_elapsed         | 7917        |
|    total_timesteps      | 868352      |
| train/                  |             |
|    approx_kl            | 0.014376055 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.766       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.36        |
|    n_updates            | 520         |
|    policy_gradient_loss | 0.000216    |
|    value_loss           | 2.05        |
-----------------------------------------
Num timesteps: 880000
Best mean reward: 70.64 - Last mean reward per episode: 59.23
Eval num_timesteps=880000, episode_reward=37.00 +/- 6.65
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 37          |
| time/                   |             |
|    total_timesteps      | 880000      |
| train/                  |             |
|    approx_kl            | 0.016259966 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.767       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.31        |
|    n_updates            | 530         |
|    policy_gradient_loss | 0.00283     |
|    value_loss           | 2.52        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.21e+03 |
|    ep_rew_mean     | 62.7     |
| time/              |          |
|    fps             | 109      |
|    iterations      | 54       |
|    time_elapsed    | 8104     |
|    total_timesteps | 884736   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22e+03    |
|    ep_rew_mean          | 63.1        |
| time/                   |             |
|    fps                  | 109         |
|    iterations           | 55          |
|    time_elapsed         | 8237        |
|    total_timesteps      | 901120      |
| train/                  |             |
|    approx_kl            | 0.017086364 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.769       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.39        |
|    n_updates            | 540         |
|    policy_gradient_loss | 0.00187     |
|    value_loss           | 2.05        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.23e+03    |
|    ep_rew_mean          | 62.3        |
| time/                   |             |
|    fps                  | 109         |
|    iterations           | 56          |
|    time_elapsed         | 8367        |
|    total_timesteps      | 917504      |
| train/                  |             |
|    approx_kl            | 0.016509999 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.06        |
|    n_updates            | 550         |
|    policy_gradient_loss | 0.00569     |
|    value_loss           | 2.12        |
-----------------------------------------
Eval num_timesteps=920000, episode_reward=48.30 +/- 10.44
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 48.3        |
| time/                   |             |
|    total_timesteps      | 920000      |
| train/                  |             |
|    approx_kl            | 0.015086505 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.717       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.599       |
|    n_updates            | 560         |
|    policy_gradient_loss | 0.00388     |
|    value_loss           | 2.03        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.26e+03 |
|    ep_rew_mean     | 62.6     |
| time/              |          |
|    fps             | 109      |
|    iterations      | 57       |
|    time_elapsed    | 8553     |
|    total_timesteps | 933888   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.27e+03    |
|    ep_rew_mean          | 61.8        |
| time/                   |             |
|    fps                  | 109         |
|    iterations           | 58          |
|    time_elapsed         | 8684        |
|    total_timesteps      | 950272      |
| train/                  |             |
|    approx_kl            | 0.014482362 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.756       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.401       |
|    n_updates            | 570         |
|    policy_gradient_loss | 0.00319     |
|    value_loss           | 1.95        |
-----------------------------------------
Num timesteps: 960000
Best mean reward: 70.64 - Last mean reward per episode: 59.27
Eval num_timesteps=960000, episode_reward=45.50 +/- 14.03
Episode length: 3000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3e+03        |
|    mean_reward          | 45.5         |
| time/                   |              |
|    total_timesteps      | 960000       |
| train/                  |              |
|    approx_kl            | 0.0152660655 |
|    clip_fraction        | 0.14         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.991       |
|    explained_variance   | 0.77         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.928        |
|    n_updates            | 580          |
|    policy_gradient_loss | 0.0054       |
|    value_loss           | 2.22         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.28e+03 |
|    ep_rew_mean     | 64.2     |
| time/              |          |
|    fps             | 108      |
|    iterations      | 59       |
|    time_elapsed    | 8870     |
|    total_timesteps | 966656   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.3e+03    |
|    ep_rew_mean          | 62.3       |
| time/                   |            |
|    fps                  | 109        |
|    iterations           | 60         |
|    time_elapsed         | 9002       |
|    total_timesteps      | 983040     |
| train/                  |            |
|    approx_kl            | 0.01611301 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1         |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.79       |
|    n_updates            | 590        |
|    policy_gradient_loss | 0.00555    |
|    value_loss           | 2          |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.27e+03    |
|    ep_rew_mean          | 63.9        |
| time/                   |             |
|    fps                  | 109         |
|    iterations           | 61          |
|    time_elapsed         | 9134        |
|    total_timesteps      | 999424      |
| train/                  |             |
|    approx_kl            | 0.015837327 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.755       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.23        |
|    n_updates            | 600         |
|    policy_gradient_loss | 0.00469     |
|    value_loss           | 3.22        |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=66.00 +/- 25.46
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 66          |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.017946012 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.757       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.761       |
|    n_updates            | 610         |
|    policy_gradient_loss | 0.00422     |
|    value_loss           | 2.98        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.29e+03 |
|    ep_rew_mean     | 63.2     |
| time/              |          |
|    fps             | 108      |
|    iterations      | 62       |
|    time_elapsed    | 9323     |
|    total_timesteps | 1015808  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.27e+03   |
|    ep_rew_mean          | 60.8       |
| time/                   |            |
|    fps                  | 109        |
|    iterations           | 63         |
|    time_elapsed         | 9454       |
|    total_timesteps      | 1032192    |
| train/                  |            |
|    approx_kl            | 0.01740568 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.03       |
|    n_updates            | 620        |
|    policy_gradient_loss | 0.00814    |
|    value_loss           | 2.53       |
----------------------------------------
Num timesteps: 1040000
Best mean reward: 70.64 - Last mean reward per episode: 60.26
Eval num_timesteps=1040000, episode_reward=42.10 +/- 6.58
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 42.1        |
| time/                   |             |
|    total_timesteps      | 1040000     |
| train/                  |             |
|    approx_kl            | 0.015392797 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.998      |
|    explained_variance   | 0.741       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.65        |
|    n_updates            | 630         |
|    policy_gradient_loss | 0.00568     |
|    value_loss           | 3.28        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.26e+03 |
|    ep_rew_mean     | 60.8     |
| time/              |          |
|    fps             | 108      |
|    iterations      | 64       |
|    time_elapsed    | 9640     |
|    total_timesteps | 1048576  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.26e+03    |
|    ep_rew_mean          | 64.4        |
| time/                   |             |
|    fps                  | 108         |
|    iterations           | 65          |
|    time_elapsed         | 9771        |
|    total_timesteps      | 1064960     |
| train/                  |             |
|    approx_kl            | 0.018541079 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.883       |
|    n_updates            | 640         |
|    policy_gradient_loss | 0.00731     |
|    value_loss           | 2.25        |
-----------------------------------------
Eval num_timesteps=1080000, episode_reward=66.60 +/- 19.07
Episode length: 2443.60 +/- 698.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.44e+03    |
|    mean_reward          | 66.6        |
| time/                   |             |
|    total_timesteps      | 1080000     |
| train/                  |             |
|    approx_kl            | 0.016820524 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.994      |
|    explained_variance   | 0.779       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.824       |
|    n_updates            | 650         |
|    policy_gradient_loss | 0.0142      |
|    value_loss           | 2.67        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.27e+03 |
|    ep_rew_mean     | 63.9     |
| time/              |          |
|    fps             | 108      |
|    iterations      | 66       |
|    time_elapsed    | 9947     |
|    total_timesteps | 1081344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.29e+03    |
|    ep_rew_mean          | 64.5        |
| time/                   |             |
|    fps                  | 108         |
|    iterations           | 67          |
|    time_elapsed         | 10078       |
|    total_timesteps      | 1097728     |
| train/                  |             |
|    approx_kl            | 0.015652822 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.988      |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.92        |
|    n_updates            | 660         |
|    policy_gradient_loss | 0.00603     |
|    value_loss           | 2.24        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.28e+03    |
|    ep_rew_mean          | 63.9        |
| time/                   |             |
|    fps                  | 109         |
|    iterations           | 68          |
|    time_elapsed         | 10207       |
|    total_timesteps      | 1114112     |
| train/                  |             |
|    approx_kl            | 0.019030826 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.743       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.12        |
|    n_updates            | 670         |
|    policy_gradient_loss | 0.00534     |
|    value_loss           | 2.49        |
-----------------------------------------
Num timesteps: 1120000
Best mean reward: 70.64 - Last mean reward per episode: 63.75
Eval num_timesteps=1120000, episode_reward=42.90 +/- 8.67
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 42.9        |
| time/                   |             |
|    total_timesteps      | 1120000     |
| train/                  |             |
|    approx_kl            | 0.015725933 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.791       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.603       |
|    n_updates            | 680         |
|    policy_gradient_loss | 0.00662     |
|    value_loss           | 2.92        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.26e+03 |
|    ep_rew_mean     | 64.7     |
| time/              |          |
|    fps             | 108      |
|    iterations      | 69       |
|    time_elapsed    | 10394    |
|    total_timesteps | 1130496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.24e+03    |
|    ep_rew_mean          | 64.9        |
| time/                   |             |
|    fps                  | 108         |
|    iterations           | 70          |
|    time_elapsed         | 10525       |
|    total_timesteps      | 1146880     |
| train/                  |             |
|    approx_kl            | 0.015097214 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.707       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.43        |
|    n_updates            | 690         |
|    policy_gradient_loss | 0.00683     |
|    value_loss           | 2.76        |
-----------------------------------------
Eval num_timesteps=1160000, episode_reward=55.60 +/- 20.92
Episode length: 2376.20 +/- 777.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2.38e+03   |
|    mean_reward          | 55.6       |
| time/                   |            |
|    total_timesteps      | 1160000    |
| train/                  |            |
|    approx_kl            | 0.01714797 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.01      |
|    explained_variance   | 0.755      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.596      |
|    n_updates            | 700        |
|    policy_gradient_loss | 0.00836    |
|    value_loss           | 2.44       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.23e+03 |
|    ep_rew_mean     | 66.2     |
| time/              |          |
|    fps             | 108      |
|    iterations      | 71       |
|    time_elapsed    | 10699    |
|    total_timesteps | 1163264  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.2e+03     |
|    ep_rew_mean          | 64.9        |
| time/                   |             |
|    fps                  | 108         |
|    iterations           | 72          |
|    time_elapsed         | 10830       |
|    total_timesteps      | 1179648     |
| train/                  |             |
|    approx_kl            | 0.014549695 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.04        |
|    n_updates            | 710         |
|    policy_gradient_loss | 0.00555     |
|    value_loss           | 2.95        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.19e+03    |
|    ep_rew_mean          | 66.4        |
| time/                   |             |
|    fps                  | 109         |
|    iterations           | 73          |
|    time_elapsed         | 10956       |
|    total_timesteps      | 1196032     |
| train/                  |             |
|    approx_kl            | 0.021855846 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.998      |
|    explained_variance   | 0.773       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.734       |
|    n_updates            | 720         |
|    policy_gradient_loss | 0.0117      |
|    value_loss           | 2.72        |
-----------------------------------------
Num timesteps: 1200000
Best mean reward: 70.64 - Last mean reward per episode: 62.53
Eval num_timesteps=1200000, episode_reward=38.00 +/- 0.77
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 38          |
| time/                   |             |
|    total_timesteps      | 1200000     |
| train/                  |             |
|    approx_kl            | 0.021201462 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.978      |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.03        |
|    n_updates            | 730         |
|    policy_gradient_loss | 0.0098      |
|    value_loss           | 3.13        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.2e+03  |
|    ep_rew_mean     | 65.2     |
| time/              |          |
|    fps             | 108      |
|    iterations      | 74       |
|    time_elapsed    | 11142    |
|    total_timesteps | 1212416  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22e+03    |
|    ep_rew_mean          | 63.9        |
| time/                   |             |
|    fps                  | 109         |
|    iterations           | 75          |
|    time_elapsed         | 11273       |
|    total_timesteps      | 1228800     |
| train/                  |             |
|    approx_kl            | 0.021933377 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.98       |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.36        |
|    n_updates            | 740         |
|    policy_gradient_loss | 0.012       |
|    value_loss           | 2.42        |
-----------------------------------------
Eval num_timesteps=1240000, episode_reward=53.60 +/- 12.78
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 53.6        |
| time/                   |             |
|    total_timesteps      | 1240000     |
| train/                  |             |
|    approx_kl            | 0.015580349 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.96       |
|    explained_variance   | 0.79        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.15        |
|    n_updates            | 750         |
|    policy_gradient_loss | 0.0113      |
|    value_loss           | 3.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.23e+03 |
|    ep_rew_mean     | 62.9     |
| time/              |          |
|    fps             | 108      |
|    iterations      | 76       |
|    time_elapsed    | 11455    |
|    total_timesteps | 1245184  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.22e+03   |
|    ep_rew_mean          | 62.1       |
| time/                   |            |
|    fps                  | 108        |
|    iterations           | 77         |
|    time_elapsed         | 11586      |
|    total_timesteps      | 1261568    |
| train/                  |            |
|    approx_kl            | 0.01986505 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.972     |
|    explained_variance   | 0.737      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.34       |
|    n_updates            | 760        |
|    policy_gradient_loss | 0.0125     |
|    value_loss           | 3.41       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.15e+03    |
|    ep_rew_mean          | 58          |
| time/                   |             |
|    fps                  | 109         |
|    iterations           | 78          |
|    time_elapsed         | 11713       |
|    total_timesteps      | 1277952     |
| train/                  |             |
|    approx_kl            | 0.026568376 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.993      |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.26        |
|    n_updates            | 770         |
|    policy_gradient_loss | 0.0203      |
|    value_loss           | 4.14        |
-----------------------------------------
Num timesteps: 1280000
Best mean reward: 70.64 - Last mean reward per episode: 53.28
Eval num_timesteps=1280000, episode_reward=40.10 +/- 9.38
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 40.1        |
| time/                   |             |
|    total_timesteps      | 1280000     |
| train/                  |             |
|    approx_kl            | 0.020536192 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.726       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.11        |
|    n_updates            | 780         |
|    policy_gradient_loss | 0.0121      |
|    value_loss           | 4.83        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.11e+03 |
|    ep_rew_mean     | 53.1     |
| time/              |          |
|    fps             | 108      |
|    iterations      | 79       |
|    time_elapsed    | 11900    |
|    total_timesteps | 1294336  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.06e+03    |
|    ep_rew_mean          | 49.6        |
| time/                   |             |
|    fps                  | 108         |
|    iterations           | 80          |
|    time_elapsed         | 12031       |
|    total_timesteps      | 1310720     |
| train/                  |             |
|    approx_kl            | 0.027073491 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.997      |
|    explained_variance   | 0.741       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.91        |
|    n_updates            | 790         |
|    policy_gradient_loss | 0.0148      |
|    value_loss           | 4.55        |
-----------------------------------------
Eval num_timesteps=1320000, episode_reward=43.80 +/- 14.88
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 43.8        |
| time/                   |             |
|    total_timesteps      | 1320000     |
| train/                  |             |
|    approx_kl            | 0.019128915 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.997      |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.54        |
|    n_updates            | 800         |
|    policy_gradient_loss | 0.0125      |
|    value_loss           | 3.71        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.04e+03 |
|    ep_rew_mean     | 45.3     |
| time/              |          |
|    fps             | 108      |
|    iterations      | 81       |
|    time_elapsed    | 12217    |
|    total_timesteps | 1327104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.01e+03    |
|    ep_rew_mean          | 40.2        |
| time/                   |             |
|    fps                  | 108         |
|    iterations           | 82          |
|    time_elapsed         | 12349       |
|    total_timesteps      | 1343488     |
| train/                  |             |
|    approx_kl            | 0.020789066 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.988      |
|    explained_variance   | 0.754       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.03        |
|    n_updates            | 810         |
|    policy_gradient_loss | 0.0123      |
|    value_loss           | 4.79        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.95e+03    |
|    ep_rew_mean          | 34.3        |
| time/                   |             |
|    fps                  | 108         |
|    iterations           | 83          |
|    time_elapsed         | 12481       |
|    total_timesteps      | 1359872     |
| train/                  |             |
|    approx_kl            | 0.017213043 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.999      |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.3         |
|    n_updates            | 820         |
|    policy_gradient_loss | 0.00667     |
|    value_loss           | 4.48        |
-----------------------------------------
Num timesteps: 1360000
Best mean reward: 70.64 - Last mean reward per episode: 31.43
Eval num_timesteps=1360000, episode_reward=-56.60 +/- 14.90
Episode length: 738.20 +/- 157.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 738         |
|    mean_reward          | -56.6       |
| time/                   |             |
|    total_timesteps      | 1360000     |
| train/                  |             |
|    approx_kl            | 0.022843052 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.985      |
|    explained_variance   | 0.746       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.13        |
|    n_updates            | 830         |
|    policy_gradient_loss | 0.00863     |
|    value_loss           | 4.87        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.93e+03 |
|    ep_rew_mean     | 34.5     |
| time/              |          |
|    fps             | 109      |
|    iterations      | 84       |
|    time_elapsed    | 12625    |
|    total_timesteps | 1376256  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.8e+03     |
|    ep_rew_mean          | 20.1        |
| time/                   |             |
|    fps                  | 109         |
|    iterations           | 85          |
|    time_elapsed         | 12756       |
|    total_timesteps      | 1392640     |
| train/                  |             |
|    approx_kl            | 0.020367313 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.994      |
|    explained_variance   | 0.708       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.68        |
|    n_updates            | 840         |
|    policy_gradient_loss | 0.02        |
|    value_loss           | 4.85        |
-----------------------------------------
Eval num_timesteps=1400000, episode_reward=4.50 +/- 48.44
Episode length: 2066.40 +/- 792.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.07e+03    |
|    mean_reward          | 4.5         |
| time/                   |             |
|    total_timesteps      | 1400000     |
| train/                  |             |
|    approx_kl            | 0.012955303 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.94        |
|    n_updates            | 850         |
|    policy_gradient_loss | 0.00476     |
|    value_loss           | 6.16        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | 5.98     |
| time/              |          |
|    fps             | 109      |
|    iterations      | 86       |
|    time_elapsed    | 12921    |
|    total_timesteps | 1409024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.52e+03    |
|    ep_rew_mean          | -2.79       |
| time/                   |             |
|    fps                  | 109         |
|    iterations           | 87          |
|    time_elapsed         | 13052       |
|    total_timesteps      | 1425408     |
| train/                  |             |
|    approx_kl            | 0.011813599 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.985      |
|    explained_variance   | 0.756       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.04        |
|    n_updates            | 860         |
|    policy_gradient_loss | 0.00139     |
|    value_loss           | 6.9         |
-----------------------------------------
Num timesteps: 1440000
Best mean reward: 70.64 - Last mean reward per episode: -11.53
Eval num_timesteps=1440000, episode_reward=14.80 +/- 26.34
Episode length: 2391.20 +/- 746.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.39e+03    |
|    mean_reward          | 14.8        |
| time/                   |             |
|    total_timesteps      | 1440000     |
| train/                  |             |
|    approx_kl            | 0.019024516 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.969      |
|    explained_variance   | 0.699       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.53        |
|    n_updates            | 870         |
|    policy_gradient_loss | 0.00316     |
|    value_loss           | 7.14        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.51e+03 |
|    ep_rew_mean     | -6.97    |
| time/              |          |
|    fps             | 109      |
|    iterations      | 88       |
|    time_elapsed    | 13227    |
|    total_timesteps | 1441792  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.53e+03   |
|    ep_rew_mean          | -6.7       |
| time/                   |            |
|    fps                  | 109        |
|    iterations           | 89         |
|    time_elapsed         | 13358      |
|    total_timesteps      | 1458176    |
| train/                  |            |
|    approx_kl            | 0.01961567 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.956     |
|    explained_variance   | 0.691      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.75       |
|    n_updates            | 880        |
|    policy_gradient_loss | 0.0107     |
|    value_loss           | 6.39       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.53e+03    |
|    ep_rew_mean          | -13.3       |
| time/                   |             |
|    fps                  | 109         |
|    iterations           | 90          |
|    time_elapsed         | 13489       |
|    total_timesteps      | 1474560     |
| train/                  |             |
|    approx_kl            | 0.031290546 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.976      |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.09        |
|    n_updates            | 890         |
|    policy_gradient_loss | 0.0202      |
|    value_loss           | 5.35        |
-----------------------------------------
Eval num_timesteps=1480000, episode_reward=78.50 +/- 35.75
Episode length: 2552.40 +/- 599.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2.55e+03   |
|    mean_reward          | 78.5       |
| time/                   |            |
|    total_timesteps      | 1480000    |
| train/                  |            |
|    approx_kl            | 0.02439478 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.998     |
|    explained_variance   | 0.668      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.23       |
|    n_updates            | 900        |
|    policy_gradient_loss | 0.013      |
|    value_loss           | 5.52       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.52e+03 |
|    ep_rew_mean     | -13.9    |
| time/              |          |
|    fps             | 109      |
|    iterations      | 91       |
|    time_elapsed    | 13666    |
|    total_timesteps | 1490944  |
---------------------------------
slurmstepd-cn-402: error: *** JOB 8013096 ON cn-402 CANCELLED AT 2024-07-10T23:09:41 ***
--------------------- Slurm Task Epilog ------------------------
Job ID: 8013096
Time: Wed Jul 10 23:09:41 CEST 2024
Elapsed Time: 03:49:56
Billing per second for TRES: billing=704,cpu=32,gres/gpu=1,mem=32G,node=1
Show resource usage with e.g.:
sacct -j 8013096 -o Elapsed,TotalCPU,UserCPU,SystemCPU,MaxRSS,ReqTRES%60,MaxDiskRead,MaxDiskWrite
--------------------- Slurm Task Epilog ------------------------
