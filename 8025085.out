--------------------- Slurm Task Prolog ------------------------
Job ID: 8025085
Job name: freeway_supervised_cnn
Host: cn-406
Date: Fri Jul 12 03:09:22 CEST 2024
User: rishabh.bhatia
Slurm account: rleap
Slurm partition: rleap_gpu_24gb
Work dir: 
------------------
Node usage:
JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
8023766 rleap_gpu hiking2. blai.bon  R    3:09:32      1 cn-406
8025085 rleap_gpu freeway_ rishabh.  R       0:01      1 cn-406
8023892 rleap_gpu   tunnel rishabh.  R    2:47:25      1 cn-406
------------------
Show launch script with:
sacct -B -j 
------------------
--------------------- Slurm Task Prolog ------------------------
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.[0m
  logger.warn(
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.[0m
  logger.warn(
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.[0m
  logger.warn(
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.[0m
  logger.warn(
wandb: Currently logged in as: rishabh-bhatia (rwth-ml). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.1
wandb: Run data is saved locally in /work/rleap1/rishabh.bhatia/Master-Thesis-GNN/wandb/run-20240712_030934-a0393trx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-glitter-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/rwth-ml/cnn_atari_freeway
wandb: üöÄ View run at https://wandb.ai/rwth-ml/cnn_atari_freeway/runs/a0393trx
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f2f657b3ee0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f2fc1e435e0>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
Using cuda device
-----------------------------
| time/              |      |
|    fps             | 372  |
|    iterations      | 1    |
|    time_elapsed    | 21   |
|    total_timesteps | 8192 |
-----------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 436         |
|    iterations           | 2           |
|    time_elapsed         | 37          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.008599967 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0916      |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.00128     |
|    value_loss           | 1.35        |
-----------------------------------------
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:250: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  data_frame = pandas.concat(data_frames)
Num timesteps: 20000
Best mean reward: -inf - Last mean reward per episode: -1500.00
Saving new best model at 12000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=20000, episode_reward=-1500.00 +/- 0.00
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | -1.5e+03    |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.008678286 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | -0.00016    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.163       |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.00239     |
|    value_loss           | 1.49        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -1.5e+03 |
| time/              |          |
|    fps             | 396      |
|    iterations      | 3        |
|    time_elapsed    | 62       |
|    total_timesteps | 24576    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 441         |
|    iterations           | 4           |
|    time_elapsed         | 74          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.037448842 |
|    clip_fraction        | 0.307       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.00202     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.812       |
|    n_updates            | 30          |
|    policy_gradient_loss | 0.00553     |
|    value_loss           | 2.31        |
-----------------------------------------
Num timesteps: 40000
Best mean reward: -1500.00 - Last mean reward per episode: -1494.71
Saving new best model at 51000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=40000, episode_reward=-252.00 +/- 147.70
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | -252        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.024258547 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.0064      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.205       |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.00401     |
|    value_loss           | 1.36        |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 3e+03     |
|    ep_rew_mean     | -1.49e+03 |
| time/              |           |
|    fps             | 413       |
|    iterations      | 5         |
|    time_elapsed    | 98        |
|    total_timesteps | 40960     |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | -1.48e+03   |
| time/                   |             |
|    fps                  | 442         |
|    iterations           | 6           |
|    time_elapsed         | 111         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.025075413 |
|    clip_fraction        | 0.322       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | -0.00214    |
|    learning_rate        | 0.0003      |
|    loss                 | 11.4        |
|    n_updates            | 50          |
|    policy_gradient_loss | 0.0098      |
|    value_loss           | 7.6         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | -1.48e+03   |
| time/                   |             |
|    fps                  | 463         |
|    iterations           | 7           |
|    time_elapsed         | 123         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.030678984 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.922      |
|    explained_variance   | -0.00237    |
|    learning_rate        | 0.0003      |
|    loss                 | 7.31        |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.00812     |
|    value_loss           | 13.9        |
-----------------------------------------
Num timesteps: 60000
Best mean reward: -1494.71 - Last mean reward per episode: -1226.00
Saving new best model at 90000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=60000, episode_reward=-264.00 +/- 159.20
Episode length: 3000.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 3e+03     |
|    mean_reward          | -264      |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 0.0171945 |
|    clip_fraction        | 0.234     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.809    |
|    explained_variance   | 0.000268  |
|    learning_rate        | 0.0003    |
|    loss                 | 16.3      |
|    n_updates            | 70        |
|    policy_gradient_loss | 0.00639   |
|    value_loss           | 31.1      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -1.4e+03 |
| time/              |          |
|    fps             | 443      |
|    iterations      | 8        |
|    time_elapsed    | 147      |
|    total_timesteps | 65536    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3e+03      |
|    ep_rew_mean          | -1.3e+03   |
| time/                   |            |
|    fps                  | 461        |
|    iterations           | 9          |
|    time_elapsed         | 159        |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.04775136 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.667     |
|    explained_variance   | -0.00476   |
|    learning_rate        | 0.0003     |
|    loss                 | 37.9       |
|    n_updates            | 80         |
|    policy_gradient_loss | 0.0067     |
|    value_loss           | 41.1       |
----------------------------------------
Num timesteps: 80000
Best mean reward: -1226.00 - Last mean reward per episode: -1060.77
Saving new best model at 117000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=80000, episode_reward=-270.00 +/- 110.63
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | -270        |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.010630135 |
|    clip_fraction        | 0.088       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.45       |
|    explained_variance   | 0.00469     |
|    learning_rate        | 0.0003      |
|    loss                 | 31.4        |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.00621     |
|    value_loss           | 59.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -1.3e+03 |
| time/              |          |
|    fps             | 445      |
|    iterations      | 10       |
|    time_elapsed    | 183      |
|    total_timesteps | 81920    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | -1.16e+03   |
| time/                   |             |
|    fps                  | 459         |
|    iterations           | 11          |
|    time_elapsed         | 195         |
|    total_timesteps      | 90112       |
| train/                  |             |
|    approx_kl            | 0.013762398 |
|    clip_fraction        | 0.09        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.538      |
|    explained_variance   | 0.04        |
|    learning_rate        | 0.0003      |
|    loss                 | 31.7        |
|    n_updates            | 100         |
|    policy_gradient_loss | 0.00282     |
|    value_loss           | 65.6        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -1.06e+03    |
| time/                   |              |
|    fps                  | 472          |
|    iterations           | 12           |
|    time_elapsed         | 208          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0062736184 |
|    clip_fraction        | 0.0624       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.46        |
|    explained_variance   | 0.0577       |
|    learning_rate        | 0.0003       |
|    loss                 | 41.4         |
|    n_updates            | 110          |
|    policy_gradient_loss | 0.00395      |
|    value_loss           | 68.7         |
------------------------------------------
Num timesteps: 100000
Best mean reward: -1060.77 - Last mean reward per episode: -874.62
Saving new best model at 156000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=100000, episode_reward=-204.00 +/- 102.88
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | -204        |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.006120338 |
|    clip_fraction        | 0.0464      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.41       |
|    explained_variance   | 0.0427      |
|    learning_rate        | 0.0003      |
|    loss                 | 37.7        |
|    n_updates            | 120         |
|    policy_gradient_loss | 0.00253     |
|    value_loss           | 65.4        |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 3e+03     |
|    ep_rew_mean     | -1.06e+03 |
| time/              |           |
|    fps             | 458       |
|    iterations      | 13        |
|    time_elapsed    | 232       |
|    total_timesteps | 106496    |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | -1e+03      |
| time/                   |             |
|    fps                  | 469         |
|    iterations           | 14          |
|    time_elapsed         | 244         |
|    total_timesteps      | 114688      |
| train/                  |             |
|    approx_kl            | 0.009296553 |
|    clip_fraction        | 0.0882      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.539      |
|    explained_variance   | 0.14        |
|    learning_rate        | 0.0003      |
|    loss                 | 29.9        |
|    n_updates            | 130         |
|    policy_gradient_loss | 0.000811    |
|    value_loss           | 58.1        |
-----------------------------------------
Num timesteps: 120000
Best mean reward: -874.62 - Last mean reward per episode: -767.54
Saving new best model at 195000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=120000, episode_reward=-198.00 +/- 153.67
Episode length: 3000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3e+03        |
|    mean_reward          | -198         |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 0.0042471876 |
|    clip_fraction        | 0.0555       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.399       |
|    explained_variance   | 0.0159       |
|    learning_rate        | 0.0003       |
|    loss                 | 27.7         |
|    n_updates            | 140          |
|    policy_gradient_loss | 0.000612     |
|    value_loss           | 67.7         |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -936     |
| time/              |          |
|    fps             | 457      |
|    iterations      | 15       |
|    time_elapsed    | 268      |
|    total_timesteps | 122880   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | -936        |
| time/                   |             |
|    fps                  | 466         |
|    iterations           | 16          |
|    time_elapsed         | 280         |
|    total_timesteps      | 131072      |
| train/                  |             |
|    approx_kl            | 0.007938758 |
|    clip_fraction        | 0.0576      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.4        |
|    explained_variance   | 0.0217      |
|    learning_rate        | 0.0003      |
|    loss                 | 37          |
|    n_updates            | 150         |
|    policy_gradient_loss | 0.00174     |
|    value_loss           | 67          |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -882         |
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 17           |
|    time_elapsed         | 292          |
|    total_timesteps      | 139264       |
| train/                  |              |
|    approx_kl            | 0.0035619023 |
|    clip_fraction        | 0.0492       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.311       |
|    explained_variance   | 0.0527       |
|    learning_rate        | 0.0003       |
|    loss                 | 24           |
|    n_updates            | 160          |
|    policy_gradient_loss | 0.00263      |
|    value_loss           | 63.3         |
------------------------------------------
Num timesteps: 140000
Best mean reward: -767.54 - Last mean reward per episode: -705.81
Saving new best model at 222000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=140000, episode_reward=-306.00 +/- 125.00
Episode length: 3000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3e+03        |
|    mean_reward          | -306         |
| time/                   |              |
|    total_timesteps      | 140000       |
| train/                  |              |
|    approx_kl            | 0.0028018032 |
|    clip_fraction        | 0.034        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.339       |
|    explained_variance   | 0.0783       |
|    learning_rate        | 0.0003       |
|    loss                 | 30           |
|    n_updates            | 170          |
|    policy_gradient_loss | 0.000624     |
|    value_loss           | 63.4         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -844     |
| time/              |          |
|    fps             | 465      |
|    iterations      | 18       |
|    time_elapsed    | 316      |
|    total_timesteps | 147456   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -844         |
| time/                   |              |
|    fps                  | 473          |
|    iterations           | 19           |
|    time_elapsed         | 328          |
|    total_timesteps      | 155648       |
| train/                  |              |
|    approx_kl            | 0.0032508573 |
|    clip_fraction        | 0.0396       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.264       |
|    explained_variance   | 0.049        |
|    learning_rate        | 0.0003       |
|    loss                 | 20.6         |
|    n_updates            | 180          |
|    policy_gradient_loss | 0.00107      |
|    value_loss           | 67.1         |
------------------------------------------
Num timesteps: 160000
Best mean reward: -705.81 - Last mean reward per episode: -651.38
Saving new best model at 261000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=160000, episode_reward=-198.00 +/- 129.52
Episode length: 3000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3e+03        |
|    mean_reward          | -198         |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0032103758 |
|    clip_fraction        | 0.0203       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.155       |
|    explained_variance   | 0.0043       |
|    learning_rate        | 0.0003       |
|    loss                 | 35.8         |
|    n_updates            | 190          |
|    policy_gradient_loss | 0.00158      |
|    value_loss           | 68.7         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -802     |
| time/              |          |
|    fps             | 464      |
|    iterations      | 20       |
|    time_elapsed    | 353      |
|    total_timesteps | 163840   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | -768        |
| time/                   |             |
|    fps                  | 471         |
|    iterations           | 21          |
|    time_elapsed         | 365         |
|    total_timesteps      | 172032      |
| train/                  |             |
|    approx_kl            | 0.003243592 |
|    clip_fraction        | 0.0167      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.138      |
|    explained_variance   | 0.018       |
|    learning_rate        | 0.0003      |
|    loss                 | 40.8        |
|    n_updates            | 200         |
|    policy_gradient_loss | 0.000897    |
|    value_loss           | 72.4        |
-----------------------------------------
Num timesteps: 180000
Best mean reward: -651.38 - Last mean reward per episode: -598.20
Saving new best model at 300000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=180000, episode_reward=-192.00 +/- 208.37
Episode length: 3000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3e+03        |
|    mean_reward          | -192         |
| time/                   |              |
|    total_timesteps      | 180000       |
| train/                  |              |
|    approx_kl            | 0.0009583371 |
|    clip_fraction        | 0.0158       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.155       |
|    explained_variance   | 0.0718       |
|    learning_rate        | 0.0003       |
|    loss                 | 37.7         |
|    n_updates            | 210          |
|    policy_gradient_loss | 0.00108      |
|    value_loss           | 70.4         |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -731     |
| time/              |          |
|    fps             | 462      |
|    iterations      | 22       |
|    time_elapsed    | 389      |
|    total_timesteps | 180224   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -731         |
| time/                   |              |
|    fps                  | 469          |
|    iterations           | 23           |
|    time_elapsed         | 401          |
|    total_timesteps      | 188416       |
| train/                  |              |
|    approx_kl            | 0.0017265808 |
|    clip_fraction        | 0.0153       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.158       |
|    explained_variance   | 0.00501      |
|    learning_rate        | 0.0003       |
|    loss                 | 43.8         |
|    n_updates            | 220          |
|    policy_gradient_loss | 0.000524     |
|    value_loss           | 70.5         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -699         |
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 24           |
|    time_elapsed         | 413          |
|    total_timesteps      | 196608       |
| train/                  |              |
|    approx_kl            | 0.0013985727 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.199       |
|    explained_variance   | 0.0268       |
|    learning_rate        | 0.0003       |
|    loss                 | 27.7         |
|    n_updates            | 230          |
|    policy_gradient_loss | 0.00057      |
|    value_loss           | 72.3         |
------------------------------------------
Num timesteps: 200000
Best mean reward: -598.20 - Last mean reward per episode: -481.50
Saving new best model at 327000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=200000, episode_reward=-318.00 +/- 101.47
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | -318        |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.001983963 |
|    clip_fraction        | 0.0212      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.179      |
|    explained_variance   | 0.0227      |
|    learning_rate        | 0.0003      |
|    loss                 | 55.2        |
|    n_updates            | 240         |
|    policy_gradient_loss | 0.000468    |
|    value_loss           | 67.6        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -683     |
| time/              |          |
|    fps             | 467      |
|    iterations      | 25       |
|    time_elapsed    | 438      |
|    total_timesteps | 204800   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -683          |
| time/                   |               |
|    fps                  | 472           |
|    iterations           | 26            |
|    time_elapsed         | 450           |
|    total_timesteps      | 212992        |
| train/                  |               |
|    approx_kl            | 0.00063015777 |
|    clip_fraction        | 0.0102        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.141        |
|    explained_variance   | 0.0198        |
|    learning_rate        | 0.0003        |
|    loss                 | 43.4          |
|    n_updates            | 250           |
|    policy_gradient_loss | 0.00119       |
|    value_loss           | 66.3          |
-------------------------------------------
Num timesteps: 220000
Best mean reward: -481.50 - Last mean reward per episode: -391.80
Saving new best model at 366000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=220000, episode_reward=-366.00 +/- 66.81
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | -366        |
| time/                   |             |
|    total_timesteps      | 220000      |
| train/                  |             |
|    approx_kl            | 0.001004606 |
|    clip_fraction        | 0.0171      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.151      |
|    explained_variance   | 0.0477      |
|    learning_rate        | 0.0003      |
|    loss                 | 27.3        |
|    n_updates            | 260         |
|    policy_gradient_loss | 0.000222    |
|    value_loss           | 69          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -658     |
| time/              |          |
|    fps             | 466      |
|    iterations      | 27       |
|    time_elapsed    | 474      |
|    total_timesteps | 221184   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -636          |
| time/                   |               |
|    fps                  | 471           |
|    iterations           | 28            |
|    time_elapsed         | 486           |
|    total_timesteps      | 229376        |
| train/                  |               |
|    approx_kl            | 0.00088355824 |
|    clip_fraction        | 0.00946       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.125        |
|    explained_variance   | 0.0227        |
|    learning_rate        | 0.0003        |
|    loss                 | 35.9          |
|    n_updates            | 270           |
|    policy_gradient_loss | 0.000269      |
|    value_loss           | 76.4          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -636         |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 29           |
|    time_elapsed         | 499          |
|    total_timesteps      | 237568       |
| train/                  |              |
|    approx_kl            | 0.0011361956 |
|    clip_fraction        | 0.00862      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0863      |
|    explained_variance   | 0.0128       |
|    learning_rate        | 0.0003       |
|    loss                 | 32.3         |
|    n_updates            | 280          |
|    policy_gradient_loss | 0.000579     |
|    value_loss           | 70.3         |
------------------------------------------
Num timesteps: 240000
Best mean reward: -391.80 - Last mean reward per episode: -313.50
Saving new best model at 405000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=240000, episode_reward=-216.00 +/- 106.32
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -216          |
| time/                   |               |
|    total_timesteps      | 240000        |
| train/                  |               |
|    approx_kl            | 0.00029819936 |
|    clip_fraction        | 0.00551       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0711       |
|    explained_variance   | 0.0271        |
|    learning_rate        | 0.0003        |
|    loss                 | 35.9          |
|    n_updates            | 290           |
|    policy_gradient_loss | 0.000318      |
|    value_loss           | 71.4          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -614     |
| time/              |          |
|    fps             | 469      |
|    iterations      | 30       |
|    time_elapsed    | 523      |
|    total_timesteps | 245760   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -599         |
| time/                   |              |
|    fps                  | 474          |
|    iterations           | 31           |
|    time_elapsed         | 535          |
|    total_timesteps      | 253952       |
| train/                  |              |
|    approx_kl            | 0.0005553291 |
|    clip_fraction        | 0.00734      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0575      |
|    explained_variance   | 0.0248       |
|    learning_rate        | 0.0003       |
|    loss                 | 30.9         |
|    n_updates            | 300          |
|    policy_gradient_loss | 7.16e-05     |
|    value_loss           | 70.4         |
------------------------------------------
Num timesteps: 260000
Best mean reward: -313.50 - Last mean reward per episode: -289.80
Saving new best model at 432000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=260000, episode_reward=-186.00 +/- 203.82
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -186          |
| time/                   |               |
|    total_timesteps      | 260000        |
| train/                  |               |
|    approx_kl            | 0.00064036157 |
|    clip_fraction        | 0.0067        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0696       |
|    explained_variance   | 0.00674       |
|    learning_rate        | 0.0003        |
|    loss                 | 27.3          |
|    n_updates            | 310           |
|    policy_gradient_loss | 0.000444      |
|    value_loss           | 71            |
-------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -599     |
| time/              |          |
|    fps             | 467      |
|    iterations      | 32       |
|    time_elapsed    | 560      |
|    total_timesteps | 262144   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -575         |
| time/                   |              |
|    fps                  | 472          |
|    iterations           | 33           |
|    time_elapsed         | 572          |
|    total_timesteps      | 270336       |
| train/                  |              |
|    approx_kl            | 0.0004322639 |
|    clip_fraction        | 0.00612      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.07        |
|    explained_variance   | 0.0427       |
|    learning_rate        | 0.0003       |
|    loss                 | 45           |
|    n_updates            | 320          |
|    policy_gradient_loss | 0.000477     |
|    value_loss           | 73.7         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -562          |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 34            |
|    time_elapsed         | 584           |
|    total_timesteps      | 278528        |
| train/                  |               |
|    approx_kl            | 0.00030468544 |
|    clip_fraction        | 0.00532       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0444       |
|    explained_variance   | 0.0254        |
|    learning_rate        | 0.0003        |
|    loss                 | 24.7          |
|    n_updates            | 330           |
|    policy_gradient_loss | 5.57e-05      |
|    value_loss           | 71.9          |
-------------------------------------------
Num timesteps: 280000
Best mean reward: -289.80 - Last mean reward per episode: -275.40
Saving new best model at 471000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=280000, episode_reward=-270.00 +/- 94.87
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -270          |
| time/                   |               |
|    total_timesteps      | 280000        |
| train/                  |               |
|    approx_kl            | 0.00015180952 |
|    clip_fraction        | 0.00217       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0463       |
|    explained_variance   | 0.018         |
|    learning_rate        | 0.0003        |
|    loss                 | 50.3          |
|    n_updates            | 340           |
|    policy_gradient_loss | 0.0002        |
|    value_loss           | 68.9          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -562     |
| time/              |          |
|    fps             | 470      |
|    iterations      | 35       |
|    time_elapsed    | 609      |
|    total_timesteps | 286720   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -547          |
| time/                   |               |
|    fps                  | 474           |
|    iterations           | 36            |
|    time_elapsed         | 621           |
|    total_timesteps      | 294912        |
| train/                  |               |
|    approx_kl            | 0.00012035924 |
|    clip_fraction        | 0.00358       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.054        |
|    explained_variance   | 0.0323        |
|    learning_rate        | 0.0003        |
|    loss                 | 34.3          |
|    n_updates            | 350           |
|    policy_gradient_loss | 0.000266      |
|    value_loss           | 74.9          |
-------------------------------------------
Num timesteps: 300000
Best mean reward: -275.40 - Last mean reward per episode: -263.10
Saving new best model at 510000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=300000, episode_reward=-264.00 +/- 122.08
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -264          |
| time/                   |               |
|    total_timesteps      | 300000        |
| train/                  |               |
|    approx_kl            | 0.00040883984 |
|    clip_fraction        | 0.00339       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0489       |
|    explained_variance   | 0.0105        |
|    learning_rate        | 0.0003        |
|    loss                 | 28.9          |
|    n_updates            | 360           |
|    policy_gradient_loss | 2.32e-05      |
|    value_loss           | 69.8          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -535     |
| time/              |          |
|    fps             | 469      |
|    iterations      | 37       |
|    time_elapsed    | 645      |
|    total_timesteps | 303104   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -535         |
| time/                   |              |
|    fps                  | 473          |
|    iterations           | 38           |
|    time_elapsed         | 657          |
|    total_timesteps      | 311296       |
| train/                  |              |
|    approx_kl            | 0.0007174058 |
|    clip_fraction        | 0.0051       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0492      |
|    explained_variance   | 0.00961      |
|    learning_rate        | 0.0003       |
|    loss                 | 44           |
|    n_updates            | 370          |
|    policy_gradient_loss | 0.000273     |
|    value_loss           | 71.3         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -478          |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 39            |
|    time_elapsed         | 670           |
|    total_timesteps      | 319488        |
| train/                  |               |
|    approx_kl            | 0.00028795702 |
|    clip_fraction        | 0.00629       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0587       |
|    explained_variance   | 0.0875        |
|    learning_rate        | 0.0003        |
|    loss                 | 36.5          |
|    n_updates            | 380           |
|    policy_gradient_loss | 0.000768      |
|    value_loss           | 75.7          |
-------------------------------------------
Num timesteps: 320000
Best mean reward: -263.10 - Last mean reward per episode: -250.80
Saving new best model at 537000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=320000, episode_reward=-234.00 +/- 160.32
Episode length: 3000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3e+03        |
|    mean_reward          | -234         |
| time/                   |              |
|    total_timesteps      | 320000       |
| train/                  |              |
|    approx_kl            | 0.0005060482 |
|    clip_fraction        | 0.0073       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0574      |
|    explained_variance   | 0.116        |
|    learning_rate        | 0.0003       |
|    loss                 | 37.4         |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.000146    |
|    value_loss           | 71.2         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 471      |
|    iterations      | 40       |
|    time_elapsed    | 695      |
|    total_timesteps | 327680   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -424          |
| time/                   |               |
|    fps                  | 474           |
|    iterations           | 41            |
|    time_elapsed         | 707           |
|    total_timesteps      | 335872        |
| train/                  |               |
|    approx_kl            | 0.00034234484 |
|    clip_fraction        | 0.0041        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.035        |
|    explained_variance   | 0.0525        |
|    learning_rate        | 0.0003        |
|    loss                 | 35.2          |
|    n_updates            | 400           |
|    policy_gradient_loss | -0.000158     |
|    value_loss           | 75.4          |
-------------------------------------------
Num timesteps: 340000
Best mean reward: -250.80 - Last mean reward per episode: -235.50
Saving new best model at 576000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=340000, episode_reward=-234.00 +/- 117.58
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -234          |
| time/                   |               |
|    total_timesteps      | 340000        |
| train/                  |               |
|    approx_kl            | 0.00012575513 |
|    clip_fraction        | 0.00228       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0251       |
|    explained_variance   | 0.0736        |
|    learning_rate        | 0.0003        |
|    loss                 | 26.1          |
|    n_updates            | 410           |
|    policy_gradient_loss | 0.000119      |
|    value_loss           | 73.6          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -371     |
| time/              |          |
|    fps             | 470      |
|    iterations      | 42       |
|    time_elapsed    | 731      |
|    total_timesteps | 344064   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -323          |
| time/                   |               |
|    fps                  | 473           |
|    iterations           | 43            |
|    time_elapsed         | 744           |
|    total_timesteps      | 352256        |
| train/                  |               |
|    approx_kl            | 0.00045611497 |
|    clip_fraction        | 0.00245       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0264       |
|    explained_variance   | 0.059         |
|    learning_rate        | 0.0003        |
|    loss                 | 27            |
|    n_updates            | 420           |
|    policy_gradient_loss | 2e-05         |
|    value_loss           | 70            |
-------------------------------------------
Num timesteps: 360000
Best mean reward: -235.50 - Last mean reward per episode: -231.60
Saving new best model at 615000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=360000, episode_reward=-216.00 +/- 133.36
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -216          |
| time/                   |               |
|    total_timesteps      | 360000        |
| train/                  |               |
|    approx_kl            | 0.00034456333 |
|    clip_fraction        | 0.00234       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0266       |
|    explained_variance   | 0.013         |
|    learning_rate        | 0.0003        |
|    loss                 | 25.1          |
|    n_updates            | 430           |
|    policy_gradient_loss | -9.25e-05     |
|    value_loss           | 73.7          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -286     |
| time/              |          |
|    fps             | 469      |
|    iterations      | 44       |
|    time_elapsed    | 768      |
|    total_timesteps | 360448   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -286          |
| time/                   |               |
|    fps                  | 472           |
|    iterations           | 45            |
|    time_elapsed         | 780           |
|    total_timesteps      | 368640        |
| train/                  |               |
|    approx_kl            | 0.00033663996 |
|    clip_fraction        | 0.00216       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0235       |
|    explained_variance   | 0.0216        |
|    learning_rate        | 0.0003        |
|    loss                 | 36.9          |
|    n_updates            | 440           |
|    policy_gradient_loss | 5.5e-05       |
|    value_loss           | 74.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -261          |
| time/                   |               |
|    fps                  | 475           |
|    iterations           | 46            |
|    time_elapsed         | 792           |
|    total_timesteps      | 376832        |
| train/                  |               |
|    approx_kl            | 0.00018530173 |
|    clip_fraction        | 0.00133       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0141       |
|    explained_variance   | 0.0787        |
|    learning_rate        | 0.0003        |
|    loss                 | 40.9          |
|    n_updates            | 450           |
|    policy_gradient_loss | 1.65e-05      |
|    value_loss           | 72.9          |
-------------------------------------------
Num timesteps: 380000
Best mean reward: -231.60 - Last mean reward per episode: -225.90
Saving new best model at 642000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=380000, episode_reward=-132.00 +/- 225.78
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -132          |
| time/                   |               |
|    total_timesteps      | 380000        |
| train/                  |               |
|    approx_kl            | 0.00011420539 |
|    clip_fraction        | 0.00121       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0167       |
|    explained_variance   | -0.000846     |
|    learning_rate        | 0.0003        |
|    loss                 | 33.3          |
|    n_updates            | 460           |
|    policy_gradient_loss | -7.25e-05     |
|    value_loss           | 70.5          |
-------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -259     |
| time/              |          |
|    fps             | 471      |
|    iterations      | 47       |
|    time_elapsed    | 816      |
|    total_timesteps | 385024   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -259          |
| time/                   |               |
|    fps                  | 473           |
|    iterations           | 48            |
|    time_elapsed         | 829           |
|    total_timesteps      | 393216        |
| train/                  |               |
|    approx_kl            | 0.00036269368 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.012        |
|    explained_variance   | 0.04          |
|    learning_rate        | 0.0003        |
|    loss                 | 34            |
|    n_updates            | 470           |
|    policy_gradient_loss | 1.24e-05      |
|    value_loss           | 74.1          |
-------------------------------------------
Num timesteps: 400000
Best mean reward: -225.90 - Last mean reward per episode: -202.20
Saving new best model at 681000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=400000, episode_reward=-210.00 +/- 200.80
Episode length: 3000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3e+03        |
|    mean_reward          | -210         |
| time/                   |              |
|    total_timesteps      | 400000       |
| train/                  |              |
|    approx_kl            | 0.0003314434 |
|    clip_fraction        | 0.00193      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0125      |
|    explained_variance   | 0.0214       |
|    learning_rate        | 0.0003       |
|    loss                 | 36.3         |
|    n_updates            | 480          |
|    policy_gradient_loss | -3.71e-05    |
|    value_loss           | 75.2         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -248     |
| time/              |          |
|    fps             | 469      |
|    iterations      | 49       |
|    time_elapsed    | 854      |
|    total_timesteps | 401408   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -229         |
| time/                   |              |
|    fps                  | 472          |
|    iterations           | 50           |
|    time_elapsed         | 866          |
|    total_timesteps      | 409600       |
| train/                  |              |
|    approx_kl            | 0.0003800993 |
|    clip_fraction        | 0.000525     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0054      |
|    explained_variance   | 0.0127       |
|    learning_rate        | 0.0003       |
|    loss                 | 36.8         |
|    n_updates            | 490          |
|    policy_gradient_loss | 9.25e-05     |
|    value_loss           | 76.2         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -229         |
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 51           |
|    time_elapsed         | 878          |
|    total_timesteps      | 417792       |
| train/                  |              |
|    approx_kl            | 6.204282e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00466     |
|    explained_variance   | 0.0402       |
|    learning_rate        | 0.0003       |
|    loss                 | 36           |
|    n_updates            | 500          |
|    policy_gradient_loss | 5.35e-05     |
|    value_loss           | 73.1         |
------------------------------------------
Num timesteps: 420000
Best mean reward: -202.20 - Last mean reward per episode: -196.20
Saving new best model at 720000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=420000, episode_reward=-168.00 +/- 163.88
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -168          |
| time/                   |               |
|    total_timesteps      | 420000        |
| train/                  |               |
|    approx_kl            | 2.3428584e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000205     |
|    explained_variance   | 0.0318        |
|    learning_rate        | 0.0003        |
|    loss                 | 25.1          |
|    n_updates            | 510           |
|    policy_gradient_loss | 7.6e-07       |
|    value_loss           | 71.2          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -226     |
| time/              |          |
|    fps             | 471      |
|    iterations      | 52       |
|    time_elapsed    | 903      |
|    total_timesteps | 425984   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | -220        |
| time/                   |             |
|    fps                  | 474         |
|    iterations           | 53          |
|    time_elapsed         | 915         |
|    total_timesteps      | 434176      |
| train/                  |             |
|    approx_kl            | 7.52334e-09 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000212   |
|    explained_variance   | 0.0103      |
|    learning_rate        | 0.0003      |
|    loss                 | 27.3        |
|    n_updates            | 520         |
|    policy_gradient_loss | 7.8e-07     |
|    value_loss           | 72.6        |
-----------------------------------------
Num timesteps: 440000
Best mean reward: -196.20 - Last mean reward per episode: -192.00
Saving new best model at 747000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=440000, episode_reward=-216.00 +/- 222.41
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -216          |
| time/                   |               |
|    total_timesteps      | 440000        |
| train/                  |               |
|    approx_kl            | 2.9260264e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00171      |
|    explained_variance   | 0.0352        |
|    learning_rate        | 0.0003        |
|    loss                 | 31.6          |
|    n_updates            | 530           |
|    policy_gradient_loss | 5.54e-05      |
|    value_loss           | 74.7          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -220     |
| time/              |          |
|    fps             | 470      |
|    iterations      | 54       |
|    time_elapsed    | 939      |
|    total_timesteps | 442368   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -208         |
| time/                   |              |
|    fps                  | 473          |
|    iterations           | 55           |
|    time_elapsed         | 952          |
|    total_timesteps      | 450560       |
| train/                  |              |
|    approx_kl            | 0.0004787029 |
|    clip_fraction        | 0.000708     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00237     |
|    explained_variance   | 0.0652       |
|    learning_rate        | 0.0003       |
|    loss                 | 32.3         |
|    n_updates            | 540          |
|    policy_gradient_loss | -2.15e-05    |
|    value_loss           | 72.8         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -206          |
| time/                   |               |
|    fps                  | 475           |
|    iterations           | 56            |
|    time_elapsed         | 964           |
|    total_timesteps      | 458752        |
| train/                  |               |
|    approx_kl            | 1.3990211e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00154      |
|    explained_variance   | 0.0592        |
|    learning_rate        | 0.0003        |
|    loss                 | 25.1          |
|    n_updates            | 550           |
|    policy_gradient_loss | 3.9e-05       |
|    value_loss           | 71.2          |
-------------------------------------------
Num timesteps: 460000
Best mean reward: -192.00 - Last mean reward per episode: -189.90
Saving new best model at 786000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=460000, episode_reward=-324.00 +/- 173.27
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -324          |
| time/                   |               |
|    total_timesteps      | 460000        |
| train/                  |               |
|    approx_kl            | 3.1868694e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000223     |
|    explained_variance   | 0.0246        |
|    learning_rate        | 0.0003        |
|    loss                 | 26.2          |
|    n_updates            | 560           |
|    policy_gradient_loss | 5.25e-07      |
|    value_loss           | 74.1          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -206     |
| time/              |          |
|    fps             | 472      |
|    iterations      | 57       |
|    time_elapsed    | 988      |
|    total_timesteps | 466944   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -203          |
| time/                   |               |
|    fps                  | 474           |
|    iterations           | 58            |
|    time_elapsed         | 1000          |
|    total_timesteps      | 475136        |
| train/                  |               |
|    approx_kl            | 3.6379788e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000271     |
|    explained_variance   | 0.00678       |
|    learning_rate        | 0.0003        |
|    loss                 | 40.2          |
|    n_updates            | 570           |
|    policy_gradient_loss | 1.24e-06      |
|    value_loss           | 70.3          |
-------------------------------------------
Num timesteps: 480000
Best mean reward: -189.90 - Last mean reward per episode: -192.60
Eval num_timesteps=480000, episode_reward=-138.00 +/- 129.52
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -138          |
| time/                   |               |
|    total_timesteps      | 480000        |
| train/                  |               |
|    approx_kl            | 2.4956535e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000172     |
|    explained_variance   | -0.00261      |
|    learning_rate        | 0.0003        |
|    loss                 | 40.4          |
|    n_updates            | 580           |
|    policy_gradient_loss | 1.03e-06      |
|    value_loss           | 74.2          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -202     |
| time/              |          |
|    fps             | 471      |
|    iterations      | 59       |
|    time_elapsed    | 1024     |
|    total_timesteps | 483328   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -202         |
| time/                   |              |
|    fps                  | 473          |
|    iterations           | 60           |
|    time_elapsed         | 1037         |
|    total_timesteps      | 491520       |
| train/                  |              |
|    approx_kl            | 8.507312e-06 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000941    |
|    explained_variance   | 0.0235       |
|    learning_rate        | 0.0003       |
|    loss                 | 59.4         |
|    n_updates            | 590          |
|    policy_gradient_loss | -2.24e-05    |
|    value_loss           | 70.2         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -202         |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 61           |
|    time_elapsed         | 1049         |
|    total_timesteps      | 499712       |
| train/                  |              |
|    approx_kl            | 5.066213e-06 |
|    clip_fraction        | 7.32e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00184     |
|    explained_variance   | 0.0428       |
|    learning_rate        | 0.0003       |
|    loss                 | 20.6         |
|    n_updates            | 600          |
|    policy_gradient_loss | 6.38e-06     |
|    value_loss           | 71.2         |
------------------------------------------
Num timesteps: 500000
Best mean reward: -189.90 - Last mean reward per episode: -193.20
Eval num_timesteps=500000, episode_reward=-252.00 +/- 185.52
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -252          |
| time/                   |               |
|    total_timesteps      | 500000        |
| train/                  |               |
|    approx_kl            | 4.4368062e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00272      |
|    explained_variance   | 0.0133        |
|    learning_rate        | 0.0003        |
|    loss                 | 32.9          |
|    n_updates            | 610           |
|    policy_gradient_loss | 8.87e-05      |
|    value_loss           | 75.4          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -187     |
| time/              |          |
|    fps             | 473      |
|    iterations      | 62       |
|    time_elapsed    | 1073     |
|    total_timesteps | 507904   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -181         |
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 63           |
|    time_elapsed         | 1085         |
|    total_timesteps      | 516096       |
| train/                  |              |
|    approx_kl            | 8.163959e-06 |
|    clip_fraction        | 0.00011      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00105     |
|    explained_variance   | 0.0993       |
|    learning_rate        | 0.0003       |
|    loss                 | 40           |
|    n_updates            | 620          |
|    policy_gradient_loss | 1.04e-05     |
|    value_loss           | 73.6         |
------------------------------------------
Num timesteps: 520000
Best mean reward: -189.90 - Last mean reward per episode: -187.20
Saving new best model at 891000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=520000, episode_reward=-288.00 +/- 113.21
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -288          |
| time/                   |               |
|    total_timesteps      | 520000        |
| train/                  |               |
|    approx_kl            | 1.5090336e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000746     |
|    explained_variance   | 0.164         |
|    learning_rate        | 0.0003        |
|    loss                 | 46            |
|    n_updates            | 630           |
|    policy_gradient_loss | 7.2e-08       |
|    value_loss           | 73.7          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -181     |
| time/              |          |
|    fps             | 472      |
|    iterations      | 64       |
|    time_elapsed    | 1109     |
|    total_timesteps | 524288   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3e+03     |
|    ep_rew_mean          | -178      |
| time/                   |           |
|    fps                  | 474       |
|    iterations           | 65        |
|    time_elapsed         | 1122      |
|    total_timesteps      | 532480    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.06e-05 |
|    explained_variance   | 0.0017    |
|    learning_rate        | 0.0003    |
|    loss                 | 46.6      |
|    n_updates            | 640       |
|    policy_gradient_loss | 6.46e-09  |
|    value_loss           | 73.2      |
---------------------------------------
Num timesteps: 540000
Best mean reward: -187.20 - Last mean reward per episode: -185.70
Saving new best model at 930000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=540000, episode_reward=-228.00 +/- 90.20
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -228          |
| time/                   |               |
|    total_timesteps      | 540000        |
| train/                  |               |
|    approx_kl            | 4.4237822e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.39e-05     |
|    explained_variance   | 0.0424        |
|    learning_rate        | 0.0003        |
|    loss                 | 34.5          |
|    n_updates            | 650           |
|    policy_gradient_loss | 1.41e-07      |
|    value_loss           | 76.7          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -174     |
| time/              |          |
|    fps             | 471      |
|    iterations      | 66       |
|    time_elapsed    | 1146     |
|    total_timesteps | 540672   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -174          |
| time/                   |               |
|    fps                  | 473           |
|    iterations           | 67            |
|    time_elapsed         | 1158          |
|    total_timesteps      | 548864        |
| train/                  |               |
|    approx_kl            | 7.1304385e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000213     |
|    explained_variance   | 0.0452        |
|    learning_rate        | 0.0003        |
|    loss                 | 43.5          |
|    n_updates            | 660           |
|    policy_gradient_loss | 1.6e-06       |
|    value_loss           | 75.4          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -166         |
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 68           |
|    time_elapsed         | 1170         |
|    total_timesteps      | 557056       |
| train/                  |              |
|    approx_kl            | 0.0002735636 |
|    clip_fraction        | 0.000122     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00449     |
|    explained_variance   | 0.0549       |
|    learning_rate        | 0.0003       |
|    loss                 | 44           |
|    n_updates            | 670          |
|    policy_gradient_loss | 7.56e-06     |
|    value_loss           | 77.4         |
------------------------------------------
Num timesteps: 560000
Best mean reward: -185.70 - Last mean reward per episode: -186.30
Eval num_timesteps=560000, episode_reward=-258.00 +/- 145.24
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -258          |
| time/                   |               |
|    total_timesteps      | 560000        |
| train/                  |               |
|    approx_kl            | 0.00089765736 |
|    clip_fraction        | 0.000366      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00057      |
|    explained_variance   | 0.0958        |
|    learning_rate        | 0.0003        |
|    loss                 | 33.2          |
|    n_updates            | 680           |
|    policy_gradient_loss | -5.42e-05     |
|    value_loss           | 73            |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -169     |
| time/              |          |
|    fps             | 473      |
|    iterations      | 69       |
|    time_elapsed    | 1194     |
|    total_timesteps | 565248   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -169          |
| time/                   |               |
|    fps                  | 475           |
|    iterations           | 70            |
|    time_elapsed         | 1207          |
|    total_timesteps      | 573440        |
| train/                  |               |
|    approx_kl            | 1.1496013e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92e-05     |
|    explained_variance   | 0.114         |
|    learning_rate        | 0.0003        |
|    loss                 | 36            |
|    n_updates            | 690           |
|    policy_gradient_loss | 3.99e-07      |
|    value_loss           | 73.4          |
-------------------------------------------
Num timesteps: 580000
Best mean reward: -185.70 - Last mean reward per episode: -181.80
Saving new best model at 996000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=580000, episode_reward=-216.00 +/- 101.11
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | -216        |
| time/                   |             |
|    total_timesteps      | 580000      |
| train/                  |             |
|    approx_kl            | 6.85468e-08 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0012     |
|    explained_variance   | 0.0636      |
|    learning_rate        | 0.0003      |
|    loss                 | 35.2        |
|    n_updates            | 700         |
|    policy_gradient_loss | 2.2e-06     |
|    value_loss           | 77.1        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 472      |
|    iterations      | 71       |
|    time_elapsed    | 1231     |
|    total_timesteps | 581632   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -155          |
| time/                   |               |
|    fps                  | 474           |
|    iterations           | 72            |
|    time_elapsed         | 1243          |
|    total_timesteps      | 589824        |
| train/                  |               |
|    approx_kl            | 2.8532166e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0183       |
|    explained_variance   | 0.0226        |
|    learning_rate        | 0.0003        |
|    loss                 | 34.6          |
|    n_updates            | 710           |
|    policy_gradient_loss | 5.7e-05       |
|    value_loss           | 74.2          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -155          |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 73            |
|    time_elapsed         | 1255          |
|    total_timesteps      | 598016        |
| train/                  |               |
|    approx_kl            | 0.00042370122 |
|    clip_fraction        | 0.00192       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0165       |
|    explained_variance   | 0.015         |
|    learning_rate        | 0.0003        |
|    loss                 | 50.7          |
|    n_updates            | 720           |
|    policy_gradient_loss | 0.000175      |
|    value_loss           | 76.7          |
-------------------------------------------
Num timesteps: 600000
Best mean reward: -181.80 - Last mean reward per episode: -182.70
Eval num_timesteps=600000, episode_reward=-204.00 +/- 242.54
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -204          |
| time/                   |               |
|    total_timesteps      | 600000        |
| train/                  |               |
|    approx_kl            | 0.00019409158 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00915      |
|    explained_variance   | 0.0734        |
|    learning_rate        | 0.0003        |
|    loss                 | 27.7          |
|    n_updates            | 730           |
|    policy_gradient_loss | 0.000351      |
|    value_loss           | 70.8          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 473      |
|    iterations      | 74       |
|    time_elapsed    | 1280     |
|    total_timesteps | 606208   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -154         |
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 75           |
|    time_elapsed         | 1292         |
|    total_timesteps      | 614400       |
| train/                  |              |
|    approx_kl            | 0.0007624397 |
|    clip_fraction        | 0.00234      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00928     |
|    explained_variance   | 0.0421       |
|    learning_rate        | 0.0003       |
|    loss                 | 36           |
|    n_updates            | 740          |
|    policy_gradient_loss | 0.000307     |
|    value_loss           | 72           |
------------------------------------------
Num timesteps: 620000
Best mean reward: -181.80 - Last mean reward per episode: -179.70
Saving new best model at 1062000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=620000, episode_reward=-108.00 +/- 168.21
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -108          |
| time/                   |               |
|    total_timesteps      | 620000        |
| train/                  |               |
|    approx_kl            | 0.00017912612 |
|    clip_fraction        | 0.000952      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0144       |
|    explained_variance   | 0.0146        |
|    learning_rate        | 0.0003        |
|    loss                 | 31.9          |
|    n_updates            | 750           |
|    policy_gradient_loss | 0.00022       |
|    value_loss           | 72.1          |
-------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -154     |
| time/              |          |
|    fps             | 472      |
|    iterations      | 76       |
|    time_elapsed    | 1316     |
|    total_timesteps | 622592   |
---------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 3e+03          |
|    ep_rew_mean          | -153           |
| time/                   |                |
|    fps                  | 474            |
|    iterations           | 77             |
|    time_elapsed         | 1328           |
|    total_timesteps      | 630784         |
| train/                  |                |
|    approx_kl            | 0.000119768396 |
|    clip_fraction        | 0.00033        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00554       |
|    explained_variance   | 0.0275         |
|    learning_rate        | 0.0003         |
|    loss                 | 39.6           |
|    n_updates            | 760            |
|    policy_gradient_loss | 7.83e-05       |
|    value_loss           | 77.5           |
--------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -156          |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 78            |
|    time_elapsed         | 1340          |
|    total_timesteps      | 638976        |
| train/                  |               |
|    approx_kl            | 0.00022386495 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00183      |
|    explained_variance   | 0.00682       |
|    learning_rate        | 0.0003        |
|    loss                 | 44.7          |
|    n_updates            | 770           |
|    policy_gradient_loss | -3.36e-06     |
|    value_loss           | 71.5          |
-------------------------------------------
Num timesteps: 640000
Best mean reward: -179.70 - Last mean reward per episode: -168.00
Saving new best model at 1101000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=640000, episode_reward=-276.00 +/- 148.67
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -276          |
| time/                   |               |
|    total_timesteps      | 640000        |
| train/                  |               |
|    approx_kl            | 1.3907993e-06 |
|    clip_fraction        | 8.54e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00161      |
|    explained_variance   | 0.0236        |
|    learning_rate        | 0.0003        |
|    loss                 | 25.7          |
|    n_updates            | 780           |
|    policy_gradient_loss | 4.78e-05      |
|    value_loss           | 70.9          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -156     |
| time/              |          |
|    fps             | 474      |
|    iterations      | 79       |
|    time_elapsed    | 1365     |
|    total_timesteps | 647168   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -155          |
| time/                   |               |
|    fps                  | 475           |
|    iterations           | 80            |
|    time_elapsed         | 1377          |
|    total_timesteps      | 655360        |
| train/                  |               |
|    approx_kl            | 0.00020800263 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00198      |
|    explained_variance   | 0.034         |
|    learning_rate        | 0.0003        |
|    loss                 | 48.7          |
|    n_updates            | 790           |
|    policy_gradient_loss | 5.48e-05      |
|    value_loss           | 72.8          |
-------------------------------------------
Num timesteps: 660000
Best mean reward: -168.00 - Last mean reward per episode: -167.10
Saving new best model at 1140000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=660000, episode_reward=-54.00 +/- 119.10
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -54           |
| time/                   |               |
|    total_timesteps      | 660000        |
| train/                  |               |
|    approx_kl            | 0.00035306992 |
|    clip_fraction        | 0.000256      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00104      |
|    explained_variance   | 0.0141        |
|    learning_rate        | 0.0003        |
|    loss                 | 46.8          |
|    n_updates            | 800           |
|    policy_gradient_loss | 8.55e-05      |
|    value_loss           | 75.2          |
-------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -151     |
| time/              |          |
|    fps             | 473      |
|    iterations      | 81       |
|    time_elapsed    | 1401     |
|    total_timesteps | 663552   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -151          |
| time/                   |               |
|    fps                  | 474           |
|    iterations           | 82            |
|    time_elapsed         | 1414          |
|    total_timesteps      | 671744        |
| train/                  |               |
|    approx_kl            | 1.6938226e-05 |
|    clip_fraction        | 0.000122      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000827     |
|    explained_variance   | 0.0156        |
|    learning_rate        | 0.0003        |
|    loss                 | 33.2          |
|    n_updates            | 810           |
|    policy_gradient_loss | -6.72e-06     |
|    value_loss           | 74.7          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -153          |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 83            |
|    time_elapsed         | 1426          |
|    total_timesteps      | 679936        |
| train/                  |               |
|    approx_kl            | 1.7468876e-05 |
|    clip_fraction        | 0.00033       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000767     |
|    explained_variance   | 0.049         |
|    learning_rate        | 0.0003        |
|    loss                 | 41.6          |
|    n_updates            | 820           |
|    policy_gradient_loss | 3.12e-05      |
|    value_loss           | 69.6          |
-------------------------------------------
Num timesteps: 680000
Best mean reward: -167.10 - Last mean reward per episode: -159.00
Saving new best model at 1167000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=680000, episode_reward=-168.00 +/- 123.84
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -168          |
| time/                   |               |
|    total_timesteps      | 680000        |
| train/                  |               |
|    approx_kl            | 2.2453605e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00165      |
|    explained_variance   | -0.00308      |
|    learning_rate        | 0.0003        |
|    loss                 | 33.9          |
|    n_updates            | 830           |
|    policy_gradient_loss | 3.71e-06      |
|    value_loss           | 76.1          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -144     |
| time/              |          |
|    fps             | 474      |
|    iterations      | 84       |
|    time_elapsed    | 1450     |
|    total_timesteps | 688128   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -148          |
| time/                   |               |
|    fps                  | 475           |
|    iterations           | 85            |
|    time_elapsed         | 1462          |
|    total_timesteps      | 696320        |
| train/                  |               |
|    approx_kl            | 1.1918964e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00514      |
|    explained_variance   | 0.0314        |
|    learning_rate        | 0.0003        |
|    loss                 | 41            |
|    n_updates            | 840           |
|    policy_gradient_loss | 0.00013       |
|    value_loss           | 72.9          |
-------------------------------------------
Num timesteps: 700000
Best mean reward: -159.00 - Last mean reward per episode: -156.60
Saving new best model at 1206000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=700000, episode_reward=-162.00 +/- 226.57
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -162          |
| time/                   |               |
|    total_timesteps      | 700000        |
| train/                  |               |
|    approx_kl            | 4.9246664e-06 |
|    clip_fraction        | 6.1e-05       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00115      |
|    explained_variance   | 0.056         |
|    learning_rate        | 0.0003        |
|    loss                 | 33.4          |
|    n_updates            | 850           |
|    policy_gradient_loss | 1.5e-05       |
|    value_loss           | 67.7          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -148     |
| time/              |          |
|    fps             | 473      |
|    iterations      | 86       |
|    time_elapsed    | 1487     |
|    total_timesteps | 704512   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -153          |
| time/                   |               |
|    fps                  | 475           |
|    iterations           | 87            |
|    time_elapsed         | 1499          |
|    total_timesteps      | 712704        |
| train/                  |               |
|    approx_kl            | 2.6571863e-05 |
|    clip_fraction        | 0.000476      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00905      |
|    explained_variance   | 0.0589        |
|    learning_rate        | 0.0003        |
|    loss                 | 35.3          |
|    n_updates            | 860           |
|    policy_gradient_loss | 2.88e-05      |
|    value_loss           | 74.5          |
-------------------------------------------
Num timesteps: 720000
Best mean reward: -156.60 - Last mean reward per episode: -154.50
Saving new best model at 1245000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=720000, episode_reward=-282.00 +/- 120.90
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -282          |
| time/                   |               |
|    total_timesteps      | 720000        |
| train/                  |               |
|    approx_kl            | 0.00033760478 |
|    clip_fraction        | 0.00244       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0203       |
|    explained_variance   | 0.0907        |
|    learning_rate        | 0.0003        |
|    loss                 | 32.3          |
|    n_updates            | 870           |
|    policy_gradient_loss | -1.59e-05     |
|    value_loss           | 73.4          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -147     |
| time/              |          |
|    fps             | 473      |
|    iterations      | 88       |
|    time_elapsed    | 1523     |
|    total_timesteps | 720896   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -147         |
| time/                   |              |
|    fps                  | 474          |
|    iterations           | 89           |
|    time_elapsed         | 1536         |
|    total_timesteps      | 729088       |
| train/                  |              |
|    approx_kl            | 9.222737e-05 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0237      |
|    explained_variance   | 0.0887       |
|    learning_rate        | 0.0003       |
|    loss                 | 41.7         |
|    n_updates            | 880          |
|    policy_gradient_loss | 0.000128     |
|    value_loss           | 72.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -153         |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 90           |
|    time_elapsed         | 1548         |
|    total_timesteps      | 737280       |
| train/                  |              |
|    approx_kl            | 8.575917e-05 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00133     |
|    explained_variance   | 0.044        |
|    learning_rate        | 0.0003       |
|    loss                 | 42.6         |
|    n_updates            | 890          |
|    policy_gradient_loss | -2.47e-05    |
|    value_loss           | 67.4         |
------------------------------------------
Num timesteps: 740000
Best mean reward: -154.50 - Last mean reward per episode: -166.20
Eval num_timesteps=740000, episode_reward=-306.00 +/- 162.55
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -306          |
| time/                   |               |
|    total_timesteps      | 740000        |
| train/                  |               |
|    approx_kl            | 2.5658963e-05 |
|    clip_fraction        | 0.000183      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00245      |
|    explained_variance   | 0.0754        |
|    learning_rate        | 0.0003        |
|    loss                 | 48.7          |
|    n_updates            | 900           |
|    policy_gradient_loss | -7.79e-06     |
|    value_loss           | 74.5          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -155     |
| time/              |          |
|    fps             | 474      |
|    iterations      | 91       |
|    time_elapsed    | 1572     |
|    total_timesteps | 745472   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -155          |
| time/                   |               |
|    fps                  | 475           |
|    iterations           | 92            |
|    time_elapsed         | 1584          |
|    total_timesteps      | 753664        |
| train/                  |               |
|    approx_kl            | 0.00025696662 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00342      |
|    explained_variance   | 0.123         |
|    learning_rate        | 0.0003        |
|    loss                 | 34.7          |
|    n_updates            | 910           |
|    policy_gradient_loss | 8.45e-05      |
|    value_loss           | 69.6          |
-------------------------------------------
Num timesteps: 760000
Best mean reward: -154.50 - Last mean reward per episode: -170.40
Eval num_timesteps=760000, episode_reward=-282.00 +/- 220.13
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -282          |
| time/                   |               |
|    total_timesteps      | 760000        |
| train/                  |               |
|    approx_kl            | 2.0897118e-05 |
|    clip_fraction        | 0.000635      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00926      |
|    explained_variance   | 0.0144        |
|    learning_rate        | 0.0003        |
|    loss                 | 26.6          |
|    n_updates            | 920           |
|    policy_gradient_loss | 5.08e-05      |
|    value_loss           | 73.9          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -146     |
| time/              |          |
|    fps             | 473      |
|    iterations      | 93       |
|    time_elapsed    | 1608     |
|    total_timesteps | 761856   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -150         |
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 94           |
|    time_elapsed         | 1621         |
|    total_timesteps      | 770048       |
| train/                  |              |
|    approx_kl            | 0.0006749699 |
|    clip_fraction        | 0.000964     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00251     |
|    explained_variance   | 0.16         |
|    learning_rate        | 0.0003       |
|    loss                 | 45.6         |
|    n_updates            | 930          |
|    policy_gradient_loss | -0.00014     |
|    value_loss           | 69           |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -150          |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 95            |
|    time_elapsed         | 1633          |
|    total_timesteps      | 778240        |
| train/                  |               |
|    approx_kl            | 2.4158151e-05 |
|    clip_fraction        | 8.54e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00155      |
|    explained_variance   | 0.103         |
|    learning_rate        | 0.0003        |
|    loss                 | 30.3          |
|    n_updates            | 940           |
|    policy_gradient_loss | -1.57e-06     |
|    value_loss           | 69            |
-------------------------------------------
Num timesteps: 780000
Best mean reward: -154.50 - Last mean reward per episode: -186.90
Eval num_timesteps=780000, episode_reward=-216.00 +/- 168.00
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -216          |
| time/                   |               |
|    total_timesteps      | 780000        |
| train/                  |               |
|    approx_kl            | 3.9340157e-06 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00342      |
|    explained_variance   | -0.000718     |
|    learning_rate        | 0.0003        |
|    loss                 | 41.3          |
|    n_updates            | 950           |
|    policy_gradient_loss | 7.22e-05      |
|    value_loss           | 73.1          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -151     |
| time/              |          |
|    fps             | 474      |
|    iterations      | 96       |
|    time_elapsed    | 1657     |
|    total_timesteps | 786432   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -146         |
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 97           |
|    time_elapsed         | 1670         |
|    total_timesteps      | 794624       |
| train/                  |              |
|    approx_kl            | 0.0003001226 |
|    clip_fraction        | 0.000427     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00307     |
|    explained_variance   | 0.0352       |
|    learning_rate        | 0.0003       |
|    loss                 | 43           |
|    n_updates            | 960          |
|    policy_gradient_loss | -4.83e-05    |
|    value_loss           | 73.9         |
------------------------------------------
Num timesteps: 800000
Best mean reward: -154.50 - Last mean reward per episode: -190.20
Eval num_timesteps=800000, episode_reward=-174.00 +/- 220.78
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -174          |
| time/                   |               |
|    total_timesteps      | 800000        |
| train/                  |               |
|    approx_kl            | 2.3065455e-05 |
|    clip_fraction        | 0.000208      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00711      |
|    explained_variance   | 0.0252        |
|    learning_rate        | 0.0003        |
|    loss                 | 48.7          |
|    n_updates            | 970           |
|    policy_gradient_loss | 2.08e-05      |
|    value_loss           | 78.1          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -146     |
| time/              |          |
|    fps             | 473      |
|    iterations      | 98       |
|    time_elapsed    | 1694     |
|    total_timesteps | 802816   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -146          |
| time/                   |               |
|    fps                  | 475           |
|    iterations           | 99            |
|    time_elapsed         | 1706          |
|    total_timesteps      | 811008        |
| train/                  |               |
|    approx_kl            | 0.00014099378 |
|    clip_fraction        | 0.000696      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00511      |
|    explained_variance   | 0.0257        |
|    learning_rate        | 0.0003        |
|    loss                 | 46.7          |
|    n_updates            | 980           |
|    policy_gradient_loss | 0.000163      |
|    value_loss           | 77            |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -146          |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 100           |
|    time_elapsed         | 1718          |
|    total_timesteps      | 819200        |
| train/                  |               |
|    approx_kl            | 9.9375065e-05 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0107       |
|    explained_variance   | 0.0196        |
|    learning_rate        | 0.0003        |
|    loss                 | 48.3          |
|    n_updates            | 990           |
|    policy_gradient_loss | 0.000165      |
|    value_loss           | 76.6          |
-------------------------------------------
Num timesteps: 820000
Best mean reward: -154.50 - Last mean reward per episode: -176.10
Eval num_timesteps=820000, episode_reward=-162.00 +/- 92.17
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | -162        |
| time/                   |             |
|    total_timesteps      | 820000      |
| train/                  |             |
|    approx_kl            | 0.000771913 |
|    clip_fraction        | 0.000366    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00066    |
|    explained_variance   | 0.101       |
|    learning_rate        | 0.0003      |
|    loss                 | 49.2        |
|    n_updates            | 1000        |
|    policy_gradient_loss | 0.00034     |
|    value_loss           | 68.5        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -146     |
| time/              |          |
|    fps             | 474      |
|    iterations      | 101      |
|    time_elapsed    | 1742     |
|    total_timesteps | 827392   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3e+03     |
|    ep_rew_mean          | -146      |
| time/                   |           |
|    fps                  | 476       |
|    iterations           | 102       |
|    time_elapsed         | 1754      |
|    total_timesteps      | 835584    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.56e-05 |
|    explained_variance   | 0.184     |
|    learning_rate        | 0.0003    |
|    loss                 | 37.2      |
|    n_updates            | 1010      |
|    policy_gradient_loss | -6.52e-07 |
|    value_loss           | 73.8      |
---------------------------------------
Num timesteps: 840000
Best mean reward: -154.50 - Last mean reward per episode: -185.40
Eval num_timesteps=840000, episode_reward=-408.00 +/- 116.34
Episode length: 3000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3e+03        |
|    mean_reward          | -408         |
| time/                   |              |
|    total_timesteps      | 840000       |
| train/                  |              |
|    approx_kl            | 4.467438e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00153     |
|    explained_variance   | 0.119        |
|    learning_rate        | 0.0003       |
|    loss                 | 42.2         |
|    n_updates            | 1020         |
|    policy_gradient_loss | -8.39e-08    |
|    value_loss           | 73.3         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -149     |
| time/              |          |
|    fps             | 474      |
|    iterations      | 103      |
|    time_elapsed    | 1778     |
|    total_timesteps | 843776   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -149         |
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 104          |
|    time_elapsed         | 1791         |
|    total_timesteps      | 851968       |
| train/                  |              |
|    approx_kl            | 0.0016698858 |
|    clip_fraction        | 0.00061      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000318    |
|    explained_variance   | 0.173        |
|    learning_rate        | 0.0003       |
|    loss                 | 36.9         |
|    n_updates            | 1030         |
|    policy_gradient_loss | 3.63e-05     |
|    value_loss           | 72.6         |
------------------------------------------
Num timesteps: 860000
Best mean reward: -154.50 - Last mean reward per episode: -199.20
Eval num_timesteps=860000, episode_reward=-198.00 +/- 88.18
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -198          |
| time/                   |               |
|    total_timesteps      | 860000        |
| train/                  |               |
|    approx_kl            | 2.2628228e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000256     |
|    explained_variance   | 0.113         |
|    learning_rate        | 0.0003        |
|    loss                 | 24.2          |
|    n_updates            | 1040          |
|    policy_gradient_loss | 6.49e-07      |
|    value_loss           | 70.4          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -158     |
| time/              |          |
|    fps             | 473      |
|    iterations      | 105      |
|    time_elapsed    | 1815     |
|    total_timesteps | 860160   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -161          |
| time/                   |               |
|    fps                  | 475           |
|    iterations           | 106           |
|    time_elapsed         | 1827          |
|    total_timesteps      | 868352        |
| train/                  |               |
|    approx_kl            | 1.0404619e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00041      |
|    explained_variance   | 0.114         |
|    learning_rate        | 0.0003        |
|    loss                 | 52.9          |
|    n_updates            | 1050          |
|    policy_gradient_loss | -9.58e-07     |
|    value_loss           | 72.8          |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3e+03     |
|    ep_rew_mean          | -165      |
| time/                   |           |
|    fps                  | 476       |
|    iterations           | 107       |
|    time_elapsed         | 1839      |
|    total_timesteps      | 876544    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0.113     |
|    learning_rate        | 0.0003    |
|    loss                 | 21        |
|    n_updates            | 1060      |
|    policy_gradient_loss | 9.17e-07  |
|    value_loss           | 69.3      |
---------------------------------------
Num timesteps: 880000
Best mean reward: -154.50 - Last mean reward per episode: -207.30
Eval num_timesteps=880000, episode_reward=-384.00 +/- 111.28
Episode length: 3000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 3e+03          |
|    mean_reward          | -384           |
| time/                   |                |
|    total_timesteps      | 880000         |
| train/                  |                |
|    approx_kl            | -3.6525307e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000106      |
|    explained_variance   | 0.0743         |
|    learning_rate        | 0.0003         |
|    loss                 | 44.7           |
|    n_updates            | 1070           |
|    policy_gradient_loss | 1.9e-07        |
|    value_loss           | 72.8           |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 474      |
|    iterations      | 108      |
|    time_elapsed    | 1863     |
|    total_timesteps | 884736   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3e+03     |
|    ep_rew_mean          | -171      |
| time/                   |           |
|    fps                  | 475       |
|    iterations           | 109       |
|    time_elapsed         | 1875      |
|    total_timesteps      | 892928    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0.118     |
|    learning_rate        | 0.0003    |
|    loss                 | 36.8      |
|    n_updates            | 1080      |
|    policy_gradient_loss | 3.35e-07  |
|    value_loss           | 72.6      |
---------------------------------------
Num timesteps: 900000
Best mean reward: -154.50 - Last mean reward per episode: -221.70
Eval num_timesteps=900000, episode_reward=-198.00 +/- 108.33
Episode length: 3000.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 3e+03     |
|    mean_reward          | -198      |
| time/                   |           |
|    total_timesteps      | 900000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.05e-05 |
|    explained_variance   | 0.0949    |
|    learning_rate        | 0.0003    |
|    loss                 | 37.9      |
|    n_updates            | 1090      |
|    policy_gradient_loss | 9.95e-08  |
|    value_loss           | 72.5      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -176     |
| time/              |          |
|    fps             | 474      |
|    iterations      | 110      |
|    time_elapsed    | 1900     |
|    total_timesteps | 901120   |
---------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 3e+03          |
|    ep_rew_mean          | -176           |
| time/                   |                |
|    fps                  | 474            |
|    iterations           | 111            |
|    time_elapsed         | 1914           |
|    total_timesteps      | 909312         |
| train/                  |                |
|    approx_kl            | -2.8303475e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -6.72e-05      |
|    explained_variance   | 0.0461         |
|    learning_rate        | 0.0003         |
|    loss                 | 30.9           |
|    n_updates            | 1100           |
|    policy_gradient_loss | 1.23e-07       |
|    value_loss           | 66.4           |
--------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -173          |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 112           |
|    time_elapsed         | 1926          |
|    total_timesteps      | 917504        |
| train/                  |               |
|    approx_kl            | 2.3850589e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000368     |
|    explained_variance   | 0.0226        |
|    learning_rate        | 0.0003        |
|    loss                 | 38            |
|    n_updates            | 1110          |
|    policy_gradient_loss | 2.26e-07      |
|    value_loss           | 78.3          |
-------------------------------------------
Num timesteps: 920000
Best mean reward: -154.50 - Last mean reward per episode: -204.90
Eval num_timesteps=920000, episode_reward=-258.00 +/- 145.24
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -258          |
| time/                   |               |
|    total_timesteps      | 920000        |
| train/                  |               |
|    approx_kl            | 4.7992216e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00163      |
|    explained_variance   | 0.0308        |
|    learning_rate        | 0.0003        |
|    loss                 | 51.2          |
|    n_updates            | 1120          |
|    policy_gradient_loss | 3.29e-06      |
|    value_loss           | 76.4          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -168     |
| time/              |          |
|    fps             | 474      |
|    iterations      | 113      |
|    time_elapsed    | 1950     |
|    total_timesteps | 925696   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -168          |
| time/                   |               |
|    fps                  | 475           |
|    iterations           | 114           |
|    time_elapsed         | 1962          |
|    total_timesteps      | 933888        |
| train/                  |               |
|    approx_kl            | 3.3803386e-05 |
|    clip_fraction        | 0.00011       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00123      |
|    explained_variance   | 0.0117        |
|    learning_rate        | 0.0003        |
|    loss                 | 50.7          |
|    n_updates            | 1130          |
|    policy_gradient_loss | -6.55e-06     |
|    value_loss           | 75.3          |
-------------------------------------------
Num timesteps: 940000
Best mean reward: -154.50 - Last mean reward per episode: -208.80
Eval num_timesteps=940000, episode_reward=-198.00 +/- 219.31
Episode length: 3000.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 3e+03     |
|    mean_reward          | -198      |
| time/                   |           |
|    total_timesteps      | 940000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00031  |
|    explained_variance   | 0.028     |
|    learning_rate        | 0.0003    |
|    loss                 | 31.9      |
|    n_updates            | 1140      |
|    policy_gradient_loss | -1.69e-06 |
|    value_loss           | 67.7      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -171     |
| time/              |          |
|    fps             | 474      |
|    iterations      | 115      |
|    time_elapsed    | 1986     |
|    total_timesteps | 942080   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3e+03     |
|    ep_rew_mean          | -174      |
| time/                   |           |
|    fps                  | 475       |
|    iterations           | 116       |
|    time_elapsed         | 1998      |
|    total_timesteps      | 950272    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.23e-05 |
|    explained_variance   | 0.08      |
|    learning_rate        | 0.0003    |
|    loss                 | 17.9      |
|    n_updates            | 1150      |
|    policy_gradient_loss | 3.09e-07  |
|    value_loss           | 69.8      |
---------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -174         |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 117          |
|    time_elapsed         | 2010         |
|    total_timesteps      | 958464       |
| train/                  |              |
|    approx_kl            | 9.313226e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.37e-05    |
|    explained_variance   | 0.078        |
|    learning_rate        | 0.0003       |
|    loss                 | 39.9         |
|    n_updates            | 1160         |
|    policy_gradient_loss | 1.44e-07     |
|    value_loss           | 68.9         |
------------------------------------------
Num timesteps: 960000
Best mean reward: -154.50 - Last mean reward per episode: -201.30
Eval num_timesteps=960000, episode_reward=-276.00 +/- 127.84
Episode length: 3000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 3e+03          |
|    mean_reward          | -276           |
| time/                   |                |
|    total_timesteps      | 960000         |
| train/                  |                |
|    approx_kl            | -1.6007107e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000118      |
|    explained_variance   | 0.145          |
|    learning_rate        | 0.0003         |
|    loss                 | 46.9           |
|    n_updates            | 1170           |
|    policy_gradient_loss | 5.38e-08       |
|    value_loss           | 73.8           |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -178     |
| time/              |          |
|    fps             | 475      |
|    iterations      | 118      |
|    time_elapsed    | 2034     |
|    total_timesteps | 966656   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3e+03         |
|    ep_rew_mean          | -170          |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 119           |
|    time_elapsed         | 2046          |
|    total_timesteps      | 974848        |
| train/                  |               |
|    approx_kl            | -6.184564e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.73e-05     |
|    explained_variance   | 0.0455        |
|    learning_rate        | 0.0003        |
|    loss                 | 43.5          |
|    n_updates            | 1180          |
|    policy_gradient_loss | -8.55e-08     |
|    value_loss           | 73.4          |
-------------------------------------------
Num timesteps: 980000
Best mean reward: -154.50 - Last mean reward per episode: -205.80
Eval num_timesteps=980000, episode_reward=-144.00 +/- 145.00
Episode length: 3000.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 3e+03     |
|    mean_reward          | -144      |
| time/                   |           |
|    total_timesteps      | 980000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.54e-05 |
|    explained_variance   | 0.0682    |
|    learning_rate        | 0.0003    |
|    loss                 | 31.4      |
|    n_updates            | 1190      |
|    policy_gradient_loss | 2.55e-07  |
|    value_loss           | 75.7      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -170     |
| time/              |          |
|    fps             | 474      |
|    iterations      | 120      |
|    time_elapsed    | 2070     |
|    total_timesteps | 983040   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -183         |
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 121          |
|    time_elapsed         | 2083         |
|    total_timesteps      | 991232       |
| train/                  |              |
|    approx_kl            | 9.458745e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.72e-05    |
|    explained_variance   | 0.151        |
|    learning_rate        | 0.0003       |
|    loss                 | 29.7         |
|    n_updates            | 1200         |
|    policy_gradient_loss | -2.53e-08    |
|    value_loss           | 68.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -176         |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 122          |
|    time_elapsed         | 2095         |
|    total_timesteps      | 999424       |
| train/                  |              |
|    approx_kl            | 8.731149e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000186    |
|    explained_variance   | 0.106        |
|    learning_rate        | 0.0003       |
|    loss                 | 47.7         |
|    n_updates            | 1210         |
|    policy_gradient_loss | -4.08e-07    |
|    value_loss           | 74.2         |
------------------------------------------
Num timesteps: 1000000
Best mean reward: -154.50 - Last mean reward per episode: -213.90
Eval num_timesteps=1000000, episode_reward=-216.00 +/- 141.22
Episode length: 3000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 3e+03         |
|    mean_reward          | -216          |
| time/                   |               |
|    total_timesteps      | 1000000       |
| train/                  |               |
|    approx_kl            | 6.5774657e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000258     |
|    explained_variance   | 0.0907        |
|    learning_rate        | 0.0003        |
|    loss                 | 39            |
|    n_updates            | 1220          |
|    policy_gradient_loss | 8.71e-07      |
|    value_loss           | 76.5          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -176     |
| time/              |          |
|    fps             | 475      |
|    iterations      | 123      |
|    time_elapsed    | 2119     |
|    total_timesteps | 1007616  |
---------------------------------
wandb: - 0.353 MB of 0.353 MB uploadedwandb: \ 0.353 MB of 0.358 MB uploadedwandb: | 0.487 MB of 0.487 MB uploadedwandb: 
wandb: Run history:
wandb: mean_reward ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:   timesteps ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: mean_reward -213.9
wandb:   timesteps 1000000
wandb: 
wandb: üöÄ View run warm-glitter-2 at: https://wandb.ai/rwth-ml/cnn_atari_freeway/runs/a0393trx
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/rwth-ml/cnn_atari_freeway
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240712_030934-a0393trx/logs
slurmstepd-cn-406: error: *** JOB 8025085 ON cn-406 CANCELLED AT 2024-07-13T03:09:36 DUE TO TIME LIMIT ***
--------------------- Slurm Task Epilog ------------------------
Job ID: 8025085
Time: Sat Jul 13 03:09:36 CEST 2024
Elapsed Time: 1-00:00:15
Billing per second for TRES: billing=704,cpu=32,gres/gpu=1,mem=32G,node=1
Show resource usage with e.g.:
sacct -j 8025085 -o Elapsed,TotalCPU,UserCPU,SystemCPU,MaxRSS,ReqTRES%60,MaxDiskRead,MaxDiskWrite
--------------------- Slurm Task Epilog ------------------------
