--------------------- Slurm Task Prolog ------------------------
Job ID: 8048762
Job name: pong_supervised_gnn
Host: cn-406
Date: Mon Jul 15 12:54:45 CEST 2024
User: rishabh.bhatia
Slurm account: rleap
Slurm partition: rleap_gpu_24gb
Work dir: 
------------------
Node usage:
JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
8044499 rleap_gpu exp7_log markus.f  R   22:50:09      1 cn-406
8044498 rleap_gpu exp6_mic markus.f  R   22:50:13      1 cn-406
8048762 rleap_gpu pong_sup rishabh.  R       0:01      1 cn-406
8045911 rleap_gpu   tunnel rishabh.  R    3:54:28      1 cn-406
8044553 rleap_gpu train.sl samuel.s  R   20:27:51      1 cn-406
8044552 rleap_gpu train.sl samuel.s  R   20:28:03      1 cn-406
------------------
Show launch script with:
sacct -B -j 
------------------
--------------------- Slurm Task Prolog ------------------------
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.[0m
  logger.warn(
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.[0m
  logger.warn(
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.[0m
  logger.warn(
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.[0m
  logger.warn(
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.[0m
  logger.warn(
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.[0m
  logger.warn(
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.[0m
  logger.warn(
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.[0m
  logger.warn(
wandb: Currently logged in as: rishabh-bhatia (rwth-ml). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.1
wandb: Run data is saved locally in /work/rleap1/rishabh.bhatia/Master-Thesis-GNN/wandb/run-20240715_125454-7cw9smqu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run woven-cosmos-25
wandb: ‚≠êÔ∏è View project at https://wandb.ai/rwth-ml/cnn_atari_pong
wandb: üöÄ View run at https://wandb.ai/rwth-ml/cnn_atari_pong/runs/7cw9smqu
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fd304908640> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fd2b218a8c0>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
Using cpu device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 751      |
|    ep_rew_mean     | -60.6    |
| time/              |          |
|    fps             | 872      |
|    iterations      | 1        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 815          |
|    ep_rew_mean          | -57.4        |
| time/                   |              |
|    fps                  | 359          |
|    iterations           | 2            |
|    time_elapsed         | 91           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0047505335 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 2.8          |
|    n_updates            | 10           |
|    policy_gradient_loss | -2.98e-05    |
|    value_loss           | 10.4         |
------------------------------------------
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:250: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  data_frame = pandas.concat(data_frames)
Num timesteps: 40000
Best mean reward: -inf - Last mean reward per episode: -60.02
Saving new best model at 35240 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=40000, episode_reward=-79.70 +/- 16.68
Episode length: 736.80 +/- 431.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 737          |
|    mean_reward          | -79.7        |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0052453615 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.555        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.08         |
|    n_updates            | 20           |
|    policy_gradient_loss | 6.39e-05     |
|    value_loss           | 8.18         |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 825      |
|    ep_rew_mean     | -59.2    |
| time/              |          |
|    fps             | 282      |
|    iterations      | 3        |
|    time_elapsed    | 173      |
|    total_timesteps | 49152    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 850         |
|    ep_rew_mean          | -58.4       |
| time/                   |             |
|    fps                  | 266         |
|    iterations           | 4           |
|    time_elapsed         | 245         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.005065716 |
|    clip_fraction        | 0.0534      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.659       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.74        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.000456   |
|    value_loss           | 8.32        |
-----------------------------------------
Num timesteps: 80000
Best mean reward: -60.02 - Last mean reward per episode: -60.59
Eval num_timesteps=80000, episode_reward=-30.00 +/- 50.82
Episode length: 1977.60 +/- 1256.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.98e+03    |
|    mean_reward          | -30         |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.007079605 |
|    clip_fraction        | 0.0702      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.21        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00182    |
|    value_loss           | 7.64        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 826      |
|    ep_rew_mean     | -59.7    |
| time/              |          |
|    fps             | 236      |
|    iterations      | 5        |
|    time_elapsed    | 345      |
|    total_timesteps | 81920    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 867          |
|    ep_rew_mean          | -57.6        |
| time/                   |              |
|    fps                  | 235          |
|    iterations           | 6            |
|    time_elapsed         | 418          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0071804244 |
|    clip_fraction        | 0.0661       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.71         |
|    learning_rate        | 0.0003       |
|    loss                 | 2.76         |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00301     |
|    value_loss           | 7.14         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 902          |
|    ep_rew_mean          | -55.9        |
| time/                   |              |
|    fps                  | 234          |
|    iterations           | 7            |
|    time_elapsed         | 488          |
|    total_timesteps      | 114688       |
| train/                  |              |
|    approx_kl            | 0.0069548916 |
|    clip_fraction        | 0.0724       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.722        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.79         |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00325     |
|    value_loss           | 6.55         |
------------------------------------------
Num timesteps: 120000
Best mean reward: -60.02 - Last mean reward per episode: -54.78
Saving new best model at 129327 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=120000, episode_reward=-6.30 +/- 57.81
Episode length: 2047.00 +/- 1167.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.05e+03     |
|    mean_reward          | -6.3         |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 0.0070074853 |
|    clip_fraction        | 0.077        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0.724        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.86         |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00418     |
|    value_loss           | 6.43         |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 932      |
|    ep_rew_mean     | -53.8    |
| time/              |          |
|    fps             | 222      |
|    iterations      | 8        |
|    time_elapsed    | 589      |
|    total_timesteps | 131072   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 977          |
|    ep_rew_mean          | -49.9        |
| time/                   |              |
|    fps                  | 222          |
|    iterations           | 9            |
|    time_elapsed         | 661          |
|    total_timesteps      | 147456       |
| train/                  |              |
|    approx_kl            | 0.0070497063 |
|    clip_fraction        | 0.0562       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0.709        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.66         |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00222     |
|    value_loss           | 6.97         |
------------------------------------------
Num timesteps: 160000
Best mean reward: -54.78 - Last mean reward per episode: -41.10
Saving new best model at 180454 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=160000, episode_reward=14.60 +/- 23.68
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 14.6        |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.007273858 |
|    clip_fraction        | 0.0637      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.735       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.53        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00213    |
|    value_loss           | 6.47        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.05e+03 |
|    ep_rew_mean     | -42.5    |
| time/              |          |
|    fps             | 211      |
|    iterations      | 10       |
|    time_elapsed    | 774      |
|    total_timesteps | 163840   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.1e+03      |
|    ep_rew_mean          | -38.5        |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 11           |
|    time_elapsed         | 844          |
|    total_timesteps      | 180224       |
| train/                  |              |
|    approx_kl            | 0.0077124345 |
|    clip_fraction        | 0.0651       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.02        |
|    explained_variance   | 0.764        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.34         |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00125     |
|    value_loss           | 6.27         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.17e+03     |
|    ep_rew_mean          | -32.6        |
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 12           |
|    time_elapsed         | 914          |
|    total_timesteps      | 196608       |
| train/                  |              |
|    approx_kl            | 0.0071584145 |
|    clip_fraction        | 0.0724       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.02        |
|    explained_variance   | 0.72         |
|    learning_rate        | 0.0003       |
|    loss                 | 6.42         |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00139     |
|    value_loss           | 7.27         |
------------------------------------------
Num timesteps: 200000
Best mean reward: -41.10 - Last mean reward per episode: -24.64
Saving new best model at 231626 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=200000, episode_reward=20.60 +/- 14.42
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 20.6        |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.008576458 |
|    clip_fraction        | 0.0883      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.689       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.94        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0044     |
|    value_loss           | 4.55        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.24e+03 |
|    ep_rew_mean     | -23.6    |
| time/              |          |
|    fps             | 207      |
|    iterations      | 13       |
|    time_elapsed    | 1028     |
|    total_timesteps | 212992   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.29e+03    |
|    ep_rew_mean          | -19.8       |
| time/                   |             |
|    fps                  | 208         |
|    iterations           | 14          |
|    time_elapsed         | 1099        |
|    total_timesteps      | 229376      |
| train/                  |             |
|    approx_kl            | 0.007038512 |
|    clip_fraction        | 0.0727      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.77        |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00306    |
|    value_loss           | 4.87        |
-----------------------------------------
Num timesteps: 240000
Best mean reward: -24.64 - Last mean reward per episode: -7.72
Saving new best model at 287721 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=240000, episode_reward=2.40 +/- 10.55
Episode length: 3000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 3e+03      |
|    mean_reward          | 2.4        |
| time/                   |            |
|    total_timesteps      | 240000     |
| train/                  |            |
|    approx_kl            | 0.00708089 |
|    clip_fraction        | 0.0718     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.72       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.46       |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.00349   |
|    value_loss           | 4.86       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.35e+03 |
|    ep_rew_mean     | -11.9    |
| time/              |          |
|    fps             | 202      |
|    iterations      | 15       |
|    time_elapsed    | 1213     |
|    total_timesteps | 245760   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.4e+03    |
|    ep_rew_mean          | -3.42      |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 16         |
|    time_elapsed         | 1283       |
|    total_timesteps      | 262144     |
| train/                  |            |
|    approx_kl            | 0.00797062 |
|    clip_fraction        | 0.0648     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.998     |
|    explained_variance   | 0.756      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.38       |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.00342   |
|    value_loss           | 4.61       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.48e+03    |
|    ep_rew_mean          | 4           |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 17          |
|    time_elapsed         | 1355        |
|    total_timesteps      | 278528      |
| train/                  |             |
|    approx_kl            | 0.007769825 |
|    clip_fraction        | 0.076       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.978      |
|    explained_variance   | 0.735       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.23        |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00296    |
|    value_loss           | 3.98        |
-----------------------------------------
Num timesteps: 280000
Best mean reward: -7.72 - Last mean reward per episode: 8.78
Saving new best model at 341590 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=280000, episode_reward=4.40 +/- 31.93
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 4.4         |
| time/                   |             |
|    total_timesteps      | 280000      |
| train/                  |             |
|    approx_kl            | 0.006860571 |
|    clip_fraction        | 0.0731      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.965      |
|    explained_variance   | 0.722       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.66        |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00327    |
|    value_loss           | 4.55        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.51e+03 |
|    ep_rew_mean     | 7.99     |
| time/              |          |
|    fps             | 200      |
|    iterations      | 18       |
|    time_elapsed    | 1469     |
|    total_timesteps | 294912   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.57e+03    |
|    ep_rew_mean          | 13          |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 19          |
|    time_elapsed         | 1541        |
|    total_timesteps      | 311296      |
| train/                  |             |
|    approx_kl            | 0.006422056 |
|    clip_fraction        | 0.0791      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.967      |
|    explained_variance   | 0.757       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.05        |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00346    |
|    value_loss           | 3.65        |
-----------------------------------------
Num timesteps: 320000
Best mean reward: 8.78 - Last mean reward per episode: 17.29
Saving new best model at 394637 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=320000, episode_reward=14.50 +/- 25.80
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 14.5        |
| time/                   |             |
|    total_timesteps      | 320000      |
| train/                  |             |
|    approx_kl            | 0.008064134 |
|    clip_fraction        | 0.0801      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.955      |
|    explained_variance   | 0.706       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.21        |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00391    |
|    value_loss           | 3.99        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 197      |
|    iterations      | 20       |
|    time_elapsed    | 1657     |
|    total_timesteps | 327680   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.65e+03    |
|    ep_rew_mean          | 20.8        |
| time/                   |             |
|    fps                  | 199         |
|    iterations           | 21          |
|    time_elapsed         | 1728        |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.007497074 |
|    clip_fraction        | 0.0749      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.943      |
|    explained_variance   | 0.729       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.23        |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00221    |
|    value_loss           | 3.68        |
-----------------------------------------
Num timesteps: 360000
Best mean reward: 17.29 - Last mean reward per episode: 28.23
Saving new best model at 451076 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=360000, episode_reward=-20.80 +/- 54.04
Episode length: 2051.40 +/- 1163.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.05e+03     |
|    mean_reward          | -20.8        |
| time/                   |              |
|    total_timesteps      | 360000       |
| train/                  |              |
|    approx_kl            | 0.0081659295 |
|    clip_fraction        | 0.0786       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.954       |
|    explained_variance   | 0.705        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.25         |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00473     |
|    value_loss           | 2.68         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.73e+03 |
|    ep_rew_mean     | 28.6     |
| time/              |          |
|    fps             | 197      |
|    iterations      | 22       |
|    time_elapsed    | 1828     |
|    total_timesteps | 360448   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.77e+03    |
|    ep_rew_mean          | 33.4        |
| time/                   |             |
|    fps                  | 198         |
|    iterations           | 23          |
|    time_elapsed         | 1900        |
|    total_timesteps      | 376832      |
| train/                  |             |
|    approx_kl            | 0.010018184 |
|    clip_fraction        | 0.0946      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.926      |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.821       |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00552    |
|    value_loss           | 2.94        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.78e+03    |
|    ep_rew_mean          | 39.5        |
| time/                   |             |
|    fps                  | 199         |
|    iterations           | 24          |
|    time_elapsed         | 1973        |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.008341128 |
|    clip_fraction        | 0.0912      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.894      |
|    explained_variance   | 0.761       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.13        |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00268    |
|    value_loss           | 2.7         |
-----------------------------------------
Num timesteps: 400000
Best mean reward: 28.23 - Last mean reward per episode: 42.25
Saving new best model at 499509 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=400000, episode_reward=22.90 +/- 5.66
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 22.9        |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.009296143 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.855      |
|    explained_variance   | 0.763       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.477       |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00626    |
|    value_loss           | 2.31        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.83e+03 |
|    ep_rew_mean     | 43.5     |
| time/              |          |
|    fps             | 196      |
|    iterations      | 25       |
|    time_elapsed    | 2088     |
|    total_timesteps | 409600   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.87e+03    |
|    ep_rew_mean          | 47.5        |
| time/                   |             |
|    fps                  | 197         |
|    iterations           | 26          |
|    time_elapsed         | 2161        |
|    total_timesteps      | 425984      |
| train/                  |             |
|    approx_kl            | 0.011213507 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.852      |
|    explained_variance   | 0.755       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.57        |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00686    |
|    value_loss           | 2.24        |
-----------------------------------------
Num timesteps: 440000
Best mean reward: 42.25 - Last mean reward per episode: 43.95
Saving new best model at 553622 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=440000, episode_reward=-20.20 +/- 33.01
Episode length: 2055.60 +/- 1158.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.06e+03    |
|    mean_reward          | -20.2       |
| time/                   |             |
|    total_timesteps      | 440000      |
| train/                  |             |
|    approx_kl            | 0.009378003 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.865      |
|    explained_variance   | 0.761       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.417       |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00756    |
|    value_loss           | 2.2         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | 49.9     |
| time/              |          |
|    fps             | 195      |
|    iterations      | 27       |
|    time_elapsed    | 2263     |
|    total_timesteps | 442368   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.99e+03    |
|    ep_rew_mean          | 52.8        |
| time/                   |             |
|    fps                  | 196         |
|    iterations           | 28          |
|    time_elapsed         | 2336        |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.010886408 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.845      |
|    explained_variance   | 0.759       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.872       |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00585    |
|    value_loss           | 2.66        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.01e+03    |
|    ep_rew_mean          | 56.9        |
| time/                   |             |
|    fps                  | 197         |
|    iterations           | 29          |
|    time_elapsed         | 2406        |
|    total_timesteps      | 475136      |
| train/                  |             |
|    approx_kl            | 0.009936383 |
|    clip_fraction        | 0.0969      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.826      |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.69        |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00625    |
|    value_loss           | 2.06        |
-----------------------------------------
Num timesteps: 480000
Best mean reward: 43.95 - Last mean reward per episode: 51.78
Saving new best model at 602294 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=480000, episode_reward=-0.50 +/- 38.41
Episode length: 2515.80 +/- 968.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.52e+03    |
|    mean_reward          | -0.5        |
| time/                   |             |
|    total_timesteps      | 480000      |
| train/                  |             |
|    approx_kl            | 0.011594364 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.828      |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.927       |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00638    |
|    value_loss           | 1.79        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.03e+03 |
|    ep_rew_mean     | 61.4     |
| time/              |          |
|    fps             | 195      |
|    iterations      | 30       |
|    time_elapsed    | 2513     |
|    total_timesteps | 491520   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.07e+03    |
|    ep_rew_mean          | 66.6        |
| time/                   |             |
|    fps                  | 196         |
|    iterations           | 31          |
|    time_elapsed         | 2585        |
|    total_timesteps      | 507904      |
| train/                  |             |
|    approx_kl            | 0.011950705 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.825      |
|    explained_variance   | 0.766       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.431       |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00661    |
|    value_loss           | 1.97        |
-----------------------------------------
Num timesteps: 520000
Best mean reward: 51.78 - Last mean reward per episode: 57.17
Saving new best model at 662798 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=520000, episode_reward=10.30 +/- 44.73
Episode length: 2525.80 +/- 948.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.53e+03    |
|    mean_reward          | 10.3        |
| time/                   |             |
|    total_timesteps      | 520000      |
| train/                  |             |
|    approx_kl            | 0.012783522 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.801      |
|    explained_variance   | 0.786       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.812       |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00674    |
|    value_loss           | 1.71        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.07e+03 |
|    ep_rew_mean     | 70       |
| time/              |          |
|    fps             | 194      |
|    iterations      | 32       |
|    time_elapsed    | 2691     |
|    total_timesteps | 524288   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.07e+03     |
|    ep_rew_mean          | 72.8         |
| time/                   |              |
|    fps                  | 195          |
|    iterations           | 33           |
|    time_elapsed         | 2763         |
|    total_timesteps      | 540672       |
| train/                  |              |
|    approx_kl            | 0.0119006075 |
|    clip_fraction        | 0.112        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.801       |
|    explained_variance   | 0.787        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.808        |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.0091      |
|    value_loss           | 1.74         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.04e+03    |
|    ep_rew_mean          | 74.2        |
| time/                   |             |
|    fps                  | 196         |
|    iterations           | 34          |
|    time_elapsed         | 2836        |
|    total_timesteps      | 557056      |
| train/                  |             |
|    approx_kl            | 0.011908046 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.805      |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.652       |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00744    |
|    value_loss           | 1.83        |
-----------------------------------------
