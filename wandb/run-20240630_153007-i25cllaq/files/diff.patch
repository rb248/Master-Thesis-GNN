diff --git a/games/__pycache__/__init__.cpython-310.pyc b/games/__pycache__/__init__.cpython-310.pyc
index 347bbb7..1b00a1b 100644
Binary files a/games/__pycache__/__init__.cpython-310.pyc and b/games/__pycache__/__init__.cpython-310.pyc differ
diff --git a/games/encoder/__pycache__/GraphEncoder.cpython-310.pyc b/games/encoder/__pycache__/GraphEncoder.cpython-310.pyc
index 3dc7f23..e12bbb9 100644
Binary files a/games/encoder/__pycache__/GraphEncoder.cpython-310.pyc and b/games/encoder/__pycache__/GraphEncoder.cpython-310.pyc differ
diff --git a/games/encoder/__pycache__/__init__.cpython-310.pyc b/games/encoder/__pycache__/__init__.cpython-310.pyc
index 7335618..e4059e9 100644
Binary files a/games/encoder/__pycache__/__init__.cpython-310.pyc and b/games/encoder/__pycache__/__init__.cpython-310.pyc differ
diff --git a/games/freeway/__pycache__/__init__.cpython-310.pyc b/games/freeway/__pycache__/__init__.cpython-310.pyc
index ee44eda..8f6a241 100644
Binary files a/games/freeway/__pycache__/__init__.cpython-310.pyc and b/games/freeway/__pycache__/__init__.cpython-310.pyc differ
diff --git a/games/freeway/__pycache__/run_supervised_cnn.cpython-310.pyc b/games/freeway/__pycache__/run_supervised_cnn.cpython-310.pyc
index f8965e6..971bba3 100644
Binary files a/games/freeway/__pycache__/run_supervised_cnn.cpython-310.pyc and b/games/freeway/__pycache__/run_supervised_cnn.cpython-310.pyc differ
diff --git a/games/freeway/__pycache__/run_supervised_gnn.cpython-310.pyc b/games/freeway/__pycache__/run_supervised_gnn.cpython-310.pyc
index 01b141a..2dba0c8 100644
Binary files a/games/freeway/__pycache__/run_supervised_gnn.cpython-310.pyc and b/games/freeway/__pycache__/run_supervised_gnn.cpython-310.pyc differ
diff --git a/games/freeway/freeway_envs/__pycache__/freeway_env.cpython-310.pyc b/games/freeway/freeway_envs/__pycache__/freeway_env.cpython-310.pyc
index cf3f63c..47e6b2c 100644
Binary files a/games/freeway/freeway_envs/__pycache__/freeway_env.cpython-310.pyc and b/games/freeway/freeway_envs/__pycache__/freeway_env.cpython-310.pyc differ
diff --git a/games/freeway/freeway_envs/freeway_env.py b/games/freeway/freeway_envs/freeway_env.py
index c17efff..ea187d7 100644
--- a/games/freeway/freeway_envs/freeway_env.py
+++ b/games/freeway/freeway_envs/freeway_env.py
@@ -4,12 +4,7 @@ import numpy as np
 import gymnasium as gym
 from gymnasium import spaces
 import torch
-import networkx as nx
-from torch_geometric.data import HeteroData, Batch
-from collections import defaultdict
-from itertools import combinations
 from stable_baselines3 import PPO
-from stable_baselines3.common.evaluation import evaluate_policy
 
 class FreewayEnv(gym.Env):
     metadata = {'render_modes': ['human', 'rgb_array']}
@@ -20,27 +15,22 @@ class FreewayEnv(gym.Env):
         self.last_time = pygame.time.get_ticks()
         self.render_mode = render_mode
         self.observation_type = observation_type
-        #self.window_width = 800
         self.window_width = 210
-        #self.window_height = 600
         self.window_height = 160
         self.player_width = 5
         self.player_height = 5
         self.car_width = 20
         self.car_height = 20
         self.frame_stack = frame_stack
-
-        #self.lanes = [100, 200, 300, 400, 500, 600, 700]
         self.lanes = [50,100,150]
         self.max_cars = 10
-        # Define action and observation space
-        # Actions: 0 - Stay, 1 - Move Up, 2 - Move Down
+
         self.action_space = spaces.Discrete(3)
 
         if observation_type == "pixel":
             self.observation_space = spaces.Box(low=0, high=255, shape=(self.frame_stack, 84, 84), dtype=np.uint8)
         else:
-            self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.max_cars+ len(self.lanes)+1, 7), dtype=np.float32)
+            self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.max_cars + len(self.lanes) + 1, 7), dtype=np.float32)
 
         self.window = pygame.display.set_mode((self.window_width, self.window_height))
         self.background_image = pygame.image.load("games/images/Atari - background.png")
@@ -50,9 +40,10 @@ class FreewayEnv(gym.Env):
         self.car_image = pygame.image.load("games/images/car2.png").convert_alpha()
         self.car_image = pygame.transform.scale(self.car_image, (self.car_width, self.car_height))
         self.frame_buffer = np.zeros((self.frame_stack, 84, 84), dtype=np.uint8)
-
+        self.episode_rewards = []
         self.clock = pygame.time.Clock()
         self.reset()
+
     def seed(self, seed=None):
         self.np_random, seed = gym.utils.seeding.np_random(seed)
         random.seed(seed)
@@ -81,36 +72,33 @@ class FreewayEnv(gym.Env):
             return self.get_object_data(), {}
 
     def step(self, action):
-        reward = 0
         reward = -0.5
         current_time = pygame.time.get_ticks()
-        if action == 1:  # Up
+        if action == 1:
             self.player_rect.y = max(0, self.player_rect.y - 5)
-        elif action == 2:  # Down
+        elif action == 2:
             self.player_rect.y = min(self.window_height - self.player_height, self.player_rect.y + 5)
 
         for car in self.cars:
             car['x'] += car['speed']
             if car['x'] > self.window_width:
                 car['x'] = 0
-                car['speed'] = random.randint(1,2)
+                car['speed'] = random.randint(1, 2)
 
-        # Collision detection
         hit = any(self.player_rect.colliderect(pygame.Rect(car['x'], car['lane'], self.car_width, self.car_height)) for car in self.cars)
         if hit:
-            #self.score = -1
             self.player_rect.y = self.window_height - self.player_height - 10
-        
             self.last_time = current_time
-        if current_time - self.episode_start_time >= 60000:  # 60000 milliseconds = 1 minute
+        if current_time - self.episode_start_time >= 60000:
             self.done = True
             
-        if self.player_rect.y <= 0:  # Reached top
-            self.score +=1
-            reward += 10*(len(self.lanes))
-
+        if self.player_rect.y <= 0:
+            self.score += 1
+            reward += 10 * len(self.lanes)
             self.player_rect.y = self.window_height - self.player_height - 10
 
+        self.episode_rewards.append(reward)
+
         if self.observation_type == "pixel":
             self.update_frame_buffer()
             observation = self.get_observation()
@@ -119,9 +107,12 @@ class FreewayEnv(gym.Env):
 
         return observation, reward, self.done, False, {}
 
+    def get_rewards(self):
+        return self.episode_rewards
+
     def update_frame_buffer(self):
         frame = self.render_to_array()
-        grayscale = np.dot(frame[..., :3], [0.2989, 0.5870, 0.1140]).astype(np.uint8)  # Convert to grayscale
+        grayscale = np.dot(frame[..., :3], [0.2989, 0.5870, 0.1140]).astype(np.uint8)
         resized_frame = pygame.transform.scale(pygame.surfarray.make_surface(grayscale), (84, 84))
         frame_array = pygame.surfarray.array3d(resized_frame).transpose(1, 0, 2)[:, :, 0]
 
@@ -139,20 +130,11 @@ class FreewayEnv(gym.Env):
         return self.frame_buffer
 
     def get_object_data(self):
-        objects = [
-            [self.player_rect.x, self.player_rect.y, 0, 0, 1, 0, 0],  # Player
-            
-        ] 
-        # add lanes
+        objects = [[self.player_rect.x, self.player_rect.y, 0, 0, 1, 0, 0]]
         for lane in self.lanes:
-            objects.append([self.window_width//2, lane, 0, 0, 0, 1, 0])
-
-        for i, car in enumerate(self.cars):
+            objects.append([self.window_width // 2, lane, 0, 0, 0, 1, 0])
+        for car in self.cars:
             objects.append([car['x'], car['lane'], car['speed'], 0, 0, 0, 1])
-
-        # while len(objects) < self.max_cars + 10:  # Ensure the list has a constant length
-        #     objects.append([0, 0, 0, 0, 0, 0, 0])
-
         return torch.tensor(objects, dtype=torch.float32)
 
     def render(self, mode='human'):
@@ -165,24 +147,16 @@ class FreewayEnv(gym.Env):
     def close(self):
         pygame.quit()
 
-
 if __name__=="__main__":
     env = FreewayEnv(render_mode='human', observation_type='graph')
+    model = PPO.load("path_to_your_model")
 
-    #model = PPO.load("ppo_freeway_pixel")
-    model = PPO.load("ppo_custom_heterognn")
-
-    # # Evaluate the agent
-    # mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=1, render=True)
-    # print(f"Mean reward: {mean_reward} Â± {std_reward}")
-
-    obs,_ = env.reset()
+    obs, _ = env.reset()
     done = False
     total_reward = 0
     while not done:
         action, _ = model.predict(obs)
-        #action = env.action_space.sample()
-        obs, reward, done, _,_ = env.step(action)
+        obs, reward, done, _, _ = env.step(action)
         total_reward += reward
         pygame.time.delay(50)
         env.render()
diff --git a/games/freeway/run_supervised_cnn.py b/games/freeway/run_supervised_cnn.py
index 8d1ce97..d0b898d 100644
--- a/games/freeway/run_supervised_cnn.py
+++ b/games/freeway/run_supervised_cnn.py
@@ -11,10 +11,7 @@ import pygame
 from stable_baselines3.common.vec_env import VecFrameStack
 from stable_baselines3.common.vec_env import DummyVecEnv
 from stable_baselines3.common.evaluation import evaluate_policy
-
-
-
-
+import wandb
 import torch as th
 import torch.nn as nn
 from gymnasium import spaces
@@ -78,12 +75,19 @@ if __name__ == "__main__":
             return env
         return _init
 
-    env = DummyVecEnv([make_env(i) for i in range(num_envs)])
+    #env = DummyVecEnv([make_env(i) for i in range(num_envs)])
+    #model = PPO.load("ppo_freeway_pixel")
     env = FreewayEnv(render_mode='human', observation_type='pixel')
     # env = DummyVecEnv([lambda: env])    
     # env = VecFrameStack(env, n_stack=4)
+    wandb.init(
+        project="cnn_atari_freeway",  # Replace with your project name
+        sync_tensorboard=True,        # Automatically sync SB3 logs with wandb
+        monitor_gym=True,             # Automatically log gym environments
+        save_code=True                # Save the code used for this run
+    )
     
     device = "cuda" if th.cuda.is_available() else "cpu"
-    model = PPO("CnnPolicy", env, verbose=2, device=device)
-    model.learn(total_timesteps=100000)
+    model = PPO("CnnPolicy", env, verbose=2, device=device, )
+    model.learn(total_timesteps=1000000)
     model.save("ppo_freeway_pixel")   
\ No newline at end of file
diff --git a/games/freeway/run_supervised_gnn.py b/games/freeway/run_supervised_gnn.py
index 317183e..b1755a4 100644
--- a/games/freeway/run_supervised_gnn.py
+++ b/games/freeway/run_supervised_gnn.py
@@ -2,29 +2,10 @@ import wandb
 from stable_baselines3 import PPO
 from stable_baselines3.common.env_util import make_vec_env
 from wandb.integration.sb3 import WandbCallback
-#from games.model.policy import CustomActorCriticPolicy
 from games.freeway.freeway_envs.freeway_env import FreewayEnv
-from games.model.policy import CustomCNN, CustomHeteroGNN
-import numpy as np
-import pygame
-# #Initialize wandb
-wandb.init(
-    project="gnn_atari_freeway",  # Replace with your project name
-    sync_tensorboard=True,        # Automatically sync SB3 logs with wandb
-    monitor_gym=True,             # Automatically log gym environments
-    save_code=True                # Save the code used for this run
-)
-
-# wandb.init(
-#     project="cnn_g",  # Replace with your project name
-#     sync_tensorboard=True,        # Automatically sync SB3 logs with wandb
-#     monitor_gym=True,             # Automatically log gym environments
-#     save_code=True                # Save the code used for this run
-# )
-
-# Wrap the environment 
-
+from games.model.policy import CustomHeteroGNN
 import os
+import numpy as np
 from stable_baselines3.common.callbacks import BaseCallback
 
 class SaveOnBestTrainingRewardCallback(BaseCallback):
@@ -41,8 +22,11 @@ class SaveOnBestTrainingRewardCallback(BaseCallback):
 
     def _on_step(self) -> bool:
         if self.n_calls % self.check_freq == 0:
-            x, y = self.training_env.get_attr('reward')
-            mean_reward = np.mean(y)
+            rewards = []
+            for idx in range(self.training_env.num_envs):
+                env_rewards = self.training_env.get_attr('get_rewards', indices=idx)
+                rewards.extend(env_rewards[0] if isinstance(env_rewards, list) else env_rewards)
+            mean_reward = np.mean(rewards)
             if self.verbose > 0:
                 print(f"Num timesteps: {self.num_timesteps}")
                 print(f"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward: {mean_reward:.2f}")
@@ -54,15 +38,6 @@ class SaveOnBestTrainingRewardCallback(BaseCallback):
                 self.model.save(self.save_path)
 
         return True
-import wandb
-from stable_baselines3 import PPO
-from stable_baselines3.common.env_util import make_vec_env
-from wandb.integration.sb3 import WandbCallback
-#from games.model.policy import CustomActorCriticPolicy
-from games.freeway.freeway_envs.freeway_env import FreewayEnv
-from games.model.policy import CustomCNN, CustomHeteroGNN
-import pygame
-import os
 
 # Initialize wandb
 wandb.init(
@@ -73,7 +48,7 @@ wandb.init(
 )
 
 # Wrap the environment
-env = FreewayEnv(render_mode='human', observation_type='graph')
+env = make_vec_env(lambda: FreewayEnv(render_mode='human', observation_type='graph'), n_envs=4)
 
 policy_kwargs = dict(
     features_extractor_class=CustomHeteroGNN,
@@ -99,3 +74,4 @@ save_best_callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=l
 
 # Train the model with WandbCallback and the custom callback
 model.learn(total_timesteps=1000000, callback=[WandbCallback(), save_best_callback])
+
diff --git a/games/model/__pycache__/cnn_model.cpython-310.pyc b/games/model/__pycache__/cnn_model.cpython-310.pyc
index 9e82db6..aaea007 100644
Binary files a/games/model/__pycache__/cnn_model.cpython-310.pyc and b/games/model/__pycache__/cnn_model.cpython-310.pyc differ
diff --git a/games/model/__pycache__/hetero_gnn.cpython-310.pyc b/games/model/__pycache__/hetero_gnn.cpython-310.pyc
index 7853617..5154077 100644
Binary files a/games/model/__pycache__/hetero_gnn.cpython-310.pyc and b/games/model/__pycache__/hetero_gnn.cpython-310.pyc differ
diff --git a/games/model/__pycache__/hetero_message_passing.cpython-310.pyc b/games/model/__pycache__/hetero_message_passing.cpython-310.pyc
index 243e3b4..2d8f804 100644
Binary files a/games/model/__pycache__/hetero_message_passing.cpython-310.pyc and b/games/model/__pycache__/hetero_message_passing.cpython-310.pyc differ
diff --git a/games/model/__pycache__/policy.cpython-310.pyc b/games/model/__pycache__/policy.cpython-310.pyc
index 18a2c62..a8bcf5c 100644
Binary files a/games/model/__pycache__/policy.cpython-310.pyc and b/games/model/__pycache__/policy.cpython-310.pyc differ
Submodule games/pacman contains modified content
diff --git a/games/pacman/pacman_env.py b/games/pacman/pacman_env.py
index e185f02..2556ba7 100644
--- a/games/pacman/pacman_env.py
+++ b/games/pacman/pacman_env.py
@@ -14,8 +14,7 @@ from games.pacman.level import level
 from games.pacman.game import game
 from games.pacman.fruit import fruit
 from games.pacman.config import *
-import networkx as nx
-from games.encoder.GraphEncoder import GraphConverter
+import torch
 
 class PacmanEnv(gym.Env):
     metadata = {'render.modes': ['human', 'rgb_array']}
@@ -211,117 +210,36 @@ class PacmanEnv(gym.Env):
         return self.frame_buffer
 
     
-    def get_graph_data(self):
-        # Initialize a NetworkX graph
-        graph = nx.Graph()
+    def get_object_data(self):
+        max_ghosts = 6  # Maximum number of ghosts
+        max_pellets = 100  # Maximum number of pellets
+        max_power_pellets = 4  # Maximum number of power pellets
 
-        # Define object features and add nodes
+        # Initialize fixed-size arrays for objects
         pacman_features = [self.player.x, self.player.y, self.player.velX, self.player.velY, 1, 0, 0, 0]
-        graph.add_node("pacman", type="object", features=pacman_features)
-
-        ghost_features = [[ghost.x, ghost.y, ghost.velX, ghost.velY,0,1,0,0] for ghost in self.ghosts.values()]
-        for i, features in enumerate(ghost_features):
-            graph.add_node(f"ghost_{i}", type="object", features=features)
-
-        pellet_features = [[pellet[0], pellet[1],0,0,0,0,1,0] for pellet in self.level.GetPelletLocations()]
-        for i, features in enumerate(pellet_features):
-            graph.add_node(f"pellet_{i}", type="object", features=features)
-
-        power_pellet_features = [[pellet[0], pellet[1],0,0,0,0,0,1] for pellet in self.level.GetPowerPelletLocations()]
-        for i, features in enumerate(power_pellet_features):
-            graph.add_node(f"power_pellet_{i}", type="object", features=features)
-
-        # Combine object positions
-        object_positions = {
-            "pacman": pacman_features[:2],
-        }
+        
+        ghost_features = [[0, 0, 0, 0, 0, 0, 0, 0] for _ in range(max_ghosts)]
         for i, ghost in enumerate(self.ghosts.values()):
-            object_positions[f"ghost_{i}"] = ghost_features[i][:2]
-        for i, pellet in enumerate(self.level.GetPelletLocations()):
-            object_positions[f"pellet_{i}"] = pellet_features[i][:2]
-        for i, power_pellet in enumerate(self.level.GetPowerPelletLocations()):
-            object_positions[f"power_pellet_{i}"] = power_pellet_features[i][:2]
-
-        # Proximity threshold for creating atoms
-        #proximity_threshold = self.proximity_threshold
+            if i < max_ghosts:
+                ghost_features[i] = [ghost.x, ghost.y, ghost.velX, ghost.velY, 0, 1, 0, 0]
 
-        # Create atom nodes and edges based on proximity
-        atom_index = len(object_positions)  # Start indexing atoms after all objects
+        pellet_features = [[0, 0, 0, 0, 0, 0, 1, 0] for _ in range(max_pellets)]
+        current_pellets = self.level.GetPelletLocations()
+        for i, pellet in enumerate(current_pellets):
+            if i < max_pellets:
+                pellet_features[i] = [pellet[0], pellet[1], 0, 0, 0, 0, 1, 0]
 
-        # Determine wall proximity around Pac-Man
-        walls = {
-            'up': self.level.CheckIfHitWall(self.player.x, self.player.y - self.player.velY, self.player.y - 1, self.player.x),
-            'down': self.level.CheckIfHitWall(self.player.x, self.player.y + self.player.velY, self.player.y + 1, self.player.x),
-            'left': self.level.CheckIfHitWall(self.player.x - self.player.velX, self.player.y, self.player.y, self.player.x - 1),
-            'right': self.level.CheckIfHitWall(self.player.x + self.player.velX, self.player.y, self.player.y, self.player.x + 1)
-        }
-
-        standard_feature_vector_size = len(pacman_features)
-        empty_feature_vector = [0] *(2* standard_feature_vector_size)
-
-        # Add wall direction atoms and edges for Pac-Man
-        for direction, hit in walls.items():
-            if hit:
-                atom_node = f"Wall{direction.capitalize()}_{atom_index}"
-                graph.add_node(atom_node, type="atom", features=empty_feature_vector, predicate=f"Wall{direction.capitalize()}")
-                graph.add_edge("pacman", atom_node, position=0)
-                atom_index += 1
-
-        # Add distance and direction atoms and edges for ghosts
-        for i, ghost_pos in enumerate(ghost_features):
-            distance = np.linalg.norm(np.array(pacman_features[:2]) - np.array(ghost_pos[:2]))
-            direction = np.array(pacman_features[:2]) - np.array(ghost_pos[:2])
-            
-            atom_node_distance = f"Distance_Pacman_Ghost_{i}_{atom_index}"
-            graph.add_node(atom_node_distance,features=empty_feature_vector, type="atom", predicate="Distance")
-            graph.add_edge("pacman", atom_node_distance, position=0)
-            graph.add_edge(f"ghost_{i}", atom_node_distance, position=1)
-            
-            atom_index += 1
+        power_pellet_features = [[0, 0, 0, 0, 0, 0, 0, 1] for _ in range(max_power_pellets)]
+        current_power_pellets = self.level.GetPowerPelletLocations()
+        for i, power_pellet in enumerate(current_power_pellets):
+            if i < max_power_pellets:
+                power_pellet_features[i] = [power_pellet[0], power_pellet[1], 0, 0, 0, 0, 0, 1]
 
-            if pacman_features[0] == ghost_pos[0]:
-                atom_node_same_row = f"SameRow_Pacman_Ghost_{i}_{atom_index}"
-                graph.add_node(atom_node_same_row, features=empty_feature_vector,type="atom", predicate="SameRow")
-                graph.add_edge("pacman", atom_node_same_row, position=0)
-                graph.add_edge(f"ghost_{i}", atom_node_same_row, position=1)
-                atom_index += 1
+        # Combine all features into a single tensor
+        features = [pacman_features] + ghost_features + pellet_features + power_pellet_features
 
-            if pacman_features[1] == ghost_pos[1]:
-                atom_node_same_column = f"SameColumn_Pacman_Ghost_{i}_{atom_index}"
-                graph.add_node(atom_node_same_column, features=empty_feature_vector, type="atom", predicate="SameColumn")
-                graph.add_edge("pacman", atom_node_same_column, position=0)
-                graph.add_edge(f"ghost_{i}", atom_node_same_column, position=1)
-                atom_index += 1
-
-        # Add distance and direction atoms and edges for pellets
-        for i, pellet_pos in enumerate(pellet_features):
-            distance = np.linalg.norm(np.array(pacman_features[:2]) - np.array(pellet_pos[:2]))
-            
-            atom_node_distance = f"Distance_Pacman_Pellet_{i}_{atom_index}"
-            graph.add_node(atom_node_distance, features=empty_feature_vector,type="atom", predicate="Distance")
-            graph.add_edge("pacman", atom_node_distance, position=0)
-            graph.add_edge(f"pellet_{i}", atom_node_distance, position=1)
-            
-            atom_index += 1
-
-        # Add distance and direction atoms and edges for power pellets
-        for i, power_pellet_pos in enumerate(power_pellet_features):
-            distance = np.linalg.norm(np.array(pacman_features[:2]) - np.array(power_pellet_pos[:2]))
-            
-            atom_node_distance = f"Distance_Pacman_PowerPellet_{i}_{atom_index}"
-            graph.add_node(atom_node_distance,features=empty_feature_vector, type="atom", predicate="Distance")
-            graph.add_edge("pacman", atom_node_distance, position=0)
-            graph.add_edge(f"power_pellet_{i}", atom_node_distance, position=1)
-            
-            atom_index += 1
-
-        # Create a GraphConverter object
-        converter = GraphConverter()
-
-        # Convert the NetworkX graph to a PyG Data object
-        data = converter.to_pyg_data(graph)
-        return data
-    
+        return torch.tensor(features, dtype=torch.float32)
+        
 
     
     def render(self, mode='human'):
diff --git a/ppo_freeway_pixel.zip b/ppo_freeway_pixel.zip
index 72cd310..c2cab28 100644
Binary files a/ppo_freeway_pixel.zip and b/ppo_freeway_pixel.zip differ
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index a7e2754..a2c420c 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20240627_175509-kfssyjeq/logs/debug-internal.log
\ No newline at end of file
+run-20240630_153007-i25cllaq/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index 78950f7..840a34e 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20240627_175509-kfssyjeq/logs/debug.log
\ No newline at end of file
+run-20240630_153007-i25cllaq/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 847fe7f..1e9b53f 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20240627_175509-kfssyjeq
\ No newline at end of file
+run-20240630_153007-i25cllaq
\ No newline at end of file
