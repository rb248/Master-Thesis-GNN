
Using cpu device
Num timesteps: 4000
Best mean reward: -inf - Last mean reward: -0.50
Saving new best model to ./logs/best_model
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.get_rewards to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_rewards` for environment variables or `env.get_wrapper_attr('get_rewards')` that will search the reminding wrappers.
  logger.warn(
Num timesteps: 8000
Best mean reward: -0.50 - Last mean reward: -0.50
-----------------------------
| time/              |      |
|    fps             | 220  |
|    iterations      | 1    |
|    time_elapsed    | 37   |
|    total_timesteps | 8192 |
-----------------------------
Num timesteps: 12000
Best mean reward: -0.50 - Last mean reward: -0.48
Saving new best model to ./logs/best_model
Num timesteps: 16000
Best mean reward: -0.48 - Last mean reward: -0.48
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.05e+03   |
|    ep_rew_mean          | -1.02e+03  |
| time/                   |            |
|    fps                  | 50         |
|    iterations           | 2          |
|    time_elapsed         | 325        |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.00770852 |
|    clip_fraction        | 0.0296     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.09      |
|    explained_variance   | -0.00355   |
|    learning_rate        | 0.0003     |
|    loss                 | 0.38       |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.000366  |
|    value_loss           | 1.47       |
----------------------------------------
Num timesteps: 20000
Best mean reward: -0.48 - Last mean reward: -0.45
Saving new best model to ./logs/best_model
Num timesteps: 24000
Best mean reward: -0.45 - Last mean reward: -0.45
Saving new best model to ./logs/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -1.01e+03   |
| time/                   |             |
|    fps                  | 40          |
|    iterations           | 3           |
|    time_elapsed         | 613         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.010347146 |
|    clip_fraction        | 0.0821      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 2           |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00349    |
|    value_loss           | 7.49        |
-----------------------------------------
Num timesteps: 28000
Best mean reward: -0.45 - Last mean reward: -0.40
Saving new best model to ./logs/best_model
Num timesteps: 32000
Best mean reward: -0.40 - Last mean reward: -0.42
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -979        |
| time/                   |             |
|    fps                  | 36          |
|    iterations           | 4           |
|    time_elapsed         | 891         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.013356933 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 11.9        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00457    |
|    value_loss           | 13.7        |
-----------------------------------------
Num timesteps: 36000
Best mean reward: -0.40 - Last mean reward: -0.33
Saving new best model to ./logs/best_model
Num timesteps: 40000
Best mean reward: -0.33 - Last mean reward: -0.31
Saving new best model to ./logs/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -947        |
| time/                   |             |
|    fps                  | 34          |
|    iterations           | 5           |
|    time_elapsed         | 1178        |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.017418426 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.981      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 6.14        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00624    |
|    value_loss           | 20.8        |
-----------------------------------------
Num timesteps: 44000
Best mean reward: -0.31 - Last mean reward: -0.15
Saving new best model to ./logs/best_model
Num timesteps: 48000
Best mean reward: -0.15 - Last mean reward: -0.20
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -882        |
| time/                   |             |
|    fps                  | 33          |
|    iterations           | 6           |
|    time_elapsed         | 1463        |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.014351524 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.873      |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 22          |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00775    |
|    value_loss           | 41          |
-----------------------------------------
Num timesteps: 52000
Best mean reward: -0.15 - Last mean reward: -0.19
Num timesteps: 56000
Best mean reward: -0.15 - Last mean reward: -0.19
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -804         |
| time/                   |              |
|    fps                  | 32           |
|    iterations           | 7            |
|    time_elapsed         | 1743         |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0087851845 |
|    clip_fraction        | 0.0836       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.774       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 31.8         |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00579     |
|    value_loss           | 55.2         |
------------------------------------------
Num timesteps: 60000
Best mean reward: -0.15 - Last mean reward: -0.14
Saving new best model to ./logs/best_model
Num timesteps: 64000
Best mean reward: -0.14 - Last mean reward: -0.13
Saving new best model to ./logs/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -748         |
| time/                   |              |
|    fps                  | 32           |
|    iterations           | 8            |
|    time_elapsed         | 2023         |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0077772043 |
|    clip_fraction        | 0.0685       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.702       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 28.4         |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00455     |
|    value_loss           | 56           |
------------------------------------------
Num timesteps: 68000
Best mean reward: -0.13 - Last mean reward: -0.04
Saving new best model to ./logs/best_model
Num timesteps: 72000
Best mean reward: -0.04 - Last mean reward: -0.08
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -682         |
| time/                   |              |
|    fps                  | 31           |
|    iterations           | 9            |
|    time_elapsed         | 2306         |
|    total_timesteps      | 73728        |
| train/                  |              |
|    approx_kl            | 0.0063307104 |
|    clip_fraction        | 0.0794       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.619       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 38.5         |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00517     |
|    value_loss           | 65.8         |
------------------------------------------
Num timesteps: 76000
Best mean reward: -0.04 - Last mean reward: -0.08
Num timesteps: 80000
Best mean reward: -0.04 - Last mean reward: -0.03
Saving new best model to ./logs/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -624         |
| time/                   |              |
|    fps                  | 31           |
|    iterations           | 10           |
|    time_elapsed         | 2593         |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.0047633895 |
|    clip_fraction        | 0.0486       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.536       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 36.6         |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00318     |
|    value_loss           | 69.3         |
------------------------------------------
Num timesteps: 84000
Best mean reward: -0.03 - Last mean reward: -0.02
Saving new best model to ./logs/best_model
Num timesteps: 88000
Best mean reward: -0.02 - Last mean reward: -0.05
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -566         |
| time/                   |              |
|    fps                  | 31           |
|    iterations           | 11           |
|    time_elapsed         | 2874         |
|    total_timesteps      | 90112        |
| train/                  |              |
|    approx_kl            | 0.0033358391 |
|    clip_fraction        | 0.0421       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.469       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 34.6         |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00306     |
|    value_loss           | 74.2         |
------------------------------------------
Num timesteps: 92000
Best mean reward: -0.02 - Last mean reward: -0.01
Saving new best model to ./logs/best_model
Num timesteps: 96000
Best mean reward: -0.01 - Last mean reward: -0.02
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -521         |
| time/                   |              |
|    fps                  | 31           |
|    iterations           | 12           |
|    time_elapsed         | 3157         |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0007854465 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.437       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 40.7         |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.000311    |
|    value_loss           | 74.7         |
------------------------------------------
Num timesteps: 100000
Best mean reward: -0.01 - Last mean reward: -0.06
Num timesteps: 104000
Best mean reward: -0.01 - Last mean reward: -0.02
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -478         |
| time/                   |              |
|    fps                  | 30           |
|    iterations           | 13           |
|    time_elapsed         | 3437         |
|    total_timesteps      | 106496       |
| train/                  |              |
|    approx_kl            | 0.0010829852 |
|    clip_fraction        | 0.0276       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.38        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 30.6         |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.000325    |
|    value_loss           | 76.5         |
------------------------------------------
Num timesteps: 108000
Best mean reward: -0.01 - Last mean reward: 0.08
Saving new best model to ./logs/best_model
Num timesteps: 112000
Best mean reward: 0.08 - Last mean reward: -0.04
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -444         |
| time/                   |              |
|    fps                  | 30           |
|    iterations           | 14           |
|    time_elapsed         | 3721         |
|    total_timesteps      | 114688       |
| train/                  |              |
|    approx_kl            | 0.0011553447 |
|    clip_fraction        | 0.0265       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.334       |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 56.6         |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00135     |
|    value_loss           | 78           |
------------------------------------------
Num timesteps: 116000
Best mean reward: 0.08 - Last mean reward: -0.04
Num timesteps: 120000
Best mean reward: 0.08 - Last mean reward: 0.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -416          |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 15            |
|    time_elapsed         | 4004          |
|    total_timesteps      | 122880        |
| train/                  |               |
|    approx_kl            | 0.00094857346 |
|    clip_fraction        | 0.0185        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.296        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 44.2          |
|    n_updates            | 140           |
|    policy_gradient_loss | -0.000575     |
|    value_loss           | 75.5          |
-------------------------------------------
Num timesteps: 124000
Best mean reward: 0.08 - Last mean reward: 0.15
Saving new best model to ./logs/best_model
Num timesteps: 128000
Best mean reward: 0.15 - Last mean reward: 0.09
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -392         |
| time/                   |              |
|    fps                  | 30           |
|    iterations           | 16           |
|    time_elapsed         | 4290         |
|    total_timesteps      | 131072       |
| train/                  |              |
|    approx_kl            | 0.0013286402 |
|    clip_fraction        | 0.0131       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.297       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 30.4         |
|    n_updates            | 150          |
|    policy_gradient_loss | 1.39e-05     |
|    value_loss           | 75.4         |
------------------------------------------
Num timesteps: 132000
Best mean reward: 0.15 - Last mean reward: -0.08
Num timesteps: 136000
Best mean reward: 0.15 - Last mean reward: 0.02
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -360        |
| time/                   |             |
|    fps                  | 30          |
|    iterations           | 17          |
|    time_elapsed         | 4579        |
|    total_timesteps      | 139264      |
| train/                  |             |
|    approx_kl            | 0.001005599 |
|    clip_fraction        | 0.0151      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.25       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 34.4        |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.000726   |
|    value_loss           | 81.2        |
-----------------------------------------
Num timesteps: 140000
Best mean reward: 0.15 - Last mean reward: -0.09
Num timesteps: 144000
Best mean reward: 0.15 - Last mean reward: 0.04
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -336         |
| time/                   |              |
|    fps                  | 30           |
|    iterations           | 18           |
|    time_elapsed         | 4862         |
|    total_timesteps      | 147456       |
| train/                  |              |
|    approx_kl            | 0.0004818356 |
|    clip_fraction        | 0.00948      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.224       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 39.1         |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.000132    |
|    value_loss           | 76.4         |
------------------------------------------
Num timesteps: 148000
Best mean reward: 0.15 - Last mean reward: 0.06
Num timesteps: 152000
Best mean reward: 0.15 - Last mean reward: 0.07
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -313         |
| time/                   |              |
|    fps                  | 30           |
|    iterations           | 19           |
|    time_elapsed         | 5142         |
|    total_timesteps      | 155648       |
| train/                  |              |
|    approx_kl            | 0.0010680137 |
|    clip_fraction        | 0.0182       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.2         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 28.6         |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00107     |
|    value_loss           | 80.7         |
------------------------------------------
Num timesteps: 156000
Best mean reward: 0.15 - Last mean reward: -0.16
Num timesteps: 160000
Best mean reward: 0.15 - Last mean reward: 0.02
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -287         |
| time/                   |              |
|    fps                  | 30           |
|    iterations           | 20           |
|    time_elapsed         | 5429         |
|    total_timesteps      | 163840       |
| train/                  |              |
|    approx_kl            | 0.0012596481 |
|    clip_fraction        | 0.0199       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.194       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 43.8         |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.000845    |
|    value_loss           | 81.9         |
------------------------------------------
Num timesteps: 164000
Best mean reward: 0.15 - Last mean reward: 0.08
Num timesteps: 168000
Best mean reward: 0.15 - Last mean reward: 0.06
Num timesteps: 172000
Best mean reward: 0.15 - Last mean reward: 0.07
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -269         |
| time/                   |              |
|    fps                  | 30           |
|    iterations           | 21           |
|    time_elapsed         | 5714         |
|    total_timesteps      | 172032       |
| train/                  |              |
|    approx_kl            | 0.0007219109 |
|    clip_fraction        | 0.00948      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.169       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 57           |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.000177    |
|    value_loss           | 81.5         |
------------------------------------------
Num timesteps: 176000
Best mean reward: 0.15 - Last mean reward: 0.07
Num timesteps: 180000
Best mean reward: 0.15 - Last mean reward: 0.06
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -249          |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 22            |
|    time_elapsed         | 6001          |
|    total_timesteps      | 180224        |
| train/                  |               |
|    approx_kl            | 0.00033863354 |
|    clip_fraction        | 0.00559       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.194        |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 43.8          |
|    n_updates            | 210           |
|    policy_gradient_loss | 0.000273      |
|    value_loss           | 82.9          |
-------------------------------------------
Num timesteps: 184000
Best mean reward: 0.15 - Last mean reward: 0.06
Num timesteps: 188000
Best mean reward: 0.15 - Last mean reward: 0.06
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -232         |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 23           |
|    time_elapsed         | 6286         |
|    total_timesteps      | 188416       |
| train/                  |              |
|    approx_kl            | 0.0006784543 |
|    clip_fraction        | 0.00714      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.186       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 35.3         |
|    n_updates            | 220          |
|    policy_gradient_loss | 0.000162     |
|    value_loss           | 79.3         |
------------------------------------------
Num timesteps: 192000
Best mean reward: 0.15 - Last mean reward: 0.04
Num timesteps: 196000
Best mean reward: 0.15 - Last mean reward: 0.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -217        |
| time/                   |             |
|    fps                  | 29          |
|    iterations           | 24          |
|    time_elapsed         | 6571        |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.000729598 |
|    clip_fraction        | 0.00995     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.149      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 44.5        |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.000349   |
|    value_loss           | 82.7        |
-----------------------------------------
Num timesteps: 200000
Best mean reward: 0.15 - Last mean reward: 0.05
Num timesteps: 204000
Best mean reward: 0.15 - Last mean reward: 0.04
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -203          |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 25            |
|    time_elapsed         | 6856          |
|    total_timesteps      | 204800        |
| train/                  |               |
|    approx_kl            | 0.00027020837 |
|    clip_fraction        | 0.00685       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.141        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 39.7          |
|    n_updates            | 240           |
|    policy_gradient_loss | 0.000169      |
|    value_loss           | 81.2          |
-------------------------------------------
Num timesteps: 208000
Best mean reward: 0.15 - Last mean reward: 0.05
Num timesteps: 212000
Best mean reward: 0.15 - Last mean reward: 0.06
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -191          |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 26            |
|    time_elapsed         | 7138          |
|    total_timesteps      | 212992        |
| train/                  |               |
|    approx_kl            | 0.00019937701 |
|    clip_fraction        | 0.00396       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.143        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 30.6          |
|    n_updates            | 250           |
|    policy_gradient_loss | 0.000136      |
|    value_loss           | 80.4          |
-------------------------------------------
Num timesteps: 216000
Best mean reward: 0.15 - Last mean reward: 0.07
Num timesteps: 220000
Best mean reward: 0.15 - Last mean reward: 0.06
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -146         |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 27           |
|    time_elapsed         | 7420         |
|    total_timesteps      | 221184       |
| train/                  |              |
|    approx_kl            | 0.0003474425 |
|    clip_fraction        | 0.00869      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.123       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 46           |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.000384    |
|    value_loss           | 82.3         |
------------------------------------------
Num timesteps: 224000
Best mean reward: 0.15 - Last mean reward: 0.04
Num timesteps: 228000
Best mean reward: 0.15 - Last mean reward: 0.03
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -101        |
| time/                   |             |
|    fps                  | 29          |
|    iterations           | 28          |
|    time_elapsed         | 7709        |
|    total_timesteps      | 229376      |
| train/                  |             |
|    approx_kl            | 0.000232305 |
|    clip_fraction        | 0.00334     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.137      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 38.6        |
|    n_updates            | 270         |
|    policy_gradient_loss | 2.86e-05    |
|    value_loss           | 79.8        |
-----------------------------------------
Num timesteps: 232000
Best mean reward: 0.15 - Last mean reward: 0.12
Num timesteps: 236000
Best mean reward: 0.15 - Last mean reward: 0.08
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -61.9        |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 29           |
|    time_elapsed         | 7993         |
|    total_timesteps      | 237568       |
| train/                  |              |
|    approx_kl            | 0.0006655946 |
|    clip_fraction        | 0.00997      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.109       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 43.9         |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.000521    |
|    value_loss           | 81.5         |
------------------------------------------
Num timesteps: 240000
Best mean reward: 0.15 - Last mean reward: 0.04
Num timesteps: 244000
Best mean reward: 0.15 - Last mean reward: 0.06
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -21.4         |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 30            |
|    time_elapsed         | 8276          |
|    total_timesteps      | 245760        |
| train/                  |               |
|    approx_kl            | 0.00035362336 |
|    clip_fraction        | 0.00538       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.107        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 40.8          |
|    n_updates            | 290           |
|    policy_gradient_loss | -4.38e-05     |
|    value_loss           | 80.5          |
-------------------------------------------
Num timesteps: 248000
Best mean reward: 0.15 - Last mean reward: 0.02
Num timesteps: 252000
Best mean reward: 0.15 - Last mean reward: 0.05
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 8.3           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 31            |
|    time_elapsed         | 8565          |
|    total_timesteps      | 253952        |
| train/                  |               |
|    approx_kl            | 0.00040446734 |
|    clip_fraction        | 0.00702       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 41.6          |
|    n_updates            | 300           |
|    policy_gradient_loss | -0.000342     |
|    value_loss           | 80            |
-------------------------------------------
Num timesteps: 256000
Best mean reward: 0.15 - Last mean reward: 0.04
Num timesteps: 260000
Best mean reward: 0.15 - Last mean reward: 0.07
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 29            |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 32            |
|    time_elapsed         | 8846          |
|    total_timesteps      | 262144        |
| train/                  |               |
|    approx_kl            | 0.00032186363 |
|    clip_fraction        | 0.00587       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0937       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 48            |
|    n_updates            | 310           |
|    policy_gradient_loss | -0.000115     |
|    value_loss           | 80.2          |
-------------------------------------------
Num timesteps: 264000
Best mean reward: 0.15 - Last mean reward: 0.03
Num timesteps: 268000
Best mean reward: 0.15 - Last mean reward: 0.06
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | 51.5         |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 33           |
|    time_elapsed         | 9130         |
|    total_timesteps      | 270336       |
| train/                  |              |
|    approx_kl            | 0.0004303124 |
|    clip_fraction        | 0.00668      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0774      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 37.7         |
|    n_updates            | 320          |
|    policy_gradient_loss | -3.29e-05    |
|    value_loss           | 81.7         |
------------------------------------------
Num timesteps: 272000
Best mean reward: 0.15 - Last mean reward: 0.10
Num timesteps: 276000
Best mean reward: 0.15 - Last mean reward: 0.11
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 64.4          |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 34            |
|    time_elapsed         | 9408          |
|    total_timesteps      | 278528        |
| train/                  |               |
|    approx_kl            | 0.00026070664 |
|    clip_fraction        | 0.00774       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0682       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 38.3          |
|    n_updates            | 330           |
|    policy_gradient_loss | -0.000466     |
|    value_loss           | 83.8          |
-------------------------------------------
Num timesteps: 280000
Best mean reward: 0.15 - Last mean reward: 0.07
Num timesteps: 284000
Best mean reward: 0.15 - Last mean reward: 0.11
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 81.5          |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 35            |
|    time_elapsed         | 9691          |
|    total_timesteps      | 286720        |
| train/                  |               |
|    approx_kl            | 0.00033581007 |
|    clip_fraction        | 0.00507       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0653       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 29.5          |
|    n_updates            | 340           |
|    policy_gradient_loss | -9.55e-05     |
|    value_loss           | 84.9          |
-------------------------------------------
Num timesteps: 288000
Best mean reward: 0.15 - Last mean reward: 0.02
Num timesteps: 292000
Best mean reward: 0.15 - Last mean reward: 0.05
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 2.05e+03       |
|    ep_rew_mean          | 92.6           |
| time/                   |                |
|    fps                  | 29             |
|    iterations           | 36             |
|    time_elapsed         | 9971           |
|    total_timesteps      | 294912         |
| train/                  |                |
|    approx_kl            | 0.000117250725 |
|    clip_fraction        | 0.00333        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0578        |
|    explained_variance   | 1.19e-07       |
|    learning_rate        | 0.0003         |
|    loss                 | 28.9           |
|    n_updates            | 350            |
|    policy_gradient_loss | -1.76e-05      |
|    value_loss           | 83.9           |
--------------------------------------------
Num timesteps: 296000
Best mean reward: 0.15 - Last mean reward: -0.00
Num timesteps: 300000
Best mean reward: 0.15 - Last mean reward: 0.03
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 100           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 37            |
|    time_elapsed         | 10256         |
|    total_timesteps      | 303104        |
| train/                  |               |
|    approx_kl            | 0.00015909816 |
|    clip_fraction        | 0.00316       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0552       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 37.3          |
|    n_updates            | 360           |
|    policy_gradient_loss | -0.00022      |
|    value_loss           | 82.7          |
-------------------------------------------
Num timesteps: 304000
Best mean reward: 0.15 - Last mean reward: 0.07
Num timesteps: 308000
Best mean reward: 0.15 - Last mean reward: 0.04
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 103           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 38            |
|    time_elapsed         | 10536         |
|    total_timesteps      | 311296        |
| train/                  |               |
|    approx_kl            | 0.00020741022 |
|    clip_fraction        | 0.00317       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0488       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 36.4          |
|    n_updates            | 370           |
|    policy_gradient_loss | 9.9e-06       |
|    value_loss           | 80.5          |
-------------------------------------------
Num timesteps: 312000
Best mean reward: 0.15 - Last mean reward: -0.07
Num timesteps: 316000
Best mean reward: 0.15 - Last mean reward: 0.09
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | 106          |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 39           |
|    time_elapsed         | 10824        |
|    total_timesteps      | 319488       |
| train/                  |              |
|    approx_kl            | 9.478333e-05 |
|    clip_fraction        | 0.00193      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0455      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 36.4         |
|    n_updates            | 380          |
|    policy_gradient_loss | 0.000146     |
|    value_loss           | 79.9         |
------------------------------------------
Num timesteps: 320000
Best mean reward: 0.15 - Last mean reward: -0.09
Num timesteps: 324000
Best mean reward: 0.15 - Last mean reward: 0.08
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 115           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 40            |
|    time_elapsed         | 11103         |
|    total_timesteps      | 327680        |
| train/                  |               |
|    approx_kl            | 0.00012498524 |
|    clip_fraction        | 0.00248       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0392       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 44.4          |
|    n_updates            | 390           |
|    policy_gradient_loss | -0.00011      |
|    value_loss           | 83            |
-------------------------------------------
Num timesteps: 328000
Best mean reward: 0.15 - Last mean reward: -0.03
Num timesteps: 332000
Best mean reward: 0.15 - Last mean reward: 0.04
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 125           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 41            |
|    time_elapsed         | 11385         |
|    total_timesteps      | 335872        |
| train/                  |               |
|    approx_kl            | 5.5262622e-05 |
|    clip_fraction        | 0.00228       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0389       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 40.1          |
|    n_updates            | 400           |
|    policy_gradient_loss | 5.07e-05      |
|    value_loss           | 84.6          |
-------------------------------------------
Num timesteps: 336000
Best mean reward: 0.15 - Last mean reward: -0.02
Num timesteps: 340000
Best mean reward: 0.15 - Last mean reward: 0.08
Num timesteps: 344000
Best mean reward: 0.15 - Last mean reward: 0.07
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 123           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 42            |
|    time_elapsed         | 11667         |
|    total_timesteps      | 344064        |
| train/                  |               |
|    approx_kl            | 3.7188984e-05 |
|    clip_fraction        | 0.00225       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0473       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 33.5          |
|    n_updates            | 410           |
|    policy_gradient_loss | 2.71e-05      |
|    value_loss           | 80.2          |
-------------------------------------------
Num timesteps: 348000
Best mean reward: 0.15 - Last mean reward: 0.10
Num timesteps: 352000
Best mean reward: 0.15 - Last mean reward: 0.08
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | 126          |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 43           |
|    time_elapsed         | 11956        |
|    total_timesteps      | 352256       |
| train/                  |              |
|    approx_kl            | 4.746137e-05 |
|    clip_fraction        | 0.0014       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.041       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 35.2         |
|    n_updates            | 420          |
|    policy_gradient_loss | 3.45e-05     |
|    value_loss           | 81.4         |
------------------------------------------
Num timesteps: 356000
Best mean reward: 0.15 - Last mean reward: 0.08
Num timesteps: 360000
Best mean reward: 0.15 - Last mean reward: 0.09
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | 130          |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 44           |
|    time_elapsed         | 12240        |
|    total_timesteps      | 360448       |
| train/                  |              |
|    approx_kl            | 0.0002304546 |
|    clip_fraction        | 0.00333      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.036       |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 41           |
|    n_updates            | 430          |
|    policy_gradient_loss | -4.08e-05    |
|    value_loss           | 82.3         |
------------------------------------------
Num timesteps: 364000
Best mean reward: 0.15 - Last mean reward: -0.03
Num timesteps: 368000
Best mean reward: 0.15 - Last mean reward: 0.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 130           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 45            |
|    time_elapsed         | 12524         |
|    total_timesteps      | 368640        |
| train/                  |               |
|    approx_kl            | 0.00016541674 |
|    clip_fraction        | 0.00173       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0321       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 47.5          |
|    n_updates            | 440           |
|    policy_gradient_loss | -9.55e-05     |
|    value_loss           | 87.8          |
-------------------------------------------
Num timesteps: 372000
Best mean reward: 0.15 - Last mean reward: 0.10
Num timesteps: 376000
Best mean reward: 0.15 - Last mean reward: 0.08
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 128           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 46            |
|    time_elapsed         | 12804         |
|    total_timesteps      | 376832        |
| train/                  |               |
|    approx_kl            | 1.9820283e-05 |
|    clip_fraction        | 0.00194       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0238       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 37.3          |
|    n_updates            | 450           |
|    policy_gradient_loss | 4.11e-05      |
|    value_loss           | 80.6          |
-------------------------------------------
Num timesteps: 380000
Best mean reward: 0.15 - Last mean reward: -0.02
Num timesteps: 384000
Best mean reward: 0.15 - Last mean reward: -0.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 129           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 47            |
|    time_elapsed         | 13089         |
|    total_timesteps      | 385024        |
| train/                  |               |
|    approx_kl            | 0.00010643657 |
|    clip_fraction        | 0.0015        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0267       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 35.5          |
|    n_updates            | 460           |
|    policy_gradient_loss | -2.61e-05     |
|    value_loss           | 82.6          |
-------------------------------------------
Num timesteps: 388000
Best mean reward: 0.15 - Last mean reward: 0.18
Saving new best model to ./logs/best_model
Num timesteps: 392000
Best mean reward: 0.18 - Last mean reward: 0.22
Saving new best model to ./logs/best_model
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 124           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 48            |
|    time_elapsed         | 13374         |
|    total_timesteps      | 393216        |
| train/                  |               |
|    approx_kl            | 0.00019822187 |
|    clip_fraction        | 0.00265       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0285       |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 50.1          |
|    n_updates            | 470           |
|    policy_gradient_loss | 8.42e-06      |
|    value_loss           | 80.6          |
-------------------------------------------
Num timesteps: 396000
Best mean reward: 0.22 - Last mean reward: 0.14
Num timesteps: 400000
Best mean reward: 0.22 - Last mean reward: 0.10
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | 138         |
| time/                   |             |
|    fps                  | 29          |
|    iterations           | 49          |
|    time_elapsed         | 13651       |
|    total_timesteps      | 401408      |
| train/                  |             |
|    approx_kl            | 6.53786e-05 |
|    clip_fraction        | 0.00194     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0219     |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 54          |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.000257   |
|    value_loss           | 87.3        |
-----------------------------------------
Num timesteps: 404000
Best mean reward: 0.22 - Last mean reward: 0.07
Num timesteps: 408000
Best mean reward: 0.22 - Last mean reward: 0.06
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | 141          |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 50           |
|    time_elapsed         | 13933        |
|    total_timesteps      | 409600       |
| train/                  |              |
|    approx_kl            | 5.379848e-05 |
|    clip_fraction        | 0.00175      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0262      |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 50.7         |
|    n_updates            | 490          |
|    policy_gradient_loss | -2.93e-05    |
|    value_loss           | 85.7         |
------------------------------------------
Num timesteps: 412000
Best mean reward: 0.22 - Last mean reward: 0.04
Num timesteps: 416000
Best mean reward: 0.22 - Last mean reward: 0.07
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | 142          |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 51           |
|    time_elapsed         | 14221        |
|    total_timesteps      | 417792       |
| train/                  |              |
|    approx_kl            | 7.004243e-05 |
|    clip_fraction        | 0.00118      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0172      |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 37.8         |
|    n_updates            | 500          |
|    policy_gradient_loss | -4.58e-06    |
|    value_loss           | 81.3         |
------------------------------------------
Num timesteps: 420000
Best mean reward: 0.22 - Last mean reward: 0.04
Num timesteps: 424000
Best mean reward: 0.22 - Last mean reward: 0.05
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | 142          |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 52           |
|    time_elapsed         | 14504        |
|    total_timesteps      | 425984       |
| train/                  |              |
|    approx_kl            | 4.186306e-05 |
|    clip_fraction        | 0.00116      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0146      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 28.6         |
|    n_updates            | 510          |
|    policy_gradient_loss | -1.18e-05    |
|    value_loss           | 81.7         |
------------------------------------------
Num timesteps: 428000
Best mean reward: 0.22 - Last mean reward: -0.10
Num timesteps: 432000
Best mean reward: 0.22 - Last mean reward: -0.04
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 141           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 53            |
|    time_elapsed         | 14782         |
|    total_timesteps      | 434176        |
| train/                  |               |
|    approx_kl            | 0.00027713607 |
|    clip_fraction        | 0.00149       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0201       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 23.9          |
|    n_updates            | 520           |
|    policy_gradient_loss | 4.3e-05       |
|    value_loss           | 84            |
-------------------------------------------
Num timesteps: 436000
Best mean reward: 0.22 - Last mean reward: 0.11
Num timesteps: 440000
Best mean reward: 0.22 - Last mean reward: 0.11
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 138           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 54            |
|    time_elapsed         | 15073         |
|    total_timesteps      | 442368        |
| train/                  |               |
|    approx_kl            | 0.00015723947 |
|    clip_fraction        | 0.000867      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0145       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 43.4          |
|    n_updates            | 530           |
|    policy_gradient_loss | 6.13e-05      |
|    value_loss           | 79.5          |
-------------------------------------------
Num timesteps: 444000
Best mean reward: 0.22 - Last mean reward: 0.09
Num timesteps: 448000
Best mean reward: 0.22 - Last mean reward: 0.12
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | 141          |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 55           |
|    time_elapsed         | 15359        |
|    total_timesteps      | 450560       |
| train/                  |              |
|    approx_kl            | 5.074723e-05 |
|    clip_fraction        | 0.00101      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0105      |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 59.2         |
|    n_updates            | 540          |
|    policy_gradient_loss | 2.27e-05     |
|    value_loss           | 87.1         |
------------------------------------------
Num timesteps: 452000
Best mean reward: 0.22 - Last mean reward: 0.11
Num timesteps: 456000
Best mean reward: 0.22 - Last mean reward: 0.12
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | 148          |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 56           |
|    time_elapsed         | 15640        |
|    total_timesteps      | 458752       |
| train/                  |              |
|    approx_kl            | 8.242281e-05 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00949     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 46.1         |
|    n_updates            | 550          |
|    policy_gradient_loss | 2.96e-05     |
|    value_loss           | 85.9         |
------------------------------------------
Num timesteps: 460000
Best mean reward: 0.22 - Last mean reward: 0.10
Num timesteps: 464000
Best mean reward: 0.22 - Last mean reward: 0.12
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 152           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 57            |
|    time_elapsed         | 15929         |
|    total_timesteps      | 466944        |
| train/                  |               |
|    approx_kl            | 0.00020550456 |
|    clip_fraction        | 0.000562      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.009        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 44.1          |
|    n_updates            | 560           |
|    policy_gradient_loss | -3.89e-05     |
|    value_loss           | 86.3          |
-------------------------------------------
Num timesteps: 468000
Best mean reward: 0.22 - Last mean reward: 0.07
Num timesteps: 472000
Best mean reward: 0.22 - Last mean reward: 0.08
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | 156          |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 58           |
|    time_elapsed         | 16215        |
|    total_timesteps      | 475136       |
| train/                  |              |
|    approx_kl            | 5.713734e-05 |
|    clip_fraction        | 0.000989     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00894     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 41.7         |
|    n_updates            | 570          |
|    policy_gradient_loss | -5.52e-05    |
|    value_loss           | 84.9         |
------------------------------------------
Num timesteps: 476000
Best mean reward: 0.22 - Last mean reward: 0.13
Num timesteps: 480000
Best mean reward: 0.22 - Last mean reward: 0.09
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | 158         |
| time/                   |             |
|    fps                  | 29          |
|    iterations           | 59          |
|    time_elapsed         | 16499       |
|    total_timesteps      | 483328      |
| train/                  |             |
|    approx_kl            | 0.000244886 |
|    clip_fraction        | 0.00118     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00921    |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 42.2        |
|    n_updates            | 580         |
|    policy_gradient_loss | -3.97e-06   |
|    value_loss           | 84          |
-----------------------------------------
Num timesteps: 484000
Best mean reward: 0.22 - Last mean reward: 0.17
Num timesteps: 488000
Best mean reward: 0.22 - Last mean reward: 0.16
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | 156         |
| time/                   |             |
|    fps                  | 29          |
|    iterations           | 60          |
|    time_elapsed         | 16786       |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 8.36841e-05 |
|    clip_fraction        | 0.000854    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00586    |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 45.2        |
|    n_updates            | 590         |
|    policy_gradient_loss | -6.68e-05   |
|    value_loss           | 81          |
-----------------------------------------
Num timesteps: 492000
Best mean reward: 0.22 - Last mean reward: 0.13
Num timesteps: 496000
Best mean reward: 0.22 - Last mean reward: 0.08
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 159           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 61            |
|    time_elapsed         | 17067         |
|    total_timesteps      | 499712        |
| train/                  |               |
|    approx_kl            | 4.5367153e-05 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00496      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 41.6          |
|    n_updates            | 600           |
|    policy_gradient_loss | -8.18e-06     |
|    value_loss           | 88.6          |
-------------------------------------------
Num timesteps: 500000
Best mean reward: 0.22 - Last mean reward: -0.08
Num timesteps: 504000
Best mean reward: 0.22 - Last mean reward: 0.07
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 160           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 62            |
|    time_elapsed         | 17350         |
|    total_timesteps      | 507904        |
| train/                  |               |
|    approx_kl            | 1.5498743e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00411      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 41            |
|    n_updates            | 610           |
|    policy_gradient_loss | 2.24e-05      |
|    value_loss           | 85            |
-------------------------------------------
Num timesteps: 508000
Best mean reward: 0.22 - Last mean reward: -0.50
Num timesteps: 512000
Best mean reward: 0.22 - Last mean reward: 0.06
Num timesteps: 516000
Best mean reward: 0.22 - Last mean reward: 0.04
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 164           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 63            |
|    time_elapsed         | 17633         |
|    total_timesteps      | 516096        |
| train/                  |               |
|    approx_kl            | 0.00025941228 |
|    clip_fraction        | 0.000366      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00444      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 40.5          |
|    n_updates            | 620           |
|    policy_gradient_loss | 8.15e-05      |
|    value_loss           | 81.8          |
-------------------------------------------
Num timesteps: 520000
Best mean reward: 0.22 - Last mean reward: 0.07
Num timesteps: 524000
Best mean reward: 0.22 - Last mean reward: 0.09
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | 166          |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 64           |
|    time_elapsed         | 17915        |
|    total_timesteps      | 524288       |
| train/                  |              |
|    approx_kl            | 8.667517e-05 |
|    clip_fraction        | 0.000793     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00588     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 35.6         |
|    n_updates            | 630          |
|    policy_gradient_loss | -1.18e-05    |
|    value_loss           | 82.6         |
------------------------------------------
Num timesteps: 528000
Best mean reward: 0.22 - Last mean reward: 0.12
Num timesteps: 532000
Best mean reward: 0.22 - Last mean reward: 0.11
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 166           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 65            |
|    time_elapsed         | 18202         |
|    total_timesteps      | 532480        |
| train/                  |               |
|    approx_kl            | 2.4512803e-05 |
|    clip_fraction        | 0.0005        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00607      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 48.2          |
|    n_updates            | 640           |
|    policy_gradient_loss | -4.14e-05     |
|    value_loss           | 86.3          |
-------------------------------------------
Num timesteps: 536000
Best mean reward: 0.22 - Last mean reward: 0.10
Num timesteps: 540000
Best mean reward: 0.22 - Last mean reward: 0.08
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 168           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 66            |
|    time_elapsed         | 18496         |
|    total_timesteps      | 540672        |
| train/                  |               |
|    approx_kl            | 2.3719862e-05 |
|    clip_fraction        | 0.000696      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00654      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 49.7          |
|    n_updates            | 650           |
|    policy_gradient_loss | 6.93e-05      |
|    value_loss           | 82.6          |
-------------------------------------------
Num timesteps: 544000
Best mean reward: 0.22 - Last mean reward: 0.08
Num timesteps: 548000
Best mean reward: 0.22 - Last mean reward: 0.07
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | 172          |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 67           |
|    time_elapsed         | 18778        |
|    total_timesteps      | 548864       |
| train/                  |              |
|    approx_kl            | 1.083887e-05 |
|    clip_fraction        | 0.000366     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00405     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 42.2         |
|    n_updates            | 660          |
|    policy_gradient_loss | -1.24e-05    |
|    value_loss           | 79.4         |
------------------------------------------
Num timesteps: 552000
Best mean reward: 0.22 - Last mean reward: 0.07
Num timesteps: 556000
Best mean reward: 0.22 - Last mean reward: 0.08
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 172           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 68            |
|    time_elapsed         | 19065         |
|    total_timesteps      | 557056        |
| train/                  |               |
|    approx_kl            | 2.5927366e-06 |
|    clip_fraction        | 0.000415      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00336      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 38.4          |
|    n_updates            | 670           |
|    policy_gradient_loss | 6.43e-06      |
|    value_loss           | 82.4          |
-------------------------------------------
Num timesteps: 560000
Best mean reward: 0.22 - Last mean reward: 0.06
Num timesteps: 564000
Best mean reward: 0.22 - Last mean reward: 0.08
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | 171          |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 69           |
|    time_elapsed         | 19354        |
|    total_timesteps      | 565248       |
| train/                  |              |
|    approx_kl            | 6.362243e-06 |
|    clip_fraction        | 0.000269     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00264     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    n_updates            | 680          |
|    policy_gradient_loss | 1.94e-05     |
|    value_loss           | 82.5         |
------------------------------------------
Num timesteps: 568000
Best mean reward: 0.22 - Last mean reward: 0.13
Num timesteps: 572000
Best mean reward: 0.22 - Last mean reward: 0.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | 170          |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 70           |
|    time_elapsed         | 19634        |
|    total_timesteps      | 573440       |
| train/                  |              |
|    approx_kl            | 5.967602e-06 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.004       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 36.9         |
|    n_updates            | 690          |
|    policy_gradient_loss | 2.29e-05     |
|    value_loss           | 82.1         |
------------------------------------------
Num timesteps: 576000
Best mean reward: 0.22 - Last mean reward: 0.03
Num timesteps: 580000
Best mean reward: 0.22 - Last mean reward: 0.08
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 181           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 71            |
|    time_elapsed         | 19918         |
|    total_timesteps      | 581632        |
| train/                  |               |
|    approx_kl            | 1.9727631e-05 |
|    clip_fraction        | 0.00033       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0025       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 41.8          |
|    n_updates            | 700           |
|    policy_gradient_loss | -6.72e-05     |
|    value_loss           | 85.9          |
-------------------------------------------
Num timesteps: 584000
Best mean reward: 0.22 - Last mean reward: 0.06
Num timesteps: 588000
Best mean reward: 0.22 - Last mean reward: 0.05
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 180           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 72            |
|    time_elapsed         | 20205         |
|    total_timesteps      | 589824        |
| train/                  |               |
|    approx_kl            | 1.3303041e-05 |
|    clip_fraction        | 0.000183      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00194      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 51.8          |
|    n_updates            | 710           |
|    policy_gradient_loss | -4.5e-06      |
|    value_loss           | 82.1          |
-------------------------------------------
Num timesteps: 592000
Best mean reward: 0.22 - Last mean reward: 0.02
Num timesteps: 596000
Best mean reward: 0.22 - Last mean reward: 0.02
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 184           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 73            |
|    time_elapsed         | 20488         |
|    total_timesteps      | 598016        |
| train/                  |               |
|    approx_kl            | 3.0914161e-06 |
|    clip_fraction        | 0.00011       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00152      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 41.6          |
|    n_updates            | 720           |
|    policy_gradient_loss | -2.11e-05     |
|    value_loss           | 86.6          |
-------------------------------------------
Num timesteps: 600000
Best mean reward: 0.22 - Last mean reward: 0.12
Num timesteps: 604000
Best mean reward: 0.22 - Last mean reward: 0.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | 167          |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 74           |
|    time_elapsed         | 20768        |
|    total_timesteps      | 606208       |
| train/                  |              |
|    approx_kl            | 3.203837e-06 |
|    clip_fraction        | 8.54e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00177     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 37           |
|    n_updates            | 730          |
|    policy_gradient_loss | -2.15e-06    |
|    value_loss           | 80.4         |
------------------------------------------
Num timesteps: 608000
Best mean reward: 0.22 - Last mean reward: -0.05
Num timesteps: 612000
Best mean reward: 0.22 - Last mean reward: 0.01
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | 169          |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 75           |
|    time_elapsed         | 21053        |
|    total_timesteps      | 614400       |
| train/                  |              |
|    approx_kl            | 4.682486e-06 |
|    clip_fraction        | 0.00011      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00143     |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 50.3         |
|    n_updates            | 740          |
|    policy_gradient_loss | -1.49e-05    |
|    value_loss           | 88.4         |
------------------------------------------
Num timesteps: 616000
Best mean reward: 0.22 - Last mean reward: -0.05
Num timesteps: 620000
Best mean reward: 0.22 - Last mean reward: 0.02
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | 166          |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 76           |
|    time_elapsed         | 21340        |
|    total_timesteps      | 622592       |
| train/                  |              |
|    approx_kl            | 1.282664e-06 |
|    clip_fraction        | 0.000232     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00144     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 28.9         |
|    n_updates            | 750          |
|    policy_gradient_loss | 4.76e-05     |
|    value_loss           | 78.6         |
------------------------------------------
Num timesteps: 624000
Best mean reward: 0.22 - Last mean reward: 0.06
Num timesteps: 628000
Best mean reward: 0.22 - Last mean reward: 0.13
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 162       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 77        |
|    time_elapsed         | 21624     |
|    total_timesteps      | 630784    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00177  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 37.4      |
|    n_updates            | 760       |
|    policy_gradient_loss | -3.88e-10 |
|    value_loss           | 81.5      |
---------------------------------------
Num timesteps: 632000
Best mean reward: 0.22 - Last mean reward: 0.02
Num timesteps: 636000
Best mean reward: 0.22 - Last mean reward: 0.09
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 167           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 78            |
|    time_elapsed         | 21914         |
|    total_timesteps      | 638976        |
| train/                  |               |
|    approx_kl            | 1.7181359e-05 |
|    clip_fraction        | 0.00011       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00201      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 49.5          |
|    n_updates            | 770           |
|    policy_gradient_loss | -7.78e-06     |
|    value_loss           | 83.4          |
-------------------------------------------
Num timesteps: 640000
Best mean reward: 0.22 - Last mean reward: 0.06
Num timesteps: 644000
Best mean reward: 0.22 - Last mean reward: 0.06
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | 177          |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 79           |
|    time_elapsed         | 22203        |
|    total_timesteps      | 647168       |
| train/                  |              |
|    approx_kl            | 5.680271e-05 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00101     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 37.5         |
|    n_updates            | 780          |
|    policy_gradient_loss | -1.63e-05    |
|    value_loss           | 83.8         |
------------------------------------------
Num timesteps: 648000
Best mean reward: 0.22 - Last mean reward: 0.01
Num timesteps: 652000
Best mean reward: 0.22 - Last mean reward: 0.02
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 172       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 80        |
|    time_elapsed         | 22486     |
|    total_timesteps      | 655360    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0008   |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 37.5      |
|    n_updates            | 790       |
|    policy_gradient_loss | -5.16e-11 |
|    value_loss           | 85.5      |
---------------------------------------
Num timesteps: 656000
Best mean reward: 0.22 - Last mean reward: 0.11
Num timesteps: 660000
Best mean reward: 0.22 - Last mean reward: 0.18
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | 163          |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 81           |
|    time_elapsed         | 22773        |
|    total_timesteps      | 663552       |
| train/                  |              |
|    approx_kl            | 3.754707e-06 |
|    clip_fraction        | 0.000208     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000568    |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 38.1         |
|    n_updates            | 800          |
|    policy_gradient_loss | 3.06e-06     |
|    value_loss           | 79           |
------------------------------------------
Num timesteps: 664000
Best mean reward: 0.22 - Last mean reward: 0.11
Num timesteps: 668000
Best mean reward: 0.22 - Last mean reward: 0.10
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 167       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 82        |
|    time_elapsed         | 23059     |
|    total_timesteps      | 671744    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000724 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 40.8      |
|    n_updates            | 810       |
|    policy_gradient_loss | 8.99e-10  |
|    value_loss           | 87.3      |
---------------------------------------
Num timesteps: 672000
Best mean reward: 0.22 - Last mean reward: -0.14
Num timesteps: 676000
Best mean reward: 0.22 - Last mean reward: -0.01
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 167           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 83            |
|    time_elapsed         | 23337         |
|    total_timesteps      | 679936        |
| train/                  |               |
|    approx_kl            | 3.0045601e-05 |
|    clip_fraction        | 0.00011       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00118      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 31.9          |
|    n_updates            | 820           |
|    policy_gradient_loss | -2.38e-05     |
|    value_loss           | 86.3          |
-------------------------------------------
Num timesteps: 680000
Best mean reward: 0.22 - Last mean reward: -0.50
Num timesteps: 684000
Best mean reward: 0.22 - Last mean reward: 0.11
Num timesteps: 688000
Best mean reward: 0.22 - Last mean reward: 0.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | 161          |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 84           |
|    time_elapsed         | 23630        |
|    total_timesteps      | 688128       |
| train/                  |              |
|    approx_kl            | 8.906207e-05 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000553    |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 33.3         |
|    n_updates            | 830          |
|    policy_gradient_loss | -3.1e-05     |
|    value_loss           | 78.1         |
------------------------------------------
Num timesteps: 692000
Best mean reward: 0.22 - Last mean reward: 0.07
Num timesteps: 696000
Best mean reward: 0.22 - Last mean reward: 0.07
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 163           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 85            |
|    time_elapsed         | 23917         |
|    total_timesteps      | 696320        |
| train/                  |               |
|    approx_kl            | 3.0725234e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000302     |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 42.9          |
|    n_updates            | 840           |
|    policy_gradient_loss | -2.65e-05     |
|    value_loss           | 84.6          |
-------------------------------------------
Num timesteps: 700000
Best mean reward: 0.22 - Last mean reward: 0.09
Num timesteps: 704000
Best mean reward: 0.22 - Last mean reward: 0.09
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 156       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 86        |
|    time_elapsed         | 24202     |
|    total_timesteps      | 704512    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000296 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 44        |
|    n_updates            | 850       |
|    policy_gradient_loss | 3.8e-10   |
|    value_loss           | 83.2      |
---------------------------------------
Num timesteps: 708000
Best mean reward: 0.22 - Last mean reward: 0.16
Num timesteps: 712000
Best mean reward: 0.22 - Last mean reward: 0.14
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 158       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 87        |
|    time_elapsed         | 24484     |
|    total_timesteps      | 712704    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000296 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 50.5      |
|    n_updates            | 860       |
|    policy_gradient_loss | -2e-10    |
|    value_loss           | 84.5      |
---------------------------------------
Num timesteps: 716000
Best mean reward: 0.22 - Last mean reward: 0.08
Num timesteps: 720000
Best mean reward: 0.22 - Last mean reward: 0.10
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 163       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 88        |
|    time_elapsed         | 24767     |
|    total_timesteps      | 720896    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000296 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 36.8      |
|    n_updates            | 870       |
|    policy_gradient_loss | 4.93e-10  |
|    value_loss           | 86.5      |
---------------------------------------
Num timesteps: 724000
Best mean reward: 0.22 - Last mean reward: 0.06
Num timesteps: 728000
Best mean reward: 0.22 - Last mean reward: 0.07
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 167       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 89        |
|    time_elapsed         | 25055     |
|    total_timesteps      | 729088    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000296 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 50.1      |
|    n_updates            | 880       |
|    policy_gradient_loss | 5.9e-10   |
|    value_loss           | 86.6      |
---------------------------------------
Num timesteps: 732000
Best mean reward: 0.22 - Last mean reward: 0.01
Num timesteps: 736000
Best mean reward: 0.22 - Last mean reward: 0.05
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 165       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 90        |
|    time_elapsed         | 25343     |
|    total_timesteps      | 737280    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000296 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 49.4      |
|    n_updates            | 890       |
|    policy_gradient_loss | -1.38e-10 |
|    value_loss           | 83.6      |
---------------------------------------
Num timesteps: 740000
Best mean reward: 0.22 - Last mean reward: 0.01
Num timesteps: 744000
Best mean reward: 0.22 - Last mean reward: 0.02
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 161       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 91        |
|    time_elapsed         | 25628     |
|    total_timesteps      | 745472    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000296 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 40.6      |
|    n_updates            | 900       |
|    policy_gradient_loss | 2.06e-10  |
|    value_loss           | 83.7      |
---------------------------------------
Num timesteps: 748000
Best mean reward: 0.22 - Last mean reward: 0.03
Num timesteps: 752000
Best mean reward: 0.22 - Last mean reward: 0.04
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 160       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 92        |
|    time_elapsed         | 25915     |
|    total_timesteps      | 753664    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000296 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 38.2      |
|    n_updates            | 910       |
|    policy_gradient_loss | -3.79e-10 |
|    value_loss           | 83.9      |
---------------------------------------
Num timesteps: 756000
Best mean reward: 0.22 - Last mean reward: 0.08
Num timesteps: 760000
Best mean reward: 0.22 - Last mean reward: 0.08
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 156       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 93        |
|    time_elapsed         | 26204     |
|    total_timesteps      | 761856    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000296 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 39.3      |
|    n_updates            | 920       |
|    policy_gradient_loss | -5.05e-10 |
|    value_loss           | 79.4      |
---------------------------------------
Num timesteps: 764000
Best mean reward: 0.22 - Last mean reward: 0.05
Num timesteps: 768000
Best mean reward: 0.22 - Last mean reward: 0.12
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 159       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 94        |
|    time_elapsed         | 26493     |
|    total_timesteps      | 770048    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000296 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 52.2      |
|    n_updates            | 930       |
|    policy_gradient_loss | -4.49e-11 |
|    value_loss           | 82.2      |
---------------------------------------
Num timesteps: 772000
Best mean reward: 0.22 - Last mean reward: 0.10
Num timesteps: 776000
Best mean reward: 0.22 - Last mean reward: 0.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 161           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 95            |
|    time_elapsed         | 26778         |
|    total_timesteps      | 778240        |
| train/                  |               |
|    approx_kl            | 1.3488105e-05 |
|    clip_fraction        | 0.00011       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000204     |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 53            |
|    n_updates            | 940           |
|    policy_gradient_loss | -1.23e-05     |
|    value_loss           | 83.4          |
-------------------------------------------
Num timesteps: 780000
Best mean reward: 0.22 - Last mean reward: -0.07
Num timesteps: 784000
Best mean reward: 0.22 - Last mean reward: -0.03
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 160       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 96        |
|    time_elapsed         | 27059     |
|    total_timesteps      | 786432    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000195 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 47.3      |
|    n_updates            | 950       |
|    policy_gradient_loss | -2.32e-11 |
|    value_loss           | 83.4      |
---------------------------------------
Num timesteps: 788000
Best mean reward: 0.22 - Last mean reward: 0.13
Num timesteps: 792000
Best mean reward: 0.22 - Last mean reward: 0.15
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 152       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 97        |
|    time_elapsed         | 27348     |
|    total_timesteps      | 794624    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000195 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 31.1      |
|    n_updates            | 960       |
|    policy_gradient_loss | -5.33e-10 |
|    value_loss           | 79        |
---------------------------------------
Num timesteps: 796000
Best mean reward: 0.22 - Last mean reward: -0.02
Num timesteps: 800000
Best mean reward: 0.22 - Last mean reward: -0.01
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 160       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 98        |
|    time_elapsed         | 27631     |
|    total_timesteps      | 802816    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000195 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 32.8      |
|    n_updates            | 970       |
|    policy_gradient_loss | -5.91e-11 |
|    value_loss           | 86.8      |
---------------------------------------
Num timesteps: 804000
Best mean reward: 0.22 - Last mean reward: 0.03
Num timesteps: 808000
Best mean reward: 0.22 - Last mean reward: 0.09
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 157       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 99        |
|    time_elapsed         | 27910     |
|    total_timesteps      | 811008    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000195 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 45.9      |
|    n_updates            | 980       |
|    policy_gradient_loss | 8.84e-10  |
|    value_loss           | 79.8      |
---------------------------------------
Num timesteps: 812000
Best mean reward: 0.22 - Last mean reward: -0.01
Num timesteps: 816000
Best mean reward: 0.22 - Last mean reward: -0.01
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 155       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 100       |
|    time_elapsed         | 28196     |
|    total_timesteps      | 819200    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000195 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 56        |
|    n_updates            | 990       |
|    policy_gradient_loss | 3.06e-10  |
|    value_loss           | 82.2      |
---------------------------------------
Num timesteps: 820000
Best mean reward: 0.22 - Last mean reward: 0.10
Num timesteps: 824000
Best mean reward: 0.22 - Last mean reward: 0.08
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 152       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 101       |
|    time_elapsed         | 28482     |
|    total_timesteps      | 827392    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000195 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 41.2      |
|    n_updates            | 1000      |
|    policy_gradient_loss | -8.54e-10 |
|    value_loss           | 78.3      |
---------------------------------------
Num timesteps: 828000
Best mean reward: 0.22 - Last mean reward: -0.05
Num timesteps: 832000
Best mean reward: 0.22 - Last mean reward: 0.13
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 157       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 102       |
|    time_elapsed         | 28770     |
|    total_timesteps      | 835584    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000195 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 38.3      |
|    n_updates            | 1010      |
|    policy_gradient_loss | -1.27e-11 |
|    value_loss           | 80.4      |
---------------------------------------
Num timesteps: 836000
Best mean reward: 0.22 - Last mean reward: -0.28
Num timesteps: 840000
Best mean reward: 0.22 - Last mean reward: 0.03
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 159       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 103       |
|    time_elapsed         | 29051     |
|    total_timesteps      | 843776    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000195 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 38.2      |
|    n_updates            | 1020      |
|    policy_gradient_loss | -1.55e-10 |
|    value_loss           | 83.7      |
---------------------------------------
Num timesteps: 844000
Best mean reward: 0.22 - Last mean reward: 0.05
Num timesteps: 848000
Best mean reward: 0.22 - Last mean reward: 0.23
Saving new best model to ./logs/best_model
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 156       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 104       |
|    time_elapsed         | 29330     |
|    total_timesteps      | 851968    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000195 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 46        |
|    n_updates            | 1030      |
|    policy_gradient_loss | -7.85e-10 |
|    value_loss           | 81.7      |
---------------------------------------
Num timesteps: 852000
Best mean reward: 0.23 - Last mean reward: -0.50
Num timesteps: 856000
Best mean reward: 0.23 - Last mean reward: 0.01
Num timesteps: 860000
Best mean reward: 0.23 - Last mean reward: 0.03
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 166       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 105       |
|    time_elapsed         | 29612     |
|    total_timesteps      | 860160    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000195 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 54.4      |
|    n_updates            | 1040      |
|    policy_gradient_loss | -1.35e-10 |
|    value_loss           | 89.5      |
---------------------------------------
Num timesteps: 864000
Best mean reward: 0.23 - Last mean reward: -0.04
Num timesteps: 868000
Best mean reward: 0.23 - Last mean reward: -0.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 167       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 106       |
|    time_elapsed         | 29905     |
|    total_timesteps      | 868352    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000195 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 41.3      |
|    n_updates            | 1050      |
|    policy_gradient_loss | -3.36e-10 |
|    value_loss           | 82.5      |
---------------------------------------
Num timesteps: 872000
Best mean reward: 0.23 - Last mean reward: -0.01
Num timesteps: 876000
Best mean reward: 0.23 - Last mean reward: 0.01
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 154       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 107       |
|    time_elapsed         | 30192     |
|    total_timesteps      | 876544    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000195 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 36.7      |
|    n_updates            | 1060      |
|    policy_gradient_loss | 4.57e-10  |
|    value_loss           | 78.6      |
---------------------------------------
Num timesteps: 880000
Best mean reward: 0.23 - Last mean reward: 0.16
Num timesteps: 884000
Best mean reward: 0.23 - Last mean reward: 0.18
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 145       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 108       |
|    time_elapsed         | 30477     |
|    total_timesteps      | 884736    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000195 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 39.7      |
|    n_updates            | 1070      |
|    policy_gradient_loss | -4.52e-10 |
|    value_loss           | 80        |
---------------------------------------
Num timesteps: 888000
Best mean reward: 0.23 - Last mean reward: 0.13
Num timesteps: 892000
Best mean reward: 0.23 - Last mean reward: 0.11
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 160       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 109       |
|    time_elapsed         | 30756     |
|    total_timesteps      | 892928    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000195 |
|    explained_variance   | 1.79e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 36.2      |
|    n_updates            | 1080      |
|    policy_gradient_loss | 4.62e-11  |
|    value_loss           | 84.4      |
---------------------------------------
Num timesteps: 896000
Best mean reward: 0.23 - Last mean reward: 0.09
Num timesteps: 900000
Best mean reward: 0.23 - Last mean reward: 0.08
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 158       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 110       |
|    time_elapsed         | 31037     |
|    total_timesteps      | 901120    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000195 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 42.9      |
|    n_updates            | 1090      |
|    policy_gradient_loss | -5.84e-11 |
|    value_loss           | 88.1      |
---------------------------------------
Num timesteps: 904000
Best mean reward: 0.23 - Last mean reward: 0.08
Num timesteps: 908000
Best mean reward: 0.23 - Last mean reward: 0.07
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | 160           |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 111           |
|    time_elapsed         | 31323         |
|    total_timesteps      | 909312        |
| train/                  |               |
|    approx_kl            | 2.5219451e-05 |
|    clip_fraction        | 0.00011       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000117     |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 57.9          |
|    n_updates            | 1100          |
|    policy_gradient_loss | -2.27e-05     |
|    value_loss           | 86.3          |
-------------------------------------------
Num timesteps: 912000
Best mean reward: 0.23 - Last mean reward: 0.09
Num timesteps: 916000
Best mean reward: 0.23 - Last mean reward: 0.10
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 158       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 112       |
|    time_elapsed         | 31608     |
|    total_timesteps      | 917504    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00011  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 41.1      |
|    n_updates            | 1110      |
|    policy_gradient_loss | -5.41e-10 |
|    value_loss           | 84.7      |
---------------------------------------
Num timesteps: 920000
Best mean reward: 0.23 - Last mean reward: 0.11
Num timesteps: 924000
Best mean reward: 0.23 - Last mean reward: 0.16
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 2.05e+03 |
|    ep_rew_mean          | 156      |
| time/                   |          |
|    fps                  | 29       |
|    iterations           | 113      |
|    time_elapsed         | 31895    |
|    total_timesteps      | 925696   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00011 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 52.5     |
|    n_updates            | 1120     |
|    policy_gradient_loss | 6.15e-10 |
|    value_loss           | 86.8     |
--------------------------------------
Num timesteps: 928000
Best mean reward: 0.23 - Last mean reward: 0.01
Num timesteps: 932000
Best mean reward: 0.23 - Last mean reward: 0.06
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 2.05e+03 |
|    ep_rew_mean          | 162      |
| time/                   |          |
|    fps                  | 29       |
|    iterations           | 114      |
|    time_elapsed         | 32189    |
|    total_timesteps      | 933888   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00011 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 40.5     |
|    n_updates            | 1130     |
|    policy_gradient_loss | -5e-11   |
|    value_loss           | 87.1     |
--------------------------------------
Num timesteps: 936000
Best mean reward: 0.23 - Last mean reward: 0.14
Num timesteps: 940000
Best mean reward: 0.23 - Last mean reward: 0.18
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 161       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 115       |
|    time_elapsed         | 32474     |
|    total_timesteps      | 942080    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00011  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 36.8      |
|    n_updates            | 1140      |
|    policy_gradient_loss | 4.42e-10  |
|    value_loss           | 85.2      |
---------------------------------------
Num timesteps: 944000
Best mean reward: 0.23 - Last mean reward: 0.13
Num timesteps: 948000
Best mean reward: 0.23 - Last mean reward: 0.14
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 172       |
| time/                   |           |
|    fps                  | 29        |
|    iterations           | 116       |
|    time_elapsed         | 32766     |
|    total_timesteps      | 950272    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00011  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 41.4      |
|    n_updates            | 1150      |
|    policy_gradient_loss | -3.19e-10 |
|    value_loss           | 86        |
---------------------------------------
Num timesteps: 952000
Best mean reward: 0.23 - Last mean reward: 0.02
Num timesteps: 956000
Best mean reward: 0.23 - Last mean reward: 0.06
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 178       |
| time/                   |           |
|    fps                  | 28        |
|    iterations           | 117       |
|    time_elapsed         | 33050     |
|    total_timesteps      | 958464    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00011  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 40.3      |
|    n_updates            | 1160      |
|    policy_gradient_loss | -7.79e-11 |
|    value_loss           | 84.4      |
---------------------------------------
Num timesteps: 960000
Best mean reward: 0.23 - Last mean reward: 0.13
Num timesteps: 964000
Best mean reward: 0.23 - Last mean reward: 0.09
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 2.05e+03 |
|    ep_rew_mean          | 181      |
| time/                   |          |
|    fps                  | 28       |
|    iterations           | 118      |
|    time_elapsed         | 33337    |
|    total_timesteps      | 966656   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00011 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 47       |
|    n_updates            | 1170     |
|    policy_gradient_loss | 4.92e-10 |
|    value_loss           | 81.3     |
--------------------------------------
Num timesteps: 968000
Best mean reward: 0.23 - Last mean reward: 0.01
Num timesteps: 972000
Best mean reward: 0.23 - Last mean reward: 0.01
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 2.05e+03 |
|    ep_rew_mean          | 181      |
| time/                   |          |
|    fps                  | 28       |
|    iterations           | 119      |
|    time_elapsed         | 33621    |
|    total_timesteps      | 974848   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00011 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 50.5     |
|    n_updates            | 1180     |
|    policy_gradient_loss | 3.27e-10 |
|    value_loss           | 87       |
--------------------------------------
Num timesteps: 976000
Best mean reward: 0.23 - Last mean reward: 0.07
Num timesteps: 980000
Best mean reward: 0.23 - Last mean reward: 0.14
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 2.05e+03 |
|    ep_rew_mean          | 173      |
| time/                   |          |
|    fps                  | 28       |
|    iterations           | 120      |
|    time_elapsed         | 33909    |
|    total_timesteps      | 983040   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00011 |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.0003   |
|    loss                 | 48.9     |
|    n_updates            | 1190     |
|    policy_gradient_loss | 2.52e-10 |
|    value_loss           | 81.2     |
--------------------------------------
Num timesteps: 984000
Best mean reward: 0.23 - Last mean reward: 0.10
Num timesteps: 988000
Best mean reward: 0.23 - Last mean reward: 0.14
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | 172       |
| time/                   |           |
|    fps                  | 28        |
|    iterations           | 121       |
|    time_elapsed         | 34194     |
|    total_timesteps      | 991232    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00011  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 43.2      |
|    n_updates            | 1200      |
|    policy_gradient_loss | -5.13e-10 |
|    value_loss           | 86.4      |
