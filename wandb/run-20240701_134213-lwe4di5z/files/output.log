
Using cpu device
Num timesteps: 4000
Best mean reward: -inf - Last mean reward: -0.50
Saving new best model to ./logs/best_model
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.get_rewards to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_rewards` for environment variables or `env.get_wrapper_attr('get_rewards')` that will search the reminding wrappers.
  logger.warn(
Num timesteps: 8000
Best mean reward: -0.50 - Last mean reward: -0.50
-----------------------------
| time/              |      |
|    fps             | 224  |
|    iterations      | 1    |
|    time_elapsed    | 36   |
|    total_timesteps | 8192 |
-----------------------------
Num timesteps: 12000
Best mean reward: -0.50 - Last mean reward: -0.49
Saving new best model to ./logs/best_model
Num timesteps: 16000
Best mean reward: -0.49 - Last mean reward: -0.49
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -1.02e+03   |
| time/                   |             |
|    fps                  | 53          |
|    iterations           | 2           |
|    time_elapsed         | 308         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.009612698 |
|    clip_fraction        | 0.0425      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | -0.00319    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0434      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00129    |
|    value_loss           | 1.58        |
-----------------------------------------
Num timesteps: 20000
Best mean reward: -0.49 - Last mean reward: -0.46
Saving new best model to ./logs/best_model
Num timesteps: 24000
Best mean reward: -0.46 - Last mean reward: -0.44
Saving new best model to ./logs/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -1.02e+03   |
| time/                   |             |
|    fps                  | 42          |
|    iterations           | 3           |
|    time_elapsed         | 584         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.009243336 |
|    clip_fraction        | 0.0154      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 7.75e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.464       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.000132   |
|    value_loss           | 5.03        |
-----------------------------------------
Num timesteps: 28000
Best mean reward: -0.44 - Last mean reward: -0.35
Saving new best model to ./logs/best_model
Num timesteps: 32000
Best mean reward: -0.35 - Last mean reward: -0.37
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -984        |
| time/                   |             |
|    fps                  | 38          |
|    iterations           | 4           |
|    time_elapsed         | 859         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.016148444 |
|    clip_fraction        | 0.0754      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 6.49        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00372    |
|    value_loss           | 14.2        |
-----------------------------------------
Num timesteps: 36000
Best mean reward: -0.35 - Last mean reward: -0.32
Saving new best model to ./logs/best_model
Num timesteps: 40000
Best mean reward: -0.32 - Last mean reward: -0.31
Saving new best model to ./logs/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -930        |
| time/                   |             |
|    fps                  | 36          |
|    iterations           | 5           |
|    time_elapsed         | 1132        |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.016846396 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.932      |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 14.7        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00642    |
|    value_loss           | 28.7        |
-----------------------------------------
Num timesteps: 44000
Best mean reward: -0.31 - Last mean reward: -0.19
Saving new best model to ./logs/best_model
Num timesteps: 48000
Best mean reward: -0.19 - Last mean reward: -0.18
Saving new best model to ./logs/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -873        |
| time/                   |             |
|    fps                  | 34          |
|    iterations           | 6           |
|    time_elapsed         | 1408        |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.012371783 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.844      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 16          |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00754    |
|    value_loss           | 39.6        |
-----------------------------------------
Num timesteps: 52000
Best mean reward: -0.18 - Last mean reward: -0.15
Saving new best model to ./logs/best_model
Num timesteps: 56000
Best mean reward: -0.15 - Last mean reward: -0.13
Saving new best model to ./logs/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -785        |
| time/                   |             |
|    fps                  | 33          |
|    iterations           | 7           |
|    time_elapsed         | 1689        |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.008574041 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.748      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 24          |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00748    |
|    value_loss           | 58.6        |
-----------------------------------------
Num timesteps: 60000
Best mean reward: -0.13 - Last mean reward: -0.17
Num timesteps: 64000
Best mean reward: -0.13 - Last mean reward: -0.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -711         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 8            |
|    time_elapsed         | 1966         |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0052118255 |
|    clip_fraction        | 0.0526       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.681       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 35.5         |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00326     |
|    value_loss           | 63.3         |
------------------------------------------
Num timesteps: 68000
Best mean reward: -0.13 - Last mean reward: -0.01
Saving new best model to ./logs/best_model
Num timesteps: 72000
Best mean reward: -0.01 - Last mean reward: -0.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -661        |
| time/                   |             |
|    fps                  | 32          |
|    iterations           | 9           |
|    time_elapsed         | 2253        |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.005537777 |
|    clip_fraction        | 0.067       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 27.7        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00389    |
|    value_loss           | 61.2        |
-----------------------------------------
Num timesteps: 76000
Best mean reward: -0.01 - Last mean reward: -0.10
Num timesteps: 80000
Best mean reward: -0.01 - Last mean reward: -0.09
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -600         |
| time/                   |              |
|    fps                  | 32           |
|    iterations           | 10           |
|    time_elapsed         | 2531         |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.0049073976 |
|    clip_fraction        | 0.0534       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.521       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 30           |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00355     |
|    value_loss           | 71.5         |
------------------------------------------
Num timesteps: 84000
Best mean reward: -0.01 - Last mean reward: -0.11
Num timesteps: 88000
Best mean reward: -0.01 - Last mean reward: -0.10
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -556         |
| time/                   |              |
|    fps                  | 32           |
|    iterations           | 11           |
|    time_elapsed         | 2809         |
|    total_timesteps      | 90112        |
| train/                  |              |
|    approx_kl            | 0.0015152794 |
|    clip_fraction        | 0.0237       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.482       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 41.7         |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00103     |
|    value_loss           | 69.4         |
------------------------------------------
Num timesteps: 92000
Best mean reward: -0.01 - Last mean reward: -0.07
Num timesteps: 96000
Best mean reward: -0.01 - Last mean reward: -0.09
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -523         |
| time/                   |              |
|    fps                  | 31           |
|    iterations           | 12           |
|    time_elapsed         | 3084         |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0015498716 |
|    clip_fraction        | 0.0304       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.468       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 30.7         |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00122     |
|    value_loss           | 69.5         |
------------------------------------------
Num timesteps: 100000
Best mean reward: -0.01 - Last mean reward: -0.00
Saving new best model to ./logs/best_model
Num timesteps: 104000
Best mean reward: -0.00 - Last mean reward: -0.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -495         |
| time/                   |              |
|    fps                  | 31           |
|    iterations           | 13           |
|    time_elapsed         | 3357         |
|    total_timesteps      | 106496       |
| train/                  |              |
|    approx_kl            | 0.0028289973 |
|    clip_fraction        | 0.0345       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.476       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 31.3         |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00144     |
|    value_loss           | 68.2         |
------------------------------------------
Traceback (most recent call last):
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/work/rleap1/rishabh.bhatia/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 76, in <module>
    model.learn(total_timesteps=100000, callback=[WandbCallback(), save_best_callback])
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 313, in learn
    self.train()
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 217, in train
    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 730, in evaluate_actions
    features = self.extract_features(obs)
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/rleap1/rishabh.bhatia/Master-Thesis-GNN/games/model/policy.py", line 40, in forward
    pyg_data = self.encoder.encode(observations)
  File "/work/rleap1/rishabh.bhatia/Master-Thesis-GNN/games/encoder/GraphEncoder.py", line 214, in encode
    atom_features = torch.zeros((2, object_feature_length)).tolist()
KeyboardInterrupt