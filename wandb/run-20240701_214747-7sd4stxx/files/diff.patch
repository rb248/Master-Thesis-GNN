diff --git a/games/__pycache__/__init__.cpython-310.pyc b/games/__pycache__/__init__.cpython-310.pyc
index 347bbb7..1b00a1b 100644
Binary files a/games/__pycache__/__init__.cpython-310.pyc and b/games/__pycache__/__init__.cpython-310.pyc differ
diff --git a/games/encoder/GraphEncoder.py b/games/encoder/GraphEncoder.py
index 436b348..04951d9 100644
--- a/games/encoder/GraphEncoder.py
+++ b/games/encoder/GraphEncoder.py
@@ -769,3 +769,113 @@ class GraphEncoderBreakout:
             data_list.append(data)
 
         return data_list
+
+
+import torch
+import networkx as nx
+from torch_geometric.data import HeteroData, Batch
+from itertools import combinations
+import torch_geometric.utils as pyg_utils
+
+class HeteroGNNEncoderShooting:
+    def __init__(self, obj_type_id: str = "obj", atom_type_id: str = "atom"):
+        self.obj_type_id = obj_type_id
+        self.atom_type_id = atom_type_id
+
+    def encode(self, batch_node_features: torch.Tensor, proximity_threshold: float = 1000) -> Batch:
+        batch_data = []
+        batch_size = batch_node_features.size(0)
+
+        for b in range(batch_size):
+            node_features = batch_node_features[b]
+            num_nodes = node_features.size(0)
+            graph = nx.Graph()
+
+            # Adding object nodes
+            for i in range(num_nodes):
+                graph.add_node(i, type=self.obj_type_id, features=node_features[i].tolist())
+
+            # Adding atom nodes
+            atom_index = num_nodes
+            object_feature_length = node_features.size(1)
+
+            for i, j in combinations(range(num_nodes), 2):
+                if self.is_ball_bullet_pair(node_features[i], node_features[j]):
+                    atom_features = torch.zeros((2, object_feature_length)).tolist()
+                    graph.add_node(atom_index, type=self.atom_type_id, features=atom_features)
+                    graph.add_edge(i, atom_index, position=0)
+                    graph.add_edge(j, atom_index, position=1)
+                    atom_index += 1
+                else:
+                    dist = torch.norm(node_features[i, :2] - node_features[j, :2]).item()
+                    if dist < proximity_threshold:
+                        atom_features = torch.zeros((2, object_feature_length)).tolist()
+                        graph.add_node(atom_index, type=self.atom_type_id, features=atom_features)
+                        graph.add_edge(i, atom_index, position=0)
+                        graph.add_edge(j, atom_index, position=1)
+                        atom_index += 1
+
+            pos = nx.spring_layout(graph)  # positions for all nodes
+            batch_data.append(graph)
+
+        return Batch.from_data_list(self.to_pyg_data(batch_data))
+
+    def is_ball_bullet_pair(self, feature1, feature2):
+        return (feature1[-1] == 0 and feature2[-1] == 1) or (feature1[-1] == 1 and feature2[-1] == 0)
+
+    def to_pyg_data(self, batch_graphs):
+        data_list = []
+
+        for graph in batch_graphs:
+            data = HeteroData()
+            node_index_mapping = defaultdict(dict)
+            obj_features = []
+            atom_features_dict = defaultdict(list)
+            edge_dict = defaultdict(list)
+
+            current_obj_features = []
+            current_atom_features_dict = defaultdict(list)
+
+            for node, attrs in graph.nodes(data=True):
+                node_type = attrs['type']
+                features = torch.tensor(attrs['features'])
+                if node_type == self.obj_type_id:
+                    node_index_mapping[node_type][node] = len(current_obj_features)
+                    current_obj_features.append(features)
+                else:
+                    node_index_mapping[node_type][node] = len(current_atom_features_dict[node_type])
+                    current_atom_features_dict[node_type].append(features)
+
+            if current_obj_features:
+                obj_features.append(torch.stack(current_obj_features))
+            for node_type, features_list in current_atom_features_dict.items():
+                if features_list:
+                    flattened_features = [f.view(-1) for f in features_list]
+                    atom_features_dict[node_type].append(torch.stack(flattened_features))
+
+            if obj_features:
+                data[self.obj_type_id].x = torch.cat(obj_features)
+            for node_type, features_list in atom_features_dict.items():
+                if features_list:
+                    data[node_type].x = torch.cat(features_list)
+
+            for src, dst, attr in graph.edges(data=True):
+                src_type = graph.nodes[src]['type']
+                dst_type = graph.nodes[dst]['type']
+                pos = str(attr['position'])
+                edge_type = (src_type, pos, dst_type)
+
+                src_idx = node_index_mapping[src_type][src]
+                dst_idx = node_index_mapping[dst_type][dst]
+                edge_dict[edge_type].append((src_idx, dst_idx))
+                # Add reverse edges for bidirectionality
+                reverse_edge_type = (dst_type, pos, src_type)
+                edge_dict[reverse_edge_type].append((dst_idx, src_idx))
+
+            for edge_type, edges in edge_dict.items():
+                edge_tensor = torch.tensor(edges, dtype=torch.long).t().contiguous()
+                data[edge_type].edge_index = edge_tensor
+
+            data_list.append(data)
+
+        return data_list
\ No newline at end of file
diff --git a/games/encoder/__pycache__/GraphEncoder.cpython-310.pyc b/games/encoder/__pycache__/GraphEncoder.cpython-310.pyc
index 3dc7f23..cf4fe34 100644
Binary files a/games/encoder/__pycache__/GraphEncoder.cpython-310.pyc and b/games/encoder/__pycache__/GraphEncoder.cpython-310.pyc differ
diff --git a/games/encoder/__pycache__/__init__.cpython-310.pyc b/games/encoder/__pycache__/__init__.cpython-310.pyc
index 7335618..e4059e9 100644
Binary files a/games/encoder/__pycache__/__init__.cpython-310.pyc and b/games/encoder/__pycache__/__init__.cpython-310.pyc differ
diff --git a/games/freeway/__pycache__/__init__.cpython-310.pyc b/games/freeway/__pycache__/__init__.cpython-310.pyc
index ee44eda..8f6a241 100644
Binary files a/games/freeway/__pycache__/__init__.cpython-310.pyc and b/games/freeway/__pycache__/__init__.cpython-310.pyc differ
diff --git a/games/freeway/__pycache__/run_supervised_cnn.cpython-310.pyc b/games/freeway/__pycache__/run_supervised_cnn.cpython-310.pyc
index f8965e6..971bba3 100644
Binary files a/games/freeway/__pycache__/run_supervised_cnn.cpython-310.pyc and b/games/freeway/__pycache__/run_supervised_cnn.cpython-310.pyc differ
diff --git a/games/freeway/__pycache__/run_supervised_gnn.cpython-310.pyc b/games/freeway/__pycache__/run_supervised_gnn.cpython-310.pyc
index 01b141a..331a117 100644
Binary files a/games/freeway/__pycache__/run_supervised_gnn.cpython-310.pyc and b/games/freeway/__pycache__/run_supervised_gnn.cpython-310.pyc differ
diff --git a/games/freeway/freeway_envs/__pycache__/freeway_env.cpython-310.pyc b/games/freeway/freeway_envs/__pycache__/freeway_env.cpython-310.pyc
index cf3f63c..0c3546c 100644
Binary files a/games/freeway/freeway_envs/__pycache__/freeway_env.cpython-310.pyc and b/games/freeway/freeway_envs/__pycache__/freeway_env.cpython-310.pyc differ
diff --git a/games/freeway/freeway_envs/freeway_env.py b/games/freeway/freeway_envs/freeway_env.py
index c17efff..5798f7a 100644
--- a/games/freeway/freeway_envs/freeway_env.py
+++ b/games/freeway/freeway_envs/freeway_env.py
@@ -4,12 +4,7 @@ import numpy as np
 import gymnasium as gym
 from gymnasium import spaces
 import torch
-import networkx as nx
-from torch_geometric.data import HeteroData, Batch
-from collections import defaultdict
-from itertools import combinations
 from stable_baselines3 import PPO
-from stable_baselines3.common.evaluation import evaluate_policy
 
 class FreewayEnv(gym.Env):
     metadata = {'render_modes': ['human', 'rgb_array']}
@@ -20,27 +15,22 @@ class FreewayEnv(gym.Env):
         self.last_time = pygame.time.get_ticks()
         self.render_mode = render_mode
         self.observation_type = observation_type
-        #self.window_width = 800
         self.window_width = 210
-        #self.window_height = 600
         self.window_height = 160
         self.player_width = 5
         self.player_height = 5
         self.car_width = 20
         self.car_height = 20
         self.frame_stack = frame_stack
-
-        #self.lanes = [100, 200, 300, 400, 500, 600, 700]
-        self.lanes = [50,100,150]
+        self.lanes = [50,80,120]
         self.max_cars = 10
-        # Define action and observation space
-        # Actions: 0 - Stay, 1 - Move Up, 2 - Move Down
+
         self.action_space = spaces.Discrete(3)
 
         if observation_type == "pixel":
             self.observation_space = spaces.Box(low=0, high=255, shape=(self.frame_stack, 84, 84), dtype=np.uint8)
         else:
-            self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.max_cars+ len(self.lanes)+1, 7), dtype=np.float32)
+            self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.max_cars + len(self.lanes) + 1, 7), dtype=np.float32)
 
         self.window = pygame.display.set_mode((self.window_width, self.window_height))
         self.background_image = pygame.image.load("games/images/Atari - background.png")
@@ -50,9 +40,10 @@ class FreewayEnv(gym.Env):
         self.car_image = pygame.image.load("games/images/car2.png").convert_alpha()
         self.car_image = pygame.transform.scale(self.car_image, (self.car_width, self.car_height))
         self.frame_buffer = np.zeros((self.frame_stack, 84, 84), dtype=np.uint8)
-
+        self.episode_rewards = []
         self.clock = pygame.time.Clock()
         self.reset()
+
     def seed(self, seed=None):
         self.np_random, seed = gym.utils.seeding.np_random(seed)
         random.seed(seed)
@@ -73,6 +64,7 @@ class FreewayEnv(gym.Env):
         self.done = False
         self.episode_start_time = pygame.time.get_ticks()
         self.frame_buffer = np.zeros((self.frame_stack, 84, 84), dtype=np.uint8)
+        self.episode_rewards = []
         if self.observation_type == "pixel":
             for _ in range(self.frame_stack):
                 self.update_frame_buffer()
@@ -81,36 +73,33 @@ class FreewayEnv(gym.Env):
             return self.get_object_data(), {}
 
     def step(self, action):
-        reward = 0
         reward = -0.5
         current_time = pygame.time.get_ticks()
-        if action == 1:  # Up
+        if action == 1:
             self.player_rect.y = max(0, self.player_rect.y - 5)
-        elif action == 2:  # Down
+        elif action == 2:
             self.player_rect.y = min(self.window_height - self.player_height, self.player_rect.y + 5)
 
         for car in self.cars:
             car['x'] += car['speed']
             if car['x'] > self.window_width:
                 car['x'] = 0
-                car['speed'] = random.randint(1,2)
+                car['speed'] = random.randint(1, 2)
 
-        # Collision detection
         hit = any(self.player_rect.colliderect(pygame.Rect(car['x'], car['lane'], self.car_width, self.car_height)) for car in self.cars)
         if hit:
-            #self.score = -1
             self.player_rect.y = self.window_height - self.player_height - 10
-        
             self.last_time = current_time
-        if current_time - self.episode_start_time >= 60000:  # 60000 milliseconds = 1 minute
+        if current_time - self.episode_start_time >= 60000:
             self.done = True
             
-        if self.player_rect.y <= 0:  # Reached top
-            self.score +=1
-            reward += 10*(len(self.lanes))
-
+        if self.player_rect.y <= 0:
+            self.score += 1
+            reward += 10 * len(self.lanes)
             self.player_rect.y = self.window_height - self.player_height - 10
 
+        self.episode_rewards.append(reward)
+
         if self.observation_type == "pixel":
             self.update_frame_buffer()
             observation = self.get_observation()
@@ -119,9 +108,12 @@ class FreewayEnv(gym.Env):
 
         return observation, reward, self.done, False, {}
 
+    def get_rewards(self):
+        return self.episode_rewards
+
     def update_frame_buffer(self):
         frame = self.render_to_array()
-        grayscale = np.dot(frame[..., :3], [0.2989, 0.5870, 0.1140]).astype(np.uint8)  # Convert to grayscale
+        grayscale = np.dot(frame[..., :3], [0.2989, 0.5870, 0.1140]).astype(np.uint8)
         resized_frame = pygame.transform.scale(pygame.surfarray.make_surface(grayscale), (84, 84))
         frame_array = pygame.surfarray.array3d(resized_frame).transpose(1, 0, 2)[:, :, 0]
 
@@ -139,20 +131,11 @@ class FreewayEnv(gym.Env):
         return self.frame_buffer
 
     def get_object_data(self):
-        objects = [
-            [self.player_rect.x, self.player_rect.y, 0, 0, 1, 0, 0],  # Player
-            
-        ] 
-        # add lanes
+        objects = [[self.player_rect.x, self.player_rect.y, 0, 0, 1, 0, 0]]
         for lane in self.lanes:
-            objects.append([self.window_width//2, lane, 0, 0, 0, 1, 0])
-
-        for i, car in enumerate(self.cars):
+            objects.append([self.window_width // 2, lane, 0, 0, 0, 1, 0])
+        for car in self.cars:
             objects.append([car['x'], car['lane'], car['speed'], 0, 0, 0, 1])
-
-        # while len(objects) < self.max_cars + 10:  # Ensure the list has a constant length
-        #     objects.append([0, 0, 0, 0, 0, 0, 0])
-
         return torch.tensor(objects, dtype=torch.float32)
 
     def render(self, mode='human'):
@@ -165,24 +148,16 @@ class FreewayEnv(gym.Env):
     def close(self):
         pygame.quit()
 
-
 if __name__=="__main__":
     env = FreewayEnv(render_mode='human', observation_type='graph')
+    model = PPO.load("path_to_your_model")
 
-    #model = PPO.load("ppo_freeway_pixel")
-    model = PPO.load("ppo_custom_heterognn")
-
-    # # Evaluate the agent
-    # mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=1, render=True)
-    # print(f"Mean reward: {mean_reward} ± {std_reward}")
-
-    obs,_ = env.reset()
+    obs, _ = env.reset()
     done = False
     total_reward = 0
     while not done:
         action, _ = model.predict(obs)
-        #action = env.action_space.sample()
-        obs, reward, done, _,_ = env.step(action)
+        obs, reward, done, _, _ = env.step(action)
         total_reward += reward
         pygame.time.delay(50)
         env.render()
diff --git a/games/freeway/run_supervised_cnn.py b/games/freeway/run_supervised_cnn.py
index 8d1ce97..d0b898d 100644
--- a/games/freeway/run_supervised_cnn.py
+++ b/games/freeway/run_supervised_cnn.py
@@ -11,10 +11,7 @@ import pygame
 from stable_baselines3.common.vec_env import VecFrameStack
 from stable_baselines3.common.vec_env import DummyVecEnv
 from stable_baselines3.common.evaluation import evaluate_policy
-
-
-
-
+import wandb
 import torch as th
 import torch.nn as nn
 from gymnasium import spaces
@@ -78,12 +75,19 @@ if __name__ == "__main__":
             return env
         return _init
 
-    env = DummyVecEnv([make_env(i) for i in range(num_envs)])
+    #env = DummyVecEnv([make_env(i) for i in range(num_envs)])
+    #model = PPO.load("ppo_freeway_pixel")
     env = FreewayEnv(render_mode='human', observation_type='pixel')
     # env = DummyVecEnv([lambda: env])    
     # env = VecFrameStack(env, n_stack=4)
+    wandb.init(
+        project="cnn_atari_freeway",  # Replace with your project name
+        sync_tensorboard=True,        # Automatically sync SB3 logs with wandb
+        monitor_gym=True,             # Automatically log gym environments
+        save_code=True                # Save the code used for this run
+    )
     
     device = "cuda" if th.cuda.is_available() else "cpu"
-    model = PPO("CnnPolicy", env, verbose=2, device=device)
-    model.learn(total_timesteps=100000)
+    model = PPO("CnnPolicy", env, verbose=2, device=device, )
+    model.learn(total_timesteps=1000000)
     model.save("ppo_freeway_pixel")   
\ No newline at end of file
diff --git a/games/freeway/run_supervised_gnn.py b/games/freeway/run_supervised_gnn.py
index 317183e..d7b3bcf 100644
--- a/games/freeway/run_supervised_gnn.py
+++ b/games/freeway/run_supervised_gnn.py
@@ -2,29 +2,10 @@ import wandb
 from stable_baselines3 import PPO
 from stable_baselines3.common.env_util import make_vec_env
 from wandb.integration.sb3 import WandbCallback
-#from games.model.policy import CustomActorCriticPolicy
 from games.freeway.freeway_envs.freeway_env import FreewayEnv
-from games.model.policy import CustomCNN, CustomHeteroGNN
-import numpy as np
-import pygame
-# #Initialize wandb
-wandb.init(
-    project="gnn_atari_freeway",  # Replace with your project name
-    sync_tensorboard=True,        # Automatically sync SB3 logs with wandb
-    monitor_gym=True,             # Automatically log gym environments
-    save_code=True                # Save the code used for this run
-)
-
-# wandb.init(
-#     project="cnn_g",  # Replace with your project name
-#     sync_tensorboard=True,        # Automatically sync SB3 logs with wandb
-#     monitor_gym=True,             # Automatically log gym environments
-#     save_code=True                # Save the code used for this run
-# )
-
-# Wrap the environment 
-
+from games.model.policy import CustomHeteroGNN
 import os
+import numpy as np
 from stable_baselines3.common.callbacks import BaseCallback
 
 class SaveOnBestTrainingRewardCallback(BaseCallback):
@@ -41,8 +22,11 @@ class SaveOnBestTrainingRewardCallback(BaseCallback):
 
     def _on_step(self) -> bool:
         if self.n_calls % self.check_freq == 0:
-            x, y = self.training_env.get_attr('reward')
-            mean_reward = np.mean(y)
+            rewards = []
+            for idx in range(self.training_env.num_envs):
+                env_rewards = self.training_env.get_attr('get_rewards', indices=idx)[0]()
+                rewards.extend(env_rewards)
+            mean_reward = np.mean(rewards)
             if self.verbose > 0:
                 print(f"Num timesteps: {self.num_timesteps}")
                 print(f"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward: {mean_reward:.2f}")
@@ -54,15 +38,6 @@ class SaveOnBestTrainingRewardCallback(BaseCallback):
                 self.model.save(self.save_path)
 
         return True
-import wandb
-from stable_baselines3 import PPO
-from stable_baselines3.common.env_util import make_vec_env
-from wandb.integration.sb3 import WandbCallback
-#from games.model.policy import CustomActorCriticPolicy
-from games.freeway.freeway_envs.freeway_env import FreewayEnv
-from games.model.policy import CustomCNN, CustomHeteroGNN
-import pygame
-import os
 
 # Initialize wandb
 wandb.init(
@@ -73,7 +48,7 @@ wandb.init(
 )
 
 # Wrap the environment
-env = FreewayEnv(render_mode='human', observation_type='graph')
+env = make_vec_env(lambda: FreewayEnv(render_mode='human', observation_type='graph'), n_envs=4)
 
 policy_kwargs = dict(
     features_extractor_class=CustomHeteroGNN,
@@ -98,4 +73,6 @@ os.makedirs(log_dir, exist_ok=True)
 save_best_callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)
 
 # Train the model with WandbCallback and the custom callback
-model.learn(total_timesteps=1000000, callback=[WandbCallback(), save_best_callback])
+model.learn(total_timesteps=100000, callback=[WandbCallback(), save_best_callback])
+
+model.save("ppo_custom_heterognn_freeway")
\ No newline at end of file
diff --git a/games/model/__pycache__/cnn_model.cpython-310.pyc b/games/model/__pycache__/cnn_model.cpython-310.pyc
index 9e82db6..aaea007 100644
Binary files a/games/model/__pycache__/cnn_model.cpython-310.pyc and b/games/model/__pycache__/cnn_model.cpython-310.pyc differ
diff --git a/games/model/__pycache__/hetero_gnn.cpython-310.pyc b/games/model/__pycache__/hetero_gnn.cpython-310.pyc
index 7853617..5154077 100644
Binary files a/games/model/__pycache__/hetero_gnn.cpython-310.pyc and b/games/model/__pycache__/hetero_gnn.cpython-310.pyc differ
diff --git a/games/model/__pycache__/hetero_message_passing.cpython-310.pyc b/games/model/__pycache__/hetero_message_passing.cpython-310.pyc
index 243e3b4..2d8f804 100644
Binary files a/games/model/__pycache__/hetero_message_passing.cpython-310.pyc and b/games/model/__pycache__/hetero_message_passing.cpython-310.pyc differ
diff --git a/games/model/__pycache__/policy.cpython-310.pyc b/games/model/__pycache__/policy.cpython-310.pyc
index 18a2c62..a8bcf5c 100644
Binary files a/games/model/__pycache__/policy.cpython-310.pyc and b/games/model/__pycache__/policy.cpython-310.pyc differ
Submodule games/pacman contains modified content
diff --git a/games/pacman/pacman_env.py b/games/pacman/pacman_env.py
index e185f02..2556ba7 100644
--- a/games/pacman/pacman_env.py
+++ b/games/pacman/pacman_env.py
@@ -14,8 +14,7 @@ from games.pacman.level import level
 from games.pacman.game import game
 from games.pacman.fruit import fruit
 from games.pacman.config import *
-import networkx as nx
-from games.encoder.GraphEncoder import GraphConverter
+import torch
 
 class PacmanEnv(gym.Env):
     metadata = {'render.modes': ['human', 'rgb_array']}
@@ -211,117 +210,36 @@ class PacmanEnv(gym.Env):
         return self.frame_buffer
 
     
-    def get_graph_data(self):
-        # Initialize a NetworkX graph
-        graph = nx.Graph()
+    def get_object_data(self):
+        max_ghosts = 6  # Maximum number of ghosts
+        max_pellets = 100  # Maximum number of pellets
+        max_power_pellets = 4  # Maximum number of power pellets
 
-        # Define object features and add nodes
+        # Initialize fixed-size arrays for objects
         pacman_features = [self.player.x, self.player.y, self.player.velX, self.player.velY, 1, 0, 0, 0]
-        graph.add_node("pacman", type="object", features=pacman_features)
-
-        ghost_features = [[ghost.x, ghost.y, ghost.velX, ghost.velY,0,1,0,0] for ghost in self.ghosts.values()]
-        for i, features in enumerate(ghost_features):
-            graph.add_node(f"ghost_{i}", type="object", features=features)
-
-        pellet_features = [[pellet[0], pellet[1],0,0,0,0,1,0] for pellet in self.level.GetPelletLocations()]
-        for i, features in enumerate(pellet_features):
-            graph.add_node(f"pellet_{i}", type="object", features=features)
-
-        power_pellet_features = [[pellet[0], pellet[1],0,0,0,0,0,1] for pellet in self.level.GetPowerPelletLocations()]
-        for i, features in enumerate(power_pellet_features):
-            graph.add_node(f"power_pellet_{i}", type="object", features=features)
-
-        # Combine object positions
-        object_positions = {
-            "pacman": pacman_features[:2],
-        }
+        
+        ghost_features = [[0, 0, 0, 0, 0, 0, 0, 0] for _ in range(max_ghosts)]
         for i, ghost in enumerate(self.ghosts.values()):
-            object_positions[f"ghost_{i}"] = ghost_features[i][:2]
-        for i, pellet in enumerate(self.level.GetPelletLocations()):
-            object_positions[f"pellet_{i}"] = pellet_features[i][:2]
-        for i, power_pellet in enumerate(self.level.GetPowerPelletLocations()):
-            object_positions[f"power_pellet_{i}"] = power_pellet_features[i][:2]
-
-        # Proximity threshold for creating atoms
-        #proximity_threshold = self.proximity_threshold
+            if i < max_ghosts:
+                ghost_features[i] = [ghost.x, ghost.y, ghost.velX, ghost.velY, 0, 1, 0, 0]
 
-        # Create atom nodes and edges based on proximity
-        atom_index = len(object_positions)  # Start indexing atoms after all objects
+        pellet_features = [[0, 0, 0, 0, 0, 0, 1, 0] for _ in range(max_pellets)]
+        current_pellets = self.level.GetPelletLocations()
+        for i, pellet in enumerate(current_pellets):
+            if i < max_pellets:
+                pellet_features[i] = [pellet[0], pellet[1], 0, 0, 0, 0, 1, 0]
 
-        # Determine wall proximity around Pac-Man
-        walls = {
-            'up': self.level.CheckIfHitWall(self.player.x, self.player.y - self.player.velY, self.player.y - 1, self.player.x),
-            'down': self.level.CheckIfHitWall(self.player.x, self.player.y + self.player.velY, self.player.y + 1, self.player.x),
-            'left': self.level.CheckIfHitWall(self.player.x - self.player.velX, self.player.y, self.player.y, self.player.x - 1),
-            'right': self.level.CheckIfHitWall(self.player.x + self.player.velX, self.player.y, self.player.y, self.player.x + 1)
-        }
-
-        standard_feature_vector_size = len(pacman_features)
-        empty_feature_vector = [0] *(2* standard_feature_vector_size)
-
-        # Add wall direction atoms and edges for Pac-Man
-        for direction, hit in walls.items():
-            if hit:
-                atom_node = f"Wall{direction.capitalize()}_{atom_index}"
-                graph.add_node(atom_node, type="atom", features=empty_feature_vector, predicate=f"Wall{direction.capitalize()}")
-                graph.add_edge("pacman", atom_node, position=0)
-                atom_index += 1
-
-        # Add distance and direction atoms and edges for ghosts
-        for i, ghost_pos in enumerate(ghost_features):
-            distance = np.linalg.norm(np.array(pacman_features[:2]) - np.array(ghost_pos[:2]))
-            direction = np.array(pacman_features[:2]) - np.array(ghost_pos[:2])
-            
-            atom_node_distance = f"Distance_Pacman_Ghost_{i}_{atom_index}"
-            graph.add_node(atom_node_distance,features=empty_feature_vector, type="atom", predicate="Distance")
-            graph.add_edge("pacman", atom_node_distance, position=0)
-            graph.add_edge(f"ghost_{i}", atom_node_distance, position=1)
-            
-            atom_index += 1
+        power_pellet_features = [[0, 0, 0, 0, 0, 0, 0, 1] for _ in range(max_power_pellets)]
+        current_power_pellets = self.level.GetPowerPelletLocations()
+        for i, power_pellet in enumerate(current_power_pellets):
+            if i < max_power_pellets:
+                power_pellet_features[i] = [power_pellet[0], power_pellet[1], 0, 0, 0, 0, 0, 1]
 
-            if pacman_features[0] == ghost_pos[0]:
-                atom_node_same_row = f"SameRow_Pacman_Ghost_{i}_{atom_index}"
-                graph.add_node(atom_node_same_row, features=empty_feature_vector,type="atom", predicate="SameRow")
-                graph.add_edge("pacman", atom_node_same_row, position=0)
-                graph.add_edge(f"ghost_{i}", atom_node_same_row, position=1)
-                atom_index += 1
+        # Combine all features into a single tensor
+        features = [pacman_features] + ghost_features + pellet_features + power_pellet_features
 
-            if pacman_features[1] == ghost_pos[1]:
-                atom_node_same_column = f"SameColumn_Pacman_Ghost_{i}_{atom_index}"
-                graph.add_node(atom_node_same_column, features=empty_feature_vector, type="atom", predicate="SameColumn")
-                graph.add_edge("pacman", atom_node_same_column, position=0)
-                graph.add_edge(f"ghost_{i}", atom_node_same_column, position=1)
-                atom_index += 1
-
-        # Add distance and direction atoms and edges for pellets
-        for i, pellet_pos in enumerate(pellet_features):
-            distance = np.linalg.norm(np.array(pacman_features[:2]) - np.array(pellet_pos[:2]))
-            
-            atom_node_distance = f"Distance_Pacman_Pellet_{i}_{atom_index}"
-            graph.add_node(atom_node_distance, features=empty_feature_vector,type="atom", predicate="Distance")
-            graph.add_edge("pacman", atom_node_distance, position=0)
-            graph.add_edge(f"pellet_{i}", atom_node_distance, position=1)
-            
-            atom_index += 1
-
-        # Add distance and direction atoms and edges for power pellets
-        for i, power_pellet_pos in enumerate(power_pellet_features):
-            distance = np.linalg.norm(np.array(pacman_features[:2]) - np.array(power_pellet_pos[:2]))
-            
-            atom_node_distance = f"Distance_Pacman_PowerPellet_{i}_{atom_index}"
-            graph.add_node(atom_node_distance,features=empty_feature_vector, type="atom", predicate="Distance")
-            graph.add_edge("pacman", atom_node_distance, position=0)
-            graph.add_edge(f"power_pellet_{i}", atom_node_distance, position=1)
-            
-            atom_index += 1
-
-        # Create a GraphConverter object
-        converter = GraphConverter()
-
-        # Convert the NetworkX graph to a PyG Data object
-        data = converter.to_pyg_data(graph)
-        return data
-    
+        return torch.tensor(features, dtype=torch.float32)
+        
 
     
     def render(self, mode='human'):
diff --git a/games/pong/pong_envs/pong_env.py b/games/pong/pong_envs/pong_env.py
index 2ce3468..faf7f8b 100644
--- a/games/pong/pong_envs/pong_env.py
+++ b/games/pong/pong_envs/pong_env.py
@@ -4,14 +4,12 @@ import pygame
 import numpy as np
 import random
 import torch
-from torch_geometric.data import Data
-import stable_baselines3.common.env_checker
 from skimage.transform import resize
 
 class PongEnvNew(gym.Env):
     metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 60, "observation_types": ["pixel", "graph"]}
 
-    def __init__(self, render_mode='human', observation_type='pixel', paddle_width=5, paddle_height=20, ball_size=5, paddle_speed=5, frame_stack=4):
+    def __init__(self, render_mode=None, observation_type='pixel', paddle_width=5, paddle_height=20, ball_size=5, paddle_speed=10, frame_stack=4):
         pygame.init()
         self.width = 210
         self.height = 160
@@ -30,14 +28,14 @@ class PongEnvNew(gym.Env):
             # Define a generic observation space for graph data
             self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(3, 7), dtype=np.float32)  # Number of objects and feature length
 
-        if self.render_mode == "human" or self.render_mode == "rgb_array":
+        if self.render_mode in ["human", "rgb_array"]:
             self.screen = pygame.display.set_mode((self.width, self.height))
             pygame.display.set_caption("Pong")
         else:
             self.screen = pygame.Surface((self.width, self.height))
         
         self.clock = pygame.time.Clock()
-        self.ai_reaction_time = 10  # milliseconds
+        self.ai_reaction_time = 50  # milliseconds
         self.np_random = None
         self.frame_buffer = np.zeros((self.height, self.width, self.frame_stack), dtype=np.uint8)
         self.proximity_threshold = 50
@@ -48,16 +46,13 @@ class PongEnvNew(gym.Env):
     
     def reset(self, seed=None, options=None):
         super().reset(seed=seed, options=options)
-        if not pygame.display.get_init():
-            pygame.display.init()
-            self.screen = pygame.display.set_mode((self.width, self.height))
         if seed is not None:
             self.seed(seed)  # Seed the RNG for the environment
         self.ball = pygame.Rect(self.width // 2 - self.ball_size // 2, self.height // 2 - self.ball_size // 2, self.ball_size, self.ball_size)
         self.left_paddle = pygame.Rect(20, self.height // 2 - self.paddle_height // 2, self.paddle_width, self.paddle_height)
         self.right_paddle = pygame.Rect(self.width - 20 - self.paddle_width, self.height // 2 - self.paddle_height // 2, self.paddle_width, self.paddle_height)
         self.ai_last_reaction_time = pygame.time.get_ticks()
-        self.ball_speed_x, self.ball_speed_y = 2 * random.choice((1, -1)), 2 * random.choice((1, -1))
+        self.ball_speed_x, self.ball_speed_y = 1 * random.choice((1, -1)), 1 * random.choice((1, -1))
         self.left_player_score = 0
         self.right_player_score = 0
         self.frame_buffer = np.zeros((self.height, self.width, self.frame_stack), dtype=np.uint8)
@@ -72,7 +67,7 @@ class PongEnvNew(gym.Env):
         return self._get_observation(), {}
 
     def render(self):
-        if not self.render_mode in ['human', 'rgb_array']:
+        if self.render_mode not in ['human', 'rgb_array']:
             # If not in a mode that requires rendering, skip the rendering.
             return None
         if not pygame.display.get_init():
@@ -93,10 +88,16 @@ class PongEnvNew(gym.Env):
             print("Pygame error:", e)
             return None
 
+    def render_to_array(self):
+        self.screen.fill((0, 0, 0))
+        pygame.draw.rect(self.screen, (255, 255, 255), self.left_paddle)
+        pygame.draw.rect(self.screen, (255, 255, 255), self.right_paddle)
+        pygame.draw.ellipse(self.screen, (255, 255, 255), self.ball)
+        return pygame.surfarray.array3d(self.screen)
+
     def _get_observation(self):
         if self.observation_type == "pixel":
-            self.render()
-            frame = pygame.surfarray.array3d(pygame.display.get_surface())
+            frame = self.render_to_array() if self.render_mode is None else pygame.surfarray.array3d(self.screen)
             frame = np.transpose(frame, (1, 0, 2))  # Transpose to match (height, width, channels)
             grayscale = np.dot(frame[..., :3], [0.2989, 0.5870, 0.1140]).astype(np.uint8)  # Convert to grayscale
             self.frame_buffer = np.roll(self.frame_buffer, shift=-1, axis=2)
@@ -167,7 +168,7 @@ class PongEnvNew(gym.Env):
             self.ball_speed_x = np.sign(self.ball_speed_x) * speed * np.cos(angle)
             
             # Ensure the ball's speed is within a reasonable range
-            max_speed = 10
+            max_speed = 4
             self.ball_speed_y = max(-max_speed, min(max_speed, self.ball_speed_y))
             
             # Adjust the ball's position to prevent sticking
@@ -189,10 +190,9 @@ class PongEnvNew(gym.Env):
 
         return collision, score
 
-
     def ball_reset(self):
         self.ball.x, self.ball.y = self.width // 2 - self.ball_size // 2, self.height // 2 - self.ball_size // 2
-        self.ball_speed_x, self.ball_speed_y = 2 * random.choice((1, -1)), 2 * random.choice((1, -1))
+        self.ball_speed_x, self.ball_speed_y = 1 * random.choice((1, -1)), 1 * random.choice((1, -1))
 
     def step(self, action):
         if not pygame.display.get_init():
@@ -258,52 +258,15 @@ class PongEnvNew(gym.Env):
         pygame.display.quit()
         pygame.quit()
 
-# if __name__ == "__main__":
-#     env = PongEnvNew(render_mode='human', observation_type='pixel')
-    
-#     env.reset()
-
-#     num_episodes = 100
-#     for i_episode in range(num_episodes):
-#         done = False
-#         try:
-#             while not done:
-#                 action = env.action_space.sample()
-#                 _, _, done, _, _ = env.step(action)
-#                 env.render()
-#                 pygame.time.wait(10)
-#         finally:
-#             env.close()
-
-from stable_baselines3 import PPO
-
-if __name__ == "__main__":
-    # #env = PongEnvNew(render_mode='human', observation_type='pixel')
-    # env = PongEnvNew(render_mode='human', observation_type='pixel')
-    # #model = PPO.load("ppo_pong_custom_cnn")
-    # model = PPO.load("ppo_custom_env")
-    # num_episodes = 100
-
-    # for i_episode in range(num_episodes):
-    #     done = False
-    #     obs, _ = env.reset()  # Reset the environment at the start of each episode
-    #     try:
-    #         while not done:
-    #             action, _ = model.predict(obs)
-    #             obs, _, done, _, _ = env.step(action)
-    #             env.render()
-    #             pygame.time.wait(10)
-    #     except Exception as e:
-    #         print(f"An error occurred: {e}")
-    #         break
-    #     finally:
-    #         env.close()
-    # Create a single instance of the environment for evaluation
-    eval_env = PongEnvNew(render_mode='human', observation_type='pixel')
-    model = PPO.load("ppo_custom_env")
-    # Evaluate the policy
-    from stable_baselines3.common.evaluation import evaluate_policy
-    mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10)
 
-    print(f"Mean reward: {mean_reward} ± {std_reward}")
+if __name__ =="__main__":
+    env = PongEnvNew(render_mode='human', observation_type='pixel')
+    done = False
+    env.reset()
+    while not done:
+        env.render()
+        action = env.action_space.sample()
+        observation, reward, done, _, _ = env.step(action)
+    env.close()
+    print("Game over!")
 
diff --git a/games/shoot/shoot_env.py b/games/shoot/shoot_env.py
index 4ef7c69..08569f8 100644
--- a/games/shoot/shoot_env.py
+++ b/games/shoot/shoot_env.py
@@ -1,21 +1,27 @@
-import gym
-from gym import spaces
+import gymnasium as gym
+from gymnasium import spaces
+from stable_baselines3.common.vec_env import SubprocVecEnv
 import pygame
 import random
 import numpy as np
-import time
+from collections import deque
+from skimage.color import rgb2gray
+from skimage.transform import resize
+from stable_baselines3.common.env_util import make_vec_env
 
 class ShootingEnv(gym.Env):
     metadata = {"render_modes": ["human"], "render_fps": 60}
 
-    def __init__(self):
+    def __init__(self, observation_type='pixel', max_steps=50, frame_stack=4):
         super(ShootingEnv, self).__init__()
-        pygame.init()
         
+        self.observation_type = observation_type
+        self.max_steps = max_steps
+        self.frame_stack = frame_stack
+        self.current_step = 0
+
         # Game window dimensions
         self.WIDTH, self.HEIGHT = 800, 600
-        self.win = pygame.display.set_mode((self.WIDTH, self.HEIGHT))
-        pygame.display.set_caption("Shooting Game")
 
         # Colors
         self.WHITE = (255, 255, 255)
@@ -25,64 +31,117 @@ class ShootingEnv(gym.Env):
         # Shooter settings
         self.shooter_pos = [self.WIDTH // 2, self.HEIGHT - 50]
         self.shooter_speed = 5
-
+        
         # Target settings
         self.target_pos = [random.randint(20, self.WIDTH - 20), random.randint(20, self.HEIGHT // 2)]
         self.target_speed = 2
         self.target_direction = 1
 
         # Bullet settings
-        self.bullet_pos = []
+        self.bullet_pos = [self.shooter_pos[0], self.shooter_pos[1]]  # Initial bullet position
         self.bullet_speed = 10
 
         # Gym spaces
         self.action_space = spaces.Discrete(2)  # 0: do nothing, 1: shoot bullet
-        self.observation_space = spaces.Box(low=0, high=255, shape=(self.HEIGHT, self.WIDTH, 3), dtype=np.uint8)
 
-        self.clock = pygame.time.Clock()
+        if observation_type == 'pixel':
+            self.observation_space = spaces.Box(low=0, high=255, shape=(self.frame_stack, 84, 84), dtype=np.uint8)
+        else:
+            self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(2, 4), dtype=np.float32)
+
+        self.frame_buffer = deque(maxlen=self.frame_stack)
 
-    def reset(self):
+    def seed(self, seed=None):
+        self.np_random, seed = gym.utils.seeding.np_random(seed)
+        return [seed]
+    
+    def reset(self, seed=None, options=None):
+        super().reset(seed=seed, options=options)
+        
+        if not hasattr(self, 'win'):
+            pygame.init()
+            self.win = pygame.display.set_mode((self.WIDTH, self.HEIGHT))
+            pygame.display.set_caption("Shooting Game")
+            self.offscreen_surface = pygame.Surface((self.WIDTH, self.HEIGHT))
+            self.clock = pygame.time.Clock()
+        
+        if seed is not None:
+            self.seed(seed)  # Seed the RNG for the environment
         self.shooter_pos = [self.WIDTH // 2, self.HEIGHT - 50]
         self.target_pos = [random.randint(20, self.WIDTH - 20), random.randint(20, self.HEIGHT // 2)]
-        self.bullet_pos = []
+        self.bullet_pos = [self.shooter_pos[0] + 20, self.shooter_pos[1]]  # Reset bullet position to shooter position
         self.target_direction = 1
-        return self._get_obs()
+        self.current_step = 0
+        self.frame_buffer.clear()
+        if self.observation_type == 'pixel':
+            obs = self._get_obs()
+            for _ in range(self.frame_stack):
+                self.frame_buffer.append(obs)
+            
+            return np.array(self.frame_buffer),{}
+        else:
+            return self._get_object_data(), {}
 
     def step(self, action):
         reward = 0
         done = False
         
-        # Shoot bullet if action is 1
-        if action == 1:
+        # Shoot bullet if action is 1 and there's no bullet currently
+        if action == 1 and self.bullet_pos is None:
             self._shoot_bullet()
         
         # Move target
         self._move_target()
         
-        # Move bullets
-        self._move_bullets()
+        # Move bullet
+        self._move_bullet()
 
-        # Check collision
-        reward, done = self._check_collision()
+        # Check collision or miss
+        reward,done = self._check_collision_or_miss()
+
+        # Increment step counter
+        self.current_step += 1
+        if self.current_step >= self.max_steps:
+            done = True
 
         obs = self._get_obs()
-        return obs, reward, done, {}
+        self.frame_buffer.append(obs)
+        
+        return np.array(self.frame_buffer), reward, done,False, {}
 
     def render(self, mode="human"):
-        self.win.fill(self.BLACK)
-        pygame.draw.circle(self.win, self.RED, self.target_pos, 20)
-        pygame.draw.rect(self.win, self.WHITE, (*self.shooter_pos, 50, 20))
-        for bullet in self.bullet_pos:
-            pygame.draw.rect(self.win, self.WHITE, (*bullet, 10, 5))
-        pygame.display.update()
+        if mode == "human":
+            self._render_on_surface(self.win)
+            pygame.display.update()
 
     def close(self):
         pygame.quit()
 
     def _get_obs(self):
-        self.render()  # Render to the pygame display
-        obs = pygame.surfarray.array3d(pygame.display.get_surface())
-        return np.transpose(obs, (1, 0, 2))  # Transpose to match the shape (height, width, channels)
+        self._render_on_surface(self.offscreen_surface)
+        obs = pygame.surfarray.array3d(self.offscreen_surface)
+        obs = np.transpose(obs, (1, 0, 2))  # Transpose to match the shape (height, width, channels)
+        obs = rgb2gray(obs)  # Convert to grayscale
+        obs = resize(obs, (84, 84), anti_aliasing=True, preserve_range=True).astype(np.uint8)
+        return obs
+
+    def _render_on_surface(self, surface):
+        surface.fill(self.BLACK)
+        pygame.draw.circle(surface, self.RED, self.target_pos, 20)
+        pygame.draw.rect(surface, self.WHITE, (*self.shooter_pos, 50, 20))
+        if self.bullet_pos:
+            pygame.draw.rect(surface, self.WHITE, (*self.bullet_pos, 10, 5))
+
+    def _get_object_data(self):
+        objects = []
+        
+        if self.bullet_pos:
+            objects.append([self.bullet_pos[0], self.bullet_pos[1], self.bullet_speed, 1])  # Bullet
+
+        # Append target
+        objects.append([self.target_pos[0], self.target_pos[1], self.target_speed * self.target_direction, 2])
+
+        return np.array(objects, dtype=np.float32)
 
     def _move_target(self):
         self.target_pos[0] += self.target_speed * self.target_direction
@@ -90,38 +149,39 @@ class ShootingEnv(gym.Env):
             self.target_direction *= -1
 
     def _shoot_bullet(self):
-        self.bullet_pos.append([self.shooter_pos[0] + 20, self.shooter_pos[1]])
+        self.bullet_pos = [self.shooter_pos[0] + 20, self.shooter_pos[1]]
 
-    def _move_bullets(self):
-        for bullet in self.bullet_pos[:]:
-            bullet[1] -= self.bullet_speed
-            if bullet[1] < 0:
-                self.bullet_pos.remove(bullet)
+    def _move_bullet(self):
+        if self.bullet_pos:
+            self.bullet_pos[1] -= self.bullet_speed
+            if self.bullet_pos[1] < 0:
+                self.bullet_pos = None  # Remove the bullet when it goes off-screen
 
-    def _check_collision(self):
-        reward = 0
+    def _check_collision_or_miss(self):
+        reward = -1  # Default to negative reward for missing
         done = False
-        for bullet in self.bullet_pos:
-            if self.target_pos[0] - 20 < bullet[0] < self.target_pos[0] + 20 and self.target_pos[1] - 20 < bullet[1] < self.target_pos[1] + 20:
-                self.shooter_pos = [self.WIDTH // 2, self.HEIGHT - 50]
-                self.target_pos = [random.randint(20, self.WIDTH - 20), random.randint(20, self.HEIGHT // 2)]
-                self.bullet_pos.remove(bullet)
-                reward = 1
-                done = True
-            else:
-                reward = 0
-        return reward, done
 
-# Example usage
-if __name__ == "__main__":
-    env = ShootingEnv()
-    obs = env.reset()
+        if self.bullet_pos:
+            if self.target_pos[0] - 20 < self.bullet_pos[0] < self.target_pos[0] + 20 and self.target_pos[1] - 20 < self.bullet_pos[1] < self.target_pos[1] + 20:
+                # Bullet hits the target
+                reward = 10  # Positive reward for hitting the target
+                self.bullet_pos = None  # Remove the bullet after hitting the target
 
-    done = False
-    while not done:
-        action = env.action_space.sample()  # Random action for illustration
-        obs, reward, done, info = env.step(action)
-        time.sleep(0.1)
-        env.render()
+        return reward, done
+    
 
+if __name__ == "__main__":
+    env = ShootingEnv(observation_type='pixel')
+    env = make_vec_env(lambda: ShootingEnv(observation_type='pixel', max_steps=1000), n_envs=4, vec_env_cls=SubprocVecEnv)
+    env.reset()
+
+    render_mode = False  # Change this to True if you want to render
+
+    for _ in range(100):
+        action = env.action_space.sample()
+        obs, reward, done, _, _ = env.step(action)
+        if render_mode:
+            env.render()
+        if done:
+            env.reset()
     env.close()
diff --git a/ppo_freeway_pixel.zip b/ppo_freeway_pixel.zip
index 72cd310..c2cab28 100644
Binary files a/ppo_freeway_pixel.zip and b/ppo_freeway_pixel.zip differ
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index a7e2754..1d095bd 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20240627_175509-kfssyjeq/logs/debug-internal.log
\ No newline at end of file
+run-20240701_214747-7sd4stxx/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index 78950f7..8eae323 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20240627_175509-kfssyjeq/logs/debug.log
\ No newline at end of file
+run-20240701_214747-7sd4stxx/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 847fe7f..79420bd 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20240627_175509-kfssyjeq
\ No newline at end of file
+run-20240701_214747-7sd4stxx
\ No newline at end of file
