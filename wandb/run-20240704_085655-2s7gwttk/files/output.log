
Using cpu device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 786      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 562      |
|    iterations      | 1        |
|    time_elapsed    | 14       |
|    total_timesteps | 8192     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 777         |
|    ep_rew_mean          | 0.237       |
| time/                   |             |
|    fps                  | 289         |
|    iterations           | 2           |
|    time_elapsed         | 56          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.014718366 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.00397     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0375     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.0126      |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=2.80 +/- 2.54
Episode length: 931.00 +/- 155.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 931         |
|    mean_reward          | 2.8         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.027052253 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.726       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0747     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0512     |
|    value_loss           | 0.00836     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 747      |
|    ep_rew_mean     | 0.452    |
| time/              |          |
|    fps             | 219      |
|    iterations      | 3        |
|    time_elapsed    | 112      |
|    total_timesteps | 24576    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 758        |
|    ep_rew_mean          | 0.975      |
| time/                   |            |
|    fps                  | 209        |
|    iterations           | 4          |
|    time_elapsed         | 156        |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.04378201 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.04      |
|    explained_variance   | 0.826      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.102     |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.076     |
|    value_loss           | 0.0143     |
----------------------------------------
Eval num_timesteps=40000, episode_reward=5.10 +/- 2.42
Episode length: 938.40 +/- 295.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 938         |
|    mean_reward          | 5.1         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.058490053 |
|    clip_fraction        | 0.448       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.996      |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.138      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0847     |
|    value_loss           | 0.0103      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 766      |
|    ep_rew_mean     | 1.08     |
| time/              |          |
|    fps             | 194      |
|    iterations      | 5        |
|    time_elapsed    | 210      |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 779        |
|    ep_rew_mean          | 1.49       |
| time/                   |            |
|    fps                  | 193        |
|    iterations           | 6          |
|    time_elapsed         | 253        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.07482523 |
|    clip_fraction        | 0.489      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.952     |
|    explained_variance   | 0.858      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.13      |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0835    |
|    value_loss           | 0.0136     |
----------------------------------------
Traceback (most recent call last):
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/work/rleap1/rishabh.bhatia/Master-Thesis-GNN/games/pong/run_supervised_cnn.py", line 75, in <module>
    model.learn(total_timesteps=1000000, callback=[WandbCallback(), eval_callback])
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 313, in learn
    self.train()
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 217, in train
    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 740, in evaluate_actions
    entropy = distribution.entropy()
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/distributions.py", line 295, in entropy
    return self.distribution.entropy()
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/torch/distributions/categorical.py", line 146, in entropy
    p_log_p = logits * self.probs
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/torch/distributions/utils.py", line 126, in __get__
    value = self.wrapped(instance)
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/torch/distributions/categorical.py", line 100, in probs
    return logits_to_probs(self.logits)
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/torch/distributions/utils.py", line 89, in logits_to_probs
    return F.softmax(logits, dim=-1)
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/torch/nn/functional.py", line 1885, in softmax
    ret = input.softmax(dim)
KeyboardInterrupt