
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 727      |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 381      |
|    iterations      | 1        |
|    time_elapsed    | 21       |
|    total_timesteps | 8192     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 767         |
|    ep_rew_mean          | 0.474       |
| time/                   |             |
|    fps                  | 442         |
|    iterations           | 2           |
|    time_elapsed         | 37          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.015958153 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.00476     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0253     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.0119      |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=0.40 +/- 3.93
Episode length: 671.20 +/- 114.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 671         |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.032144845 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0785     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0578     |
|    value_loss           | 0.00713     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 726      |
|    ep_rew_mean     | 0.0161   |
| time/              |          |
|    fps             | 460      |
|    iterations      | 3        |
|    time_elapsed    | 53       |
|    total_timesteps | 24576    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 718        |
|    ep_rew_mean          | 0.291      |
| time/                   |            |
|    fps                  | 494        |
|    iterations           | 4          |
|    time_elapsed         | 66         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.04644953 |
|    clip_fraction        | 0.391      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.875      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.115     |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0797    |
|    value_loss           | 0.00982    |
----------------------------------------
Eval num_timesteps=40000, episode_reward=4.20 +/- 1.03
Episode length: 880.40 +/- 282.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 880         |
|    mean_reward          | 4.2         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.056565084 |
|    clip_fraction        | 0.45        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.996      |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.107      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0843     |
|    value_loss           | 0.0148      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 733      |
|    ep_rew_mean     | 0.567    |
| time/              |          |
|    fps             | 484      |
|    iterations      | 5        |
|    time_elapsed    | 84       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 746        |
|    ep_rew_mean          | 0.73       |
| time/                   |            |
|    fps                  | 504        |
|    iterations           | 6          |
|    time_elapsed         | 97         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.07276456 |
|    clip_fraction        | 0.494      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.961     |
|    explained_variance   | 0.873      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.126     |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0876    |
|    value_loss           | 0.014      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 761        |
|    ep_rew_mean          | 0.979      |
| time/                   |            |
|    fps                  | 519        |
|    iterations           | 7          |
|    time_elapsed         | 110        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.09150274 |
|    clip_fraction        | 0.531      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.919     |
|    explained_variance   | 0.835      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.123     |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.087     |
|    value_loss           | 0.0155     |
----------------------------------------
Eval num_timesteps=60000, episode_reward=6.00 +/- 0.95
Episode length: 723.00 +/- 172.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 723        |
|    mean_reward          | 6          |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.10244551 |
|    clip_fraction        | 0.532      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.883     |
|    explained_variance   | 0.87       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.116     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0843    |
|    value_loss           | 0.0145     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 774      |
|    ep_rew_mean     | 1.23     |
| time/              |          |
|    fps             | 518      |
|    iterations      | 8        |
|    time_elapsed    | 126      |
|    total_timesteps | 65536    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 796        |
|    ep_rew_mean          | 1.53       |
| time/                   |            |
|    fps                  | 529        |
|    iterations           | 9          |
|    time_elapsed         | 139        |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.12231885 |
|    clip_fraction        | 0.549      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.863     |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.12      |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0892    |
|    value_loss           | 0.0157     |
----------------------------------------
Eval num_timesteps=80000, episode_reward=5.90 +/- 1.71
Episode length: 760.00 +/- 168.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 760        |
|    mean_reward          | 5.9        |
| time/                   |            |
|    total_timesteps      | 80000      |
| train/                  |            |
|    approx_kl            | 0.13863647 |
|    clip_fraction        | 0.555      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.808     |
|    explained_variance   | 0.851      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.121     |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0854    |
|    value_loss           | 0.0146     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 804      |
|    ep_rew_mean     | 1.71     |
| time/              |          |
|    fps             | 527      |
|    iterations      | 10       |
|    time_elapsed    | 155      |
|    total_timesteps | 81920    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 816       |
|    ep_rew_mean          | 2.15      |
| time/                   |           |
|    fps                  | 536       |
|    iterations           | 11        |
|    time_elapsed         | 167       |
|    total_timesteps      | 90112     |
| train/                  |           |
|    approx_kl            | 0.1600742 |
|    clip_fraction        | 0.551     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.768    |
|    explained_variance   | 0.845     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.101    |
|    n_updates            | 100       |
|    policy_gradient_loss | -0.0836   |
|    value_loss           | 0.0143    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 830        |
|    ep_rew_mean          | 2.5        |
| time/                   |            |
|    fps                  | 542        |
|    iterations           | 12         |
|    time_elapsed         | 181        |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.17163357 |
|    clip_fraction        | 0.557      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.748     |
|    explained_variance   | 0.844      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.133     |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0838    |
|    value_loss           | 0.0115     |
----------------------------------------
Eval num_timesteps=100000, episode_reward=5.30 +/- 1.03
Episode length: 704.00 +/- 97.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 704        |
|    mean_reward          | 5.3        |
| time/                   |            |
|    total_timesteps      | 100000     |
| train/                  |            |
|    approx_kl            | 0.19083071 |
|    clip_fraction        | 0.557      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.712     |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.114     |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0815    |
|    value_loss           | 0.0143     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 847      |
|    ep_rew_mean     | 2.97     |
| time/              |          |
|    fps             | 536      |
|    iterations      | 13       |
|    time_elapsed    | 198      |
|    total_timesteps | 106496   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 871       |
|    ep_rew_mean          | 3.69      |
| time/                   |           |
|    fps                  | 540       |
|    iterations           | 14        |
|    time_elapsed         | 212       |
|    total_timesteps      | 114688    |
| train/                  |           |
|    approx_kl            | 0.1955257 |
|    clip_fraction        | 0.542     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.679    |
|    explained_variance   | 0.741     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.105    |
|    n_updates            | 130       |
|    policy_gradient_loss | -0.0777   |
|    value_loss           | 0.0166    |
---------------------------------------
Eval num_timesteps=120000, episode_reward=6.80 +/- 1.03
Episode length: 766.20 +/- 153.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 766        |
|    mean_reward          | 6.8        |
| time/                   |            |
|    total_timesteps      | 120000     |
| train/                  |            |
|    approx_kl            | 0.22176415 |
|    clip_fraction        | 0.561      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.672     |
|    explained_variance   | 0.765      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.119     |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.081     |
|    value_loss           | 0.0117     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 891      |
|    ep_rew_mean     | 4.04     |
| time/              |          |
|    fps             | 534      |
|    iterations      | 15       |
|    time_elapsed    | 229      |
|    total_timesteps | 122880   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 891        |
|    ep_rew_mean          | 4.57       |
| time/                   |            |
|    fps                  | 538        |
|    iterations           | 16         |
|    time_elapsed         | 243        |
|    total_timesteps      | 131072     |
| train/                  |            |
|    approx_kl            | 0.25878185 |
|    clip_fraction        | 0.54       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.632     |
|    explained_variance   | 0.769      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0886    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0747    |
|    value_loss           | 0.0091     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 918       |
|    ep_rew_mean          | 5.12      |
| time/                   |           |
|    fps                  | 541       |
|    iterations           | 17        |
|    time_elapsed         | 256       |
|    total_timesteps      | 139264    |
| train/                  |           |
|    approx_kl            | 0.2556238 |
|    clip_fraction        | 0.542     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.609    |
|    explained_variance   | 0.759     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0601   |
|    n_updates            | 160       |
|    policy_gradient_loss | -0.0771   |
|    value_loss           | 0.0106    |
---------------------------------------
Eval num_timesteps=140000, episode_reward=6.60 +/- 1.77
Episode length: 898.40 +/- 256.21
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 898        |
|    mean_reward          | 6.6        |
| time/                   |            |
|    total_timesteps      | 140000     |
| train/                  |            |
|    approx_kl            | 0.27349526 |
|    clip_fraction        | 0.535      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.595     |
|    explained_variance   | 0.707      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.113     |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0778    |
|    value_loss           | 0.00906    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 930      |
|    ep_rew_mean     | 5.45     |
| time/              |          |
|    fps             | 538      |
|    iterations      | 18       |
|    time_elapsed    | 273      |
|    total_timesteps | 147456   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 944       |
|    ep_rew_mean          | 5.86      |
| time/                   |           |
|    fps                  | 543       |
|    iterations           | 19        |
|    time_elapsed         | 286       |
|    total_timesteps      | 155648    |
| train/                  |           |
|    approx_kl            | 0.2498454 |
|    clip_fraction        | 0.518     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.566    |
|    explained_variance   | 0.746     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.107    |
|    n_updates            | 180       |
|    policy_gradient_loss | -0.0724   |
|    value_loss           | 0.00878   |
---------------------------------------
Eval num_timesteps=160000, episode_reward=7.60 +/- 1.66
Episode length: 783.60 +/- 263.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 784        |
|    mean_reward          | 7.6        |
| time/                   |            |
|    total_timesteps      | 160000     |
| train/                  |            |
|    approx_kl            | 0.27612877 |
|    clip_fraction        | 0.525      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.559     |
|    explained_variance   | 0.803      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0879    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0731    |
|    value_loss           | 0.00767    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 942      |
|    ep_rew_mean     | 6.08     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 20       |
|    time_elapsed    | 301      |
|    total_timesteps | 163840   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 947        |
|    ep_rew_mean          | 6.44       |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 21         |
|    time_elapsed         | 315        |
|    total_timesteps      | 172032     |
| train/                  |            |
|    approx_kl            | 0.28645664 |
|    clip_fraction        | 0.496      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.522     |
|    explained_variance   | 0.778      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0975    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.068     |
|    value_loss           | 0.0103     |
----------------------------------------
Eval num_timesteps=180000, episode_reward=6.70 +/- 0.81
Episode length: 969.60 +/- 463.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 970        |
|    mean_reward          | 6.7        |
| time/                   |            |
|    total_timesteps      | 180000     |
| train/                  |            |
|    approx_kl            | 0.29903427 |
|    clip_fraction        | 0.493      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.517     |
|    explained_variance   | 0.779      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0999    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0656    |
|    value_loss           | 0.0107     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 941      |
|    ep_rew_mean     | 6.63     |
| time/              |          |
|    fps             | 539      |
|    iterations      | 22       |
|    time_elapsed    | 333      |
|    total_timesteps | 180224   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 958        |
|    ep_rew_mean          | 6.72       |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 23         |
|    time_elapsed         | 345        |
|    total_timesteps      | 188416     |
| train/                  |            |
|    approx_kl            | 0.27300173 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.5       |
|    explained_variance   | 0.753      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0502    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0639    |
|    value_loss           | 0.0116     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 969        |
|    ep_rew_mean          | 6.89       |
| time/                   |            |
|    fps                  | 551        |
|    iterations           | 24         |
|    time_elapsed         | 356        |
|    total_timesteps      | 196608     |
| train/                  |            |
|    approx_kl            | 0.31145334 |
|    clip_fraction        | 0.502      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.512     |
|    explained_variance   | 0.771      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0987    |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0724    |
|    value_loss           | 0.00881    |
----------------------------------------
Eval num_timesteps=200000, episode_reward=7.10 +/- 1.11
Episode length: 800.20 +/- 289.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 800        |
|    mean_reward          | 7.1        |
| time/                   |            |
|    total_timesteps      | 200000     |
| train/                  |            |
|    approx_kl            | 0.31544876 |
|    clip_fraction        | 0.488      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.491     |
|    explained_variance   | 0.762      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0781    |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0631    |
|    value_loss           | 0.0111     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 966      |
|    ep_rew_mean     | 7.04     |
| time/              |          |
|    fps             | 550      |
|    iterations      | 25       |
|    time_elapsed    | 372      |
|    total_timesteps | 204800   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 970       |
|    ep_rew_mean          | 7.11      |
| time/                   |           |
|    fps                  | 554       |
|    iterations           | 26        |
|    time_elapsed         | 383       |
|    total_timesteps      | 212992    |
| train/                  |           |
|    approx_kl            | 0.3192346 |
|    clip_fraction        | 0.494     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.493    |
|    explained_variance   | 0.757     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.106    |
|    n_updates            | 250       |
|    policy_gradient_loss | -0.0647   |
|    value_loss           | 0.0104    |
---------------------------------------
Eval num_timesteps=220000, episode_reward=7.00 +/- 1.34
Episode length: 774.60 +/- 182.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 775        |
|    mean_reward          | 7          |
| time/                   |            |
|    total_timesteps      | 220000     |
| train/                  |            |
|    approx_kl            | 0.33294496 |
|    clip_fraction        | 0.488      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.487     |
|    explained_variance   | 0.787      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0828    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0642    |
|    value_loss           | 0.00959    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 973      |
|    ep_rew_mean     | 7.14     |
| time/              |          |
|    fps             | 552      |
|    iterations      | 27       |
|    time_elapsed    | 400      |
|    total_timesteps | 221184   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 977        |
|    ep_rew_mean          | 7.19       |
| time/                   |            |
|    fps                  | 555        |
|    iterations           | 28         |
|    time_elapsed         | 412        |
|    total_timesteps      | 229376     |
| train/                  |            |
|    approx_kl            | 0.34527802 |
|    clip_fraction        | 0.477      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.467     |
|    explained_variance   | 0.757      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.103     |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0635    |
|    value_loss           | 0.00969    |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 975       |
|    ep_rew_mean          | 7.29      |
| time/                   |           |
|    fps                  | 558       |
|    iterations           | 29        |
|    time_elapsed         | 425       |
|    total_timesteps      | 237568    |
| train/                  |           |
|    approx_kl            | 0.3388225 |
|    clip_fraction        | 0.47      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.456    |
|    explained_variance   | 0.794     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.113    |
|    n_updates            | 280       |
|    policy_gradient_loss | -0.0637   |
|    value_loss           | 0.00691   |
---------------------------------------
Eval num_timesteps=240000, episode_reward=8.30 +/- 1.94
Episode length: 1021.60 +/- 308.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.02e+03   |
|    mean_reward          | 8.3        |
| time/                   |            |
|    total_timesteps      | 240000     |
| train/                  |            |
|    approx_kl            | 0.36541605 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.449     |
|    explained_variance   | 0.776      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0911    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0644    |
|    value_loss           | 0.00827    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 979      |
|    ep_rew_mean     | 7.3      |
| time/              |          |
|    fps             | 554      |
|    iterations      | 30       |
|    time_elapsed    | 443      |
|    total_timesteps | 245760   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 978        |
|    ep_rew_mean          | 7.29       |
| time/                   |            |
|    fps                  | 556        |
|    iterations           | 31         |
|    time_elapsed         | 456        |
|    total_timesteps      | 253952     |
| train/                  |            |
|    approx_kl            | 0.38593504 |
|    clip_fraction        | 0.452      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.415     |
|    explained_variance   | 0.778      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0468    |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0559    |
|    value_loss           | 0.00966    |
----------------------------------------
Eval num_timesteps=260000, episode_reward=5.40 +/- 0.73
Episode length: 772.20 +/- 85.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 772        |
|    mean_reward          | 5.4        |
| time/                   |            |
|    total_timesteps      | 260000     |
| train/                  |            |
|    approx_kl            | 0.35402146 |
|    clip_fraction        | 0.478      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.436     |
|    explained_variance   | 0.808      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0974    |
|    n_updates            | 310        |
|    policy_gradient_loss | -0.0629    |
|    value_loss           | 0.00793    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 985      |
|    ep_rew_mean     | 7.33     |
| time/              |          |
|    fps             | 554      |
|    iterations      | 32       |
|    time_elapsed    | 472      |
|    total_timesteps | 262144   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 961        |
|    ep_rew_mean          | 7.28       |
| time/                   |            |
|    fps                  | 556        |
|    iterations           | 33         |
|    time_elapsed         | 485        |
|    total_timesteps      | 270336     |
| train/                  |            |
|    approx_kl            | 0.37057468 |
|    clip_fraction        | 0.458      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.42      |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0838    |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.0603    |
|    value_loss           | 0.00704    |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 967       |
|    ep_rew_mean          | 7.3       |
| time/                   |           |
|    fps                  | 559       |
|    iterations           | 34        |
|    time_elapsed         | 498       |
|    total_timesteps      | 278528    |
| train/                  |           |
|    approx_kl            | 0.5144111 |
|    clip_fraction        | 0.436     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.409    |
|    explained_variance   | 0.776     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0873   |
|    n_updates            | 330       |
|    policy_gradient_loss | -0.0578   |
|    value_loss           | 0.00856   |
---------------------------------------
Eval num_timesteps=280000, episode_reward=6.80 +/- 1.21
Episode length: 942.40 +/- 420.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 942        |
|    mean_reward          | 6.8        |
| time/                   |            |
|    total_timesteps      | 280000     |
| train/                  |            |
|    approx_kl            | 0.35487443 |
|    clip_fraction        | 0.435      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.4       |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0589    |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0557    |
|    value_loss           | 0.00968    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 951      |
|    ep_rew_mean     | 7.28     |
| time/              |          |
|    fps             | 553      |
|    iterations      | 35       |
|    time_elapsed    | 517      |
|    total_timesteps | 286720   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 944        |
|    ep_rew_mean          | 7.17       |
| time/                   |            |
|    fps                  | 554        |
|    iterations           | 36         |
|    time_elapsed         | 531        |
|    total_timesteps      | 294912     |
| train/                  |            |
|    approx_kl            | 0.41035968 |
|    clip_fraction        | 0.443      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.397     |
|    explained_variance   | 0.772      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0604    |
|    n_updates            | 350        |
|    policy_gradient_loss | -0.0583    |
|    value_loss           | 0.0123     |
----------------------------------------
Eval num_timesteps=300000, episode_reward=6.70 +/- 2.09
Episode length: 991.60 +/- 285.11
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 992      |
|    mean_reward          | 6.7      |
| time/                   |          |
|    total_timesteps      | 300000   |
| train/                  |          |
|    approx_kl            | 0.38909  |
|    clip_fraction        | 0.419    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.374   |
|    explained_variance   | 0.751    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.062   |
|    n_updates            | 360      |
|    policy_gradient_loss | -0.055   |
|    value_loss           | 0.0123   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 957      |
|    ep_rew_mean     | 7.2      |
| time/              |          |
|    fps             | 552      |
|    iterations      | 37       |
|    time_elapsed    | 548      |
|    total_timesteps | 303104   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 962       |
|    ep_rew_mean          | 7.3       |
| time/                   |           |
|    fps                  | 554       |
|    iterations           | 38        |
|    time_elapsed         | 561       |
|    total_timesteps      | 311296    |
| train/                  |           |
|    approx_kl            | 0.3982655 |
|    clip_fraction        | 0.44      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.386    |
|    explained_variance   | 0.748     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0685   |
|    n_updates            | 370       |
|    policy_gradient_loss | -0.0587   |
|    value_loss           | 0.0115    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 954        |
|    ep_rew_mean          | 7.25       |
| time/                   |            |
|    fps                  | 556        |
|    iterations           | 39         |
|    time_elapsed         | 574        |
|    total_timesteps      | 319488     |
| train/                  |            |
|    approx_kl            | 0.40282983 |
|    clip_fraction        | 0.468      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.414     |
|    explained_variance   | 0.75       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0887    |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.0643    |
|    value_loss           | 0.01       |
----------------------------------------
Eval num_timesteps=320000, episode_reward=9.30 +/- 2.48
Episode length: 1307.80 +/- 466.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.31e+03  |
|    mean_reward          | 9.3       |
| time/                   |           |
|    total_timesteps      | 320000    |
| train/                  |           |
|    approx_kl            | 0.4038381 |
|    clip_fraction        | 0.465     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.426    |
|    explained_variance   | 0.734     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0873   |
|    n_updates            | 390       |
|    policy_gradient_loss | -0.063    |
|    value_loss           | 0.0118    |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 954      |
|    ep_rew_mean     | 7.17     |
| time/              |          |
|    fps             | 551      |
|    iterations      | 40       |
|    time_elapsed    | 593      |
|    total_timesteps | 327680   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 961        |
|    ep_rew_mean          | 7.11       |
| time/                   |            |
|    fps                  | 553        |
|    iterations           | 41         |
|    time_elapsed         | 606        |
|    total_timesteps      | 335872     |
| train/                  |            |
|    approx_kl            | 0.44982785 |
|    clip_fraction        | 0.459      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.409     |
|    explained_variance   | 0.794      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.107     |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.0634    |
|    value_loss           | 0.00961    |
----------------------------------------
Eval num_timesteps=340000, episode_reward=8.50 +/- 2.49
Episode length: 949.00 +/- 423.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 949        |
|    mean_reward          | 8.5        |
| time/                   |            |
|    total_timesteps      | 340000     |
| train/                  |            |
|    approx_kl            | 0.35559994 |
|    clip_fraction        | 0.457      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.41      |
|    explained_variance   | 0.767      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0974    |
|    n_updates            | 410        |
|    policy_gradient_loss | -0.0633    |
|    value_loss           | 0.0105     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 949      |
|    ep_rew_mean     | 7.08     |
| time/              |          |
|    fps             | 549      |
|    iterations      | 42       |
|    time_elapsed    | 625      |
|    total_timesteps | 344064   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 928       |
|    ep_rew_mean          | 7.06      |
| time/                   |           |
|    fps                  | 551       |
|    iterations           | 43        |
|    time_elapsed         | 638       |
|    total_timesteps      | 352256    |
| train/                  |           |
|    approx_kl            | 0.3709408 |
|    clip_fraction        | 0.46      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.414    |
|    explained_variance   | 0.785     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0762   |
|    n_updates            | 420       |
|    policy_gradient_loss | -0.0614   |
|    value_loss           | 0.00882   |
---------------------------------------
Eval num_timesteps=360000, episode_reward=9.00 +/- 3.63
Episode length: 1521.60 +/- 649.26
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.52e+03   |
|    mean_reward          | 9          |
| time/                   |            |
|    total_timesteps      | 360000     |
| train/                  |            |
|    approx_kl            | 0.37554228 |
|    clip_fraction        | 0.461      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.423     |
|    explained_variance   | 0.785      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0836    |
|    n_updates            | 430        |
|    policy_gradient_loss | -0.0614    |
|    value_loss           | 0.00915    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 920      |
|    ep_rew_mean     | 7.01     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 44       |
|    time_elapsed    | 661      |
|    total_timesteps | 360448   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 921        |
|    ep_rew_mean          | 7.03       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 45         |
|    time_elapsed         | 674        |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.35696024 |
|    clip_fraction        | 0.459      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.427     |
|    explained_variance   | 0.799      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0746    |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0556    |
|    value_loss           | 0.00847    |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 927       |
|    ep_rew_mean          | 7.14      |
| time/                   |           |
|    fps                  | 548       |
|    iterations           | 46        |
|    time_elapsed         | 686       |
|    total_timesteps      | 376832    |
| train/                  |           |
|    approx_kl            | 0.3911908 |
|    clip_fraction        | 0.465     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.432    |
|    explained_variance   | 0.778     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0641   |
|    n_updates            | 450       |
|    policy_gradient_loss | -0.0652   |
|    value_loss           | 0.00891   |
---------------------------------------
Eval num_timesteps=380000, episode_reward=9.10 +/- 3.40
Episode length: 1215.00 +/- 554.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.22e+03   |
|    mean_reward          | 9.1        |
| time/                   |            |
|    total_timesteps      | 380000     |
| train/                  |            |
|    approx_kl            | 0.37724185 |
|    clip_fraction        | 0.467      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.418     |
|    explained_variance   | 0.788      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0893    |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.0635    |
|    value_loss           | 0.00855    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 901      |
|    ep_rew_mean     | 7.13     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 47       |
|    time_elapsed    | 705      |
|    total_timesteps | 385024   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 894       |
|    ep_rew_mean          | 7.2       |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 48        |
|    time_elapsed         | 718       |
|    total_timesteps      | 393216    |
| train/                  |           |
|    approx_kl            | 0.3763611 |
|    clip_fraction        | 0.466     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.423    |
|    explained_variance   | 0.82      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0963   |
|    n_updates            | 470       |
|    policy_gradient_loss | -0.0614   |
|    value_loss           | 0.00768   |
---------------------------------------
Eval num_timesteps=400000, episode_reward=7.40 +/- 0.73
Episode length: 992.80 +/- 134.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 993       |
|    mean_reward          | 7.4       |
| time/                   |           |
|    total_timesteps      | 400000    |
| train/                  |           |
|    approx_kl            | 0.3812445 |
|    clip_fraction        | 0.457     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.427    |
|    explained_variance   | 0.788     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0684   |
|    n_updates            | 480       |
|    policy_gradient_loss | -0.0609   |
|    value_loss           | 0.00957   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 916      |
|    ep_rew_mean     | 7.29     |
| time/              |          |
|    fps             | 544      |
|    iterations      | 49       |
|    time_elapsed    | 736      |
|    total_timesteps | 401408   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 906        |
|    ep_rew_mean          | 7.21       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 50         |
|    time_elapsed         | 750        |
|    total_timesteps      | 409600     |
| train/                  |            |
|    approx_kl            | 0.37920457 |
|    clip_fraction        | 0.458      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.405     |
|    explained_variance   | 0.774      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0782    |
|    n_updates            | 490        |
|    policy_gradient_loss | -0.0604    |
|    value_loss           | 0.0096     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 918       |
|    ep_rew_mean          | 7.29      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 51        |
|    time_elapsed         | 763       |
|    total_timesteps      | 417792    |
| train/                  |           |
|    approx_kl            | 0.3689574 |
|    clip_fraction        | 0.434     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.394    |
|    explained_variance   | 0.753     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0682   |
|    n_updates            | 500       |
|    policy_gradient_loss | -0.0551   |
|    value_loss           | 0.0107    |
---------------------------------------
Eval num_timesteps=420000, episode_reward=7.30 +/- 2.06
Episode length: 721.20 +/- 330.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 721        |
|    mean_reward          | 7.3        |
| time/                   |            |
|    total_timesteps      | 420000     |
| train/                  |            |
|    approx_kl            | 0.38573045 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.429     |
|    explained_variance   | 0.79       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.106     |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.0624    |
|    value_loss           | 0.00732    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 895      |
|    ep_rew_mean     | 7.28     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 52       |
|    time_elapsed    | 779      |
|    total_timesteps | 425984   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 888       |
|    ep_rew_mean          | 7.22      |
| time/                   |           |
|    fps                  | 548       |
|    iterations           | 53        |
|    time_elapsed         | 791       |
|    total_timesteps      | 434176    |
| train/                  |           |
|    approx_kl            | 0.3606552 |
|    clip_fraction        | 0.464     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.428    |
|    explained_variance   | 0.792     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0727   |
|    n_updates            | 520       |
|    policy_gradient_loss | -0.0616   |
|    value_loss           | 0.00719   |
---------------------------------------
Eval num_timesteps=440000, episode_reward=6.00 +/- 2.74
Episode length: 850.00 +/- 305.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 850        |
|    mean_reward          | 6          |
| time/                   |            |
|    total_timesteps      | 440000     |
| train/                  |            |
|    approx_kl            | 0.35414377 |
|    clip_fraction        | 0.452      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.406     |
|    explained_variance   | 0.792      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0568    |
|    n_updates            | 530        |
|    policy_gradient_loss | -0.0565    |
|    value_loss           | 0.0116     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 888      |
|    ep_rew_mean     | 7.21     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 54       |
|    time_elapsed    | 808      |
|    total_timesteps | 442368   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 888        |
|    ep_rew_mean          | 7.21       |
| time/                   |            |
|    fps                  | 548        |
|    iterations           | 55         |
|    time_elapsed         | 822        |
|    total_timesteps      | 450560     |
| train/                  |            |
|    approx_kl            | 0.38411248 |
|    clip_fraction        | 0.441      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.408     |
|    explained_variance   | 0.776      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0997    |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.0593    |
|    value_loss           | 0.01       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 888        |
|    ep_rew_mean          | 7.12       |
| time/                   |            |
|    fps                  | 549        |
|    iterations           | 56         |
|    time_elapsed         | 834        |
|    total_timesteps      | 458752     |
| train/                  |            |
|    approx_kl            | 0.34606403 |
|    clip_fraction        | 0.446      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.403     |
|    explained_variance   | 0.817      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0416    |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.0595    |
|    value_loss           | 0.00941    |
----------------------------------------
Eval num_timesteps=460000, episode_reward=7.70 +/- 1.33
Episode length: 869.00 +/- 269.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 869       |
|    mean_reward          | 7.7       |
| time/                   |           |
|    total_timesteps      | 460000    |
| train/                  |           |
|    approx_kl            | 0.3609751 |
|    clip_fraction        | 0.434     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.388    |
|    explained_variance   | 0.76      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0838   |
|    n_updates            | 560       |
|    policy_gradient_loss | -0.0579   |
|    value_loss           | 0.0107    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 889      |
|    ep_rew_mean     | 7.07     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 57       |
|    time_elapsed    | 853      |
|    total_timesteps | 466944   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 894        |
|    ep_rew_mean          | 7.04       |
| time/                   |            |
|    fps                  | 548        |
|    iterations           | 58         |
|    time_elapsed         | 866        |
|    total_timesteps      | 475136     |
| train/                  |            |
|    approx_kl            | 0.36282253 |
|    clip_fraction        | 0.462      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.426     |
|    explained_variance   | 0.778      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0908    |
|    n_updates            | 570        |
|    policy_gradient_loss | -0.062     |
|    value_loss           | 0.00937    |
----------------------------------------
Eval num_timesteps=480000, episode_reward=5.90 +/- 1.39
Episode length: 693.20 +/- 182.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 693       |
|    mean_reward          | 5.9       |
| time/                   |           |
|    total_timesteps      | 480000    |
| train/                  |           |
|    approx_kl            | 0.3892971 |
|    clip_fraction        | 0.447     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.393    |
|    explained_variance   | 0.772     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0754   |
|    n_updates            | 580       |
|    policy_gradient_loss | -0.0621   |
|    value_loss           | 0.0126    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 905      |
|    ep_rew_mean     | 7.03     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 59       |
|    time_elapsed    | 883      |
|    total_timesteps | 483328   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 923       |
|    ep_rew_mean          | 6.99      |
| time/                   |           |
|    fps                  | 548       |
|    iterations           | 60        |
|    time_elapsed         | 896       |
|    total_timesteps      | 491520    |
| train/                  |           |
|    approx_kl            | 0.4431786 |
|    clip_fraction        | 0.468     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.406    |
|    explained_variance   | 0.725     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0556   |
|    n_updates            | 590       |
|    policy_gradient_loss | -0.0646   |
|    value_loss           | 0.0113    |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 914      |
|    ep_rew_mean          | 7.01     |
| time/                   |          |
|    fps                  | 549      |
|    iterations           | 61       |
|    time_elapsed         | 908      |
|    total_timesteps      | 499712   |
| train/                  |          |
|    approx_kl            | 0.417502 |
|    clip_fraction        | 0.47     |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.397   |
|    explained_variance   | 0.752    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0599  |
|    n_updates            | 600      |
|    policy_gradient_loss | -0.0625  |
|    value_loss           | 0.0123   |
--------------------------------------
Eval num_timesteps=500000, episode_reward=7.30 +/- 0.51
Episode length: 787.40 +/- 108.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 787        |
|    mean_reward          | 7.3        |
| time/                   |            |
|    total_timesteps      | 500000     |
| train/                  |            |
|    approx_kl            | 0.44810903 |
|    clip_fraction        | 0.465      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.382     |
|    explained_variance   | 0.803      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0644    |
|    n_updates            | 610        |
|    policy_gradient_loss | -0.0613    |
|    value_loss           | 0.00749    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 916      |
|    ep_rew_mean     | 7.08     |
| time/              |          |
|    fps             | 548      |
|    iterations      | 62       |
|    time_elapsed    | 926      |
|    total_timesteps | 507904   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 922        |
|    ep_rew_mean          | 7.13       |
| time/                   |            |
|    fps                  | 548        |
|    iterations           | 63         |
|    time_elapsed         | 940        |
|    total_timesteps      | 516096     |
| train/                  |            |
|    approx_kl            | 0.50018275 |
|    clip_fraction        | 0.457      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.374     |
|    explained_variance   | 0.805      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0661    |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.0626    |
|    value_loss           | 0.00879    |
----------------------------------------
Eval num_timesteps=520000, episode_reward=8.80 +/- 1.40
Episode length: 1069.00 +/- 211.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.07e+03   |
|    mean_reward          | 8.8        |
| time/                   |            |
|    total_timesteps      | 520000     |
| train/                  |            |
|    approx_kl            | 0.41207236 |
|    clip_fraction        | 0.444      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.377     |
|    explained_variance   | 0.803      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0683    |
|    n_updates            | 630        |
|    policy_gradient_loss | -0.0602    |
|    value_loss           | 0.00866    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 928      |
|    ep_rew_mean     | 7.17     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 64       |
|    time_elapsed    | 958      |
|    total_timesteps | 524288   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 951        |
|    ep_rew_mean          | 7.28       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 65         |
|    time_elapsed         | 972        |
|    total_timesteps      | 532480     |
| train/                  |            |
|    approx_kl            | 0.47128597 |
|    clip_fraction        | 0.462      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.393     |
|    explained_variance   | 0.759      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0776    |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.0613    |
|    value_loss           | 0.00925    |
----------------------------------------
Eval num_timesteps=540000, episode_reward=7.20 +/- 1.17
Episode length: 907.20 +/- 261.79
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 907        |
|    mean_reward          | 7.2        |
| time/                   |            |
|    total_timesteps      | 540000     |
| train/                  |            |
|    approx_kl            | 0.42237508 |
|    clip_fraction        | 0.444      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.377     |
|    explained_variance   | 0.725      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0547    |
|    n_updates            | 650        |
|    policy_gradient_loss | -0.0561    |
|    value_loss           | 0.0122     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 970      |
|    ep_rew_mean     | 7.3      |
| time/              |          |
|    fps             | 545      |
|    iterations      | 66       |
|    time_elapsed    | 991      |
|    total_timesteps | 540672   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 996       |
|    ep_rew_mean          | 7.39      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 67        |
|    time_elapsed         | 1005      |
|    total_timesteps      | 548864    |
| train/                  |           |
|    approx_kl            | 0.4342448 |
|    clip_fraction        | 0.446     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.372    |
|    explained_variance   | 0.779     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0797   |
|    n_updates            | 660       |
|    policy_gradient_loss | -0.0607   |
|    value_loss           | 0.00907   |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 7.41       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 68         |
|    time_elapsed         | 1018       |
|    total_timesteps      | 557056     |
| train/                  |            |
|    approx_kl            | 0.42338464 |
|    clip_fraction        | 0.432      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.369     |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0928    |
|    n_updates            | 670        |
|    policy_gradient_loss | -0.0565    |
|    value_loss           | 0.0126     |
----------------------------------------
Eval num_timesteps=560000, episode_reward=8.20 +/- 1.03
Episode length: 921.40 +/- 233.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 921       |
|    mean_reward          | 8.2       |
| time/                   |           |
|    total_timesteps      | 560000    |
| train/                  |           |
|    approx_kl            | 0.4223692 |
|    clip_fraction        | 0.44      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.372    |
|    explained_variance   | 0.769     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0831   |
|    n_updates            | 680       |
|    policy_gradient_loss | -0.0614   |
|    value_loss           | 0.0117    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 7.39     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 69       |
|    time_elapsed    | 1035     |
|    total_timesteps | 565248   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 990        |
|    ep_rew_mean          | 7.4        |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 70         |
|    time_elapsed         | 1049       |
|    total_timesteps      | 573440     |
| train/                  |            |
|    approx_kl            | 0.49625927 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.346     |
|    explained_variance   | 0.796      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.104     |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.0573    |
|    value_loss           | 0.0128     |
----------------------------------------
Eval num_timesteps=580000, episode_reward=9.40 +/- 1.74
Episode length: 1135.20 +/- 333.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.14e+03  |
|    mean_reward          | 9.4       |
| time/                   |           |
|    total_timesteps      | 580000    |
| train/                  |           |
|    approx_kl            | 0.4526176 |
|    clip_fraction        | 0.424     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.344    |
|    explained_variance   | 0.773     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0596   |
|    n_updates            | 700       |
|    policy_gradient_loss | -0.0541   |
|    value_loss           | 0.0105    |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 7.47     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 71       |
|    time_elapsed    | 1069     |
|    total_timesteps | 581632   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 997        |
|    ep_rew_mean          | 7.5        |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 72         |
|    time_elapsed         | 1082       |
|    total_timesteps      | 589824     |
| train/                  |            |
|    approx_kl            | 0.48872292 |
|    clip_fraction        | 0.459      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.39      |
|    explained_variance   | 0.781      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0868    |
|    n_updates            | 710        |
|    policy_gradient_loss | -0.0636    |
|    value_loss           | 0.0103     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 7.67      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 73        |
|    time_elapsed         | 1094      |
|    total_timesteps      | 598016    |
| train/                  |           |
|    approx_kl            | 0.4710434 |
|    clip_fraction        | 0.447     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.369    |
|    explained_variance   | 0.78      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0936   |
|    n_updates            | 720       |
|    policy_gradient_loss | -0.0647   |
|    value_loss           | 0.00817   |
---------------------------------------
Eval num_timesteps=600000, episode_reward=7.40 +/- 2.13
Episode length: 927.80 +/- 328.81
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 928        |
|    mean_reward          | 7.4        |
| time/                   |            |
|    total_timesteps      | 600000     |
| train/                  |            |
|    approx_kl            | 0.48788664 |
|    clip_fraction        | 0.447      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.369     |
|    explained_variance   | 0.799      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0527    |
|    n_updates            | 730        |
|    policy_gradient_loss | -0.0594    |
|    value_loss           | 0.00756    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 989      |
|    ep_rew_mean     | 7.46     |
| time/              |          |
|    fps             | 544      |
|    iterations      | 74       |
|    time_elapsed    | 1113     |
|    total_timesteps | 606208   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 947        |
|    ep_rew_mean          | 7.29       |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 75         |
|    time_elapsed         | 1125       |
|    total_timesteps      | 614400     |
| train/                  |            |
|    approx_kl            | 0.42123058 |
|    clip_fraction        | 0.446      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.372     |
|    explained_variance   | 0.804      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.061     |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.0563    |
|    value_loss           | 0.0088     |
----------------------------------------
Eval num_timesteps=620000, episode_reward=8.20 +/- 1.25
Episode length: 1149.40 +/- 271.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.15e+03   |
|    mean_reward          | 8.2        |
| time/                   |            |
|    total_timesteps      | 620000     |
| train/                  |            |
|    approx_kl            | 0.39756075 |
|    clip_fraction        | 0.457      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.402     |
|    explained_variance   | 0.832      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0808    |
|    n_updates            | 750        |
|    policy_gradient_loss | -0.0619    |
|    value_loss           | 0.00744    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 951      |
|    ep_rew_mean     | 7.3      |
| time/              |          |
|    fps             | 544      |
|    iterations      | 76       |
|    time_elapsed    | 1143     |
|    total_timesteps | 622592   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 929        |
|    ep_rew_mean          | 7.28       |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 77         |
|    time_elapsed         | 1156       |
|    total_timesteps      | 630784     |
| train/                  |            |
|    approx_kl            | 0.35382712 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.399     |
|    explained_variance   | 0.813      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0898    |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.0595    |
|    value_loss           | 0.00768    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 911        |
|    ep_rew_mean          | 7.25       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 78         |
|    time_elapsed         | 1169       |
|    total_timesteps      | 638976     |
| train/                  |            |
|    approx_kl            | 0.38669538 |
|    clip_fraction        | 0.458      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.411     |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0989    |
|    n_updates            | 770        |
|    policy_gradient_loss | -0.0615    |
|    value_loss           | 0.00812    |
----------------------------------------
Eval num_timesteps=640000, episode_reward=7.30 +/- 0.81
Episode length: 834.80 +/- 211.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 835        |
|    mean_reward          | 7.3        |
| time/                   |            |
|    total_timesteps      | 640000     |
| train/                  |            |
|    approx_kl            | 0.44184086 |
|    clip_fraction        | 0.459      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.394     |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0976    |
|    n_updates            | 780        |
|    policy_gradient_loss | -0.0635    |
|    value_loss           | 0.00591    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 899      |
|    ep_rew_mean     | 7.23     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 79       |
|    time_elapsed    | 1187     |
|    total_timesteps | 647168   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 875        |
|    ep_rew_mean          | 7.15       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 80         |
|    time_elapsed         | 1200       |
|    total_timesteps      | 655360     |
| train/                  |            |
|    approx_kl            | 0.45979702 |
|    clip_fraction        | 0.465      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.393     |
|    explained_variance   | 0.816      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0952    |
|    n_updates            | 790        |
|    policy_gradient_loss | -0.0661    |
|    value_loss           | 0.00692    |
----------------------------------------
Eval num_timesteps=660000, episode_reward=7.30 +/- 0.81
Episode length: 846.00 +/- 148.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 846        |
|    mean_reward          | 7.3        |
| time/                   |            |
|    total_timesteps      | 660000     |
| train/                  |            |
|    approx_kl            | 0.38585857 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.355     |
|    explained_variance   | 0.742      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0782    |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.0518    |
|    value_loss           | 0.0145     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 900      |
|    ep_rew_mean     | 7.22     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 81       |
|    time_elapsed    | 1216     |
|    total_timesteps | 663552   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 918        |
|    ep_rew_mean          | 7.34       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 82         |
|    time_elapsed         | 1229       |
|    total_timesteps      | 671744     |
| train/                  |            |
|    approx_kl            | 0.41356823 |
|    clip_fraction        | 0.434      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.375     |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0531    |
|    n_updates            | 810        |
|    policy_gradient_loss | -0.0561    |
|    value_loss           | 0.0098     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 920       |
|    ep_rew_mean          | 7.42      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 83        |
|    time_elapsed         | 1242      |
|    total_timesteps      | 679936    |
| train/                  |           |
|    approx_kl            | 0.4285844 |
|    clip_fraction        | 0.429     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.37     |
|    explained_variance   | 0.834     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0465   |
|    n_updates            | 820       |
|    policy_gradient_loss | -0.0529   |
|    value_loss           | 0.00722   |
---------------------------------------
Eval num_timesteps=680000, episode_reward=8.10 +/- 2.48
Episode length: 1022.20 +/- 464.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.02e+03   |
|    mean_reward          | 8.1        |
| time/                   |            |
|    total_timesteps      | 680000     |
| train/                  |            |
|    approx_kl            | 0.44818175 |
|    clip_fraction        | 0.459      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.394     |
|    explained_variance   | 0.802      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0949    |
|    n_updates            | 830        |
|    policy_gradient_loss | -0.061     |
|    value_loss           | 0.00777    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 892      |
|    ep_rew_mean     | 7.33     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 84       |
|    time_elapsed    | 1259     |
|    total_timesteps | 688128   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 898       |
|    ep_rew_mean          | 7.45      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 85        |
|    time_elapsed         | 1272      |
|    total_timesteps      | 696320    |
| train/                  |           |
|    approx_kl            | 0.5038842 |
|    clip_fraction        | 0.44      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.363    |
|    explained_variance   | 0.812     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0564   |
|    n_updates            | 840       |
|    policy_gradient_loss | -0.0612   |
|    value_loss           | 0.00659   |
---------------------------------------
Eval num_timesteps=700000, episode_reward=8.10 +/- 1.88
Episode length: 954.00 +/- 273.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 954        |
|    mean_reward          | 8.1        |
| time/                   |            |
|    total_timesteps      | 700000     |
| train/                  |            |
|    approx_kl            | 0.41066712 |
|    clip_fraction        | 0.449      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.38      |
|    explained_variance   | 0.883      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0867    |
|    n_updates            | 850        |
|    policy_gradient_loss | -0.0622    |
|    value_loss           | 0.0052     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 913      |
|    ep_rew_mean     | 7.53     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 86       |
|    time_elapsed    | 1290     |
|    total_timesteps | 704512   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 931        |
|    ep_rew_mean          | 7.56       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 87         |
|    time_elapsed         | 1303       |
|    total_timesteps      | 712704     |
| train/                  |            |
|    approx_kl            | 0.41676494 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.379     |
|    explained_variance   | 0.788      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0855    |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.0569    |
|    value_loss           | 0.01       |
----------------------------------------
Eval num_timesteps=720000, episode_reward=6.70 +/- 0.75
Episode length: 821.80 +/- 110.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 822        |
|    mean_reward          | 6.7        |
| time/                   |            |
|    total_timesteps      | 720000     |
| train/                  |            |
|    approx_kl            | 0.40794843 |
|    clip_fraction        | 0.463      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.402     |
|    explained_variance   | 0.802      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0831    |
|    n_updates            | 870        |
|    policy_gradient_loss | -0.0653    |
|    value_loss           | 0.00835    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 919      |
|    ep_rew_mean     | 7.48     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 88       |
|    time_elapsed    | 1320     |
|    total_timesteps | 720896   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 935        |
|    ep_rew_mean          | 7.54       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 89         |
|    time_elapsed         | 1333       |
|    total_timesteps      | 729088     |
| train/                  |            |
|    approx_kl            | 0.42037234 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.373     |
|    explained_variance   | 0.801      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0887    |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.0592    |
|    value_loss           | 0.00999    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 942        |
|    ep_rew_mean          | 7.58       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 90         |
|    time_elapsed         | 1346       |
|    total_timesteps      | 737280     |
| train/                  |            |
|    approx_kl            | 0.43429762 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.401     |
|    explained_variance   | 0.789      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0813    |
|    n_updates            | 890        |
|    policy_gradient_loss | -0.0655    |
|    value_loss           | 0.0099     |
----------------------------------------
Eval num_timesteps=740000, episode_reward=7.60 +/- 1.74
Episode length: 948.60 +/- 432.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 949       |
|    mean_reward          | 7.6       |
| time/                   |           |
|    total_timesteps      | 740000    |
| train/                  |           |
|    approx_kl            | 0.3818912 |
|    clip_fraction        | 0.448     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.392    |
|    explained_variance   | 0.824     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0581   |
|    n_updates            | 900       |
|    policy_gradient_loss | -0.0623   |
|    value_loss           | 0.00628   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 930      |
|    ep_rew_mean     | 7.54     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 91       |
|    time_elapsed    | 1365     |
|    total_timesteps | 745472   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 951        |
|    ep_rew_mean          | 7.64       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 92         |
|    time_elapsed         | 1378       |
|    total_timesteps      | 753664     |
| train/                  |            |
|    approx_kl            | 0.41329038 |
|    clip_fraction        | 0.452      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.407     |
|    explained_variance   | 0.791      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0826    |
|    n_updates            | 910        |
|    policy_gradient_loss | -0.0603    |
|    value_loss           | 0.00765    |
----------------------------------------
Eval num_timesteps=760000, episode_reward=7.00 +/- 1.70
Episode length: 959.80 +/- 415.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 960        |
|    mean_reward          | 7          |
| time/                   |            |
|    total_timesteps      | 760000     |
| train/                  |            |
|    approx_kl            | 0.44022456 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.371     |
|    explained_variance   | 0.759      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0932    |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.0582    |
|    value_loss           | 0.00876    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 930      |
|    ep_rew_mean     | 7.5      |
| time/              |          |
|    fps             | 545      |
|    iterations      | 93       |
|    time_elapsed    | 1397     |
|    total_timesteps | 761856   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 948        |
|    ep_rew_mean          | 7.55       |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 94         |
|    time_elapsed         | 1410       |
|    total_timesteps      | 770048     |
| train/                  |            |
|    approx_kl            | 0.41990283 |
|    clip_fraction        | 0.421      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.376     |
|    explained_variance   | 0.805      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0594    |
|    n_updates            | 930        |
|    policy_gradient_loss | -0.0507    |
|    value_loss           | 0.0117     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 957        |
|    ep_rew_mean          | 7.54       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 95         |
|    time_elapsed         | 1423       |
|    total_timesteps      | 778240     |
| train/                  |            |
|    approx_kl            | 0.35081214 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.398     |
|    explained_variance   | 0.768      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0815    |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.0592    |
|    value_loss           | 0.00892    |
----------------------------------------
Eval num_timesteps=780000, episode_reward=6.60 +/- 2.60
Episode length: 1106.60 +/- 544.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.11e+03   |
|    mean_reward          | 6.6        |
| time/                   |            |
|    total_timesteps      | 780000     |
| train/                  |            |
|    approx_kl            | 0.39104673 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.417     |
|    explained_variance   | 0.806      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0568    |
|    n_updates            | 950        |
|    policy_gradient_loss | -0.0652    |
|    value_loss           | 0.00816    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 940      |
|    ep_rew_mean     | 7.33     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 96       |
|    time_elapsed    | 1441     |
|    total_timesteps | 786432   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 964        |
|    ep_rew_mean          | 7.39       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 97         |
|    time_elapsed         | 1454       |
|    total_timesteps      | 794624     |
| train/                  |            |
|    approx_kl            | 0.38794088 |
|    clip_fraction        | 0.431      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.373     |
|    explained_variance   | 0.787      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.11      |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.0617    |
|    value_loss           | 0.0115     |
----------------------------------------
Eval num_timesteps=800000, episode_reward=8.60 +/- 0.86
Episode length: 1105.20 +/- 308.31
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1.11e+03 |
|    mean_reward          | 8.6      |
| time/                   |          |
|    total_timesteps      | 800000   |
| train/                  |          |
|    approx_kl            | 0.456747 |
|    clip_fraction        | 0.452    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.382   |
|    explained_variance   | 0.786    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0669  |
|    n_updates            | 970      |
|    policy_gradient_loss | -0.0606  |
|    value_loss           | 0.0101   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 957      |
|    ep_rew_mean     | 7.3      |
| time/              |          |
|    fps             | 545      |
|    iterations      | 98       |
|    time_elapsed    | 1471     |
|    total_timesteps | 802816   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 955        |
|    ep_rew_mean          | 7.32       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 99         |
|    time_elapsed         | 1484       |
|    total_timesteps      | 811008     |
| train/                  |            |
|    approx_kl            | 0.43501723 |
|    clip_fraction        | 0.463      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.384     |
|    explained_variance   | 0.807      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0976    |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.0617    |
|    value_loss           | 0.0105     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 974        |
|    ep_rew_mean          | 7.39       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 100        |
|    time_elapsed         | 1496       |
|    total_timesteps      | 819200     |
| train/                  |            |
|    approx_kl            | 0.43133166 |
|    clip_fraction        | 0.455      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.379     |
|    explained_variance   | 0.811      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0956    |
|    n_updates            | 990        |
|    policy_gradient_loss | -0.0622    |
|    value_loss           | 0.00722    |
----------------------------------------
Eval num_timesteps=820000, episode_reward=8.50 +/- 2.66
Episode length: 1139.00 +/- 430.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.14e+03   |
|    mean_reward          | 8.5        |
| time/                   |            |
|    total_timesteps      | 820000     |
| train/                  |            |
|    approx_kl            | 0.48076904 |
|    clip_fraction        | 0.435      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.351     |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0728    |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.0626    |
|    value_loss           | 0.0081     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 956      |
|    ep_rew_mean     | 7.29     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 101      |
|    time_elapsed    | 1516     |
|    total_timesteps | 827392   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 956        |
|    ep_rew_mean          | 7.26       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 102        |
|    time_elapsed         | 1529       |
|    total_timesteps      | 835584     |
| train/                  |            |
|    approx_kl            | 0.44562095 |
|    clip_fraction        | 0.417      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.345     |
|    explained_variance   | 0.815      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0719    |
|    n_updates            | 1010       |
|    policy_gradient_loss | -0.0587    |
|    value_loss           | 0.00908    |
----------------------------------------
Eval num_timesteps=840000, episode_reward=7.80 +/- 0.75
Episode length: 887.20 +/- 103.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 887        |
|    mean_reward          | 7.8        |
| time/                   |            |
|    total_timesteps      | 840000     |
| train/                  |            |
|    approx_kl            | 0.39034438 |
|    clip_fraction        | 0.428      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.354     |
|    explained_variance   | 0.809      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0686    |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.0588    |
|    value_loss           | 0.00848    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 967      |
|    ep_rew_mean     | 7.26     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 103      |
|    time_elapsed    | 1547     |
|    total_timesteps | 843776   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 928        |
|    ep_rew_mean          | 7.14       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 104        |
|    time_elapsed         | 1559       |
|    total_timesteps      | 851968     |
| train/                  |            |
|    approx_kl            | 0.39275408 |
|    clip_fraction        | 0.424      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.359     |
|    explained_variance   | 0.781      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0671    |
|    n_updates            | 1030       |
|    policy_gradient_loss | -0.0537    |
|    value_loss           | 0.0123     |
----------------------------------------
Eval num_timesteps=860000, episode_reward=6.80 +/- 0.75
Episode length: 660.20 +/- 127.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 660       |
|    mean_reward          | 6.8       |
| time/                   |           |
|    total_timesteps      | 860000    |
| train/                  |           |
|    approx_kl            | 0.4441123 |
|    clip_fraction        | 0.442     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.373    |
|    explained_variance   | 0.787     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.109    |
|    n_updates            | 1040      |
|    policy_gradient_loss | -0.0621   |
|    value_loss           | 0.0107    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 940      |
|    ep_rew_mean     | 7.2      |
| time/              |          |
|    fps             | 545      |
|    iterations      | 105      |
|    time_elapsed    | 1576     |
|    total_timesteps | 860160   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 931        |
|    ep_rew_mean          | 7.21       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 106        |
|    time_elapsed         | 1588       |
|    total_timesteps      | 868352     |
| train/                  |            |
|    approx_kl            | 0.47095752 |
|    clip_fraction        | 0.431      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.348     |
|    explained_variance   | 0.802      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0997    |
|    n_updates            | 1050       |
|    policy_gradient_loss | -0.0584    |
|    value_loss           | 0.00697    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 906        |
|    ep_rew_mean          | 7.11       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 107        |
|    time_elapsed         | 1601       |
|    total_timesteps      | 876544     |
| train/                  |            |
|    approx_kl            | 0.36731848 |
|    clip_fraction        | 0.418      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.356     |
|    explained_variance   | 0.827      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.072     |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.059     |
|    value_loss           | 0.00793    |
----------------------------------------
Eval num_timesteps=880000, episode_reward=8.20 +/- 2.27
Episode length: 1077.00 +/- 338.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.08e+03   |
|    mean_reward          | 8.2        |
| time/                   |            |
|    total_timesteps      | 880000     |
| train/                  |            |
|    approx_kl            | 0.45546323 |
|    clip_fraction        | 0.452      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.386     |
|    explained_variance   | 0.813      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0769    |
|    n_updates            | 1070       |
|    policy_gradient_loss | -0.0614    |
|    value_loss           | 0.0108     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 927      |
|    ep_rew_mean     | 7.26     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 108      |
|    time_elapsed    | 1619     |
|    total_timesteps | 884736   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 905        |
|    ep_rew_mean          | 7.28       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 109        |
|    time_elapsed         | 1632       |
|    total_timesteps      | 892928     |
| train/                  |            |
|    approx_kl            | 0.50085735 |
|    clip_fraction        | 0.445      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.369     |
|    explained_variance   | 0.817      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0769    |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.0623    |
|    value_loss           | 0.00887    |
----------------------------------------
Eval num_timesteps=900000, episode_reward=7.50 +/- 1.95
Episode length: 809.60 +/- 292.79
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 810        |
|    mean_reward          | 7.5        |
| time/                   |            |
|    total_timesteps      | 900000     |
| train/                  |            |
|    approx_kl            | 0.49058652 |
|    clip_fraction        | 0.424      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.341     |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0953    |
|    n_updates            | 1090       |
|    policy_gradient_loss | -0.0583    |
|    value_loss           | 0.00934    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 896      |
|    ep_rew_mean     | 7.24     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 110      |
|    time_elapsed    | 1648     |
|    total_timesteps | 901120   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 908       |
|    ep_rew_mean          | 7.28      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 111       |
|    time_elapsed         | 1661      |
|    total_timesteps      | 909312    |
| train/                  |           |
|    approx_kl            | 0.4169811 |
|    clip_fraction        | 0.431     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.354    |
|    explained_variance   | 0.817     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0781   |
|    n_updates            | 1100      |
|    policy_gradient_loss | -0.0578   |
|    value_loss           | 0.00977   |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 921        |
|    ep_rew_mean          | 7.34       |
| time/                   |            |
|    fps                  | 548        |
|    iterations           | 112        |
|    time_elapsed         | 1674       |
|    total_timesteps      | 917504     |
| train/                  |            |
|    approx_kl            | 0.44216383 |
|    clip_fraction        | 0.444      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.371     |
|    explained_variance   | 0.796      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0676    |
|    n_updates            | 1110       |
|    policy_gradient_loss | -0.0619    |
|    value_loss           | 0.00883    |
----------------------------------------
Eval num_timesteps=920000, episode_reward=8.40 +/- 1.16
Episode length: 1079.00 +/- 430.78
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1.08e+03 |
|    mean_reward          | 8.4      |
| time/                   |          |
|    total_timesteps      | 920000   |
| train/                  |          |
|    approx_kl            | 2.321965 |
|    clip_fraction        | 0.43     |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.357   |
|    explained_variance   | 0.82     |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0716  |
|    n_updates            | 1120     |
|    policy_gradient_loss | -0.0592  |
|    value_loss           | 0.00907  |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 921      |
|    ep_rew_mean     | 7.36     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 113      |
|    time_elapsed    | 1691     |
|    total_timesteps | 925696   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 914        |
|    ep_rew_mean          | 7.39       |
| time/                   |            |
|    fps                  | 548        |
|    iterations           | 114        |
|    time_elapsed         | 1703       |
|    total_timesteps      | 933888     |
| train/                  |            |
|    approx_kl            | 0.45400482 |
|    clip_fraction        | 0.439      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.367     |
|    explained_variance   | 0.807      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0639    |
|    n_updates            | 1130       |
|    policy_gradient_loss | -0.0604    |
|    value_loss           | 0.00734    |
----------------------------------------
Eval num_timesteps=940000, episode_reward=7.40 +/- 1.46
Episode length: 986.60 +/- 556.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 987        |
|    mean_reward          | 7.4        |
| time/                   |            |
|    total_timesteps      | 940000     |
| train/                  |            |
|    approx_kl            | 0.42527854 |
|    clip_fraction        | 0.445      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.373     |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0828    |
|    n_updates            | 1140       |
|    policy_gradient_loss | -0.0631    |
|    value_loss           | 0.00982    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 920      |
|    ep_rew_mean     | 7.42     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 115      |
|    time_elapsed    | 1721     |
|    total_timesteps | 942080   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 917        |
|    ep_rew_mean          | 7.42       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 116        |
|    time_elapsed         | 1734       |
|    total_timesteps      | 950272     |
| train/                  |            |
|    approx_kl            | 0.48219615 |
|    clip_fraction        | 0.428      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.348     |
|    explained_variance   | 0.75       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.078     |
|    n_updates            | 1150       |
|    policy_gradient_loss | -0.0587    |
|    value_loss           | 0.00976    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 901        |
|    ep_rew_mean          | 7.33       |
| time/                   |            |
|    fps                  | 548        |
|    iterations           | 117        |
|    time_elapsed         | 1747       |
|    total_timesteps      | 958464     |
| train/                  |            |
|    approx_kl            | 0.39628303 |
|    clip_fraction        | 0.427      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.356     |
|    explained_variance   | 0.836      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0773    |
|    n_updates            | 1160       |
|    policy_gradient_loss | -0.0598    |
|    value_loss           | 0.00927    |
----------------------------------------
Eval num_timesteps=960000, episode_reward=6.70 +/- 0.81
Episode length: 674.60 +/- 160.97
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 675      |
|    mean_reward          | 6.7      |
| time/                   |          |
|    total_timesteps      | 960000   |
| train/                  |          |
|    approx_kl            | 0.504417 |
|    clip_fraction        | 0.44     |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.369   |
|    explained_variance   | 0.804    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0559  |
|    n_updates            | 1170     |
|    policy_gradient_loss | -0.0636  |
|    value_loss           | 0.00805  |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 907      |
|    ep_rew_mean     | 7.4      |
| time/              |          |
|    fps             | 547      |
|    iterations      | 118      |
|    time_elapsed    | 1765     |
|    total_timesteps | 966656   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 907        |
|    ep_rew_mean          | 7.46       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 119        |
|    time_elapsed         | 1778       |
|    total_timesteps      | 974848     |
| train/                  |            |
|    approx_kl            | 0.41507873 |
|    clip_fraction        | 0.413      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.344     |
|    explained_variance   | 0.805      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0576    |
|    n_updates            | 1180       |
|    policy_gradient_loss | -0.0561    |
|    value_loss           | 0.0105     |
----------------------------------------
Eval num_timesteps=980000, episode_reward=8.00 +/- 0.84
Episode length: 1059.80 +/- 266.78
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.06e+03   |
|    mean_reward          | 8          |
| time/                   |            |
|    total_timesteps      | 980000     |
| train/                  |            |
|    approx_kl            | 0.43593365 |
|    clip_fraction        | 0.442      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.374     |
|    explained_variance   | 0.776      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0905    |
|    n_updates            | 1190       |
|    policy_gradient_loss | -0.0577    |
|    value_loss           | 0.00799    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 894      |
|    ep_rew_mean     | 7.34     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 120      |
|    time_elapsed    | 1795     |
|    total_timesteps | 983040   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 888        |
|    ep_rew_mean          | 7.32       |
| time/                   |            |
|    fps                  | 548        |
|    iterations           | 121        |
|    time_elapsed         | 1808       |
|    total_timesteps      | 991232     |
| train/                  |            |
|    approx_kl            | 0.42571938 |
|    clip_fraction        | 0.451      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.374     |
|    explained_variance   | 0.799      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0502    |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.0587    |
|    value_loss           | 0.00895    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 869        |
|    ep_rew_mean          | 7.25       |
| time/                   |            |
|    fps                  | 548        |
|    iterations           | 122        |
|    time_elapsed         | 1821       |
|    total_timesteps      | 999424     |
| train/                  |            |
|    approx_kl            | 0.46716878 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.386     |
|    explained_variance   | 0.803      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0692    |
|    n_updates            | 1210       |
|    policy_gradient_loss | -0.0621    |
|    value_loss           | 0.00824    |
----------------------------------------
Eval num_timesteps=1000000, episode_reward=9.30 +/- 1.03
Episode length: 1179.40 +/- 151.31
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.18e+03   |
|    mean_reward          | 9.3        |
| time/                   |            |
|    total_timesteps      | 1000000    |
| train/                  |            |
|    approx_kl            | 0.44620308 |
|    clip_fraction        | 0.456      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.391     |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0775    |
|    n_updates            | 1220       |
|    policy_gradient_loss | -0.0634    |
|    value_loss           | 0.00702    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 864      |
|    ep_rew_mean     | 7.25     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 123      |
|    time_elapsed    | 1840     |
|    total_timesteps | 1007616  |
---------------------------------