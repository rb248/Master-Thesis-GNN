
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 711      |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    fps             | 1026     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 718        |
|    ep_rew_mean          | 1.25       |
| time/                   |            |
|    fps                  | 782        |
|    iterations           | 2          |
|    time_elapsed         | 20         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.01661573 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.09      |
|    explained_variance   | -0.0277    |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0497    |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0157    |
|    value_loss           | 0.0125     |
----------------------------------------
Eval num_timesteps=20000, episode_reward=2.10 +/- 2.27
Episode length: 872.00 +/- 240.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 872         |
|    mean_reward          | 2.1         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.031102346 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.78        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0929     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0577     |
|    value_loss           | 0.00854     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 721      |
|    ep_rew_mean     | 1.55     |
| time/              |          |
|    fps             | 638      |
|    iterations      | 3        |
|    time_elapsed    | 38       |
|    total_timesteps | 24576    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 757        |
|    ep_rew_mean          | 1.43       |
| time/                   |            |
|    fps                  | 638        |
|    iterations           | 4          |
|    time_elapsed         | 51         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.04875356 |
|    clip_fraction        | 0.401      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.872      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.13      |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0808    |
|    value_loss           | 0.0104     |
----------------------------------------
Eval num_timesteps=40000, episode_reward=6.20 +/- 1.89
Episode length: 1189.60 +/- 290.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.19e+03    |
|    mean_reward          | 6.2         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.063402176 |
|    clip_fraction        | 0.468       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.989      |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.126      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0861     |
|    value_loss           | 0.0129      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 790      |
|    ep_rew_mean     | 1.54     |
| time/              |          |
|    fps             | 581      |
|    iterations      | 5        |
|    time_elapsed    | 70       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 826        |
|    ep_rew_mean          | 1.88       |
| time/                   |            |
|    fps                  | 583        |
|    iterations           | 6          |
|    time_elapsed         | 84         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.08193833 |
|    clip_fraction        | 0.516      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.941     |
|    explained_variance   | 0.809      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.123     |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0908    |
|    value_loss           | 0.0139     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 834        |
|    ep_rew_mean          | 2.28       |
| time/                   |            |
|    fps                  | 588        |
|    iterations           | 7          |
|    time_elapsed         | 97         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.10184965 |
|    clip_fraction        | 0.551      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.892     |
|    explained_variance   | 0.819      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.127     |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.092     |
|    value_loss           | 0.014      |
----------------------------------------
Eval num_timesteps=60000, episode_reward=6.70 +/- 1.12
Episode length: 828.40 +/- 271.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 828        |
|    mean_reward          | 6.7        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.12327275 |
|    clip_fraction        | 0.551      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.841     |
|    explained_variance   | 0.781      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.138     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0894    |
|    value_loss           | 0.0145     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 867      |
|    ep_rew_mean     | 2.68     |
| time/              |          |
|    fps             | 571      |
|    iterations      | 8        |
|    time_elapsed    | 114      |
|    total_timesteps | 65536    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 859        |
|    ep_rew_mean          | 3.12       |
| time/                   |            |
|    fps                  | 575        |
|    iterations           | 9          |
|    time_elapsed         | 128        |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.13485205 |
|    clip_fraction        | 0.556      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.822     |
|    explained_variance   | 0.766      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.108     |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0878    |
|    value_loss           | 0.0121     |
----------------------------------------
Eval num_timesteps=80000, episode_reward=8.30 +/- 2.29
Episode length: 1037.40 +/- 336.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.04e+03   |
|    mean_reward          | 8.3        |
| time/                   |            |
|    total_timesteps      | 80000      |
| train/                  |            |
|    approx_kl            | 0.13947679 |
|    clip_fraction        | 0.55       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.778     |
|    explained_variance   | 0.74       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.112     |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0801    |
|    value_loss           | 0.0138     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 864      |
|    ep_rew_mean     | 3.39     |
| time/              |          |
|    fps             | 554      |
|    iterations      | 10       |
|    time_elapsed    | 147      |
|    total_timesteps | 81920    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 863        |
|    ep_rew_mean          | 3.73       |
| time/                   |            |
|    fps                  | 559        |
|    iterations           | 11         |
|    time_elapsed         | 161        |
|    total_timesteps      | 90112      |
| train/                  |            |
|    approx_kl            | 0.15560427 |
|    clip_fraction        | 0.543      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.753     |
|    explained_variance   | 0.772      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.1       |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0817    |
|    value_loss           | 0.0116     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 893       |
|    ep_rew_mean          | 4.04      |
| time/                   |           |
|    fps                  | 561       |
|    iterations           | 12        |
|    time_elapsed         | 174       |
|    total_timesteps      | 98304     |
| train/                  |           |
|    approx_kl            | 0.1734725 |
|    clip_fraction        | 0.534     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.707    |
|    explained_variance   | 0.764     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0989   |
|    n_updates            | 110       |
|    policy_gradient_loss | -0.0763   |
|    value_loss           | 0.0111    |
---------------------------------------
Eval num_timesteps=100000, episode_reward=8.40 +/- 1.85
Episode length: 1057.80 +/- 323.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.06e+03   |
|    mean_reward          | 8.4        |
| time/                   |            |
|    total_timesteps      | 100000     |
| train/                  |            |
|    approx_kl            | 0.20057735 |
|    clip_fraction        | 0.556      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.71      |
|    explained_variance   | 0.79       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.133     |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0836    |
|    value_loss           | 0.0078     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 908      |
|    ep_rew_mean     | 4.63     |
| time/              |          |
|    fps             | 552      |
|    iterations      | 13       |
|    time_elapsed    | 192      |
|    total_timesteps | 106496   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 934        |
|    ep_rew_mean          | 5.24       |
| time/                   |            |
|    fps                  | 556        |
|    iterations           | 14         |
|    time_elapsed         | 206        |
|    total_timesteps      | 114688     |
| train/                  |            |
|    approx_kl            | 0.22160977 |
|    clip_fraction        | 0.539      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.661     |
|    explained_variance   | 0.731      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0926    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0786    |
|    value_loss           | 0.00989    |
----------------------------------------
Eval num_timesteps=120000, episode_reward=8.30 +/- 3.08
Episode length: 920.20 +/- 470.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 920       |
|    mean_reward          | 8.3       |
| time/                   |           |
|    total_timesteps      | 120000    |
| train/                  |           |
|    approx_kl            | 0.2239145 |
|    clip_fraction        | 0.537     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.64     |
|    explained_variance   | 0.786     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0951   |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.0791   |
|    value_loss           | 0.00785   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 970      |
|    ep_rew_mean     | 5.79     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 15       |
|    time_elapsed    | 225      |
|    total_timesteps | 122880   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 983        |
|    ep_rew_mean          | 6.21       |
| time/                   |            |
|    fps                  | 548        |
|    iterations           | 16         |
|    time_elapsed         | 238        |
|    total_timesteps      | 131072     |
| train/                  |            |
|    approx_kl            | 0.24081942 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.592     |
|    explained_variance   | 0.697      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0869    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0675    |
|    value_loss           | 0.0123     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 998        |
|    ep_rew_mean          | 6.82       |
| time/                   |            |
|    fps                  | 552        |
|    iterations           | 17         |
|    time_elapsed         | 251        |
|    total_timesteps      | 139264     |
| train/                  |            |
|    approx_kl            | 0.25239265 |
|    clip_fraction        | 0.507      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.575     |
|    explained_variance   | 0.777      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.114     |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.069     |
|    value_loss           | 0.00805    |
----------------------------------------
Eval num_timesteps=140000, episode_reward=7.70 +/- 0.98
Episode length: 896.40 +/- 264.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 896        |
|    mean_reward          | 7.7        |
| time/                   |            |
|    total_timesteps      | 140000     |
| train/                  |            |
|    approx_kl            | 0.25535405 |
|    clip_fraction        | 0.519      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.59      |
|    explained_variance   | 0.772      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.099     |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0723    |
|    value_loss           | 0.00695    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 968      |
|    ep_rew_mean     | 7.08     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 18       |
|    time_elapsed    | 269      |
|    total_timesteps | 147456   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 969        |
|    ep_rew_mean          | 7.38       |
| time/                   |            |
|    fps                  | 551        |
|    iterations           | 19         |
|    time_elapsed         | 282        |
|    total_timesteps      | 155648     |
| train/                  |            |
|    approx_kl            | 0.27570334 |
|    clip_fraction        | 0.52       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.59      |
|    explained_variance   | 0.804      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.113     |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.071     |
|    value_loss           | 0.00757    |
----------------------------------------
Eval num_timesteps=160000, episode_reward=7.80 +/- 1.08
Episode length: 1110.40 +/- 222.34
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.11e+03   |
|    mean_reward          | 7.8        |
| time/                   |            |
|    total_timesteps      | 160000     |
| train/                  |            |
|    approx_kl            | 0.25966698 |
|    clip_fraction        | 0.514      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.55      |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.107     |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0699    |
|    value_loss           | 0.00528    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 949      |
|    ep_rew_mean     | 7.5      |
| time/              |          |
|    fps             | 544      |
|    iterations      | 20       |
|    time_elapsed    | 300      |
|    total_timesteps | 163840   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 953        |
|    ep_rew_mean          | 7.58       |
| time/                   |            |
|    fps                  | 548        |
|    iterations           | 21         |
|    time_elapsed         | 313        |
|    total_timesteps      | 172032     |
| train/                  |            |
|    approx_kl            | 0.29882258 |
|    clip_fraction        | 0.497      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.53      |
|    explained_variance   | 0.807      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0866    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.07      |
|    value_loss           | 0.00767    |
----------------------------------------
Eval num_timesteps=180000, episode_reward=9.60 +/- 1.71
Episode length: 1537.60 +/- 734.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.54e+03   |
|    mean_reward          | 9.6        |
| time/                   |            |
|    total_timesteps      | 180000     |
| train/                  |            |
|    approx_kl            | 0.28019127 |
|    clip_fraction        | 0.502      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.541     |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0913    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0691    |
|    value_loss           | 0.00769    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 955      |
|    ep_rew_mean     | 7.56     |
| time/              |          |
|    fps             | 538      |
|    iterations      | 22       |
|    time_elapsed    | 334      |
|    total_timesteps | 180224   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 972        |
|    ep_rew_mean          | 7.67       |
| time/                   |            |
|    fps                  | 541        |
|    iterations           | 23         |
|    time_elapsed         | 347        |
|    total_timesteps      | 188416     |
| train/                  |            |
|    approx_kl            | 0.29401565 |
|    clip_fraction        | 0.484      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.518     |
|    explained_variance   | 0.758      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.102     |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.064     |
|    value_loss           | 0.0117     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 996        |
|    ep_rew_mean          | 7.78       |
| time/                   |            |
|    fps                  | 544        |
|    iterations           | 24         |
|    time_elapsed         | 360        |
|    total_timesteps      | 196608     |
| train/                  |            |
|    approx_kl            | 0.27953053 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.523     |
|    explained_variance   | 0.744      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.112     |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0708    |
|    value_loss           | 0.00609    |
----------------------------------------
Eval num_timesteps=200000, episode_reward=8.20 +/- 2.79
Episode length: 1112.20 +/- 323.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.11e+03  |
|    mean_reward          | 8.2       |
| time/                   |           |
|    total_timesteps      | 200000    |
| train/                  |           |
|    approx_kl            | 0.2990119 |
|    clip_fraction        | 0.497     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.512    |
|    explained_variance   | 0.782     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0911   |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.067    |
|    value_loss           | 0.0083    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 972      |
|    ep_rew_mean     | 7.63     |
| time/              |          |
|    fps             | 541      |
|    iterations      | 25       |
|    time_elapsed    | 378      |
|    total_timesteps | 204800   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 970       |
|    ep_rew_mean          | 7.54      |
| time/                   |           |
|    fps                  | 544       |
|    iterations           | 26        |
|    time_elapsed         | 391       |
|    total_timesteps      | 212992    |
| train/                  |           |
|    approx_kl            | 0.3250377 |
|    clip_fraction        | 0.498     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.484    |
|    explained_variance   | 0.787     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.12     |
|    n_updates            | 250       |
|    policy_gradient_loss | -0.0634   |
|    value_loss           | 0.00936   |
---------------------------------------
Eval num_timesteps=220000, episode_reward=7.80 +/- 0.98
Episode length: 929.40 +/- 259.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 929        |
|    mean_reward          | 7.8        |
| time/                   |            |
|    total_timesteps      | 220000     |
| train/                  |            |
|    approx_kl            | 0.32917488 |
|    clip_fraction        | 0.501      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.492     |
|    explained_variance   | 0.809      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0713    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.067     |
|    value_loss           | 0.00752    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 970      |
|    ep_rew_mean     | 7.54     |
| time/              |          |
|    fps             | 541      |
|    iterations      | 27       |
|    time_elapsed    | 408      |
|    total_timesteps | 221184   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 968        |
|    ep_rew_mean          | 7.62       |
| time/                   |            |
|    fps                  | 544        |
|    iterations           | 28         |
|    time_elapsed         | 421        |
|    total_timesteps      | 229376     |
| train/                  |            |
|    approx_kl            | 0.33112416 |
|    clip_fraction        | 0.509      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.499     |
|    explained_variance   | 0.758      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0634    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0678    |
|    value_loss           | 0.00741    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 968        |
|    ep_rew_mean          | 7.53       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 29         |
|    time_elapsed         | 434        |
|    total_timesteps      | 237568     |
| train/                  |            |
|    approx_kl            | 0.33560997 |
|    clip_fraction        | 0.492      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.481     |
|    explained_variance   | 0.774      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.076     |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0657    |
|    value_loss           | 0.00899    |
----------------------------------------
Eval num_timesteps=240000, episode_reward=7.80 +/- 2.34
Episode length: 867.80 +/- 372.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 868       |
|    mean_reward          | 7.8       |
| time/                   |           |
|    total_timesteps      | 240000    |
| train/                  |           |
|    approx_kl            | 0.4076597 |
|    clip_fraction        | 0.489     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.454    |
|    explained_variance   | 0.762     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0836   |
|    n_updates            | 290       |
|    policy_gradient_loss | -0.0635   |
|    value_loss           | 0.0101    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 998      |
|    ep_rew_mean     | 7.64     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 30       |
|    time_elapsed    | 452      |
|    total_timesteps | 245760   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.01e+03  |
|    ep_rew_mean          | 7.68      |
| time/                   |           |
|    fps                  | 545       |
|    iterations           | 31        |
|    time_elapsed         | 465       |
|    total_timesteps      | 253952    |
| train/                  |           |
|    approx_kl            | 0.3762772 |
|    clip_fraction        | 0.484     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.441    |
|    explained_variance   | 0.759     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.113    |
|    n_updates            | 300       |
|    policy_gradient_loss | -0.063    |
|    value_loss           | 0.00682   |
---------------------------------------
Eval num_timesteps=260000, episode_reward=6.50 +/- 2.28
Episode length: 851.00 +/- 268.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 851        |
|    mean_reward          | 6.5        |
| time/                   |            |
|    total_timesteps      | 260000     |
| train/                  |            |
|    approx_kl            | 0.37675777 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.438     |
|    explained_variance   | 0.77       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0434    |
|    n_updates            | 310        |
|    policy_gradient_loss | -0.0629    |
|    value_loss           | 0.0075     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 7.66     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 32       |
|    time_elapsed    | 482      |
|    total_timesteps | 262144   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.03e+03   |
|    ep_rew_mean          | 7.62       |
| time/                   |            |
|    fps                  | 544        |
|    iterations           | 33         |
|    time_elapsed         | 496        |
|    total_timesteps      | 270336     |
| train/                  |            |
|    approx_kl            | 0.41081676 |
|    clip_fraction        | 0.462      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.422     |
|    explained_variance   | 0.756      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0727    |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.0607    |
|    value_loss           | 0.00903    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.04e+03   |
|    ep_rew_mean          | 7.68       |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 34         |
|    time_elapsed         | 510        |
|    total_timesteps      | 278528     |
| train/                  |            |
|    approx_kl            | 0.42395627 |
|    clip_fraction        | 0.442      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.388     |
|    explained_variance   | 0.784      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0649    |
|    n_updates            | 330        |
|    policy_gradient_loss | -0.0601    |
|    value_loss           | 0.00814    |
----------------------------------------
Eval num_timesteps=280000, episode_reward=9.40 +/- 1.36
Episode length: 1424.20 +/- 390.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.42e+03   |
|    mean_reward          | 9.4        |
| time/                   |            |
|    total_timesteps      | 280000     |
| train/                  |            |
|    approx_kl            | 0.36400086 |
|    clip_fraction        | 0.425      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.393     |
|    explained_variance   | 0.785      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0722    |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.055     |
|    value_loss           | 0.00735    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.05e+03 |
|    ep_rew_mean     | 7.74     |
| time/              |          |
|    fps             | 539      |
|    iterations      | 35       |
|    time_elapsed    | 531      |
|    total_timesteps | 286720   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 7.58       |
| time/                   |            |
|    fps                  | 540        |
|    iterations           | 36         |
|    time_elapsed         | 545        |
|    total_timesteps      | 294912     |
| train/                  |            |
|    approx_kl            | 0.33988184 |
|    clip_fraction        | 0.443      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.406     |
|    explained_variance   | 0.837      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.077     |
|    n_updates            | 350        |
|    policy_gradient_loss | -0.0612    |
|    value_loss           | 0.00773    |
----------------------------------------
Eval num_timesteps=300000, episode_reward=4.20 +/- 2.01
Episode length: 836.00 +/- 232.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 836        |
|    mean_reward          | 4.2        |
| time/                   |            |
|    total_timesteps      | 300000     |
| train/                  |            |
|    approx_kl            | 0.40334326 |
|    clip_fraction        | 0.445      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.41      |
|    explained_variance   | 0.825      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0638    |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0593    |
|    value_loss           | 0.00934    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 7.49     |
| time/              |          |
|    fps             | 538      |
|    iterations      | 37       |
|    time_elapsed    | 563      |
|    total_timesteps | 303104   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 7.41       |
| time/                   |            |
|    fps                  | 540        |
|    iterations           | 38         |
|    time_elapsed         | 576        |
|    total_timesteps      | 311296     |
| train/                  |            |
|    approx_kl            | 0.34153223 |
|    clip_fraction        | 0.456      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.435     |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0804    |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.0597    |
|    value_loss           | 0.0129     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 7.34      |
| time/                   |           |
|    fps                  | 542       |
|    iterations           | 39        |
|    time_elapsed         | 589       |
|    total_timesteps      | 319488    |
| train/                  |           |
|    approx_kl            | 0.4033681 |
|    clip_fraction        | 0.457     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.418    |
|    explained_variance   | 0.837     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0722   |
|    n_updates            | 380       |
|    policy_gradient_loss | -0.0635   |
|    value_loss           | 0.00857   |
---------------------------------------
Eval num_timesteps=320000, episode_reward=8.20 +/- 2.32
Episode length: 1062.60 +/- 445.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.06e+03  |
|    mean_reward          | 8.2       |
| time/                   |           |
|    total_timesteps      | 320000    |
| train/                  |           |
|    approx_kl            | 0.3643186 |
|    clip_fraction        | 0.447     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.417    |
|    explained_variance   | 0.835     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0846   |
|    n_updates            | 390       |
|    policy_gradient_loss | -0.0627   |
|    value_loss           | 0.0101    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 976      |
|    ep_rew_mean     | 7.18     |
| time/              |          |
|    fps             | 539      |
|    iterations      | 40       |
|    time_elapsed    | 607      |
|    total_timesteps | 327680   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 974        |
|    ep_rew_mean          | 7.22       |
| time/                   |            |
|    fps                  | 541        |
|    iterations           | 41         |
|    time_elapsed         | 620        |
|    total_timesteps      | 335872     |
| train/                  |            |
|    approx_kl            | 0.37996602 |
|    clip_fraction        | 0.456      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.407     |
|    explained_variance   | 0.823      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0859    |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.0658    |
|    value_loss           | 0.0088     |
----------------------------------------
Eval num_timesteps=340000, episode_reward=8.20 +/- 0.98
Episode length: 958.00 +/- 221.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 958        |
|    mean_reward          | 8.2        |
| time/                   |            |
|    total_timesteps      | 340000     |
| train/                  |            |
|    approx_kl            | 0.37675995 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.419     |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.106     |
|    n_updates            | 410        |
|    policy_gradient_loss | -0.061     |
|    value_loss           | 0.00807    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 966      |
|    ep_rew_mean     | 7.22     |
| time/              |          |
|    fps             | 539      |
|    iterations      | 42       |
|    time_elapsed    | 638      |
|    total_timesteps | 344064   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 961        |
|    ep_rew_mean          | 7.16       |
| time/                   |            |
|    fps                  | 541        |
|    iterations           | 43         |
|    time_elapsed         | 650        |
|    total_timesteps      | 352256     |
| train/                  |            |
|    approx_kl            | 0.36952162 |
|    clip_fraction        | 0.449      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.414     |
|    explained_variance   | 0.765      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0797    |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.0609    |
|    value_loss           | 0.0111     |
----------------------------------------
Eval num_timesteps=360000, episode_reward=7.30 +/- 1.17
Episode length: 836.80 +/- 203.37
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 837        |
|    mean_reward          | 7.3        |
| time/                   |            |
|    total_timesteps      | 360000     |
| train/                  |            |
|    approx_kl            | 0.38040316 |
|    clip_fraction        | 0.462      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.416     |
|    explained_variance   | 0.806      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0997    |
|    n_updates            | 430        |
|    policy_gradient_loss | -0.0624    |
|    value_loss           | 0.00748    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 956      |
|    ep_rew_mean     | 7.23     |
| time/              |          |
|    fps             | 539      |
|    iterations      | 44       |
|    time_elapsed    | 668      |
|    total_timesteps | 360448   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 967        |
|    ep_rew_mean          | 7.29       |
| time/                   |            |
|    fps                  | 541        |
|    iterations           | 45         |
|    time_elapsed         | 681        |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.41528076 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.418     |
|    explained_variance   | 0.789      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0944    |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0638    |
|    value_loss           | 0.0074     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 962        |
|    ep_rew_mean          | 7.32       |
| time/                   |            |
|    fps                  | 542        |
|    iterations           | 46         |
|    time_elapsed         | 694        |
|    total_timesteps      | 376832     |
| train/                  |            |
|    approx_kl            | 0.43180853 |
|    clip_fraction        | 0.455      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.392     |
|    explained_variance   | 0.79       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0925    |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.0627    |
|    value_loss           | 0.00936    |
----------------------------------------
Eval num_timesteps=380000, episode_reward=7.40 +/- 2.27
Episode length: 916.60 +/- 462.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 917        |
|    mean_reward          | 7.4        |
| time/                   |            |
|    total_timesteps      | 380000     |
| train/                  |            |
|    approx_kl            | 0.39341998 |
|    clip_fraction        | 0.441      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.383     |
|    explained_variance   | 0.799      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0394    |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.0602    |
|    value_loss           | 0.00713    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 962      |
|    ep_rew_mean     | 7.34     |
| time/              |          |
|    fps             | 540      |
|    iterations      | 47       |
|    time_elapsed    | 712      |
|    total_timesteps | 385024   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 965        |
|    ep_rew_mean          | 7.34       |
| time/                   |            |
|    fps                  | 540        |
|    iterations           | 48         |
|    time_elapsed         | 726        |
|    total_timesteps      | 393216     |
| train/                  |            |
|    approx_kl            | 0.37702847 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.424     |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.105     |
|    n_updates            | 470        |
|    policy_gradient_loss | -0.0646    |
|    value_loss           | 0.00788    |
----------------------------------------
Eval num_timesteps=400000, episode_reward=6.30 +/- 0.93
Episode length: 727.40 +/- 226.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 727       |
|    mean_reward          | 6.3       |
| time/                   |           |
|    total_timesteps      | 400000    |
| train/                  |           |
|    approx_kl            | 0.3815054 |
|    clip_fraction        | 0.452     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.412    |
|    explained_variance   | 0.76      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0719   |
|    n_updates            | 480       |
|    policy_gradient_loss | -0.0635   |
|    value_loss           | 0.00743   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 971      |
|    ep_rew_mean     | 7.42     |
| time/              |          |
|    fps             | 539      |
|    iterations      | 49       |
|    time_elapsed    | 744      |
|    total_timesteps | 401408   |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 965      |
|    ep_rew_mean          | 7.58     |
| time/                   |          |
|    fps                  | 540      |
|    iterations           | 50       |
|    time_elapsed         | 758      |
|    total_timesteps      | 409600   |
| train/                  |          |
|    approx_kl            | 0.424796 |
|    clip_fraction        | 0.442    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.378   |
|    explained_variance   | 0.786    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0997  |
|    n_updates            | 490      |
|    policy_gradient_loss | -0.0623  |
|    value_loss           | 0.0108   |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 968       |
|    ep_rew_mean          | 7.6       |
| time/                   |           |
|    fps                  | 540       |
|    iterations           | 51        |
|    time_elapsed         | 772       |
|    total_timesteps      | 417792    |
| train/                  |           |
|    approx_kl            | 0.4088282 |
|    clip_fraction        | 0.456     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.397    |
|    explained_variance   | 0.812     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0728   |
|    n_updates            | 500       |
|    policy_gradient_loss | -0.0625   |
|    value_loss           | 0.00626   |
---------------------------------------
Eval num_timesteps=420000, episode_reward=9.00 +/- 2.10
Episode length: 1064.60 +/- 317.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.06e+03   |
|    mean_reward          | 9          |
| time/                   |            |
|    total_timesteps      | 420000     |
| train/                  |            |
|    approx_kl            | 0.46101022 |
|    clip_fraction        | 0.477      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.408     |
|    explained_variance   | 0.799      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.097     |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.0646    |
|    value_loss           | 0.00865    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 983      |
|    ep_rew_mean     | 7.61     |
| time/              |          |
|    fps             | 537      |
|    iterations      | 52       |
|    time_elapsed    | 791      |
|    total_timesteps | 425984   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 7.67      |
| time/                   |           |
|    fps                  | 538       |
|    iterations           | 53        |
|    time_elapsed         | 805       |
|    total_timesteps      | 434176    |
| train/                  |           |
|    approx_kl            | 0.4225018 |
|    clip_fraction        | 0.47      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.402    |
|    explained_variance   | 0.829     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0844   |
|    n_updates            | 520       |
|    policy_gradient_loss | -0.0655   |
|    value_loss           | 0.00791   |
---------------------------------------
Eval num_timesteps=440000, episode_reward=9.10 +/- 2.35
Episode length: 1147.20 +/- 432.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.15e+03   |
|    mean_reward          | 9.1        |
| time/                   |            |
|    total_timesteps      | 440000     |
| train/                  |            |
|    approx_kl            | 0.42045298 |
|    clip_fraction        | 0.467      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.404     |
|    explained_variance   | 0.784      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0964    |
|    n_updates            | 530        |
|    policy_gradient_loss | -0.0653    |
|    value_loss           | 0.00773    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 7.67     |
| time/              |          |
|    fps             | 535      |
|    iterations      | 54       |
|    time_elapsed    | 825      |
|    total_timesteps | 442368   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 7.64      |
| time/                   |           |
|    fps                  | 536       |
|    iterations           | 55        |
|    time_elapsed         | 839       |
|    total_timesteps      | 450560    |
| train/                  |           |
|    approx_kl            | 0.4615618 |
|    clip_fraction        | 0.45      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.384    |
|    explained_variance   | 0.791     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.101    |
|    n_updates            | 540       |
|    policy_gradient_loss | -0.0598   |
|    value_loss           | 0.00824   |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 7.6        |
| time/                   |            |
|    fps                  | 537        |
|    iterations           | 56         |
|    time_elapsed         | 852        |
|    total_timesteps      | 458752     |
| train/                  |            |
|    approx_kl            | 0.41282302 |
|    clip_fraction        | 0.452      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.388     |
|    explained_variance   | 0.766      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0616    |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.0597    |
|    value_loss           | 0.0107     |
----------------------------------------
Eval num_timesteps=460000, episode_reward=9.50 +/- 1.67
Episode length: 1296.40 +/- 391.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.3e+03    |
|    mean_reward          | 9.5        |
| time/                   |            |
|    total_timesteps      | 460000     |
| train/                  |            |
|    approx_kl            | 0.49853668 |
|    clip_fraction        | 0.447      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.374     |
|    explained_variance   | 0.796      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0882    |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.0615    |
|    value_loss           | 0.00994    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 995      |
|    ep_rew_mean     | 7.61     |
| time/              |          |
|    fps             | 535      |
|    iterations      | 57       |
|    time_elapsed    | 871      |
|    total_timesteps | 466944   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 996       |
|    ep_rew_mean          | 7.51      |
| time/                   |           |
|    fps                  | 536       |
|    iterations           | 58        |
|    time_elapsed         | 884       |
|    total_timesteps      | 475136    |
| train/                  |           |
|    approx_kl            | 0.4814531 |
|    clip_fraction        | 0.458     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.377    |
|    explained_variance   | 0.826     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.105    |
|    n_updates            | 570       |
|    policy_gradient_loss | -0.065    |
|    value_loss           | 0.00952   |
---------------------------------------
Eval num_timesteps=480000, episode_reward=6.80 +/- 1.50
Episode length: 952.00 +/- 279.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 952        |
|    mean_reward          | 6.8        |
| time/                   |            |
|    total_timesteps      | 480000     |
| train/                  |            |
|    approx_kl            | 0.49831116 |
|    clip_fraction        | 0.461      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.387     |
|    explained_variance   | 0.775      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.105     |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.0669    |
|    value_loss           | 0.00942    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 7.5      |
| time/              |          |
|    fps             | 535      |
|    iterations      | 59       |
|    time_elapsed    | 902      |
|    total_timesteps | 483328   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 985        |
|    ep_rew_mean          | 7.33       |
| time/                   |            |
|    fps                  | 536        |
|    iterations           | 60         |
|    time_elapsed         | 915        |
|    total_timesteps      | 491520     |
| train/                  |            |
|    approx_kl            | 0.48332864 |
|    clip_fraction        | 0.446      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.364     |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0914    |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.0629    |
|    value_loss           | 0.0079     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 988       |
|    ep_rew_mean          | 7.27      |
| time/                   |           |
|    fps                  | 538       |
|    iterations           | 61        |
|    time_elapsed         | 928       |
|    total_timesteps      | 499712    |
| train/                  |           |
|    approx_kl            | 0.4333821 |
|    clip_fraction        | 0.42      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.357    |
|    explained_variance   | 0.795     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0779   |
|    n_updates            | 600       |
|    policy_gradient_loss | -0.0563   |
|    value_loss           | 0.014     |
---------------------------------------
Eval num_timesteps=500000, episode_reward=6.30 +/- 2.04
Episode length: 918.20 +/- 283.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 918       |
|    mean_reward          | 6.3       |
| time/                   |           |
|    total_timesteps      | 500000    |
| train/                  |           |
|    approx_kl            | 0.4919607 |
|    clip_fraction        | 0.452     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.373    |
|    explained_variance   | 0.806     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0734   |
|    n_updates            | 610       |
|    policy_gradient_loss | -0.0651   |
|    value_loss           | 0.00822   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 982      |
|    ep_rew_mean     | 7.12     |
| time/              |          |
|    fps             | 537      |
|    iterations      | 62       |
|    time_elapsed    | 945      |
|    total_timesteps | 507904   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 977        |
|    ep_rew_mean          | 7.05       |
| time/                   |            |
|    fps                  | 538        |
|    iterations           | 63         |
|    time_elapsed         | 958        |
|    total_timesteps      | 516096     |
| train/                  |            |
|    approx_kl            | 0.44661516 |
|    clip_fraction        | 0.462      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.39      |
|    explained_variance   | 0.809      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0843    |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.0621    |
|    value_loss           | 0.00945    |
----------------------------------------
Eval num_timesteps=520000, episode_reward=7.10 +/- 2.13
Episode length: 915.40 +/- 595.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 915        |
|    mean_reward          | 7.1        |
| time/                   |            |
|    total_timesteps      | 520000     |
| train/                  |            |
|    approx_kl            | 0.44955048 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.389     |
|    explained_variance   | 0.788      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0956    |
|    n_updates            | 630        |
|    policy_gradient_loss | -0.0574    |
|    value_loss           | 0.0104     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 978      |
|    ep_rew_mean     | 7.05     |
| time/              |          |
|    fps             | 536      |
|    iterations      | 64       |
|    time_elapsed    | 977      |
|    total_timesteps | 524288   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 948        |
|    ep_rew_mean          | 6.86       |
| time/                   |            |
|    fps                  | 537        |
|    iterations           | 65         |
|    time_elapsed         | 990        |
|    total_timesteps      | 532480     |
| train/                  |            |
|    approx_kl            | 0.40968224 |
|    clip_fraction        | 0.449      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.384     |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0269    |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.0599    |
|    value_loss           | 0.0149     |
----------------------------------------
Eval num_timesteps=540000, episode_reward=8.50 +/- 1.67
Episode length: 1038.00 +/- 352.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.04e+03   |
|    mean_reward          | 8.5        |
| time/                   |            |
|    total_timesteps      | 540000     |
| train/                  |            |
|    approx_kl            | 0.41600752 |
|    clip_fraction        | 0.436      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.362     |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0754    |
|    n_updates            | 650        |
|    policy_gradient_loss | -0.0592    |
|    value_loss           | 0.0134     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 956      |
|    ep_rew_mean     | 6.82     |
| time/              |          |
|    fps             | 536      |
|    iterations      | 66       |
|    time_elapsed    | 1007     |
|    total_timesteps | 540672   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 958       |
|    ep_rew_mean          | 6.92      |
| time/                   |           |
|    fps                  | 537       |
|    iterations           | 67        |
|    time_elapsed         | 1021      |
|    total_timesteps      | 548864    |
| train/                  |           |
|    approx_kl            | 0.4192418 |
|    clip_fraction        | 0.446     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.383    |
|    explained_variance   | 0.801     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0611   |
|    n_updates            | 660       |
|    policy_gradient_loss | -0.0617   |
|    value_loss           | 0.0118    |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 961       |
|    ep_rew_mean          | 6.96      |
| time/                   |           |
|    fps                  | 538       |
|    iterations           | 68        |
|    time_elapsed         | 1034      |
|    total_timesteps      | 557056    |
| train/                  |           |
|    approx_kl            | 0.4520562 |
|    clip_fraction        | 0.473     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.397    |
|    explained_variance   | 0.782     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0701   |
|    n_updates            | 670       |
|    policy_gradient_loss | -0.0666   |
|    value_loss           | 0.00817   |
---------------------------------------
Eval num_timesteps=560000, episode_reward=7.70 +/- 1.29
Episode length: 943.40 +/- 382.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 943        |
|    mean_reward          | 7.7        |
| time/                   |            |
|    total_timesteps      | 560000     |
| train/                  |            |
|    approx_kl            | 0.43141767 |
|    clip_fraction        | 0.461      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.384     |
|    explained_variance   | 0.743      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.088     |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.0611    |
|    value_loss           | 0.00898    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 956      |
|    ep_rew_mean     | 6.9      |
| time/              |          |
|    fps             | 537      |
|    iterations      | 69       |
|    time_elapsed    | 1051     |
|    total_timesteps | 565248   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 921        |
|    ep_rew_mean          | 6.86       |
| time/                   |            |
|    fps                  | 538        |
|    iterations           | 70         |
|    time_elapsed         | 1065       |
|    total_timesteps      | 573440     |
| train/                  |            |
|    approx_kl            | 0.44157752 |
|    clip_fraction        | 0.456      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.392     |
|    explained_variance   | 0.784      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0677    |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.062     |
|    value_loss           | 0.00956    |
----------------------------------------
Eval num_timesteps=580000, episode_reward=7.20 +/- 1.94
Episode length: 861.60 +/- 282.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 862        |
|    mean_reward          | 7.2        |
| time/                   |            |
|    total_timesteps      | 580000     |
| train/                  |            |
|    approx_kl            | 0.42884314 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.386     |
|    explained_variance   | 0.775      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0853    |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.0621    |
|    value_loss           | 0.0112     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 893      |
|    ep_rew_mean     | 6.75     |
| time/              |          |
|    fps             | 537      |
|    iterations      | 71       |
|    time_elapsed    | 1082     |
|    total_timesteps | 581632   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 896        |
|    ep_rew_mean          | 6.88       |
| time/                   |            |
|    fps                  | 538        |
|    iterations           | 72         |
|    time_elapsed         | 1095       |
|    total_timesteps      | 589824     |
| train/                  |            |
|    approx_kl            | 0.40570217 |
|    clip_fraction        | 0.443      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.378     |
|    explained_variance   | 0.8        |
|    learning_rate        | 0.0003     |
|    loss                 | -0.084     |
|    n_updates            | 710        |
|    policy_gradient_loss | -0.0613    |
|    value_loss           | 0.0106     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 912        |
|    ep_rew_mean          | 7.06       |
| time/                   |            |
|    fps                  | 539        |
|    iterations           | 73         |
|    time_elapsed         | 1109       |
|    total_timesteps      | 598016     |
| train/                  |            |
|    approx_kl            | 0.40852246 |
|    clip_fraction        | 0.436      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.377     |
|    explained_variance   | 0.783      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0815    |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.055     |
|    value_loss           | 0.0123     |
----------------------------------------
Eval num_timesteps=600000, episode_reward=7.50 +/- 1.26
Episode length: 793.40 +/- 218.79
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 793        |
|    mean_reward          | 7.5        |
| time/                   |            |
|    total_timesteps      | 600000     |
| train/                  |            |
|    approx_kl            | 0.50675344 |
|    clip_fraction        | 0.463      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.387     |
|    explained_variance   | 0.826      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0954    |
|    n_updates            | 730        |
|    policy_gradient_loss | -0.0639    |
|    value_loss           | 0.0049     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 906      |
|    ep_rew_mean     | 7.17     |
| time/              |          |
|    fps             | 538      |
|    iterations      | 74       |
|    time_elapsed    | 1126     |
|    total_timesteps | 606208   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 946        |
|    ep_rew_mean          | 7.33       |
| time/                   |            |
|    fps                  | 539        |
|    iterations           | 75         |
|    time_elapsed         | 1139       |
|    total_timesteps      | 614400     |
| train/                  |            |
|    approx_kl            | 0.45931762 |
|    clip_fraction        | 0.477      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.413     |
|    explained_variance   | 0.781      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0972    |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.067     |
|    value_loss           | 0.0091     |
----------------------------------------
Eval num_timesteps=620000, episode_reward=7.20 +/- 1.03
Episode length: 808.60 +/- 107.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 809       |
|    mean_reward          | 7.2       |
| time/                   |           |
|    total_timesteps      | 620000    |
| train/                  |           |
|    approx_kl            | 0.4650511 |
|    clip_fraction        | 0.478     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.4      |
|    explained_variance   | 0.791     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0899   |
|    n_updates            | 750       |
|    policy_gradient_loss | -0.0673   |
|    value_loss           | 0.00787   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 964      |
|    ep_rew_mean     | 7.39     |
| time/              |          |
|    fps             | 538      |
|    iterations      | 76       |
|    time_elapsed    | 1156     |
|    total_timesteps | 622592   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 970       |
|    ep_rew_mean          | 7.52      |
| time/                   |           |
|    fps                  | 539       |
|    iterations           | 77        |
|    time_elapsed         | 1169      |
|    total_timesteps      | 630784    |
| train/                  |           |
|    approx_kl            | 0.5594138 |
|    clip_fraction        | 0.438     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.375    |
|    explained_variance   | 0.731     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0905   |
|    n_updates            | 760       |
|    policy_gradient_loss | -0.0597   |
|    value_loss           | 0.00903   |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 964        |
|    ep_rew_mean          | 7.51       |
| time/                   |            |
|    fps                  | 539        |
|    iterations           | 78         |
|    time_elapsed         | 1183       |
|    total_timesteps      | 638976     |
| train/                  |            |
|    approx_kl            | 0.44350797 |
|    clip_fraction        | 0.447      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.37      |
|    explained_variance   | 0.79       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0675    |
|    n_updates            | 770        |
|    policy_gradient_loss | -0.059     |
|    value_loss           | 0.00817    |
----------------------------------------
Eval num_timesteps=640000, episode_reward=6.70 +/- 1.36
Episode length: 754.20 +/- 285.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 754        |
|    mean_reward          | 6.7        |
| time/                   |            |
|    total_timesteps      | 640000     |
| train/                  |            |
|    approx_kl            | 0.45321515 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.369     |
|    explained_variance   | 0.792      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.114     |
|    n_updates            | 780        |
|    policy_gradient_loss | -0.0618    |
|    value_loss           | 0.00744    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 969      |
|    ep_rew_mean     | 7.5      |
| time/              |          |
|    fps             | 538      |
|    iterations      | 79       |
|    time_elapsed    | 1201     |
|    total_timesteps | 647168   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 941        |
|    ep_rew_mean          | 7.33       |
| time/                   |            |
|    fps                  | 539        |
|    iterations           | 80         |
|    time_elapsed         | 1214       |
|    total_timesteps      | 655360     |
| train/                  |            |
|    approx_kl            | 0.46388352 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.37      |
|    explained_variance   | 0.777      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0896    |
|    n_updates            | 790        |
|    policy_gradient_loss | -0.0585    |
|    value_loss           | 0.0104     |
----------------------------------------
Eval num_timesteps=660000, episode_reward=9.30 +/- 1.33
Episode length: 1243.60 +/- 265.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.24e+03   |
|    mean_reward          | 9.3        |
| time/                   |            |
|    total_timesteps      | 660000     |
| train/                  |            |
|    approx_kl            | 0.48320448 |
|    clip_fraction        | 0.444      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.354     |
|    explained_variance   | 0.798      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0857    |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.0625    |
|    value_loss           | 0.00711    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 944      |
|    ep_rew_mean     | 7.46     |
| time/              |          |
|    fps             | 537      |
|    iterations      | 81       |
|    time_elapsed    | 1233     |
|    total_timesteps | 663552   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 961        |
|    ep_rew_mean          | 7.54       |
| time/                   |            |
|    fps                  | 538        |
|    iterations           | 82         |
|    time_elapsed         | 1246       |
|    total_timesteps      | 671744     |
| train/                  |            |
|    approx_kl            | 0.44943926 |
|    clip_fraction        | 0.43       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.355     |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0439    |
|    n_updates            | 810        |
|    policy_gradient_loss | -0.055     |
|    value_loss           | 0.0109     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 984       |
|    ep_rew_mean          | 7.64      |
| time/                   |           |
|    fps                  | 539       |
|    iterations           | 83        |
|    time_elapsed         | 1259      |
|    total_timesteps      | 679936    |
| train/                  |           |
|    approx_kl            | 1.0209589 |
|    clip_fraction        | 0.422     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.346    |
|    explained_variance   | 0.746     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0612   |
|    n_updates            | 820       |
|    policy_gradient_loss | -0.0561   |
|    value_loss           | 0.00968   |
---------------------------------------
Eval num_timesteps=680000, episode_reward=7.60 +/- 2.01
Episode length: 1015.40 +/- 448.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.02e+03   |
|    mean_reward          | 7.6        |
| time/                   |            |
|    total_timesteps      | 680000     |
| train/                  |            |
|    approx_kl            | 0.48901695 |
|    clip_fraction        | 0.455      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.369     |
|    explained_variance   | 0.779      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0301    |
|    n_updates            | 830        |
|    policy_gradient_loss | -0.0632    |
|    value_loss           | 0.00757    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | 7.67     |
| time/              |          |
|    fps             | 538      |
|    iterations      | 84       |
|    time_elapsed    | 1277     |
|    total_timesteps | 688128   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 958       |
|    ep_rew_mean          | 7.51      |
| time/                   |           |
|    fps                  | 539       |
|    iterations           | 85        |
|    time_elapsed         | 1290      |
|    total_timesteps      | 696320    |
| train/                  |           |
|    approx_kl            | 0.5211113 |
|    clip_fraction        | 0.448     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.363    |
|    explained_variance   | 0.823     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0861   |
|    n_updates            | 840       |
|    policy_gradient_loss | -0.0625   |
|    value_loss           | 0.00816   |
---------------------------------------
Eval num_timesteps=700000, episode_reward=8.60 +/- 2.71
Episode length: 1000.80 +/- 517.31
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 8.6        |
| time/                   |            |
|    total_timesteps      | 700000     |
| train/                  |            |
|    approx_kl            | 0.42274588 |
|    clip_fraction        | 0.42       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.344     |
|    explained_variance   | 0.806      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0777    |
|    n_updates            | 850        |
|    policy_gradient_loss | -0.053     |
|    value_loss           | 0.00774    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 955      |
|    ep_rew_mean     | 7.46     |
| time/              |          |
|    fps             | 537      |
|    iterations      | 86       |
|    time_elapsed    | 1309     |
|    total_timesteps | 704512   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 947       |
|    ep_rew_mean          | 7.5       |
| time/                   |           |
|    fps                  | 538       |
|    iterations           | 87        |
|    time_elapsed         | 1323      |
|    total_timesteps      | 712704    |
| train/                  |           |
|    approx_kl            | 0.5047719 |
|    clip_fraction        | 0.453     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.36     |
|    explained_variance   | 0.777     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0424   |
|    n_updates            | 860       |
|    policy_gradient_loss | -0.062    |
|    value_loss           | 0.00672   |
---------------------------------------
Eval num_timesteps=720000, episode_reward=7.00 +/- 2.92
Episode length: 1016.20 +/- 351.03
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1.02e+03 |
|    mean_reward          | 7        |
| time/                   |          |
|    total_timesteps      | 720000   |
| train/                  |          |
|    approx_kl            | 0.533148 |
|    clip_fraction        | 0.418    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.339   |
|    explained_variance   | 0.837    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0893  |
|    n_updates            | 870      |
|    policy_gradient_loss | -0.0567  |
|    value_loss           | 0.00697  |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 930      |
|    ep_rew_mean     | 7.46     |
| time/              |          |
|    fps             | 537      |
|    iterations      | 88       |
|    time_elapsed    | 1341     |
|    total_timesteps | 720896   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 949       |
|    ep_rew_mean          | 7.63      |
| time/                   |           |
|    fps                  | 538       |
|    iterations           | 89        |
|    time_elapsed         | 1354      |
|    total_timesteps      | 729088    |
| train/                  |           |
|    approx_kl            | 0.4675914 |
|    clip_fraction        | 0.448     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.361    |
|    explained_variance   | 0.796     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0843   |
|    n_updates            | 880       |
|    policy_gradient_loss | -0.0601   |
|    value_loss           | 0.00794   |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 940      |
|    ep_rew_mean          | 7.66     |
| time/                   |          |
|    fps                  | 538      |
|    iterations           | 90       |
|    time_elapsed         | 1368     |
|    total_timesteps      | 737280   |
| train/                  |          |
|    approx_kl            | 0.574906 |
|    clip_fraction        | 0.466    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.364   |
|    explained_variance   | 0.823    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0965  |
|    n_updates            | 890      |
|    policy_gradient_loss | -0.0661  |
|    value_loss           | 0.00457  |
--------------------------------------
Eval num_timesteps=740000, episode_reward=7.80 +/- 1.63
Episode length: 883.20 +/- 348.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 883       |
|    mean_reward          | 7.8       |
| time/                   |           |
|    total_timesteps      | 740000    |
| train/                  |           |
|    approx_kl            | 0.5126945 |
|    clip_fraction        | 0.452     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.361    |
|    explained_variance   | 0.814     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0708   |
|    n_updates            | 900       |
|    policy_gradient_loss | -0.0633   |
|    value_loss           | 0.00645   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 943      |
|    ep_rew_mean     | 7.75     |
| time/              |          |
|    fps             | 537      |
|    iterations      | 91       |
|    time_elapsed    | 1386     |
|    total_timesteps | 745472   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 953       |
|    ep_rew_mean          | 7.86      |
| time/                   |           |
|    fps                  | 538       |
|    iterations           | 92        |
|    time_elapsed         | 1400      |
|    total_timesteps      | 753664    |
| train/                  |           |
|    approx_kl            | 0.5590275 |
|    clip_fraction        | 0.445     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.339    |
|    explained_variance   | 0.828     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0227   |
|    n_updates            | 910       |
|    policy_gradient_loss | -0.0604   |
|    value_loss           | 0.00521   |
---------------------------------------
Eval num_timesteps=760000, episode_reward=7.50 +/- 2.66
Episode length: 838.60 +/- 464.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 839        |
|    mean_reward          | 7.5        |
| time/                   |            |
|    total_timesteps      | 760000     |
| train/                  |            |
|    approx_kl            | 0.47243446 |
|    clip_fraction        | 0.413      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.331     |
|    explained_variance   | 0.771      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0701    |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.0526    |
|    value_loss           | 0.00835    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 963      |
|    ep_rew_mean     | 7.84     |
| time/              |          |
|    fps             | 537      |
|    iterations      | 93       |
|    time_elapsed    | 1418     |
|    total_timesteps | 761856   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 983        |
|    ep_rew_mean          | 7.73       |
| time/                   |            |
|    fps                  | 537        |
|    iterations           | 94         |
|    time_elapsed         | 1432       |
|    total_timesteps      | 770048     |
| train/                  |            |
|    approx_kl            | 0.53408194 |
|    clip_fraction        | 0.422      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.324     |
|    explained_variance   | 0.797      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0832    |
|    n_updates            | 930        |
|    policy_gradient_loss | -0.0584    |
|    value_loss           | 0.00629    |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 990       |
|    ep_rew_mean          | 7.75      |
| time/                   |           |
|    fps                  | 538       |
|    iterations           | 95        |
|    time_elapsed         | 1446      |
|    total_timesteps      | 778240    |
| train/                  |           |
|    approx_kl            | 0.5194287 |
|    clip_fraction        | 0.411     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.322    |
|    explained_variance   | 0.786     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0564   |
|    n_updates            | 940       |
|    policy_gradient_loss | -0.0554   |
|    value_loss           | 0.0109    |
---------------------------------------
Eval num_timesteps=780000, episode_reward=7.70 +/- 0.81
Episode length: 902.00 +/- 177.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 902       |
|    mean_reward          | 7.7       |
| time/                   |           |
|    total_timesteps      | 780000    |
| train/                  |           |
|    approx_kl            | 0.5452355 |
|    clip_fraction        | 0.426     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.326    |
|    explained_variance   | 0.783     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0767   |
|    n_updates            | 950       |
|    policy_gradient_loss | -0.0602   |
|    value_loss           | 0.0099    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | 7.72     |
| time/              |          |
|    fps             | 536      |
|    iterations      | 96       |
|    time_elapsed    | 1465     |
|    total_timesteps | 786432   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 985       |
|    ep_rew_mean          | 7.66      |
| time/                   |           |
|    fps                  | 537       |
|    iterations           | 97        |
|    time_elapsed         | 1479      |
|    total_timesteps      | 794624    |
| train/                  |           |
|    approx_kl            | 0.4601848 |
|    clip_fraction        | 0.406     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.315    |
|    explained_variance   | 0.799     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0703   |
|    n_updates            | 960       |
|    policy_gradient_loss | -0.0528   |
|    value_loss           | 0.0113    |
---------------------------------------
Eval num_timesteps=800000, episode_reward=6.50 +/- 0.55
Episode length: 554.80 +/- 92.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 555       |
|    mean_reward          | 6.5       |
| time/                   |           |
|    total_timesteps      | 800000    |
| train/                  |           |
|    approx_kl            | 0.5125159 |
|    clip_fraction        | 0.429     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.335    |
|    explained_variance   | 0.773     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0747   |
|    n_updates            | 970       |
|    policy_gradient_loss | -0.0555   |
|    value_loss           | 0.00971   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 981      |
|    ep_rew_mean     | 7.67     |
| time/              |          |
|    fps             | 536      |
|    iterations      | 98       |
|    time_elapsed    | 1496     |
|    total_timesteps | 802816   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 7.72       |
| time/                   |            |
|    fps                  | 537        |
|    iterations           | 99         |
|    time_elapsed         | 1510       |
|    total_timesteps      | 811008     |
| train/                  |            |
|    approx_kl            | 0.53039634 |
|    clip_fraction        | 0.451      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.353     |
|    explained_variance   | 0.783      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.07      |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.0597    |
|    value_loss           | 0.00996    |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 992       |
|    ep_rew_mean          | 7.61      |
| time/                   |           |
|    fps                  | 537       |
|    iterations           | 100       |
|    time_elapsed         | 1523      |
|    total_timesteps      | 819200    |
| train/                  |           |
|    approx_kl            | 0.5327648 |
|    clip_fraction        | 0.434     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.349    |
|    explained_variance   | 0.737     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0679   |
|    n_updates            | 990       |
|    policy_gradient_loss | -0.0579   |
|    value_loss           | 0.0129    |
---------------------------------------
Eval num_timesteps=820000, episode_reward=7.70 +/- 0.93
Episode length: 814.40 +/- 207.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 814       |
|    mean_reward          | 7.7       |
| time/                   |           |
|    total_timesteps      | 820000    |
| train/                  |           |
|    approx_kl            | 0.4889641 |
|    clip_fraction        | 0.442     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.35     |
|    explained_variance   | 0.802     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0189    |
|    n_updates            | 1000      |
|    policy_gradient_loss | -0.0591   |
|    value_loss           | 0.00667   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 978      |
|    ep_rew_mean     | 7.48     |
| time/              |          |
|    fps             | 536      |
|    iterations      | 101      |
|    time_elapsed    | 1542     |
|    total_timesteps | 827392   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 980       |
|    ep_rew_mean          | 7.44      |
| time/                   |           |
|    fps                  | 537       |
|    iterations           | 102       |
|    time_elapsed         | 1555      |
|    total_timesteps      | 835584    |
| train/                  |           |
|    approx_kl            | 0.4701532 |
|    clip_fraction        | 0.431     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.347    |
|    explained_variance   | 0.803     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0945   |
|    n_updates            | 1010      |
|    policy_gradient_loss | -0.057    |
|    value_loss           | 0.00765   |
---------------------------------------
Eval num_timesteps=840000, episode_reward=7.70 +/- 0.68
Episode length: 815.20 +/- 139.33
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 815      |
|    mean_reward          | 7.7      |
| time/                   |          |
|    total_timesteps      | 840000   |
| train/                  |          |
|    approx_kl            | 0.756878 |
|    clip_fraction        | 0.446    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.362   |
|    explained_variance   | 0.792    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0999  |
|    n_updates            | 1020     |
|    policy_gradient_loss | -0.0581  |
|    value_loss           | 0.0077   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 977      |
|    ep_rew_mean     | 7.43     |
| time/              |          |
|    fps             | 536      |
|    iterations      | 103      |
|    time_elapsed    | 1572     |
|    total_timesteps | 843776   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 956       |
|    ep_rew_mean          | 7.34      |
| time/                   |           |
|    fps                  | 537       |
|    iterations           | 104       |
|    time_elapsed         | 1586      |
|    total_timesteps      | 851968    |
| train/                  |           |
|    approx_kl            | 0.4688062 |
|    clip_fraction        | 0.448     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.368    |
|    explained_variance   | 0.816     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0853   |
|    n_updates            | 1030      |
|    policy_gradient_loss | -0.063    |
|    value_loss           | 0.00627   |
---------------------------------------
Eval num_timesteps=860000, episode_reward=9.70 +/- 3.20
Episode length: 1187.00 +/- 508.65
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1.19e+03 |
|    mean_reward          | 9.7      |
| time/                   |          |
|    total_timesteps      | 860000   |
| train/                  |          |
|    approx_kl            | 0.425105 |
|    clip_fraction        | 0.449    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.375   |
|    explained_variance   | 0.81     |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0488  |
|    n_updates            | 1040     |
|    policy_gradient_loss | -0.0584  |
|    value_loss           | 0.00912  |
--------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 919      |
|    ep_rew_mean     | 7.28     |
| time/              |          |
|    fps             | 536      |
|    iterations      | 105      |
|    time_elapsed    | 1603     |
|    total_timesteps | 860160   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 907        |
|    ep_rew_mean          | 7.26       |
| time/                   |            |
|    fps                  | 536        |
|    iterations           | 106        |
|    time_elapsed         | 1617       |
|    total_timesteps      | 868352     |
| train/                  |            |
|    approx_kl            | 0.47455347 |
|    clip_fraction        | 0.428      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.348     |
|    explained_variance   | 0.818      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0833    |
|    n_updates            | 1050       |
|    policy_gradient_loss | -0.0573    |
|    value_loss           | 0.00946    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 907        |
|    ep_rew_mean          | 7.36       |
| time/                   |            |
|    fps                  | 537        |
|    iterations           | 107        |
|    time_elapsed         | 1630       |
|    total_timesteps      | 876544     |
| train/                  |            |
|    approx_kl            | 0.48987842 |
|    clip_fraction        | 0.463      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.377     |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0875    |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.0677    |
|    value_loss           | 0.00665    |
----------------------------------------
Eval num_timesteps=880000, episode_reward=9.40 +/- 3.71
Episode length: 1267.60 +/- 894.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.27e+03  |
|    mean_reward          | 9.4       |
| time/                   |           |
|    total_timesteps      | 880000    |
| train/                  |           |
|    approx_kl            | 0.4602282 |
|    clip_fraction        | 0.446     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.368    |
|    explained_variance   | 0.807     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0928   |
|    n_updates            | 1070      |
|    policy_gradient_loss | -0.0613   |
|    value_loss           | 0.0096    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 923      |
|    ep_rew_mean     | 7.46     |
| time/              |          |
|    fps             | 535      |
|    iterations      | 108      |
|    time_elapsed    | 1650     |
|    total_timesteps | 884736   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 934        |
|    ep_rew_mean          | 7.58       |
| time/                   |            |
|    fps                  | 536        |
|    iterations           | 109        |
|    time_elapsed         | 1664       |
|    total_timesteps      | 892928     |
| train/                  |            |
|    approx_kl            | 0.49144563 |
|    clip_fraction        | 0.444      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.355     |
|    explained_variance   | 0.813      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0818    |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.0581    |
|    value_loss           | 0.00801    |
----------------------------------------
Eval num_timesteps=900000, episode_reward=8.20 +/- 2.77
Episode length: 871.80 +/- 433.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 872       |
|    mean_reward          | 8.2       |
| time/                   |           |
|    total_timesteps      | 900000    |
| train/                  |           |
|    approx_kl            | 0.4770469 |
|    clip_fraction        | 0.448     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.362    |
|    explained_variance   | 0.831     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.065    |
|    n_updates            | 1090      |
|    policy_gradient_loss | -0.0588   |
|    value_loss           | 0.00849   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 948      |
|    ep_rew_mean     | 7.63     |
| time/              |          |
|    fps             | 535      |
|    iterations      | 110      |
|    time_elapsed    | 1682     |
|    total_timesteps | 901120   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 917        |
|    ep_rew_mean          | 7.57       |
| time/                   |            |
|    fps                  | 536        |
|    iterations           | 111        |
|    time_elapsed         | 1695       |
|    total_timesteps      | 909312     |
| train/                  |            |
|    approx_kl            | 0.47612584 |
|    clip_fraction        | 0.457      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.369     |
|    explained_variance   | 0.781      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.104     |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.0597    |
|    value_loss           | 0.00832    |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 908       |
|    ep_rew_mean          | 7.57      |
| time/                   |           |
|    fps                  | 537       |
|    iterations           | 112       |
|    time_elapsed         | 1708      |
|    total_timesteps      | 917504    |
| train/                  |           |
|    approx_kl            | 0.5662682 |
|    clip_fraction        | 0.457     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.378    |
|    explained_variance   | 0.817     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.086    |
|    n_updates            | 1110      |
|    policy_gradient_loss | -0.0644   |
|    value_loss           | 0.00668   |
---------------------------------------
Eval num_timesteps=920000, episode_reward=7.80 +/- 0.93
Episode length: 829.80 +/- 212.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 830       |
|    mean_reward          | 7.8       |
| time/                   |           |
|    total_timesteps      | 920000    |
| train/                  |           |
|    approx_kl            | 0.4940462 |
|    clip_fraction        | 0.473     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.398    |
|    explained_variance   | 0.833     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0773   |
|    n_updates            | 1120      |
|    policy_gradient_loss | -0.0659   |
|    value_loss           | 0.00695   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 893      |
|    ep_rew_mean     | 7.54     |
| time/              |          |
|    fps             | 536      |
|    iterations      | 113      |
|    time_elapsed    | 1724     |
|    total_timesteps | 925696   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 902       |
|    ep_rew_mean          | 7.51      |
| time/                   |           |
|    fps                  | 537       |
|    iterations           | 114       |
|    time_elapsed         | 1737      |
|    total_timesteps      | 933888    |
| train/                  |           |
|    approx_kl            | 0.5087017 |
|    clip_fraction        | 0.453     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.363    |
|    explained_variance   | 0.802     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0779   |
|    n_updates            | 1130      |
|    policy_gradient_loss | -0.0606   |
|    value_loss           | 0.00859   |
---------------------------------------
Eval num_timesteps=940000, episode_reward=6.50 +/- 1.22
Episode length: 744.20 +/- 119.76
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 744      |
|    mean_reward          | 6.5      |
| time/                   |          |
|    total_timesteps      | 940000   |
| train/                  |          |
|    approx_kl            | 0.514892 |
|    clip_fraction        | 0.459    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.373   |
|    explained_variance   | 0.783    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0699  |
|    n_updates            | 1140     |
|    policy_gradient_loss | -0.0631  |
|    value_loss           | 0.00829  |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 913      |
|    ep_rew_mean     | 7.49     |
| time/              |          |
|    fps             | 537      |
|    iterations      | 115      |
|    time_elapsed    | 1753     |
|    total_timesteps | 942080   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 931        |
|    ep_rew_mean          | 7.53       |
| time/                   |            |
|    fps                  | 537        |
|    iterations           | 116        |
|    time_elapsed         | 1766       |
|    total_timesteps      | 950272     |
| train/                  |            |
|    approx_kl            | 0.47436953 |
|    clip_fraction        | 0.446      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.363     |
|    explained_variance   | 0.777      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0708    |
|    n_updates            | 1150       |
|    policy_gradient_loss | -0.0608    |
|    value_loss           | 0.0094     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 932       |
|    ep_rew_mean          | 7.54      |
| time/                   |           |
|    fps                  | 538       |
|    iterations           | 117       |
|    time_elapsed         | 1779      |
|    total_timesteps      | 958464    |
| train/                  |           |
|    approx_kl            | 0.5220444 |
|    clip_fraction        | 0.424     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.345    |
|    explained_variance   | 0.764     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0819   |
|    n_updates            | 1160      |
|    policy_gradient_loss | -0.0561   |
|    value_loss           | 0.00858   |
---------------------------------------
Eval num_timesteps=960000, episode_reward=7.50 +/- 1.05
Episode length: 902.00 +/- 124.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 902        |
|    mean_reward          | 7.5        |
| time/                   |            |
|    total_timesteps      | 960000     |
| train/                  |            |
|    approx_kl            | 0.47449824 |
|    clip_fraction        | 0.451      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.354     |
|    explained_variance   | 0.833      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0853    |
|    n_updates            | 1170       |
|    policy_gradient_loss | -0.0629    |
|    value_loss           | 0.00704    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 925      |
|    ep_rew_mean     | 7.57     |
| time/              |          |
|    fps             | 538      |
|    iterations      | 118      |
|    time_elapsed    | 1796     |
|    total_timesteps | 966656   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 921        |
|    ep_rew_mean          | 7.56       |
| time/                   |            |
|    fps                  | 538        |
|    iterations           | 119        |
|    time_elapsed         | 1809       |
|    total_timesteps      | 974848     |
| train/                  |            |
|    approx_kl            | 0.54922664 |
|    clip_fraction        | 0.452      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.349     |
|    explained_variance   | 0.826      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0996    |
|    n_updates            | 1180       |
|    policy_gradient_loss | -0.0642    |
|    value_loss           | 0.00602    |
----------------------------------------
Eval num_timesteps=980000, episode_reward=6.20 +/- 1.08
Episode length: 907.20 +/- 146.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 907        |
|    mean_reward          | 6.2        |
| time/                   |            |
|    total_timesteps      | 980000     |
| train/                  |            |
|    approx_kl            | 0.54161215 |
|    clip_fraction        | 0.444      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.35      |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.096     |
|    n_updates            | 1190       |
|    policy_gradient_loss | -0.0597    |
|    value_loss           | 0.00551    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 921      |
|    ep_rew_mean     | 7.57     |
| time/              |          |
|    fps             | 537      |
|    iterations      | 120      |
|    time_elapsed    | 1827     |
|    total_timesteps | 983040   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 932        |
|    ep_rew_mean          | 7.63       |
| time/                   |            |
|    fps                  | 538        |
|    iterations           | 121        |
|    time_elapsed         | 1840       |
|    total_timesteps      | 991232     |
| train/                  |            |
|    approx_kl            | 0.46120197 |
|    clip_fraction        | 0.441      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.353     |
|    explained_variance   | 0.786      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.24       |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.054     |
|    value_loss           | 0.0075     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 907       |
|    ep_rew_mean          | 7.54      |
| time/                   |           |
|    fps                  | 539       |
|    iterations           | 122       |
|    time_elapsed         | 1853      |
|    total_timesteps      | 999424    |
| train/                  |           |
|    approx_kl            | 0.4778285 |
|    clip_fraction        | 0.451     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.362    |
|    explained_variance   | 0.85      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0933   |
|    n_updates            | 1210      |
|    policy_gradient_loss | -0.0628   |
|    value_loss           | 0.00453   |
---------------------------------------
Eval num_timesteps=1000000, episode_reward=6.30 +/- 1.44
Episode length: 647.80 +/- 212.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 648       |
|    mean_reward          | 6.3       |
| time/                   |           |
|    total_timesteps      | 1000000   |
| train/                  |           |
|    approx_kl            | 0.5333288 |
|    clip_fraction        | 0.436     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.348    |
|    explained_variance   | 0.806     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0748   |
|    n_updates            | 1220      |
|    policy_gradient_loss | -0.0557   |
|    value_loss           | 0.00813   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 928      |
|    ep_rew_mean     | 7.63     |
| time/              |          |
|    fps             | 539      |
|    iterations      | 123      |
|    time_elapsed    | 1869     |
|    total_timesteps | 1007616  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 933        |
|    ep_rew_mean          | 7.65       |
| time/                   |            |
|    fps                  | 539        |
|    iterations           | 124        |
|    time_elapsed         | 1882       |
|    total_timesteps      | 1015808    |
| train/                  |            |
|    approx_kl            | 0.50623703 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.37      |
|    explained_variance   | 0.831      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0895    |
|    n_updates            | 1230       |
|    policy_gradient_loss | -0.0655    |
|    value_loss           | 0.00579    |
----------------------------------------
Eval num_timesteps=1020000, episode_reward=8.00 +/- 1.34
Episode length: 917.00 +/- 271.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 917       |
|    mean_reward          | 8         |
| time/                   |           |
|    total_timesteps      | 1020000   |
| train/                  |           |
|    approx_kl            | 0.5331716 |
|    clip_fraction        | 0.432     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.34     |
|    explained_variance   | 0.793     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0791   |
|    n_updates            | 1240      |
|    policy_gradient_loss | -0.0549   |
|    value_loss           | 0.011     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 916      |
|    ep_rew_mean     | 7.65     |
| time/              |          |
|    fps             | 539      |
|    iterations      | 125      |
|    time_elapsed    | 1899     |
|    total_timesteps | 1024000  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 920       |
|    ep_rew_mean          | 7.66      |
| time/                   |           |
|    fps                  | 539       |
|    iterations           | 126       |
|    time_elapsed         | 1912      |
|    total_timesteps      | 1032192   |
| train/                  |           |
|    approx_kl            | 0.5251459 |
|    clip_fraction        | 0.453     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.363    |
|    explained_variance   | 0.816     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0682   |
|    n_updates            | 1250      |
|    policy_gradient_loss | -0.0627   |
|    value_loss           | 0.00699   |
---------------------------------------
Eval num_timesteps=1040000, episode_reward=7.10 +/- 2.52
Episode length: 814.60 +/- 431.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 815        |
|    mean_reward          | 7.1        |
| time/                   |            |
|    total_timesteps      | 1040000    |
| train/                  |            |
|    approx_kl            | 0.46721157 |
|    clip_fraction        | 0.432      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.346     |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0852    |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.056     |
|    value_loss           | 0.00809    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 929      |
|    ep_rew_mean     | 7.84     |
| time/              |          |
|    fps             | 539      |
|    iterations      | 127      |
|    time_elapsed    | 1928     |
|    total_timesteps | 1040384  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 920        |
|    ep_rew_mean          | 7.74       |
| time/                   |            |
|    fps                  | 540        |
|    iterations           | 128        |
|    time_elapsed         | 1941       |
|    total_timesteps      | 1048576    |
| train/                  |            |
|    approx_kl            | 0.51476485 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.336     |
|    explained_variance   | 0.826      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0636    |
|    n_updates            | 1270       |
|    policy_gradient_loss | -0.0597    |
|    value_loss           | 0.00632    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 915        |
|    ep_rew_mean          | 7.71       |
| time/                   |            |
|    fps                  | 540        |
|    iterations           | 129        |
|    time_elapsed         | 1954       |
|    total_timesteps      | 1056768    |
| train/                  |            |
|    approx_kl            | 0.48117852 |
|    clip_fraction        | 0.434      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.352     |
|    explained_variance   | 0.804      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.104     |
|    n_updates            | 1280       |
|    policy_gradient_loss | -0.0558    |
|    value_loss           | 0.00985    |
----------------------------------------
Eval num_timesteps=1060000, episode_reward=8.10 +/- 1.28
Episode length: 1042.20 +/- 355.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.04e+03  |
|    mean_reward          | 8.1       |
| time/                   |           |
|    total_timesteps      | 1060000   |
| train/                  |           |
|    approx_kl            | 0.4958468 |
|    clip_fraction        | 0.439     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.341    |
|    explained_variance   | 0.761     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0972   |
|    n_updates            | 1290      |
|    policy_gradient_loss | -0.0596   |
|    value_loss           | 0.0101    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 931      |
|    ep_rew_mean     | 7.71     |
| time/              |          |
|    fps             | 539      |
|    iterations      | 130      |
|    time_elapsed    | 1974     |
|    total_timesteps | 1064960  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 936       |
|    ep_rew_mean          | 7.67      |
| time/                   |           |
|    fps                  | 540       |
|    iterations           | 131       |
|    time_elapsed         | 1987      |
|    total_timesteps      | 1073152   |
| train/                  |           |
|    approx_kl            | 0.5563139 |
|    clip_fraction        | 0.43      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.326    |
|    explained_variance   | 0.812     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0371   |
|    n_updates            | 1300      |
|    policy_gradient_loss | -0.0595   |
|    value_loss           | 0.0105    |
---------------------------------------
Eval num_timesteps=1080000, episode_reward=6.90 +/- 1.16
Episode length: 756.80 +/- 121.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 757        |
|    mean_reward          | 6.9        |
| time/                   |            |
|    total_timesteps      | 1080000    |
| train/                  |            |
|    approx_kl            | 0.51889825 |
|    clip_fraction        | 0.436      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.339     |
|    explained_variance   | 0.812      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0667    |
|    n_updates            | 1310       |
|    policy_gradient_loss | -0.0564    |
|    value_loss           | 0.00951    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 951      |
|    ep_rew_mean     | 7.75     |
| time/              |          |
|    fps             | 539      |
|    iterations      | 132      |
|    time_elapsed    | 2003     |
|    total_timesteps | 1081344  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 931        |
|    ep_rew_mean          | 7.54       |
| time/                   |            |
|    fps                  | 540        |
|    iterations           | 133        |
|    time_elapsed         | 2016       |
|    total_timesteps      | 1089536    |
| train/                  |            |
|    approx_kl            | 0.52439207 |
|    clip_fraction        | 0.42       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.332     |
|    explained_variance   | 0.812      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0669    |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.058     |
|    value_loss           | 0.00955    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 943        |
|    ep_rew_mean          | 7.57       |
| time/                   |            |
|    fps                  | 540        |
|    iterations           | 134        |
|    time_elapsed         | 2029       |
|    total_timesteps      | 1097728    |
| train/                  |            |
|    approx_kl            | 0.47598323 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.342     |
|    explained_variance   | 0.762      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.107     |
|    n_updates            | 1330       |
|    policy_gradient_loss | -0.054     |
|    value_loss           | 0.0137     |
----------------------------------------
Eval num_timesteps=1100000, episode_reward=7.30 +/- 2.46
Episode length: 726.40 +/- 400.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 726       |
|    mean_reward          | 7.3       |
| time/                   |           |
|    total_timesteps      | 1100000   |
| train/                  |           |
|    approx_kl            | 0.4881034 |
|    clip_fraction        | 0.44      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.344    |
|    explained_variance   | 0.811     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0782   |
|    n_updates            | 1340      |
|    policy_gradient_loss | -0.0583   |
|    value_loss           | 0.00887   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 939      |
|    ep_rew_mean     | 7.55     |
| time/              |          |
|    fps             | 540      |
|    iterations      | 135      |
|    time_elapsed    | 2046     |
|    total_timesteps | 1105920  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 940        |
|    ep_rew_mean          | 7.53       |
| time/                   |            |
|    fps                  | 541        |
|    iterations           | 136        |
|    time_elapsed         | 2059       |
|    total_timesteps      | 1114112    |
| train/                  |            |
|    approx_kl            | 0.43689528 |
|    clip_fraction        | 0.429      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.354     |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0819    |
|    n_updates            | 1350       |
|    policy_gradient_loss | -0.06      |
|    value_loss           | 0.00818    |
----------------------------------------
Eval num_timesteps=1120000, episode_reward=8.40 +/- 1.46
Episode length: 1186.80 +/- 303.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.19e+03   |
|    mean_reward          | 8.4        |
| time/                   |            |
|    total_timesteps      | 1120000    |
| train/                  |            |
|    approx_kl            | 0.51203656 |
|    clip_fraction        | 0.44       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.358     |
|    explained_variance   | 0.82       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0607    |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.0607    |
|    value_loss           | 0.00761    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 950      |
|    ep_rew_mean     | 7.57     |
| time/              |          |
|    fps             | 540      |
|    iterations      | 137      |
|    time_elapsed    | 2077     |
|    total_timesteps | 1122304  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 950        |
|    ep_rew_mean          | 7.61       |
| time/                   |            |
|    fps                  | 540        |
|    iterations           | 138        |
|    time_elapsed         | 2090       |
|    total_timesteps      | 1130496    |
| train/                  |            |
|    approx_kl            | 0.47778702 |
|    clip_fraction        | 0.451      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.374     |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0702    |
|    n_updates            | 1370       |
|    policy_gradient_loss | -0.0596    |
|    value_loss           | 0.00773    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 939        |
|    ep_rew_mean          | 7.45       |
| time/                   |            |
|    fps                  | 541        |
|    iterations           | 139        |
|    time_elapsed         | 2103       |
|    total_timesteps      | 1138688    |
| train/                  |            |
|    approx_kl            | 0.47589034 |
|    clip_fraction        | 0.427      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.343     |
|    explained_variance   | 0.772      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0736    |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.0531    |
|    value_loss           | 0.0107     |
----------------------------------------
Eval num_timesteps=1140000, episode_reward=7.20 +/- 1.86
Episode length: 889.60 +/- 359.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 890       |
|    mean_reward          | 7.2       |
| time/                   |           |
|    total_timesteps      | 1140000   |
| train/                  |           |
|    approx_kl            | 0.4864754 |
|    clip_fraction        | 0.435     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.359    |
|    explained_variance   | 0.831     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0861   |
|    n_updates            | 1390      |
|    policy_gradient_loss | -0.0583   |
|    value_loss           | 0.00852   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 951      |
|    ep_rew_mean     | 7.5      |
| time/              |          |
|    fps             | 540      |
|    iterations      | 140      |
|    time_elapsed    | 2120     |
|    total_timesteps | 1146880  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 903        |
|    ep_rew_mean          | 7.36       |
| time/                   |            |
|    fps                  | 541        |
|    iterations           | 141        |
|    time_elapsed         | 2133       |
|    total_timesteps      | 1155072    |
| train/                  |            |
|    approx_kl            | 0.48989183 |
|    clip_fraction        | 0.447      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.356     |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0924    |
|    n_updates            | 1400       |
|    policy_gradient_loss | -0.0616    |
|    value_loss           | 0.00869    |
----------------------------------------
Eval num_timesteps=1160000, episode_reward=7.70 +/- 1.78
Episode length: 1101.60 +/- 218.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.1e+03   |
|    mean_reward          | 7.7       |
| time/                   |           |
|    total_timesteps      | 1160000   |
| train/                  |           |
|    approx_kl            | 0.5099004 |
|    clip_fraction        | 0.44      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.347    |
|    explained_variance   | 0.844     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0932   |
|    n_updates            | 1410      |
|    policy_gradient_loss | -0.0587   |
|    value_loss           | 0.00738   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 902      |
|    ep_rew_mean     | 7.34     |
| time/              |          |
|    fps             | 540      |
|    iterations      | 142      |
|    time_elapsed    | 2150     |
|    total_timesteps | 1163264  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 927       |
|    ep_rew_mean          | 7.47      |
| time/                   |           |
|    fps                  | 541       |
|    iterations           | 143       |
|    time_elapsed         | 2163      |
|    total_timesteps      | 1171456   |
| train/                  |           |
|    approx_kl            | 0.5045645 |
|    clip_fraction        | 0.416     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.332    |
|    explained_variance   | 0.802     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0702   |
|    n_updates            | 1420      |
|    policy_gradient_loss | -0.0584   |
|    value_loss           | 0.00999   |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 910        |
|    ep_rew_mean          | 7.41       |
| time/                   |            |
|    fps                  | 541        |
|    iterations           | 144        |
|    time_elapsed         | 2176       |
|    total_timesteps      | 1179648    |
| train/                  |            |
|    approx_kl            | 0.51234984 |
|    clip_fraction        | 0.439      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.356     |
|    explained_variance   | 0.778      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.063     |
|    n_updates            | 1430       |
|    policy_gradient_loss | -0.0558    |
|    value_loss           | 0.011      |
----------------------------------------
Eval num_timesteps=1180000, episode_reward=6.70 +/- 1.12
Episode length: 873.20 +/- 234.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 873        |
|    mean_reward          | 6.7        |
| time/                   |            |
|    total_timesteps      | 1180000    |
| train/                  |            |
|    approx_kl            | 0.49376562 |
|    clip_fraction        | 0.455      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.362     |
|    explained_variance   | 0.845      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.101     |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.0621    |
|    value_loss           | 0.00718    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 924      |
|    ep_rew_mean     | 7.51     |
| time/              |          |
|    fps             | 541      |
|    iterations      | 145      |
|    time_elapsed    | 2193     |
|    total_timesteps | 1187840  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 916        |
|    ep_rew_mean          | 7.44       |
| time/                   |            |
|    fps                  | 542        |
|    iterations           | 146        |
|    time_elapsed         | 2206       |
|    total_timesteps      | 1196032    |
| train/                  |            |
|    approx_kl            | 0.47641158 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.351     |
|    explained_variance   | 0.768      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.084     |
|    n_updates            | 1450       |
|    policy_gradient_loss | -0.056     |
|    value_loss           | 0.0107     |
----------------------------------------
Eval num_timesteps=1200000, episode_reward=7.40 +/- 1.66
Episode length: 997.40 +/- 242.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 997       |
|    mean_reward          | 7.4       |
| time/                   |           |
|    total_timesteps      | 1200000   |
| train/                  |           |
|    approx_kl            | 0.5462248 |
|    clip_fraction        | 0.45      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.353    |
|    explained_variance   | 0.814     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0894   |
|    n_updates            | 1460      |
|    policy_gradient_loss | -0.064    |
|    value_loss           | 0.0071    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 947      |
|    ep_rew_mean     | 7.51     |
| time/              |          |
|    fps             | 541      |
|    iterations      | 147      |
|    time_elapsed    | 2223     |
|    total_timesteps | 1204224  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 957        |
|    ep_rew_mean          | 7.57       |
| time/                   |            |
|    fps                  | 542        |
|    iterations           | 148        |
|    time_elapsed         | 2236       |
|    total_timesteps      | 1212416    |
| train/                  |            |
|    approx_kl            | 0.48785865 |
|    clip_fraction        | 0.446      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.361     |
|    explained_variance   | 0.769      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0835    |
|    n_updates            | 1470       |
|    policy_gradient_loss | -0.0574    |
|    value_loss           | 0.0105     |
----------------------------------------
Eval num_timesteps=1220000, episode_reward=8.90 +/- 2.80
Episode length: 1129.80 +/- 561.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.13e+03   |
|    mean_reward          | 8.9        |
| time/                   |            |
|    total_timesteps      | 1220000    |
| train/                  |            |
|    approx_kl            | 0.50940174 |
|    clip_fraction        | 0.442      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.343     |
|    explained_variance   | 0.816      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0736    |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.0605    |
|    value_loss           | 0.00804    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 977      |
|    ep_rew_mean     | 7.67     |
| time/              |          |
|    fps             | 541      |
|    iterations      | 149      |
|    time_elapsed    | 2254     |
|    total_timesteps | 1220608  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 972       |
|    ep_rew_mean          | 7.61      |
| time/                   |           |
|    fps                  | 541       |
|    iterations           | 150       |
|    time_elapsed         | 2267      |
|    total_timesteps      | 1228800   |
| train/                  |           |
|    approx_kl            | 0.5394652 |
|    clip_fraction        | 0.457     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.363    |
|    explained_variance   | 0.783     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0865   |
|    n_updates            | 1490      |
|    policy_gradient_loss | -0.0629   |
|    value_loss           | 0.00908   |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 961        |
|    ep_rew_mean          | 7.54       |
| time/                   |            |
|    fps                  | 542        |
|    iterations           | 151        |
|    time_elapsed         | 2280       |
|    total_timesteps      | 1236992    |
| train/                  |            |
|    approx_kl            | 0.54713905 |
|    clip_fraction        | 0.468      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.371     |
|    explained_variance   | 0.854      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0925    |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.0679    |
|    value_loss           | 0.00567    |
----------------------------------------
Eval num_timesteps=1240000, episode_reward=7.50 +/- 1.67
Episode length: 871.20 +/- 348.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 871        |
|    mean_reward          | 7.5        |
| time/                   |            |
|    total_timesteps      | 1240000    |
| train/                  |            |
|    approx_kl            | 0.57517487 |
|    clip_fraction        | 0.43       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.335     |
|    explained_variance   | 0.8        |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0445    |
|    n_updates            | 1510       |
|    policy_gradient_loss | -0.0564    |
|    value_loss           | 0.00848    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 968      |
|    ep_rew_mean     | 7.59     |
| time/              |          |
|    fps             | 541      |
|    iterations      | 152      |
|    time_elapsed    | 2299     |
|    total_timesteps | 1245184  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 993        |
|    ep_rew_mean          | 7.62       |
| time/                   |            |
|    fps                  | 542        |
|    iterations           | 153        |
|    time_elapsed         | 2312       |
|    total_timesteps      | 1253376    |
| train/                  |            |
|    approx_kl            | 0.50419366 |
|    clip_fraction        | 0.404      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.313     |
|    explained_variance   | 0.792      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0769    |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.0521    |
|    value_loss           | 0.00946    |
----------------------------------------
Eval num_timesteps=1260000, episode_reward=6.80 +/- 0.93
Episode length: 973.40 +/- 298.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 973       |
|    mean_reward          | 6.8       |
| time/                   |           |
|    total_timesteps      | 1260000   |
| train/                  |           |
|    approx_kl            | 0.4834724 |
|    clip_fraction        | 0.431     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.35     |
|    explained_variance   | 0.769     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0812   |
|    n_updates            | 1530      |
|    policy_gradient_loss | -0.0613   |
|    value_loss           | 0.0111    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 7.48     |
| time/              |          |
|    fps             | 541      |
|    iterations      | 154      |
|    time_elapsed    | 2330     |
|    total_timesteps | 1261568  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.01e+03  |
|    ep_rew_mean          | 7.45      |
| time/                   |           |
|    fps                  | 541       |
|    iterations           | 155       |
|    time_elapsed         | 2343      |
|    total_timesteps      | 1269760   |
| train/                  |           |
|    approx_kl            | 0.5849906 |
|    clip_fraction        | 0.412     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.319    |
|    explained_variance   | 0.757     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0755   |
|    n_updates            | 1540      |
|    policy_gradient_loss | -0.0586   |
|    value_loss           | 0.0145    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 7.5        |
| time/                   |            |
|    fps                  | 542        |
|    iterations           | 156        |
|    time_elapsed         | 2356       |
|    total_timesteps      | 1277952    |
| train/                  |            |
|    approx_kl            | 0.54723483 |
|    clip_fraction        | 0.419      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.308     |
|    explained_variance   | 0.836      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0441    |
|    n_updates            | 1550       |
|    policy_gradient_loss | -0.0588    |
|    value_loss           | 0.00899    |
----------------------------------------
Eval num_timesteps=1280000, episode_reward=8.30 +/- 2.32
Episode length: 978.60 +/- 533.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 979        |
|    mean_reward          | 8.3        |
| time/                   |            |
|    total_timesteps      | 1280000    |
| train/                  |            |
|    approx_kl            | 0.57322216 |
|    clip_fraction        | 0.43       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.321     |
|    explained_variance   | 0.803      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0957    |
|    n_updates            | 1560       |
|    policy_gradient_loss | -0.0585    |
|    value_loss           | 0.00805    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 7.55     |
| time/              |          |
|    fps             | 541      |
|    iterations      | 157      |
|    time_elapsed    | 2373     |
|    total_timesteps | 1286144  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 7.62      |
| time/                   |           |
|    fps                  | 542       |
|    iterations           | 158       |
|    time_elapsed         | 2386      |
|    total_timesteps      | 1294336   |
| train/                  |           |
|    approx_kl            | 0.5542236 |
|    clip_fraction        | 0.449     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.347    |
|    explained_variance   | 0.844     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0903   |
|    n_updates            | 1570      |
|    policy_gradient_loss | -0.0643   |
|    value_loss           | 0.00572   |
---------------------------------------
Eval num_timesteps=1300000, episode_reward=7.70 +/- 1.17
Episode length: 959.60 +/- 298.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 960       |
|    mean_reward          | 7.7       |
| time/                   |           |
|    total_timesteps      | 1300000   |
| train/                  |           |
|    approx_kl            | 0.5562036 |
|    clip_fraction        | 0.423     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.321    |
|    explained_variance   | 0.815     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.078    |
|    n_updates            | 1580      |
|    policy_gradient_loss | -0.0576   |
|    value_loss           | 0.00973   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 7.6      |
| time/              |          |
|    fps             | 541      |
|    iterations      | 159      |
|    time_elapsed    | 2403     |
|    total_timesteps | 1302528  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 992        |
|    ep_rew_mean          | 7.6        |
| time/                   |            |
|    fps                  | 542        |
|    iterations           | 160        |
|    time_elapsed         | 2416       |
|    total_timesteps      | 1310720    |
| train/                  |            |
|    approx_kl            | 0.49986732 |
|    clip_fraction        | 0.434      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.339     |
|    explained_variance   | 0.838      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0674    |
|    n_updates            | 1590       |
|    policy_gradient_loss | -0.064     |
|    value_loss           | 0.00666    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 975        |
|    ep_rew_mean          | 7.49       |
| time/                   |            |
|    fps                  | 542        |
|    iterations           | 161        |
|    time_elapsed         | 2429       |
|    total_timesteps      | 1318912    |
| train/                  |            |
|    approx_kl            | 0.69688404 |
|    clip_fraction        | 0.436      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.332     |
|    explained_variance   | 0.835      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0934    |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.0609    |
|    value_loss           | 0.00717    |
----------------------------------------
Eval num_timesteps=1320000, episode_reward=7.10 +/- 0.73
Episode length: 791.00 +/- 181.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 791        |
|    mean_reward          | 7.1        |
| time/                   |            |
|    total_timesteps      | 1320000    |
| train/                  |            |
|    approx_kl            | 0.47734982 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.331     |
|    explained_variance   | 0.815      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0804    |
|    n_updates            | 1610       |
|    policy_gradient_loss | -0.0569    |
|    value_loss           | 0.00836    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 952      |
|    ep_rew_mean     | 7.4      |
| time/              |          |
|    fps             | 542      |
|    iterations      | 162      |
|    time_elapsed    | 2445     |
|    total_timesteps | 1327104  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 967        |
|    ep_rew_mean          | 7.44       |
| time/                   |            |
|    fps                  | 543        |
|    iterations           | 163        |
|    time_elapsed         | 2458       |
|    total_timesteps      | 1335296    |
| train/                  |            |
|    approx_kl            | 0.49237946 |
|    clip_fraction        | 0.414      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.318     |
|    explained_variance   | 0.832      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0703    |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.057     |
|    value_loss           | 0.0082     |
----------------------------------------
Eval num_timesteps=1340000, episode_reward=9.30 +/- 2.71
Episode length: 1098.40 +/- 476.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.1e+03   |
|    mean_reward          | 9.3       |
| time/                   |           |
|    total_timesteps      | 1340000   |
| train/                  |           |
|    approx_kl            | 0.6021558 |
|    clip_fraction        | 0.422     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.319    |
|    explained_variance   | 0.787     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0923   |
|    n_updates            | 1630      |
|    policy_gradient_loss | -0.0577   |
|    value_loss           | 0.00928   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 971      |
|    ep_rew_mean     | 7.51     |
| time/              |          |
|    fps             | 542      |
|    iterations      | 164      |
|    time_elapsed    | 2476     |
|    total_timesteps | 1343488  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 947        |
|    ep_rew_mean          | 7.48       |
| time/                   |            |
|    fps                  | 542        |
|    iterations           | 165        |
|    time_elapsed         | 2489       |
|    total_timesteps      | 1351680    |
| train/                  |            |
|    approx_kl            | 0.58665025 |
|    clip_fraction        | 0.435      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.329     |
|    explained_variance   | 0.786      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0702    |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.0599    |
|    value_loss           | 0.00883    |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 934      |
|    ep_rew_mean          | 7.67     |
| time/                   |          |
|    fps                  | 543      |
|    iterations           | 166      |
|    time_elapsed         | 2502     |
|    total_timesteps      | 1359872  |
| train/                  |          |
|    approx_kl            | 0.548097 |
|    clip_fraction        | 0.432    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.34    |
|    explained_variance   | 0.784    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0914  |
|    n_updates            | 1650     |
|    policy_gradient_loss | -0.0562  |
|    value_loss           | 0.00797  |
--------------------------------------
Eval num_timesteps=1360000, episode_reward=6.80 +/- 2.11
Episode length: 644.80 +/- 407.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 645        |
|    mean_reward          | 6.8        |
| time/                   |            |
|    total_timesteps      | 1360000    |
| train/                  |            |
|    approx_kl            | 0.48079005 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.344     |
|    explained_variance   | 0.862      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0851    |
|    n_updates            | 1660       |
|    policy_gradient_loss | -0.062     |
|    value_loss           | 0.00556    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 918      |
|    ep_rew_mean     | 7.6      |
| time/              |          |
|    fps             | 543      |
|    iterations      | 167      |
|    time_elapsed    | 2518     |
|    total_timesteps | 1368064  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 923       |
|    ep_rew_mean          | 7.62      |
| time/                   |           |
|    fps                  | 543       |
|    iterations           | 168       |
|    time_elapsed         | 2531      |
|    total_timesteps      | 1376256   |
| train/                  |           |
|    approx_kl            | 0.6567575 |
|    clip_fraction        | 0.446     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.36     |
|    explained_variance   | 0.801     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0795   |
|    n_updates            | 1670      |
|    policy_gradient_loss | -0.0572   |
|    value_loss           | 0.00975   |
---------------------------------------
Eval num_timesteps=1380000, episode_reward=7.10 +/- 1.66
Episode length: 778.00 +/- 324.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 778        |
|    mean_reward          | 7.1        |
| time/                   |            |
|    total_timesteps      | 1380000    |
| train/                  |            |
|    approx_kl            | 0.47554153 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.366     |
|    explained_variance   | 0.774      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0517    |
|    n_updates            | 1680       |
|    policy_gradient_loss | -0.0574    |
|    value_loss           | 0.0109     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 900      |
|    ep_rew_mean     | 7.45     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 169      |
|    time_elapsed    | 2547     |
|    total_timesteps | 1384448  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 906       |
|    ep_rew_mean          | 7.49      |
| time/                   |           |
|    fps                  | 543       |
|    iterations           | 170       |
|    time_elapsed         | 2560      |
|    total_timesteps      | 1392640   |
| train/                  |           |
|    approx_kl            | 0.4915064 |
|    clip_fraction        | 0.431     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.352    |
|    explained_variance   | 0.83      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0794   |
|    n_updates            | 1690      |
|    policy_gradient_loss | -0.0568   |
|    value_loss           | 0.00909   |
---------------------------------------
Eval num_timesteps=1400000, episode_reward=7.10 +/- 1.07
Episode length: 864.40 +/- 215.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 864       |
|    mean_reward          | 7.1       |
| time/                   |           |
|    total_timesteps      | 1400000   |
| train/                  |           |
|    approx_kl            | 0.5498265 |
|    clip_fraction        | 0.45      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.356    |
|    explained_variance   | 0.751     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0757   |
|    n_updates            | 1700      |
|    policy_gradient_loss | -0.0602   |
|    value_loss           | 0.0111    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 939      |
|    ep_rew_mean     | 7.54     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 171      |
|    time_elapsed    | 2578     |
|    total_timesteps | 1400832  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 944        |
|    ep_rew_mean          | 7.47       |
| time/                   |            |
|    fps                  | 543        |
|    iterations           | 172        |
|    time_elapsed         | 2591       |
|    total_timesteps      | 1409024    |
| train/                  |            |
|    approx_kl            | 0.45678788 |
|    clip_fraction        | 0.442      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.367     |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0851    |
|    n_updates            | 1710       |
|    policy_gradient_loss | -0.0598    |
|    value_loss           | 0.0109     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 928       |
|    ep_rew_mean          | 7.41      |
| time/                   |           |
|    fps                  | 544       |
|    iterations           | 173       |
|    time_elapsed         | 2604      |
|    total_timesteps      | 1417216   |
| train/                  |           |
|    approx_kl            | 0.5311672 |
|    clip_fraction        | 0.426     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.33     |
|    explained_variance   | 0.842     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0916   |
|    n_updates            | 1720      |
|    policy_gradient_loss | -0.0613   |
|    value_loss           | 0.00706   |
---------------------------------------
Eval num_timesteps=1420000, episode_reward=6.30 +/- 1.03
Episode length: 805.00 +/- 198.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 805       |
|    mean_reward          | 6.3       |
| time/                   |           |
|    total_timesteps      | 1420000   |
| train/                  |           |
|    approx_kl            | 0.4844159 |
|    clip_fraction        | 0.435     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.359    |
|    explained_variance   | 0.785     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0818   |
|    n_updates            | 1730      |
|    policy_gradient_loss | -0.0586   |
|    value_loss           | 0.0123    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 938      |
|    ep_rew_mean     | 7.46     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 174      |
|    time_elapsed    | 2620     |
|    total_timesteps | 1425408  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 939       |
|    ep_rew_mean          | 7.47      |
| time/                   |           |
|    fps                  | 544       |
|    iterations           | 175       |
|    time_elapsed         | 2633      |
|    total_timesteps      | 1433600   |
| train/                  |           |
|    approx_kl            | 0.5835021 |
|    clip_fraction        | 0.455     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.351    |
|    explained_variance   | 0.819     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0742   |
|    n_updates            | 1740      |
|    policy_gradient_loss | -0.0646   |
|    value_loss           | 0.00745   |
---------------------------------------
Eval num_timesteps=1440000, episode_reward=7.90 +/- 1.20
Episode length: 924.40 +/- 256.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 924        |
|    mean_reward          | 7.9        |
| time/                   |            |
|    total_timesteps      | 1440000    |
| train/                  |            |
|    approx_kl            | 0.47455055 |
|    clip_fraction        | 0.447      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.349     |
|    explained_variance   | 0.809      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0842    |
|    n_updates            | 1750       |
|    policy_gradient_loss | -0.0635    |
|    value_loss           | 0.00786    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 953      |
|    ep_rew_mean     | 7.51     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 176      |
|    time_elapsed    | 2651     |
|    total_timesteps | 1441792  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 955       |
|    ep_rew_mean          | 7.55      |
| time/                   |           |
|    fps                  | 544       |
|    iterations           | 177       |
|    time_elapsed         | 2664      |
|    total_timesteps      | 1449984   |
| train/                  |           |
|    approx_kl            | 0.5915249 |
|    clip_fraction        | 0.442     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.34     |
|    explained_variance   | 0.842     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0683   |
|    n_updates            | 1760      |
|    policy_gradient_loss | -0.0623   |
|    value_loss           | 0.00786   |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 937       |
|    ep_rew_mean          | 7.45      |
| time/                   |           |
|    fps                  | 544       |
|    iterations           | 178       |
|    time_elapsed         | 2676      |
|    total_timesteps      | 1458176   |
| train/                  |           |
|    approx_kl            | 0.5863905 |
|    clip_fraction        | 0.451     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.354    |
|    explained_variance   | 0.865     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0962   |
|    n_updates            | 1770      |
|    policy_gradient_loss | -0.0659   |
|    value_loss           | 0.00575   |
---------------------------------------
Eval num_timesteps=1460000, episode_reward=6.80 +/- 1.57
Episode length: 740.80 +/- 380.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 741        |
|    mean_reward          | 6.8        |
| time/                   |            |
|    total_timesteps      | 1460000    |
| train/                  |            |
|    approx_kl            | 0.47749978 |
|    clip_fraction        | 0.435      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.351     |
|    explained_variance   | 0.867      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00349    |
|    n_updates            | 1780       |
|    policy_gradient_loss | -0.0576    |
|    value_loss           | 0.00835    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 972      |
|    ep_rew_mean     | 7.59     |
| time/              |          |
|    fps             | 544      |
|    iterations      | 179      |
|    time_elapsed    | 2693     |
|    total_timesteps | 1466368  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 956        |
|    ep_rew_mean          | 7.44       |
| time/                   |            |
|    fps                  | 544        |
|    iterations           | 180        |
|    time_elapsed         | 2706       |
|    total_timesteps      | 1474560    |
| train/                  |            |
|    approx_kl            | 0.54923713 |
|    clip_fraction        | 0.447      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.364     |
|    explained_variance   | 0.803      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0757    |
|    n_updates            | 1790       |
|    policy_gradient_loss | -0.0634    |
|    value_loss           | 0.00915    |
----------------------------------------
Eval num_timesteps=1480000, episode_reward=8.70 +/- 0.75
Episode length: 1252.80 +/- 294.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.25e+03  |
|    mean_reward          | 8.7       |
| time/                   |           |
|    total_timesteps      | 1480000   |
| train/                  |           |
|    approx_kl            | 0.6590144 |
|    clip_fraction        | 0.436     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.347    |
|    explained_variance   | 0.791     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0692   |
|    n_updates            | 1800      |
|    policy_gradient_loss | -0.0572   |
|    value_loss           | 0.0123    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 967      |
|    ep_rew_mean     | 7.48     |
| time/              |          |
|    fps             | 544      |
|    iterations      | 181      |
|    time_elapsed    | 2725     |
|    total_timesteps | 1482752  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 983       |
|    ep_rew_mean          | 7.58      |
| time/                   |           |
|    fps                  | 544       |
|    iterations           | 182       |
|    time_elapsed         | 2738      |
|    total_timesteps      | 1490944   |
| train/                  |           |
|    approx_kl            | 0.5336238 |
|    clip_fraction        | 0.432     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.341    |
|    explained_variance   | 0.806     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0869   |
|    n_updates            | 1810      |
|    policy_gradient_loss | -0.059    |
|    value_loss           | 0.0101    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 947        |
|    ep_rew_mean          | 7.46       |
| time/                   |            |
|    fps                  | 544        |
|    iterations           | 183        |
|    time_elapsed         | 2750       |
|    total_timesteps      | 1499136    |
| train/                  |            |
|    approx_kl            | 0.65794194 |
|    clip_fraction        | 0.441      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.347     |
|    explained_variance   | 0.817      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0752    |
|    n_updates            | 1820       |
|    policy_gradient_loss | -0.0588    |
|    value_loss           | 0.00782    |
----------------------------------------
Eval num_timesteps=1500000, episode_reward=8.10 +/- 2.13
Episode length: 1169.60 +/- 582.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.17e+03   |
|    mean_reward          | 8.1        |
| time/                   |            |
|    total_timesteps      | 1500000    |
| train/                  |            |
|    approx_kl            | 0.54327905 |
|    clip_fraction        | 0.424      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.329     |
|    explained_variance   | 0.837      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0641    |
|    n_updates            | 1830       |
|    policy_gradient_loss | -0.0589    |
|    value_loss           | 0.00848    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 926      |
|    ep_rew_mean     | 7.4      |
| time/              |          |
|    fps             | 543      |
|    iterations      | 184      |
|    time_elapsed    | 2771     |
|    total_timesteps | 1507328  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 942        |
|    ep_rew_mean          | 7.46       |
| time/                   |            |
|    fps                  | 544        |
|    iterations           | 185        |
|    time_elapsed         | 2784       |
|    total_timesteps      | 1515520    |
| train/                  |            |
|    approx_kl            | 0.49103498 |
|    clip_fraction        | 0.428      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.333     |
|    explained_variance   | 0.792      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0777    |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.0556    |
|    value_loss           | 0.0113     |
----------------------------------------
Eval num_timesteps=1520000, episode_reward=7.40 +/- 1.39
Episode length: 834.00 +/- 342.01
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 834        |
|    mean_reward          | 7.4        |
| time/                   |            |
|    total_timesteps      | 1520000    |
| train/                  |            |
|    approx_kl            | 0.49183217 |
|    clip_fraction        | 0.447      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.353     |
|    explained_variance   | 0.806      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0791    |
|    n_updates            | 1850       |
|    policy_gradient_loss | -0.0619    |
|    value_loss           | 0.0112     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 933      |
|    ep_rew_mean     | 7.47     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 186      |
|    time_elapsed    | 2801     |
|    total_timesteps | 1523712  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 895      |
|    ep_rew_mean          | 7.34     |
| time/                   |          |
|    fps                  | 544      |
|    iterations           | 187      |
|    time_elapsed         | 2814     |
|    total_timesteps      | 1531904  |
| train/                  |          |
|    approx_kl            | 0.521617 |
|    clip_fraction        | 0.443    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.352   |
|    explained_variance   | 0.812    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0642  |
|    n_updates            | 1860     |
|    policy_gradient_loss | -0.0609  |
|    value_loss           | 0.00956  |
--------------------------------------
Eval num_timesteps=1540000, episode_reward=8.20 +/- 1.72
Episode length: 1027.80 +/- 241.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.03e+03  |
|    mean_reward          | 8.2       |
| time/                   |           |
|    total_timesteps      | 1540000   |
| train/                  |           |
|    approx_kl            | 0.5619003 |
|    clip_fraction        | 0.442     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.328    |
|    explained_variance   | 0.846     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0903   |
|    n_updates            | 1870      |
|    policy_gradient_loss | -0.0622   |
|    value_loss           | 0.00633   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 898      |
|    ep_rew_mean     | 7.33     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 188      |
|    time_elapsed    | 2831     |
|    total_timesteps | 1540096  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 898        |
|    ep_rew_mean          | 7.34       |
| time/                   |            |
|    fps                  | 544        |
|    iterations           | 189        |
|    time_elapsed         | 2844       |
|    total_timesteps      | 1548288    |
| train/                  |            |
|    approx_kl            | 0.48516157 |
|    clip_fraction        | 0.42       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.339     |
|    explained_variance   | 0.804      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0768    |
|    n_updates            | 1880       |
|    policy_gradient_loss | -0.0541    |
|    value_loss           | 0.00931    |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 911       |
|    ep_rew_mean          | 7.44      |
| time/                   |           |
|    fps                  | 544       |
|    iterations           | 190       |
|    time_elapsed         | 2857      |
|    total_timesteps      | 1556480   |
| train/                  |           |
|    approx_kl            | 0.5762563 |
|    clip_fraction        | 0.449     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.352    |
|    explained_variance   | 0.824     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0787   |
|    n_updates            | 1890      |
|    policy_gradient_loss | -0.0607   |
|    value_loss           | 0.00668   |
---------------------------------------
Eval num_timesteps=1560000, episode_reward=9.50 +/- 4.56
Episode length: 1187.60 +/- 765.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.19e+03  |
|    mean_reward          | 9.5       |
| time/                   |           |
|    total_timesteps      | 1560000   |
| train/                  |           |
|    approx_kl            | 0.5409614 |
|    clip_fraction        | 0.443     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.348    |
|    explained_variance   | 0.796     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0653   |
|    n_updates            | 1900      |
|    policy_gradient_loss | -0.0586   |
|    value_loss           | 0.00907   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 895      |
|    ep_rew_mean     | 7.46     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 191      |
|    time_elapsed    | 2876     |
|    total_timesteps | 1564672  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 880        |
|    ep_rew_mean          | 7.48       |
| time/                   |            |
|    fps                  | 544        |
|    iterations           | 192        |
|    time_elapsed         | 2889       |
|    total_timesteps      | 1572864    |
| train/                  |            |
|    approx_kl            | 0.51716053 |
|    clip_fraction        | 0.433      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.332     |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.073     |
|    n_updates            | 1910       |
|    policy_gradient_loss | -0.0566    |
|    value_loss           | 0.00828    |
----------------------------------------
Eval num_timesteps=1580000, episode_reward=6.00 +/- 1.48
Episode length: 872.40 +/- 407.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 872       |
|    mean_reward          | 6         |
| time/                   |           |
|    total_timesteps      | 1580000   |
| train/                  |           |
|    approx_kl            | 0.4791425 |
|    clip_fraction        | 0.433     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.346    |
|    explained_variance   | 0.856     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0817   |
|    n_updates            | 1920      |
|    policy_gradient_loss | -0.0575   |
|    value_loss           | 0.00644   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 881      |
|    ep_rew_mean     | 7.48     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 193      |
|    time_elapsed    | 2907     |
|    total_timesteps | 1581056  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 883        |
|    ep_rew_mean          | 7.42       |
| time/                   |            |
|    fps                  | 544        |
|    iterations           | 194        |
|    time_elapsed         | 2920       |
|    total_timesteps      | 1589248    |
| train/                  |            |
|    approx_kl            | 0.45456317 |
|    clip_fraction        | 0.435      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.363     |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0848    |
|    n_updates            | 1930       |
|    policy_gradient_loss | -0.0596    |
|    value_loss           | 0.00841    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 871        |
|    ep_rew_mean          | 7.38       |
| time/                   |            |
|    fps                  | 544        |
|    iterations           | 195        |
|    time_elapsed         | 2933       |
|    total_timesteps      | 1597440    |
| train/                  |            |
|    approx_kl            | 0.45904744 |
|    clip_fraction        | 0.431      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.35      |
|    explained_variance   | 0.826      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0936    |
|    n_updates            | 1940       |
|    policy_gradient_loss | -0.0563    |
|    value_loss           | 0.0111     |
----------------------------------------
Eval num_timesteps=1600000, episode_reward=8.50 +/- 2.35
Episode length: 966.40 +/- 392.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 966        |
|    mean_reward          | 8.5        |
| time/                   |            |
|    total_timesteps      | 1600000    |
| train/                  |            |
|    approx_kl            | 0.56024647 |
|    clip_fraction        | 0.411      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.322     |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0483    |
|    n_updates            | 1950       |
|    policy_gradient_loss | -0.0543    |
|    value_loss           | 0.0107     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 863      |
|    ep_rew_mean     | 7.42     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 196      |
|    time_elapsed    | 2952     |
|    total_timesteps | 1605632  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 865        |
|    ep_rew_mean          | 7.46       |
| time/                   |            |
|    fps                  | 544        |
|    iterations           | 197        |
|    time_elapsed         | 2965       |
|    total_timesteps      | 1613824    |
| train/                  |            |
|    approx_kl            | 0.47763538 |
|    clip_fraction        | 0.443      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.357     |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0645    |
|    n_updates            | 1960       |
|    policy_gradient_loss | -0.061     |
|    value_loss           | 0.00792    |
----------------------------------------
Eval num_timesteps=1620000, episode_reward=11.20 +/- 2.68
Episode length: 1503.40 +/- 467.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.5e+03   |
|    mean_reward          | 11.2      |
| time/                   |           |
|    total_timesteps      | 1620000   |
| train/                  |           |
|    approx_kl            | 0.5102553 |
|    clip_fraction        | 0.446     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.356    |
|    explained_variance   | 0.84      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.058    |
|    n_updates            | 1970      |
|    policy_gradient_loss | -0.0633   |
|    value_loss           | 0.00656   |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 884      |
|    ep_rew_mean     | 7.5      |
| time/              |          |
|    fps             | 543      |
|    iterations      | 198      |
|    time_elapsed    | 2983     |
|    total_timesteps | 1622016  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 918      |
|    ep_rew_mean          | 7.68     |
| time/                   |          |
|    fps                  | 544      |
|    iterations           | 199      |
|    time_elapsed         | 2996     |
|    total_timesteps      | 1630208  |
| train/                  |          |
|    approx_kl            | 0.544181 |
|    clip_fraction        | 0.458    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.379   |
|    explained_variance   | 0.856    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0817  |
|    n_updates            | 1980     |
|    policy_gradient_loss | -0.0655  |
|    value_loss           | 0.00531  |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 898       |
|    ep_rew_mean          | 7.62      |
| time/                   |           |
|    fps                  | 544       |
|    iterations           | 200       |
|    time_elapsed         | 3010      |
|    total_timesteps      | 1638400   |
| train/                  |           |
|    approx_kl            | 0.5199977 |
|    clip_fraction        | 0.468     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.369    |
|    explained_variance   | 0.842     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0882   |
|    n_updates            | 1990      |
|    policy_gradient_loss | -0.0644   |
|    value_loss           | 0.00605   |
---------------------------------------
Eval num_timesteps=1640000, episode_reward=8.70 +/- 1.81
Episode length: 968.40 +/- 304.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 968       |
|    mean_reward          | 8.7       |
| time/                   |           |
|    total_timesteps      | 1640000   |
| train/                  |           |
|    approx_kl            | 0.6166966 |
|    clip_fraction        | 0.448     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.352    |
|    explained_variance   | 0.837     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.063    |
|    n_updates            | 2000      |
|    policy_gradient_loss | -0.0587   |
|    value_loss           | 0.00792   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 896      |
|    ep_rew_mean     | 7.53     |
| time/              |          |
|    fps             | 544      |
|    iterations      | 201      |
|    time_elapsed    | 3026     |
|    total_timesteps | 1646592  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 916        |
|    ep_rew_mean          | 7.61       |
| time/                   |            |
|    fps                  | 544        |
|    iterations           | 202        |
|    time_elapsed         | 3039       |
|    total_timesteps      | 1654784    |
| train/                  |            |
|    approx_kl            | 0.56536233 |
|    clip_fraction        | 0.451      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.347     |
|    explained_variance   | 0.743      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0918    |
|    n_updates            | 2010       |
|    policy_gradient_loss | -0.0616    |
|    value_loss           | 0.00872    |
----------------------------------------
Eval num_timesteps=1660000, episode_reward=7.50 +/- 1.64
Episode length: 939.80 +/- 443.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 940        |
|    mean_reward          | 7.5        |
| time/                   |            |
|    total_timesteps      | 1660000    |
| train/                  |            |
|    approx_kl            | 0.82321775 |
|    clip_fraction        | 0.441      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.344     |
|    explained_variance   | 0.797      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0958    |
|    n_updates            | 2020       |
|    policy_gradient_loss | -0.0562    |
|    value_loss           | 0.00554    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 921      |
|    ep_rew_mean     | 7.61     |
| time/              |          |
|    fps             | 544      |
|    iterations      | 203      |
|    time_elapsed    | 3056     |
|    total_timesteps | 1662976  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 928       |
|    ep_rew_mean          | 7.62      |
| time/                   |           |
|    fps                  | 544       |
|    iterations           | 204       |
|    time_elapsed         | 3069      |
|    total_timesteps      | 1671168   |
| train/                  |           |
|    approx_kl            | 0.4698276 |
|    clip_fraction        | 0.433     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.348    |
|    explained_variance   | 0.791     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0657   |
|    n_updates            | 2030      |
|    policy_gradient_loss | -0.0565   |
|    value_loss           | 0.00977   |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 942       |
|    ep_rew_mean          | 7.67      |
| time/                   |           |
|    fps                  | 544       |
|    iterations           | 205       |
|    time_elapsed         | 3082      |
|    total_timesteps      | 1679360   |
| train/                  |           |
|    approx_kl            | 0.5330404 |
|    clip_fraction        | 0.452     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.364    |
|    explained_variance   | 0.838     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0819   |
|    n_updates            | 2040      |
|    policy_gradient_loss | -0.0606   |
|    value_loss           | 0.00682   |
---------------------------------------
Eval num_timesteps=1680000, episode_reward=8.80 +/- 2.64
Episode length: 1079.20 +/- 417.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.08e+03   |
|    mean_reward          | 8.8        |
| time/                   |            |
|    total_timesteps      | 1680000    |
| train/                  |            |
|    approx_kl            | 0.47853732 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.35      |
|    explained_variance   | 0.808      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.102     |
|    n_updates            | 2050       |
|    policy_gradient_loss | -0.0584    |
|    value_loss           | 0.00945    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 950      |
|    ep_rew_mean     | 7.78     |
| time/              |          |
|    fps             | 544      |
|    iterations      | 206      |
|    time_elapsed    | 3101     |
|    total_timesteps | 1687552  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 968        |
|    ep_rew_mean          | 7.82       |
| time/                   |            |
|    fps                  | 544        |
|    iterations           | 207        |
|    time_elapsed         | 3114       |
|    total_timesteps      | 1695744    |
| train/                  |            |
|    approx_kl            | 0.48244295 |
|    clip_fraction        | 0.424      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.361     |
|    explained_variance   | 0.805      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0676    |
|    n_updates            | 2060       |
|    policy_gradient_loss | -0.0562    |
|    value_loss           | 0.00802    |
----------------------------------------
Eval num_timesteps=1700000, episode_reward=7.70 +/- 1.54
Episode length: 841.00 +/- 280.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 841       |
|    mean_reward          | 7.7       |
| time/                   |           |
|    total_timesteps      | 1700000   |
| train/                  |           |
|    approx_kl            | 0.5037583 |
|    clip_fraction        | 0.414     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.317    |
|    explained_variance   | 0.818     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0698   |
|    n_updates            | 2070      |
|    policy_gradient_loss | -0.0577   |
|    value_loss           | 0.00935   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 940      |
|    ep_rew_mean     | 7.75     |
| time/              |          |
|    fps             | 544      |
|    iterations      | 208      |
|    time_elapsed    | 3130     |
|    total_timesteps | 1703936  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 934       |
|    ep_rew_mean          | 7.67      |
| time/                   |           |
|    fps                  | 544       |
|    iterations           | 209       |
|    time_elapsed         | 3143      |
|    total_timesteps      | 1712128   |
| train/                  |           |
|    approx_kl            | 0.5022679 |
|    clip_fraction        | 0.41      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.318    |
|    explained_variance   | 0.808     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0764   |
|    n_updates            | 2080      |
|    policy_gradient_loss | -0.0537   |
|    value_loss           | 0.0093    |
---------------------------------------
Eval num_timesteps=1720000, episode_reward=7.10 +/- 1.07
Episode length: 724.80 +/- 220.49
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 725        |
|    mean_reward          | 7.1        |
| time/                   |            |
|    total_timesteps      | 1720000    |
| train/                  |            |
|    approx_kl            | 0.53571165 |
|    clip_fraction        | 0.431      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.349     |
|    explained_variance   | 0.829      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.069     |
|    n_updates            | 2090       |
|    policy_gradient_loss | -0.0619    |
|    value_loss           | 0.00774    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 907      |
|    ep_rew_mean     | 7.52     |
| time/              |          |
|    fps             | 544      |
|    iterations      | 210      |
|    time_elapsed    | 3160     |
|    total_timesteps | 1720320  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 905        |
|    ep_rew_mean          | 7.46       |
| time/                   |            |
|    fps                  | 544        |
|    iterations           | 211        |
|    time_elapsed         | 3173       |
|    total_timesteps      | 1728512    |
| train/                  |            |
|    approx_kl            | 0.64362365 |
|    clip_fraction        | 0.407      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.314     |
|    explained_variance   | 0.817      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0686    |
|    n_updates            | 2100       |
|    policy_gradient_loss | -0.0527    |
|    value_loss           | 0.0103     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 932       |
|    ep_rew_mean          | 7.63      |
| time/                   |           |
|    fps                  | 544       |
|    iterations           | 212       |
|    time_elapsed         | 3187      |
|    total_timesteps      | 1736704   |
| train/                  |           |
|    approx_kl            | 0.5745308 |
|    clip_fraction        | 0.422     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.339    |
|    explained_variance   | 0.815     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.1      |
|    n_updates            | 2110      |
|    policy_gradient_loss | -0.0586   |
|    value_loss           | 0.0101    |
---------------------------------------
Eval num_timesteps=1740000, episode_reward=7.60 +/- 1.46
Episode length: 875.40 +/- 288.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 875       |
|    mean_reward          | 7.6       |
| time/                   |           |
|    total_timesteps      | 1740000   |
| train/                  |           |
|    approx_kl            | 1.7417624 |
|    clip_fraction        | 0.435     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.344    |
|    explained_variance   | 0.828     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.107    |
|    n_updates            | 2120      |
|    policy_gradient_loss | -0.0615   |
|    value_loss           | 0.00782   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 921      |
|    ep_rew_mean     | 7.61     |
| time/              |          |
|    fps             | 544      |
|    iterations      | 213      |
|    time_elapsed    | 3205     |
|    total_timesteps | 1744896  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 908        |
|    ep_rew_mean          | 7.54       |
| time/                   |            |
|    fps                  | 544        |
|    iterations           | 214        |
|    time_elapsed         | 3218       |
|    total_timesteps      | 1753088    |
| train/                  |            |
|    approx_kl            | 0.73152685 |
|    clip_fraction        | 0.403      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.305     |
|    explained_variance   | 0.809      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0724    |
|    n_updates            | 2130       |
|    policy_gradient_loss | -0.0538    |
|    value_loss           | 0.0103     |
----------------------------------------
Eval num_timesteps=1760000, episode_reward=9.90 +/- 3.01
Episode length: 1282.40 +/- 494.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.28e+03   |
|    mean_reward          | 9.9        |
| time/                   |            |
|    total_timesteps      | 1760000    |
| train/                  |            |
|    approx_kl            | 0.59313595 |
|    clip_fraction        | 0.422      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.324     |
|    explained_variance   | 0.853      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0719    |
|    n_updates            | 2140       |
|    policy_gradient_loss | -0.0594    |
|    value_loss           | 0.00598    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 906      |
|    ep_rew_mean     | 7.57     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 215      |
|    time_elapsed    | 3238     |
|    total_timesteps | 1761280  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 886       |
|    ep_rew_mean          | 7.55      |
| time/                   |           |
|    fps                  | 544       |
|    iterations           | 216       |
|    time_elapsed         | 3252      |
|    total_timesteps      | 1769472   |
| train/                  |           |
|    approx_kl            | 0.6019514 |
|    clip_fraction        | 0.418     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.313    |
|    explained_variance   | 0.828     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0574   |
|    n_updates            | 2150      |
|    policy_gradient_loss | -0.0577   |
|    value_loss           | 0.00785   |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 896       |
|    ep_rew_mean          | 7.59      |
| time/                   |           |
|    fps                  | 544       |
|    iterations           | 217       |
|    time_elapsed         | 3265      |
|    total_timesteps      | 1777664   |
| train/                  |           |
|    approx_kl            | 0.7957994 |
|    clip_fraction        | 0.404     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.291    |
|    explained_variance   | 0.83      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.081    |
|    n_updates            | 2160      |
|    policy_gradient_loss | -0.053    |
|    value_loss           | 0.00732   |
---------------------------------------
Eval num_timesteps=1780000, episode_reward=7.10 +/- 2.06
Episode length: 824.60 +/- 335.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 825        |
|    mean_reward          | 7.1        |
| time/                   |            |
|    total_timesteps      | 1780000    |
| train/                  |            |
|    approx_kl            | 0.65924764 |
|    clip_fraction        | 0.397      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.275     |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0686    |
|    n_updates            | 2170       |
|    policy_gradient_loss | -0.0507    |
|    value_loss           | 0.00791    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 896      |
|    ep_rew_mean     | 7.64     |
| time/              |          |
|    fps             | 544      |
|    iterations      | 218      |
|    time_elapsed    | 3281     |
|    total_timesteps | 1785856  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 900        |
|    ep_rew_mean          | 7.64       |
| time/                   |            |
|    fps                  | 544        |
|    iterations           | 219        |
|    time_elapsed         | 3294       |
|    total_timesteps      | 1794048    |
| train/                  |            |
|    approx_kl            | 0.60793823 |
|    clip_fraction        | 0.396      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.281     |
|    explained_variance   | 0.862      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.103     |
|    n_updates            | 2180       |
|    policy_gradient_loss | -0.0502    |
|    value_loss           | 0.00686    |
----------------------------------------
Eval num_timesteps=1800000, episode_reward=8.80 +/- 1.96
Episode length: 1099.80 +/- 397.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.1e+03   |
|    mean_reward          | 8.8       |
| time/                   |           |
|    total_timesteps      | 1800000   |
| train/                  |           |
|    approx_kl            | 0.5399439 |
|    clip_fraction        | 0.402     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.307    |
|    explained_variance   | 0.872     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0869   |
|    n_updates            | 2190      |
|    policy_gradient_loss | -0.0543   |
|    value_loss           | 0.00681   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 912      |
|    ep_rew_mean     | 7.66     |
| time/              |          |
|    fps             | 544      |
|    iterations      | 220      |
|    time_elapsed    | 3311     |
|    total_timesteps | 1802240  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 901       |
|    ep_rew_mean          | 7.62      |
| time/                   |           |
|    fps                  | 544       |
|    iterations           | 221       |
|    time_elapsed         | 3324      |
|    total_timesteps      | 1810432   |
| train/                  |           |
|    approx_kl            | 0.5306623 |
|    clip_fraction        | 0.395     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.297    |
|    explained_variance   | 0.789     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.038    |
|    n_updates            | 2200      |
|    policy_gradient_loss | -0.0515   |
|    value_loss           | 0.00872   |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 893       |
|    ep_rew_mean          | 7.57      |
| time/                   |           |
|    fps                  | 544       |
|    iterations           | 222       |
|    time_elapsed         | 3337      |
|    total_timesteps      | 1818624   |
| train/                  |           |
|    approx_kl            | 0.6217803 |
|    clip_fraction        | 0.382     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.293    |
|    explained_variance   | 0.828     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0404   |
|    n_updates            | 2210      |
|    policy_gradient_loss | -0.0509   |
|    value_loss           | 0.0088    |
---------------------------------------
Eval num_timesteps=1820000, episode_reward=7.70 +/- 0.98
Episode length: 1114.20 +/- 351.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.11e+03   |
|    mean_reward          | 7.7        |
| time/                   |            |
|    total_timesteps      | 1820000    |
| train/                  |            |
|    approx_kl            | 0.58283424 |
|    clip_fraction        | 0.398      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.316     |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0754    |
|    n_updates            | 2220       |
|    policy_gradient_loss | -0.0539    |
|    value_loss           | 0.0102     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 883      |
|    ep_rew_mean     | 7.5      |
| time/              |          |
|    fps             | 544      |
|    iterations      | 223      |
|    time_elapsed    | 3354     |
|    total_timesteps | 1826816  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 898       |
|    ep_rew_mean          | 7.55      |
| time/                   |           |
|    fps                  | 544       |
|    iterations           | 224       |
|    time_elapsed         | 3367      |
|    total_timesteps      | 1835008   |
| train/                  |           |
|    approx_kl            | 0.5335681 |
|    clip_fraction        | 0.412     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.314    |
|    explained_variance   | 0.819     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0549   |
|    n_updates            | 2230      |
|    policy_gradient_loss | -0.0546   |
|    value_loss           | 0.00811   |
---------------------------------------
Eval num_timesteps=1840000, episode_reward=7.70 +/- 2.29
Episode length: 993.20 +/- 212.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 993        |
|    mean_reward          | 7.7        |
| time/                   |            |
|    total_timesteps      | 1840000    |
| train/                  |            |
|    approx_kl            | 0.55963314 |
|    clip_fraction        | 0.421      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.327     |
|    explained_variance   | 0.771      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0812    |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.0559    |
|    value_loss           | 0.00855    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 902      |
|    ep_rew_mean     | 7.56     |
| time/              |          |
|    fps             | 544      |
|    iterations      | 225      |
|    time_elapsed    | 3385     |
|    total_timesteps | 1843200  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 885        |
|    ep_rew_mean          | 7.36       |
| time/                   |            |
|    fps                  | 544        |
|    iterations           | 226        |
|    time_elapsed         | 3398       |
|    total_timesteps      | 1851392    |
| train/                  |            |
|    approx_kl            | 0.61009026 |
|    clip_fraction        | 0.391      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.282     |
|    explained_variance   | 0.808      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0802    |
|    n_updates            | 2250       |
|    policy_gradient_loss | -0.0508    |
|    value_loss           | 0.00969    |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 888       |
|    ep_rew_mean          | 7.3       |
| time/                   |           |
|    fps                  | 545       |
|    iterations           | 227       |
|    time_elapsed         | 3411      |
|    total_timesteps      | 1859584   |
| train/                  |           |
|    approx_kl            | 0.5023252 |
|    clip_fraction        | 0.379     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.293    |
|    explained_variance   | 0.803     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0846   |
|    n_updates            | 2260      |
|    policy_gradient_loss | -0.0485   |
|    value_loss           | 0.0112    |
---------------------------------------
Eval num_timesteps=1860000, episode_reward=6.80 +/- 1.12
Episode length: 709.80 +/- 332.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 710        |
|    mean_reward          | 6.8        |
| time/                   |            |
|    total_timesteps      | 1860000    |
| train/                  |            |
|    approx_kl            | 0.56058824 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.298     |
|    explained_variance   | 0.827      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0578    |
|    n_updates            | 2270       |
|    policy_gradient_loss | -0.0507    |
|    value_loss           | 0.00962    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 867      |
|    ep_rew_mean     | 7.17     |
| time/              |          |
|    fps             | 544      |
|    iterations      | 228      |
|    time_elapsed    | 3428     |
|    total_timesteps | 1867776  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 866        |
|    ep_rew_mean          | 7.08       |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 229        |
|    time_elapsed         | 3441       |
|    total_timesteps      | 1875968    |
| train/                  |            |
|    approx_kl            | 0.50877476 |
|    clip_fraction        | 0.416      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.325     |
|    explained_variance   | 0.806      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0557    |
|    n_updates            | 2280       |
|    policy_gradient_loss | -0.0523    |
|    value_loss           | 0.0118     |
----------------------------------------
Eval num_timesteps=1880000, episode_reward=7.00 +/- 2.02
Episode length: 705.80 +/- 272.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 706       |
|    mean_reward          | 7         |
| time/                   |           |
|    total_timesteps      | 1880000   |
| train/                  |           |
|    approx_kl            | 0.5782933 |
|    clip_fraction        | 0.402     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.315    |
|    explained_variance   | 0.81      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0726   |
|    n_updates            | 2290      |
|    policy_gradient_loss | -0.05     |
|    value_loss           | 0.00895   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 861      |
|    ep_rew_mean     | 7.03     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 230      |
|    time_elapsed    | 3456     |
|    total_timesteps | 1884160  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 892       |
|    ep_rew_mean          | 7.17      |
| time/                   |           |
|    fps                  | 545       |
|    iterations           | 231       |
|    time_elapsed         | 3469      |
|    total_timesteps      | 1892352   |
| train/                  |           |
|    approx_kl            | 0.5162224 |
|    clip_fraction        | 0.414     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.339    |
|    explained_variance   | 0.823     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0557   |
|    n_updates            | 2300      |
|    policy_gradient_loss | -0.0569   |
|    value_loss           | 0.0111    |
---------------------------------------
Eval num_timesteps=1900000, episode_reward=6.70 +/- 0.81
Episode length: 713.60 +/- 221.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 714       |
|    mean_reward          | 6.7       |
| time/                   |           |
|    total_timesteps      | 1900000   |
| train/                  |           |
|    approx_kl            | 0.4840027 |
|    clip_fraction        | 0.413     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.324    |
|    explained_variance   | 0.787     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0589   |
|    n_updates            | 2310      |
|    policy_gradient_loss | -0.0545   |
|    value_loss           | 0.00963   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 900      |
|    ep_rew_mean     | 7.22     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 232      |
|    time_elapsed    | 3486     |
|    total_timesteps | 1900544  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 913        |
|    ep_rew_mean          | 7.33       |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 233        |
|    time_elapsed         | 3499       |
|    total_timesteps      | 1908736    |
| train/                  |            |
|    approx_kl            | 0.46996284 |
|    clip_fraction        | 0.425      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.344     |
|    explained_variance   | 0.796      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0705    |
|    n_updates            | 2320       |
|    policy_gradient_loss | -0.055     |
|    value_loss           | 0.01       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 922       |
|    ep_rew_mean          | 7.34      |
| time/                   |           |
|    fps                  | 545       |
|    iterations           | 234       |
|    time_elapsed         | 3512      |
|    total_timesteps      | 1916928   |
| train/                  |           |
|    approx_kl            | 0.5758475 |
|    clip_fraction        | 0.412     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.329    |
|    explained_variance   | 0.822     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0621   |
|    n_updates            | 2330      |
|    policy_gradient_loss | -0.0562   |
|    value_loss           | 0.00782   |
---------------------------------------
Eval num_timesteps=1920000, episode_reward=7.50 +/- 1.26
Episode length: 738.40 +/- 257.93
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 738        |
|    mean_reward          | 7.5        |
| time/                   |            |
|    total_timesteps      | 1920000    |
| train/                  |            |
|    approx_kl            | 0.45314518 |
|    clip_fraction        | 0.401      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.319     |
|    explained_variance   | 0.823      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0573    |
|    n_updates            | 2340       |
|    policy_gradient_loss | -0.0546    |
|    value_loss           | 0.0103     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 932      |
|    ep_rew_mean     | 7.38     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 235      |
|    time_elapsed    | 3529     |
|    total_timesteps | 1925120  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 932       |
|    ep_rew_mean          | 7.37      |
| time/                   |           |
|    fps                  | 545       |
|    iterations           | 236       |
|    time_elapsed         | 3542      |
|    total_timesteps      | 1933312   |
| train/                  |           |
|    approx_kl            | 0.4895212 |
|    clip_fraction        | 0.385     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.303    |
|    explained_variance   | 0.827     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0598   |
|    n_updates            | 2350      |
|    policy_gradient_loss | -0.0518   |
|    value_loss           | 0.00726   |
---------------------------------------
Eval num_timesteps=1940000, episode_reward=8.50 +/- 1.92
Episode length: 963.80 +/- 310.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 964       |
|    mean_reward          | 8.5       |
| time/                   |           |
|    total_timesteps      | 1940000   |
| train/                  |           |
|    approx_kl            | 0.5544656 |
|    clip_fraction        | 0.413     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.328    |
|    explained_variance   | 0.778     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.106    |
|    n_updates            | 2360      |
|    policy_gradient_loss | -0.0548   |
|    value_loss           | 0.0121    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 932      |
|    ep_rew_mean     | 7.42     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 237      |
|    time_elapsed    | 3558     |
|    total_timesteps | 1941504  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 959       |
|    ep_rew_mean          | 7.47      |
| time/                   |           |
|    fps                  | 545       |
|    iterations           | 238       |
|    time_elapsed         | 3571      |
|    total_timesteps      | 1949696   |
| train/                  |           |
|    approx_kl            | 0.5023041 |
|    clip_fraction        | 0.423     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.331    |
|    explained_variance   | 0.777     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0699   |
|    n_updates            | 2370      |
|    policy_gradient_loss | -0.0578   |
|    value_loss           | 0.00978   |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 959        |
|    ep_rew_mean          | 7.55       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 239        |
|    time_elapsed         | 3584       |
|    total_timesteps      | 1957888    |
| train/                  |            |
|    approx_kl            | 0.54143655 |
|    clip_fraction        | 0.399      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.317     |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0715    |
|    n_updates            | 2380       |
|    policy_gradient_loss | -0.0564    |
|    value_loss           | 0.0109     |
----------------------------------------
Eval num_timesteps=1960000, episode_reward=8.60 +/- 0.86
Episode length: 975.60 +/- 139.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 976       |
|    mean_reward          | 8.6       |
| time/                   |           |
|    total_timesteps      | 1960000   |
| train/                  |           |
|    approx_kl            | 0.5397957 |
|    clip_fraction        | 0.414     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.315    |
|    explained_variance   | 0.802     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0852   |
|    n_updates            | 2390      |
|    policy_gradient_loss | -0.0565   |
|    value_loss           | 0.0103    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 966      |
|    ep_rew_mean     | 7.61     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 240      |
|    time_elapsed    | 3601     |
|    total_timesteps | 1966080  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 992       |
|    ep_rew_mean          | 7.79      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 241       |
|    time_elapsed         | 3614      |
|    total_timesteps      | 1974272   |
| train/                  |           |
|    approx_kl            | 0.5842739 |
|    clip_fraction        | 0.399     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.293    |
|    explained_variance   | 0.845     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0771   |
|    n_updates            | 2400      |
|    policy_gradient_loss | -0.0543   |
|    value_loss           | 0.00695   |
---------------------------------------
Eval num_timesteps=1980000, episode_reward=7.30 +/- 1.44
Episode length: 882.80 +/- 289.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 883        |
|    mean_reward          | 7.3        |
| time/                   |            |
|    total_timesteps      | 1980000    |
| train/                  |            |
|    approx_kl            | 0.50247216 |
|    clip_fraction        | 0.428      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.338     |
|    explained_variance   | 0.841      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0913    |
|    n_updates            | 2410       |
|    policy_gradient_loss | -0.0606    |
|    value_loss           | 0.00736    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 7.86     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 242      |
|    time_elapsed    | 3633     |
|    total_timesteps | 1982464  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.01e+03  |
|    ep_rew_mean          | 7.85      |
| time/                   |           |
|    fps                  | 545       |
|    iterations           | 243       |
|    time_elapsed         | 3646      |
|    total_timesteps      | 1990656   |
| train/                  |           |
|    approx_kl            | 0.5895179 |
|    clip_fraction        | 0.41      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.32     |
|    explained_variance   | 0.828     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0654   |
|    n_updates            | 2420      |
|    policy_gradient_loss | -0.0584   |
|    value_loss           | 0.00926   |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.03e+03   |
|    ep_rew_mean          | 8          |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 244        |
|    time_elapsed         | 3659       |
|    total_timesteps      | 1998848    |
| train/                  |            |
|    approx_kl            | 0.52602464 |
|    clip_fraction        | 0.423      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.33      |
|    explained_variance   | 0.832      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.105     |
|    n_updates            | 2430       |
|    policy_gradient_loss | -0.0601    |
|    value_loss           | 0.00824    |
----------------------------------------
Eval num_timesteps=2000000, episode_reward=9.10 +/- 3.25
Episode length: 1315.00 +/- 638.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.32e+03   |
|    mean_reward          | 9.1        |
| time/                   |            |
|    total_timesteps      | 2000000    |
| train/                  |            |
|    approx_kl            | 0.53365964 |
|    clip_fraction        | 0.421      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.329     |
|    explained_variance   | 0.833      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0924    |
|    n_updates            | 2440       |
|    policy_gradient_loss | -0.0598    |
|    value_loss           | 0.00758    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 7.84     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 245      |
|    time_elapsed    | 3677     |
|    total_timesteps | 2007040  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 7.83      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 246       |
|    time_elapsed         | 3690      |
|    total_timesteps      | 2015232   |
| train/                  |           |
|    approx_kl            | 0.6235528 |
|    clip_fraction        | 0.42      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.305    |
|    explained_variance   | 0.844     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0583   |
|    n_updates            | 2450      |
|    policy_gradient_loss | -0.0575   |
|    value_loss           | 0.00677   |
---------------------------------------
Eval num_timesteps=2020000, episode_reward=6.20 +/- 0.51
Episode length: 614.40 +/- 126.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 614        |
|    mean_reward          | 6.2        |
| time/                   |            |
|    total_timesteps      | 2020000    |
| train/                  |            |
|    approx_kl            | 0.57168484 |
|    clip_fraction        | 0.399      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.303     |
|    explained_variance   | 0.84       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0846    |
|    n_updates            | 2460       |
|    policy_gradient_loss | -0.0543    |
|    value_loss           | 0.00966    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 989      |
|    ep_rew_mean     | 7.8      |
| time/              |          |
|    fps             | 545      |
|    iterations      | 247      |
|    time_elapsed    | 3705     |
|    total_timesteps | 2023424  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 955       |
|    ep_rew_mean          | 7.67      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 248       |
|    time_elapsed         | 3719      |
|    total_timesteps      | 2031616   |
| train/                  |           |
|    approx_kl            | 0.5441943 |
|    clip_fraction        | 0.388     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.291    |
|    explained_variance   | 0.823     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0739   |
|    n_updates            | 2470      |
|    policy_gradient_loss | -0.051    |
|    value_loss           | 0.0114    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 948        |
|    ep_rew_mean          | 7.68       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 249        |
|    time_elapsed         | 3733       |
|    total_timesteps      | 2039808    |
| train/                  |            |
|    approx_kl            | 0.64292324 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.295     |
|    explained_variance   | 0.829      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0805    |
|    n_updates            | 2480       |
|    policy_gradient_loss | -0.0547    |
|    value_loss           | 0.01       |
----------------------------------------
Eval num_timesteps=2040000, episode_reward=6.00 +/- 0.84
Episode length: 645.20 +/- 167.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 645       |
|    mean_reward          | 6         |
| time/                   |           |
|    total_timesteps      | 2040000   |
| train/                  |           |
|    approx_kl            | 0.5446965 |
|    clip_fraction        | 0.422     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.314    |
|    explained_variance   | 0.829     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.073    |
|    n_updates            | 2490      |
|    policy_gradient_loss | -0.061    |
|    value_loss           | 0.00942   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 924      |
|    ep_rew_mean     | 7.54     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 250      |
|    time_elapsed    | 3748     |
|    total_timesteps | 2048000  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 943        |
|    ep_rew_mean          | 7.63       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 251        |
|    time_elapsed         | 3762       |
|    total_timesteps      | 2056192    |
| train/                  |            |
|    approx_kl            | 0.46053264 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.31      |
|    explained_variance   | 0.803      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0879    |
|    n_updates            | 2500       |
|    policy_gradient_loss | -0.054     |
|    value_loss           | 0.0127     |
----------------------------------------
Eval num_timesteps=2060000, episode_reward=7.20 +/- 1.21
Episode length: 749.40 +/- 261.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 749       |
|    mean_reward          | 7.2       |
| time/                   |           |
|    total_timesteps      | 2060000   |
| train/                  |           |
|    approx_kl            | 0.6591765 |
|    clip_fraction        | 0.415     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.301    |
|    explained_variance   | 0.839     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.075    |
|    n_updates            | 2510      |
|    policy_gradient_loss | -0.0571   |
|    value_loss           | 0.00638   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 910      |
|    ep_rew_mean     | 7.46     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 252      |
|    time_elapsed    | 3780     |
|    total_timesteps | 2064384  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 889       |
|    ep_rew_mean          | 7.39      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 253       |
|    time_elapsed         | 3793      |
|    total_timesteps      | 2072576   |
| train/                  |           |
|    approx_kl            | 0.5250067 |
|    clip_fraction        | 0.41      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.311    |
|    explained_variance   | 0.818     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0682   |
|    n_updates            | 2520      |
|    policy_gradient_loss | -0.0512   |
|    value_loss           | 0.00996   |
---------------------------------------
Eval num_timesteps=2080000, episode_reward=7.10 +/- 0.73
Episode length: 988.00 +/- 292.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 988       |
|    mean_reward          | 7.1       |
| time/                   |           |
|    total_timesteps      | 2080000   |
| train/                  |           |
|    approx_kl            | 0.6571189 |
|    clip_fraction        | 0.404     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.31     |
|    explained_variance   | 0.848     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0665   |
|    n_updates            | 2530      |
|    policy_gradient_loss | -0.0545   |
|    value_loss           | 0.00718   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 859      |
|    ep_rew_mean     | 7.34     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 254      |
|    time_elapsed    | 3811     |
|    total_timesteps | 2080768  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 847        |
|    ep_rew_mean          | 7.3        |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 255        |
|    time_elapsed         | 3824       |
|    total_timesteps      | 2088960    |
| train/                  |            |
|    approx_kl            | 0.46554023 |
|    clip_fraction        | 0.408      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.319     |
|    explained_variance   | 0.862      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0725    |
|    n_updates            | 2540       |
|    policy_gradient_loss | -0.0585    |
|    value_loss           | 0.00686    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 859        |
|    ep_rew_mean          | 7.3        |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 256        |
|    time_elapsed         | 3837       |
|    total_timesteps      | 2097152    |
| train/                  |            |
|    approx_kl            | 0.52254736 |
|    clip_fraction        | 0.408      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.316     |
|    explained_variance   | 0.819      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.101     |
|    n_updates            | 2550       |
|    policy_gradient_loss | -0.0575    |
|    value_loss           | 0.00781    |
----------------------------------------
Eval num_timesteps=2100000, episode_reward=6.90 +/- 1.32
Episode length: 768.80 +/- 204.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 769        |
|    mean_reward          | 6.9        |
| time/                   |            |
|    total_timesteps      | 2100000    |
| train/                  |            |
|    approx_kl            | 0.51252115 |
|    clip_fraction        | 0.42       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.33      |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0894    |
|    n_updates            | 2560       |
|    policy_gradient_loss | -0.0625    |
|    value_loss           | 0.00765    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 865      |
|    ep_rew_mean     | 7.28     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 257      |
|    time_elapsed    | 3855     |
|    total_timesteps | 2105344  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 865        |
|    ep_rew_mean          | 7.28       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 258        |
|    time_elapsed         | 3868       |
|    total_timesteps      | 2113536    |
| train/                  |            |
|    approx_kl            | 0.50317574 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.288     |
|    explained_variance   | 0.768      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0927    |
|    n_updates            | 2570       |
|    policy_gradient_loss | -0.0509    |
|    value_loss           | 0.0119     |
----------------------------------------
Eval num_timesteps=2120000, episode_reward=6.50 +/- 0.89
Episode length: 718.80 +/- 216.01
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 719        |
|    mean_reward          | 6.5        |
| time/                   |            |
|    total_timesteps      | 2120000    |
| train/                  |            |
|    approx_kl            | 0.50583875 |
|    clip_fraction        | 0.398      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.3       |
|    explained_variance   | 0.832      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0793    |
|    n_updates            | 2580       |
|    policy_gradient_loss | -0.054     |
|    value_loss           | 0.0095     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 860      |
|    ep_rew_mean     | 7.24     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 259      |
|    time_elapsed    | 3885     |
|    total_timesteps | 2121728  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 850       |
|    ep_rew_mean          | 7.17      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 260       |
|    time_elapsed         | 3898      |
|    total_timesteps      | 2129920   |
| train/                  |           |
|    approx_kl            | 0.5438573 |
|    clip_fraction        | 0.392     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.306    |
|    explained_variance   | 0.828     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0554   |
|    n_updates            | 2590      |
|    policy_gradient_loss | -0.0573   |
|    value_loss           | 0.01      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 859       |
|    ep_rew_mean          | 7.2       |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 261       |
|    time_elapsed         | 3911      |
|    total_timesteps      | 2138112   |
| train/                  |           |
|    approx_kl            | 0.9941675 |
|    clip_fraction        | 0.38      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.29     |
|    explained_variance   | 0.861     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0674   |
|    n_updates            | 2600      |
|    policy_gradient_loss | -0.0558   |
|    value_loss           | 0.00785   |
---------------------------------------
Eval num_timesteps=2140000, episode_reward=6.60 +/- 1.77
Episode length: 1035.00 +/- 362.56
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1.04e+03 |
|    mean_reward          | 6.6      |
| time/                   |          |
|    total_timesteps      | 2140000  |
| train/                  |          |
|    approx_kl            | 0.512689 |
|    clip_fraction        | 0.402    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.314   |
|    explained_variance   | 0.828    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0507  |
|    n_updates            | 2610     |
|    policy_gradient_loss | -0.0533  |
|    value_loss           | 0.0107   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 876      |
|    ep_rew_mean     | 7.18     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 262      |
|    time_elapsed    | 3930     |
|    total_timesteps | 2146304  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 902        |
|    ep_rew_mean          | 7.17       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 263        |
|    time_elapsed         | 3943       |
|    total_timesteps      | 2154496    |
| train/                  |            |
|    approx_kl            | 0.50473213 |
|    clip_fraction        | 0.415      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.324     |
|    explained_variance   | 0.834      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0377    |
|    n_updates            | 2620       |
|    policy_gradient_loss | -0.0566    |
|    value_loss           | 0.013      |
----------------------------------------
Eval num_timesteps=2160000, episode_reward=10.90 +/- 2.63
Episode length: 1380.00 +/- 437.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.38e+03  |
|    mean_reward          | 10.9      |
| time/                   |           |
|    total_timesteps      | 2160000   |
| train/                  |           |
|    approx_kl            | 0.5739149 |
|    clip_fraction        | 0.401     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.305    |
|    explained_variance   | 0.801     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0925   |
|    n_updates            | 2630      |
|    policy_gradient_loss | -0.0563   |
|    value_loss           | 0.0143    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 906      |
|    ep_rew_mean     | 7.08     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 264      |
|    time_elapsed    | 3964     |
|    total_timesteps | 2162688  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 908       |
|    ep_rew_mean          | 6.9       |
| time/                   |           |
|    fps                  | 545       |
|    iterations           | 265       |
|    time_elapsed         | 3977      |
|    total_timesteps      | 2170880   |
| train/                  |           |
|    approx_kl            | 0.5782649 |
|    clip_fraction        | 0.394     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.291    |
|    explained_variance   | 0.824     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0566   |
|    n_updates            | 2640      |
|    policy_gradient_loss | -0.0581   |
|    value_loss           | 0.0125    |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 953       |
|    ep_rew_mean          | 7.01      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 266       |
|    time_elapsed         | 3990      |
|    total_timesteps      | 2179072   |
| train/                  |           |
|    approx_kl            | 0.6362952 |
|    clip_fraction        | 0.399     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.306    |
|    explained_variance   | 0.786     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0889   |
|    n_updates            | 2650      |
|    policy_gradient_loss | -0.0501   |
|    value_loss           | 0.0152    |
---------------------------------------
Eval num_timesteps=2180000, episode_reward=8.50 +/- 1.90
Episode length: 1155.20 +/- 440.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.16e+03   |
|    mean_reward          | 8.5        |
| time/                   |            |
|    total_timesteps      | 2180000    |
| train/                  |            |
|    approx_kl            | 0.55237895 |
|    clip_fraction        | 0.421      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.325     |
|    explained_variance   | 0.774      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0799    |
|    n_updates            | 2660       |
|    policy_gradient_loss | -0.0556    |
|    value_loss           | 0.0133     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 942      |
|    ep_rew_mean     | 6.87     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 267      |
|    time_elapsed    | 4009     |
|    total_timesteps | 2187264  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 944      |
|    ep_rew_mean          | 6.86     |
| time/                   |          |
|    fps                  | 545      |
|    iterations           | 268      |
|    time_elapsed         | 4022     |
|    total_timesteps      | 2195456  |
| train/                  |          |
|    approx_kl            | 0.548938 |
|    clip_fraction        | 0.405    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.315   |
|    explained_variance   | 0.78     |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0384  |
|    n_updates            | 2670     |
|    policy_gradient_loss | -0.058   |
|    value_loss           | 0.014    |
--------------------------------------
Eval num_timesteps=2200000, episode_reward=7.00 +/- 1.70
Episode length: 679.80 +/- 248.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 680        |
|    mean_reward          | 7          |
| time/                   |            |
|    total_timesteps      | 2200000    |
| train/                  |            |
|    approx_kl            | 0.55996895 |
|    clip_fraction        | 0.421      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.315     |
|    explained_variance   | 0.792      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.106     |
|    n_updates            | 2680       |
|    policy_gradient_loss | -0.0606    |
|    value_loss           | 0.0126     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 933      |
|    ep_rew_mean     | 6.86     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 269      |
|    time_elapsed    | 4039     |
|    total_timesteps | 2203648  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 926       |
|    ep_rew_mean          | 6.84      |
| time/                   |           |
|    fps                  | 545       |
|    iterations           | 270       |
|    time_elapsed         | 4053      |
|    total_timesteps      | 2211840   |
| train/                  |           |
|    approx_kl            | 0.5979159 |
|    clip_fraction        | 0.396     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.285    |
|    explained_variance   | 0.829     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0792   |
|    n_updates            | 2690      |
|    policy_gradient_loss | -0.0574   |
|    value_loss           | 0.00921   |
---------------------------------------
Eval num_timesteps=2220000, episode_reward=8.50 +/- 2.45
Episode length: 1230.20 +/- 342.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.23e+03   |
|    mean_reward          | 8.5        |
| time/                   |            |
|    total_timesteps      | 2220000    |
| train/                  |            |
|    approx_kl            | 0.54424113 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.294     |
|    explained_variance   | 0.806      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0871    |
|    n_updates            | 2700       |
|    policy_gradient_loss | -0.0535    |
|    value_loss           | 0.012      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 937      |
|    ep_rew_mean     | 6.84     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 271      |
|    time_elapsed    | 4071     |
|    total_timesteps | 2220032  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 953      |
|    ep_rew_mean          | 6.86     |
| time/                   |          |
|    fps                  | 545      |
|    iterations           | 272      |
|    time_elapsed         | 4085     |
|    total_timesteps      | 2228224  |
| train/                  |          |
|    approx_kl            | 0.513595 |
|    clip_fraction        | 0.391    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.295   |
|    explained_variance   | 0.788    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.074   |
|    n_updates            | 2710     |
|    policy_gradient_loss | -0.052   |
|    value_loss           | 0.0137   |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 946      |
|    ep_rew_mean          | 6.91     |
| time/                   |          |
|    fps                  | 545      |
|    iterations           | 273      |
|    time_elapsed         | 4098     |
|    total_timesteps      | 2236416  |
| train/                  |          |
|    approx_kl            | 0.624751 |
|    clip_fraction        | 0.373    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.264   |
|    explained_variance   | 0.831    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0815  |
|    n_updates            | 2720     |
|    policy_gradient_loss | -0.0521  |
|    value_loss           | 0.00964  |
--------------------------------------
Eval num_timesteps=2240000, episode_reward=7.20 +/- 2.42
Episode length: 780.60 +/- 391.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 781        |
|    mean_reward          | 7.2        |
| time/                   |            |
|    total_timesteps      | 2240000    |
| train/                  |            |
|    approx_kl            | 0.58960694 |
|    clip_fraction        | 0.41       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.3       |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.043     |
|    n_updates            | 2730       |
|    policy_gradient_loss | -0.0514    |
|    value_loss           | 0.00787    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 933      |
|    ep_rew_mean     | 6.89     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 274      |
|    time_elapsed    | 4115     |
|    total_timesteps | 2244608  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 944      |
|    ep_rew_mean          | 7.04     |
| time/                   |          |
|    fps                  | 545      |
|    iterations           | 275      |
|    time_elapsed         | 4128     |
|    total_timesteps      | 2252800  |
| train/                  |          |
|    approx_kl            | 0.538834 |
|    clip_fraction        | 0.41     |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.321   |
|    explained_variance   | 0.828    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0934  |
|    n_updates            | 2740     |
|    policy_gradient_loss | -0.0565  |
|    value_loss           | 0.0112   |
--------------------------------------
Eval num_timesteps=2260000, episode_reward=7.10 +/- 1.83
Episode length: 899.80 +/- 515.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 900        |
|    mean_reward          | 7.1        |
| time/                   |            |
|    total_timesteps      | 2260000    |
| train/                  |            |
|    approx_kl            | 0.67273545 |
|    clip_fraction        | 0.403      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.317     |
|    explained_variance   | 0.813      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0689    |
|    n_updates            | 2750       |
|    policy_gradient_loss | -0.059     |
|    value_loss           | 0.00939    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 982      |
|    ep_rew_mean     | 7.3      |
| time/              |          |
|    fps             | 545      |
|    iterations      | 276      |
|    time_elapsed    | 4146     |
|    total_timesteps | 2260992  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 983       |
|    ep_rew_mean          | 7.3       |
| time/                   |           |
|    fps                  | 545       |
|    iterations           | 277       |
|    time_elapsed         | 4158      |
|    total_timesteps      | 2269184   |
| train/                  |           |
|    approx_kl            | 0.5432233 |
|    clip_fraction        | 0.443     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.348    |
|    explained_variance   | 0.81      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.102    |
|    n_updates            | 2760      |
|    policy_gradient_loss | -0.0662   |
|    value_loss           | 0.00784   |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 967       |
|    ep_rew_mean          | 7.27      |
| time/                   |           |
|    fps                  | 545       |
|    iterations           | 278       |
|    time_elapsed         | 4171      |
|    total_timesteps      | 2277376   |
| train/                  |           |
|    approx_kl            | 0.4995241 |
|    clip_fraction        | 0.399     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.311    |
|    explained_variance   | 0.78      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0791   |
|    n_updates            | 2770      |
|    policy_gradient_loss | -0.0548   |
|    value_loss           | 0.00964   |
---------------------------------------
Eval num_timesteps=2280000, episode_reward=7.30 +/- 1.12
Episode length: 868.80 +/- 176.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 869       |
|    mean_reward          | 7.3       |
| time/                   |           |
|    total_timesteps      | 2280000   |
| train/                  |           |
|    approx_kl            | 0.5366061 |
|    clip_fraction        | 0.412     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.322    |
|    explained_variance   | 0.826     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0924   |
|    n_updates            | 2780      |
|    policy_gradient_loss | -0.0601   |
|    value_loss           | 0.00997   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 968      |
|    ep_rew_mean     | 7.37     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 279      |
|    time_elapsed    | 4189     |
|    total_timesteps | 2285568  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 970        |
|    ep_rew_mean          | 7.4        |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 280        |
|    time_elapsed         | 4202       |
|    total_timesteps      | 2293760    |
| train/                  |            |
|    approx_kl            | 0.65813804 |
|    clip_fraction        | 0.43       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.342     |
|    explained_variance   | 0.832      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0782    |
|    n_updates            | 2790       |
|    policy_gradient_loss | -0.0566    |
|    value_loss           | 0.01       |
----------------------------------------
Eval num_timesteps=2300000, episode_reward=8.00 +/- 1.92
Episode length: 856.60 +/- 318.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 857        |
|    mean_reward          | 8          |
| time/                   |            |
|    total_timesteps      | 2300000    |
| train/                  |            |
|    approx_kl            | 0.43271893 |
|    clip_fraction        | 0.406      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.335     |
|    explained_variance   | 0.779      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.043     |
|    n_updates            | 2800       |
|    policy_gradient_loss | -0.055     |
|    value_loss           | 0.0107     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 975      |
|    ep_rew_mean     | 7.38     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 281      |
|    time_elapsed    | 4219     |
|    total_timesteps | 2301952  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 988        |
|    ep_rew_mean          | 7.46       |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 282        |
|    time_elapsed         | 4232       |
|    total_timesteps      | 2310144    |
| train/                  |            |
|    approx_kl            | 0.57887965 |
|    clip_fraction        | 0.409      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.32      |
|    explained_variance   | 0.792      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0723    |
|    n_updates            | 2810       |
|    policy_gradient_loss | -0.0536    |
|    value_loss           | 0.014      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 980       |
|    ep_rew_mean          | 7.54      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 283       |
|    time_elapsed         | 4245      |
|    total_timesteps      | 2318336   |
| train/                  |           |
|    approx_kl            | 0.5831678 |
|    clip_fraction        | 0.416     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.313    |
|    explained_variance   | 0.852     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0632   |
|    n_updates            | 2820      |
|    policy_gradient_loss | -0.0588   |
|    value_loss           | 0.00727   |
---------------------------------------
Eval num_timesteps=2320000, episode_reward=8.10 +/- 1.83
Episode length: 966.60 +/- 382.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 967       |
|    mean_reward          | 8.1       |
| time/                   |           |
|    total_timesteps      | 2320000   |
| train/                  |           |
|    approx_kl            | 0.4861776 |
|    clip_fraction        | 0.425     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.345    |
|    explained_variance   | 0.858     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0882   |
|    n_updates            | 2830      |
|    policy_gradient_loss | -0.0604   |
|    value_loss           | 0.00774   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 966      |
|    ep_rew_mean     | 7.55     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 284      |
|    time_elapsed    | 4263     |
|    total_timesteps | 2326528  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 980       |
|    ep_rew_mean          | 7.58      |
| time/                   |           |
|    fps                  | 545       |
|    iterations           | 285       |
|    time_elapsed         | 4276      |
|    total_timesteps      | 2334720   |
| train/                  |           |
|    approx_kl            | 0.5333103 |
|    clip_fraction        | 0.426     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.333    |
|    explained_variance   | 0.863     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0864   |
|    n_updates            | 2840      |
|    policy_gradient_loss | -0.0558   |
|    value_loss           | 0.00731   |
---------------------------------------
Eval num_timesteps=2340000, episode_reward=9.60 +/- 2.84
Episode length: 1270.40 +/- 447.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.27e+03   |
|    mean_reward          | 9.6        |
| time/                   |            |
|    total_timesteps      | 2340000    |
| train/                  |            |
|    approx_kl            | 0.52303535 |
|    clip_fraction        | 0.43       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.339     |
|    explained_variance   | 0.798      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.088     |
|    n_updates            | 2850       |
|    policy_gradient_loss | -0.0593    |
|    value_loss           | 0.00868    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 983      |
|    ep_rew_mean     | 7.63     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 286      |
|    time_elapsed    | 4293     |
|    total_timesteps | 2342912  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 981        |
|    ep_rew_mean          | 7.66       |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 287        |
|    time_elapsed         | 4306       |
|    total_timesteps      | 2351104    |
| train/                  |            |
|    approx_kl            | 0.53773737 |
|    clip_fraction        | 0.412      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.319     |
|    explained_variance   | 0.817      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0759    |
|    n_updates            | 2860       |
|    policy_gradient_loss | -0.0559    |
|    value_loss           | 0.00802    |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 926       |
|    ep_rew_mean          | 7.47      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 288       |
|    time_elapsed         | 4319      |
|    total_timesteps      | 2359296   |
| train/                  |           |
|    approx_kl            | 0.6293414 |
|    clip_fraction        | 0.403     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.307    |
|    explained_variance   | 0.871     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0947   |
|    n_updates            | 2870      |
|    policy_gradient_loss | -0.0547   |
|    value_loss           | 0.00667   |
---------------------------------------
Eval num_timesteps=2360000, episode_reward=8.90 +/- 1.93
Episode length: 1161.80 +/- 331.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.16e+03  |
|    mean_reward          | 8.9       |
| time/                   |           |
|    total_timesteps      | 2360000   |
| train/                  |           |
|    approx_kl            | 0.6396213 |
|    clip_fraction        | 0.388     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.302    |
|    explained_variance   | 0.84      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0759   |
|    n_updates            | 2880      |
|    policy_gradient_loss | -0.0512   |
|    value_loss           | 0.00985   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 920      |
|    ep_rew_mean     | 7.57     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 289      |
|    time_elapsed    | 4337     |
|    total_timesteps | 2367488  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 912        |
|    ep_rew_mean          | 7.48       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 290        |
|    time_elapsed         | 4350       |
|    total_timesteps      | 2375680    |
| train/                  |            |
|    approx_kl            | 0.59830165 |
|    clip_fraction        | 0.378      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.29      |
|    explained_variance   | 0.791      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0727    |
|    n_updates            | 2890       |
|    policy_gradient_loss | -0.0484    |
|    value_loss           | 0.00937    |
----------------------------------------
Eval num_timesteps=2380000, episode_reward=8.80 +/- 3.12
Episode length: 1103.60 +/- 570.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.1e+03    |
|    mean_reward          | 8.8        |
| time/                   |            |
|    total_timesteps      | 2380000    |
| train/                  |            |
|    approx_kl            | 0.53752434 |
|    clip_fraction        | 0.382      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.288     |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0623    |
|    n_updates            | 2900       |
|    policy_gradient_loss | -0.0486    |
|    value_loss           | 0.0101     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 893      |
|    ep_rew_mean     | 7.46     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 291      |
|    time_elapsed    | 4368     |
|    total_timesteps | 2383872  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 892       |
|    ep_rew_mean          | 7.52      |
| time/                   |           |
|    fps                  | 545       |
|    iterations           | 292       |
|    time_elapsed         | 4381      |
|    total_timesteps      | 2392064   |
| train/                  |           |
|    approx_kl            | 0.5043092 |
|    clip_fraction        | 0.412     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.327    |
|    explained_variance   | 0.86      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0677   |
|    n_updates            | 2910      |
|    policy_gradient_loss | -0.0572   |
|    value_loss           | 0.00701   |
---------------------------------------
Eval num_timesteps=2400000, episode_reward=6.80 +/- 0.60
Episode length: 814.00 +/- 122.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 814        |
|    mean_reward          | 6.8        |
| time/                   |            |
|    total_timesteps      | 2400000    |
| train/                  |            |
|    approx_kl            | 0.49003172 |
|    clip_fraction        | 0.402      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.323     |
|    explained_variance   | 0.834      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0616    |
|    n_updates            | 2920       |
|    policy_gradient_loss | -0.0532    |
|    value_loss           | 0.00799    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 930      |
|    ep_rew_mean     | 7.72     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 293      |
|    time_elapsed    | 4398     |
|    total_timesteps | 2400256  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 918      |
|    ep_rew_mean          | 7.62     |
| time/                   |          |
|    fps                  | 545      |
|    iterations           | 294      |
|    time_elapsed         | 4411     |
|    total_timesteps      | 2408448  |
| train/                  |          |
|    approx_kl            | 0.5029   |
|    clip_fraction        | 0.384    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.296   |
|    explained_variance   | 0.794    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.07    |
|    n_updates            | 2930     |
|    policy_gradient_loss | -0.0524  |
|    value_loss           | 0.011    |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 946      |
|    ep_rew_mean          | 7.63     |
| time/                   |          |
|    fps                  | 546      |
|    iterations           | 295      |
|    time_elapsed         | 4424     |
|    total_timesteps      | 2416640  |
| train/                  |          |
|    approx_kl            | 0.534626 |
|    clip_fraction        | 0.415    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.331   |
|    explained_variance   | 0.852    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0742  |
|    n_updates            | 2940     |
|    policy_gradient_loss | -0.054   |
|    value_loss           | 0.00694  |
--------------------------------------
Eval num_timesteps=2420000, episode_reward=8.50 +/- 4.66
Episode length: 1347.40 +/- 640.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.35e+03  |
|    mean_reward          | 8.5       |
| time/                   |           |
|    total_timesteps      | 2420000   |
| train/                  |           |
|    approx_kl            | 0.4250671 |
|    clip_fraction        | 0.381     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.307    |
|    explained_variance   | 0.804     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.073    |
|    n_updates            | 2950      |
|    policy_gradient_loss | -0.0517   |
|    value_loss           | 0.00901   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 948      |
|    ep_rew_mean     | 7.66     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 296      |
|    time_elapsed    | 4445     |
|    total_timesteps | 2424832  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 964      |
|    ep_rew_mean          | 7.75     |
| time/                   |          |
|    fps                  | 545      |
|    iterations           | 297      |
|    time_elapsed         | 4458     |
|    total_timesteps      | 2433024  |
| train/                  |          |
|    approx_kl            | 0.539225 |
|    clip_fraction        | 0.383    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.292   |
|    explained_variance   | 0.819    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0983  |
|    n_updates            | 2960     |
|    policy_gradient_loss | -0.0478  |
|    value_loss           | 0.0116   |
--------------------------------------
Eval num_timesteps=2440000, episode_reward=7.90 +/- 2.15
Episode length: 878.00 +/- 406.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 878        |
|    mean_reward          | 7.9        |
| time/                   |            |
|    total_timesteps      | 2440000    |
| train/                  |            |
|    approx_kl            | 0.58105624 |
|    clip_fraction        | 0.405      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.319     |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0836    |
|    n_updates            | 2970       |
|    policy_gradient_loss | -0.0547    |
|    value_loss           | 0.00724    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 932      |
|    ep_rew_mean     | 7.59     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 298      |
|    time_elapsed    | 4475     |
|    total_timesteps | 2441216  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 947        |
|    ep_rew_mean          | 7.66       |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 299        |
|    time_elapsed         | 4488       |
|    total_timesteps      | 2449408    |
| train/                  |            |
|    approx_kl            | 0.49913427 |
|    clip_fraction        | 0.397      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.324     |
|    explained_variance   | 0.835      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0597    |
|    n_updates            | 2980       |
|    policy_gradient_loss | -0.0505    |
|    value_loss           | 0.0104     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 972        |
|    ep_rew_mean          | 7.76       |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 300        |
|    time_elapsed         | 4501       |
|    total_timesteps      | 2457600    |
| train/                  |            |
|    approx_kl            | 0.56676847 |
|    clip_fraction        | 0.404      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.326     |
|    explained_variance   | 0.816      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0838    |
|    n_updates            | 2990       |
|    policy_gradient_loss | -0.0531    |
|    value_loss           | 0.0105     |
----------------------------------------
Eval num_timesteps=2460000, episode_reward=8.00 +/- 1.34
Episode length: 905.00 +/- 302.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 905       |
|    mean_reward          | 8         |
| time/                   |           |
|    total_timesteps      | 2460000   |
| train/                  |           |
|    approx_kl            | 0.6355077 |
|    clip_fraction        | 0.404     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.314    |
|    explained_variance   | 0.801     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0392   |
|    n_updates            | 3000      |
|    policy_gradient_loss | -0.0546   |
|    value_loss           | 0.0119    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 983      |
|    ep_rew_mean     | 7.75     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 301      |
|    time_elapsed    | 4518     |
|    total_timesteps | 2465792  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 984       |
|    ep_rew_mean          | 7.82      |
| time/                   |           |
|    fps                  | 545       |
|    iterations           | 302       |
|    time_elapsed         | 4531      |
|    total_timesteps      | 2473984   |
| train/                  |           |
|    approx_kl            | 0.4852327 |
|    clip_fraction        | 0.399     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.324    |
|    explained_variance   | 0.841     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0598   |
|    n_updates            | 3010      |
|    policy_gradient_loss | -0.053    |
|    value_loss           | 0.0111    |
---------------------------------------
Eval num_timesteps=2480000, episode_reward=8.40 +/- 3.51
Episode length: 1001.00 +/- 477.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 8.4        |
| time/                   |            |
|    total_timesteps      | 2480000    |
| train/                  |            |
|    approx_kl            | 0.47429916 |
|    clip_fraction        | 0.396      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.318     |
|    explained_variance   | 0.827      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0622    |
|    n_updates            | 3020       |
|    policy_gradient_loss | -0.0488    |
|    value_loss           | 0.0108     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 983      |
|    ep_rew_mean     | 7.75     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 303      |
|    time_elapsed    | 4550     |
|    total_timesteps | 2482176  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 991       |
|    ep_rew_mean          | 7.68      |
| time/                   |           |
|    fps                  | 545       |
|    iterations           | 304       |
|    time_elapsed         | 4563      |
|    total_timesteps      | 2490368   |
| train/                  |           |
|    approx_kl            | 0.4847679 |
|    clip_fraction        | 0.381     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.329    |
|    explained_variance   | 0.786     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0515   |
|    n_updates            | 3030      |
|    policy_gradient_loss | -0.0453   |
|    value_loss           | 0.0153    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 960        |
|    ep_rew_mean          | 7.5        |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 305        |
|    time_elapsed         | 4576       |
|    total_timesteps      | 2498560    |
| train/                  |            |
|    approx_kl            | 0.56662774 |
|    clip_fraction        | 0.391      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.307     |
|    explained_variance   | 0.826      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0723    |
|    n_updates            | 3040       |
|    policy_gradient_loss | -0.0534    |
|    value_loss           | 0.0103     |
----------------------------------------
Eval num_timesteps=2500000, episode_reward=6.80 +/- 1.57
Episode length: 714.00 +/- 257.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 714        |
|    mean_reward          | 6.8        |
| time/                   |            |
|    total_timesteps      | 2500000    |
| train/                  |            |
|    approx_kl            | 0.52034605 |
|    clip_fraction        | 0.392      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.329     |
|    explained_variance   | 0.792      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0728    |
|    n_updates            | 3050       |
|    policy_gradient_loss | -0.0521    |
|    value_loss           | 0.0142     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 944      |
|    ep_rew_mean     | 7.45     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 306      |
|    time_elapsed    | 4592     |
|    total_timesteps | 2506752  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 947       |
|    ep_rew_mean          | 7.55      |
| time/                   |           |
|    fps                  | 545       |
|    iterations           | 307       |
|    time_elapsed         | 4606      |
|    total_timesteps      | 2514944   |
| train/                  |           |
|    approx_kl            | 0.4959045 |
|    clip_fraction        | 0.403     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.329    |
|    explained_variance   | 0.784     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0542   |
|    n_updates            | 3060      |
|    policy_gradient_loss | -0.0463   |
|    value_loss           | 0.00982   |
---------------------------------------
Eval num_timesteps=2520000, episode_reward=6.20 +/- 1.54
Episode length: 845.40 +/- 234.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 845       |
|    mean_reward          | 6.2       |
| time/                   |           |
|    total_timesteps      | 2520000   |
| train/                  |           |
|    approx_kl            | 0.5173594 |
|    clip_fraction        | 0.427     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.354    |
|    explained_variance   | 0.833     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0694   |
|    n_updates            | 3070      |
|    policy_gradient_loss | -0.0605   |
|    value_loss           | 0.00906   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 948      |
|    ep_rew_mean     | 7.51     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 308      |
|    time_elapsed    | 4623     |
|    total_timesteps | 2523136  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 945        |
|    ep_rew_mean          | 7.46       |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 309        |
|    time_elapsed         | 4636       |
|    total_timesteps      | 2531328    |
| train/                  |            |
|    approx_kl            | 0.51153874 |
|    clip_fraction        | 0.393      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.304     |
|    explained_variance   | 0.818      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0814    |
|    n_updates            | 3080       |
|    policy_gradient_loss | -0.0551    |
|    value_loss           | 0.00892    |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 936       |
|    ep_rew_mean          | 7.41      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 310       |
|    time_elapsed         | 4649      |
|    total_timesteps      | 2539520   |
| train/                  |           |
|    approx_kl            | 0.5020834 |
|    clip_fraction        | 0.377     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.294    |
|    explained_variance   | 0.818     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0339   |
|    n_updates            | 3090      |
|    policy_gradient_loss | -0.045    |
|    value_loss           | 0.0119    |
---------------------------------------
Eval num_timesteps=2540000, episode_reward=7.90 +/- 1.32
Episode length: 1090.40 +/- 376.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.09e+03   |
|    mean_reward          | 7.9        |
| time/                   |            |
|    total_timesteps      | 2540000    |
| train/                  |            |
|    approx_kl            | 0.51375794 |
|    clip_fraction        | 0.379      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.304     |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0497    |
|    n_updates            | 3100       |
|    policy_gradient_loss | -0.0506    |
|    value_loss           | 0.009      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 933      |
|    ep_rew_mean     | 7.36     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 311      |
|    time_elapsed    | 4668     |
|    total_timesteps | 2547712  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 927        |
|    ep_rew_mean          | 7.36       |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 312        |
|    time_elapsed         | 4681       |
|    total_timesteps      | 2555904    |
| train/                  |            |
|    approx_kl            | 0.47913915 |
|    clip_fraction        | 0.39       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.32      |
|    explained_variance   | 0.836      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0761    |
|    n_updates            | 3110       |
|    policy_gradient_loss | -0.0545    |
|    value_loss           | 0.00943    |
----------------------------------------
Eval num_timesteps=2560000, episode_reward=6.00 +/- 1.00
Episode length: 604.80 +/- 101.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 605        |
|    mean_reward          | 6          |
| time/                   |            |
|    total_timesteps      | 2560000    |
| train/                  |            |
|    approx_kl            | 0.48319715 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.319     |
|    explained_variance   | 0.84       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0773    |
|    n_updates            | 3120       |
|    policy_gradient_loss | -0.0532    |
|    value_loss           | 0.0077     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 940      |
|    ep_rew_mean     | 7.51     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 313      |
|    time_elapsed    | 4697     |
|    total_timesteps | 2564096  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 949        |
|    ep_rew_mean          | 7.54       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 314        |
|    time_elapsed         | 4710       |
|    total_timesteps      | 2572288    |
| train/                  |            |
|    approx_kl            | 0.51316863 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.322     |
|    explained_variance   | 0.852      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0754    |
|    n_updates            | 3130       |
|    policy_gradient_loss | -0.0575    |
|    value_loss           | 0.00695    |
----------------------------------------
Eval num_timesteps=2580000, episode_reward=8.00 +/- 2.63
Episode length: 889.40 +/- 467.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 889       |
|    mean_reward          | 8         |
| time/                   |           |
|    total_timesteps      | 2580000   |
| train/                  |           |
|    approx_kl            | 0.5287821 |
|    clip_fraction        | 0.391     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.311    |
|    explained_variance   | 0.87      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0611   |
|    n_updates            | 3140      |
|    policy_gradient_loss | -0.0555   |
|    value_loss           | 0.00659   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 945      |
|    ep_rew_mean     | 7.66     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 315      |
|    time_elapsed    | 4727     |
|    total_timesteps | 2580480  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 927      |
|    ep_rew_mean          | 7.55     |
| time/                   |          |
|    fps                  | 546      |
|    iterations           | 316      |
|    time_elapsed         | 4740     |
|    total_timesteps      | 2588672  |
| train/                  |          |
|    approx_kl            | 0.562839 |
|    clip_fraction        | 0.39     |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.31    |
|    explained_variance   | 0.824    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0813  |
|    n_updates            | 3150     |
|    policy_gradient_loss | -0.0511  |
|    value_loss           | 0.00947  |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 930       |
|    ep_rew_mean          | 7.66      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 317       |
|    time_elapsed         | 4753      |
|    total_timesteps      | 2596864   |
| train/                  |           |
|    approx_kl            | 0.5077472 |
|    clip_fraction        | 0.404     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.333    |
|    explained_variance   | 0.839     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0901   |
|    n_updates            | 3160      |
|    policy_gradient_loss | -0.0546   |
|    value_loss           | 0.00807   |
---------------------------------------
Eval num_timesteps=2600000, episode_reward=8.30 +/- 1.33
Episode length: 879.40 +/- 292.78
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 879        |
|    mean_reward          | 8.3        |
| time/                   |            |
|    total_timesteps      | 2600000    |
| train/                  |            |
|    approx_kl            | 0.51995826 |
|    clip_fraction        | 0.414      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.333     |
|    explained_variance   | 0.887      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.069     |
|    n_updates            | 3170       |
|    policy_gradient_loss | -0.0564    |
|    value_loss           | 0.00607    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 964      |
|    ep_rew_mean     | 7.83     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 318      |
|    time_elapsed    | 4769     |
|    total_timesteps | 2605056  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 941        |
|    ep_rew_mean          | 7.74       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 319        |
|    time_elapsed         | 4782       |
|    total_timesteps      | 2613248    |
| train/                  |            |
|    approx_kl            | 0.49416065 |
|    clip_fraction        | 0.391      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.323     |
|    explained_variance   | 0.837      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0776    |
|    n_updates            | 3180       |
|    policy_gradient_loss | -0.0539    |
|    value_loss           | 0.00816    |
----------------------------------------
Eval num_timesteps=2620000, episode_reward=7.70 +/- 1.96
Episode length: 925.00 +/- 326.07
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 925      |
|    mean_reward          | 7.7      |
| time/                   |          |
|    total_timesteps      | 2620000  |
| train/                  |          |
|    approx_kl            | 0.523776 |
|    clip_fraction        | 0.392    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.321   |
|    explained_variance   | 0.87     |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0697  |
|    n_updates            | 3190     |
|    policy_gradient_loss | -0.0539  |
|    value_loss           | 0.00641  |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 928      |
|    ep_rew_mean     | 7.78     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 320      |
|    time_elapsed    | 4800     |
|    total_timesteps | 2621440  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 941       |
|    ep_rew_mean          | 7.88      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 321       |
|    time_elapsed         | 4813      |
|    total_timesteps      | 2629632   |
| train/                  |           |
|    approx_kl            | 0.4933925 |
|    clip_fraction        | 0.384     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.308    |
|    explained_variance   | 0.86      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.083    |
|    n_updates            | 3200      |
|    policy_gradient_loss | -0.0553   |
|    value_loss           | 0.00823   |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 945        |
|    ep_rew_mean          | 7.94       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 322        |
|    time_elapsed         | 4826       |
|    total_timesteps      | 2637824    |
| train/                  |            |
|    approx_kl            | 0.61050296 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.299     |
|    explained_variance   | 0.835      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0591    |
|    n_updates            | 3210       |
|    policy_gradient_loss | -0.0465    |
|    value_loss           | 0.00931    |
----------------------------------------
Eval num_timesteps=2640000, episode_reward=6.30 +/- 1.29
Episode length: 853.60 +/- 199.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 854        |
|    mean_reward          | 6.3        |
| time/                   |            |
|    total_timesteps      | 2640000    |
| train/                  |            |
|    approx_kl            | 0.53416795 |
|    clip_fraction        | 0.396      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.321     |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.057     |
|    n_updates            | 3220       |
|    policy_gradient_loss | -0.0554    |
|    value_loss           | 0.00663    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 923      |
|    ep_rew_mean     | 7.88     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 323      |
|    time_elapsed    | 4843     |
|    total_timesteps | 2646016  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 950       |
|    ep_rew_mean          | 7.97      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 324       |
|    time_elapsed         | 4856      |
|    total_timesteps      | 2654208   |
| train/                  |           |
|    approx_kl            | 0.6022404 |
|    clip_fraction        | 0.378     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.304    |
|    explained_variance   | 0.767     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0722   |
|    n_updates            | 3230      |
|    policy_gradient_loss | -0.0499   |
|    value_loss           | 0.0128    |
---------------------------------------
Eval num_timesteps=2660000, episode_reward=7.70 +/- 1.44
Episode length: 882.20 +/- 281.29
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 882      |
|    mean_reward          | 7.7      |
| time/                   |          |
|    total_timesteps      | 2660000  |
| train/                  |          |
|    approx_kl            | 2.291529 |
|    clip_fraction        | 0.373    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.292   |
|    explained_variance   | 0.772    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0402  |
|    n_updates            | 3240     |
|    policy_gradient_loss | -0.0481  |
|    value_loss           | 0.0113   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 938      |
|    ep_rew_mean     | 7.87     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 325      |
|    time_elapsed    | 4874     |
|    total_timesteps | 2662400  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 947       |
|    ep_rew_mean          | 7.88      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 326       |
|    time_elapsed         | 4887      |
|    total_timesteps      | 2670592   |
| train/                  |           |
|    approx_kl            | 0.6248197 |
|    clip_fraction        | 0.402     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.298    |
|    explained_variance   | 0.833     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.086    |
|    n_updates            | 3250      |
|    policy_gradient_loss | -0.0506   |
|    value_loss           | 0.00801   |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 950       |
|    ep_rew_mean          | 7.92      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 327       |
|    time_elapsed         | 4900      |
|    total_timesteps      | 2678784   |
| train/                  |           |
|    approx_kl            | 0.5364321 |
|    clip_fraction        | 0.408     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.32     |
|    explained_variance   | 0.841     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0686   |
|    n_updates            | 3260      |
|    policy_gradient_loss | -0.0561   |
|    value_loss           | 0.00845   |
---------------------------------------
Eval num_timesteps=2680000, episode_reward=7.00 +/- 1.38
Episode length: 758.60 +/- 429.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 759       |
|    mean_reward          | 7         |
| time/                   |           |
|    total_timesteps      | 2680000   |
| train/                  |           |
|    approx_kl            | 0.5624764 |
|    clip_fraction        | 0.378     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.289    |
|    explained_variance   | 0.852     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0664   |
|    n_updates            | 3270      |
|    policy_gradient_loss | -0.0514   |
|    value_loss           | 0.00856   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 971      |
|    ep_rew_mean     | 8.02     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 328      |
|    time_elapsed    | 4916     |
|    total_timesteps | 2686976  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 976       |
|    ep_rew_mean          | 8.01      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 329       |
|    time_elapsed         | 4929      |
|    total_timesteps      | 2695168   |
| train/                  |           |
|    approx_kl            | 0.5494455 |
|    clip_fraction        | 0.377     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.291    |
|    explained_variance   | 0.823     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0961   |
|    n_updates            | 3280      |
|    policy_gradient_loss | -0.0512   |
|    value_loss           | 0.01      |
---------------------------------------
Eval num_timesteps=2700000, episode_reward=7.20 +/- 1.08
Episode length: 868.40 +/- 211.99
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 868      |
|    mean_reward          | 7.2      |
| time/                   |          |
|    total_timesteps      | 2700000  |
| train/                  |          |
|    approx_kl            | 0.56356  |
|    clip_fraction        | 0.379    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.291   |
|    explained_variance   | 0.858    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.077   |
|    n_updates            | 3290     |
|    policy_gradient_loss | -0.0461  |
|    value_loss           | 0.00853  |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 956      |
|    ep_rew_mean     | 7.94     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 330      |
|    time_elapsed    | 4946     |
|    total_timesteps | 2703360  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 970        |
|    ep_rew_mean          | 7.96       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 331        |
|    time_elapsed         | 4959       |
|    total_timesteps      | 2711552    |
| train/                  |            |
|    approx_kl            | 0.43301392 |
|    clip_fraction        | 0.381      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.316     |
|    explained_variance   | 0.845      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0607    |
|    n_updates            | 3300       |
|    policy_gradient_loss | -0.0482    |
|    value_loss           | 0.00983    |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 954       |
|    ep_rew_mean          | 7.79      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 332       |
|    time_elapsed         | 4972      |
|    total_timesteps      | 2719744   |
| train/                  |           |
|    approx_kl            | 0.6450674 |
|    clip_fraction        | 0.379     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.312    |
|    explained_variance   | 0.813     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0803   |
|    n_updates            | 3310      |
|    policy_gradient_loss | -0.0507   |
|    value_loss           | 0.0107    |
---------------------------------------
Eval num_timesteps=2720000, episode_reward=6.40 +/- 0.58
Episode length: 687.00 +/- 86.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 687       |
|    mean_reward          | 6.4       |
| time/                   |           |
|    total_timesteps      | 2720000   |
| train/                  |           |
|    approx_kl            | 0.5213747 |
|    clip_fraction        | 0.394     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.321    |
|    explained_variance   | 0.85      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0865   |
|    n_updates            | 3320      |
|    policy_gradient_loss | -0.0547   |
|    value_loss           | 0.00827   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 953      |
|    ep_rew_mean     | 7.75     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 333      |
|    time_elapsed    | 4988     |
|    total_timesteps | 2727936  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 966        |
|    ep_rew_mean          | 7.77       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 334        |
|    time_elapsed         | 5001       |
|    total_timesteps      | 2736128    |
| train/                  |            |
|    approx_kl            | 0.54735017 |
|    clip_fraction        | 0.385      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.295     |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0633    |
|    n_updates            | 3330       |
|    policy_gradient_loss | -0.0515    |
|    value_loss           | 0.0117     |
----------------------------------------
Eval num_timesteps=2740000, episode_reward=7.80 +/- 2.34
Episode length: 1016.00 +/- 322.31
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.02e+03   |
|    mean_reward          | 7.8        |
| time/                   |            |
|    total_timesteps      | 2740000    |
| train/                  |            |
|    approx_kl            | 0.55459815 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.313     |
|    explained_variance   | 0.788      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0463    |
|    n_updates            | 3340       |
|    policy_gradient_loss | -0.0504    |
|    value_loss           | 0.0117     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 987      |
|    ep_rew_mean     | 7.88     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 335      |
|    time_elapsed    | 5020     |
|    total_timesteps | 2744320  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 958        |
|    ep_rew_mean          | 7.79       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 336        |
|    time_elapsed         | 5033       |
|    total_timesteps      | 2752512    |
| train/                  |            |
|    approx_kl            | 0.55773467 |
|    clip_fraction        | 0.39       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.308     |
|    explained_variance   | 0.801      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0679    |
|    n_updates            | 3350       |
|    policy_gradient_loss | -0.0508    |
|    value_loss           | 0.0106     |
----------------------------------------
Eval num_timesteps=2760000, episode_reward=7.50 +/- 1.52
Episode length: 818.20 +/- 337.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 818       |
|    mean_reward          | 7.5       |
| time/                   |           |
|    total_timesteps      | 2760000   |
| train/                  |           |
|    approx_kl            | 0.5307334 |
|    clip_fraction        | 0.398     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.313    |
|    explained_variance   | 0.833     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0408   |
|    n_updates            | 3360      |
|    policy_gradient_loss | -0.051    |
|    value_loss           | 0.00777   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 936      |
|    ep_rew_mean     | 7.68     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 337      |
|    time_elapsed    | 5051     |
|    total_timesteps | 2760704  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 942        |
|    ep_rew_mean          | 7.73       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 338        |
|    time_elapsed         | 5064       |
|    total_timesteps      | 2768896    |
| train/                  |            |
|    approx_kl            | 0.50055355 |
|    clip_fraction        | 0.393      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.323     |
|    explained_variance   | 0.842      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0661    |
|    n_updates            | 3370       |
|    policy_gradient_loss | -0.0497    |
|    value_loss           | 0.00979    |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 948       |
|    ep_rew_mean          | 7.78      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 339       |
|    time_elapsed         | 5076      |
|    total_timesteps      | 2777088   |
| train/                  |           |
|    approx_kl            | 0.5776125 |
|    clip_fraction        | 0.414     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.327    |
|    explained_variance   | 0.859     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0805   |
|    n_updates            | 3380      |
|    policy_gradient_loss | -0.0545   |
|    value_loss           | 0.00617   |
---------------------------------------
Eval num_timesteps=2780000, episode_reward=10.90 +/- 5.45
Episode length: 1539.00 +/- 1129.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.54e+03  |
|    mean_reward          | 10.9      |
| time/                   |           |
|    total_timesteps      | 2780000   |
| train/                  |           |
|    approx_kl            | 0.5449741 |
|    clip_fraction        | 0.384     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.309    |
|    explained_variance   | 0.813     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.103    |
|    n_updates            | 3390      |
|    policy_gradient_loss | -0.0514   |
|    value_loss           | 0.00831   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 916      |
|    ep_rew_mean     | 7.63     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 340      |
|    time_elapsed    | 5098     |
|    total_timesteps | 2785280  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 923       |
|    ep_rew_mean          | 7.67      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 341       |
|    time_elapsed         | 5111      |
|    total_timesteps      | 2793472   |
| train/                  |           |
|    approx_kl            | 0.4646634 |
|    clip_fraction        | 0.398     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.342    |
|    explained_variance   | 0.828     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0756   |
|    n_updates            | 3400      |
|    policy_gradient_loss | -0.0506   |
|    value_loss           | 0.00793   |
---------------------------------------
Eval num_timesteps=2800000, episode_reward=7.70 +/- 2.06
Episode length: 985.40 +/- 458.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 985       |
|    mean_reward          | 7.7       |
| time/                   |           |
|    total_timesteps      | 2800000   |
| train/                  |           |
|    approx_kl            | 0.5086272 |
|    clip_fraction        | 0.414     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.343    |
|    explained_variance   | 0.886     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0733   |
|    n_updates            | 3410      |
|    policy_gradient_loss | -0.0571   |
|    value_loss           | 0.00567   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 922      |
|    ep_rew_mean     | 7.68     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 342      |
|    time_elapsed    | 5130     |
|    total_timesteps | 2801664  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 921        |
|    ep_rew_mean          | 7.76       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 343        |
|    time_elapsed         | 5143       |
|    total_timesteps      | 2809856    |
| train/                  |            |
|    approx_kl            | 0.49459082 |
|    clip_fraction        | 0.385      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.322     |
|    explained_variance   | 0.823      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0857    |
|    n_updates            | 3420       |
|    policy_gradient_loss | -0.0496    |
|    value_loss           | 0.00963    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 950        |
|    ep_rew_mean          | 7.92       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 344        |
|    time_elapsed         | 5156       |
|    total_timesteps      | 2818048    |
| train/                  |            |
|    approx_kl            | 0.56832194 |
|    clip_fraction        | 0.411      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.337     |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0738    |
|    n_updates            | 3430       |
|    policy_gradient_loss | -0.0545    |
|    value_loss           | 0.00642    |
----------------------------------------
Eval num_timesteps=2820000, episode_reward=7.00 +/- 0.84
Episode length: 669.60 +/- 179.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 670       |
|    mean_reward          | 7         |
| time/                   |           |
|    total_timesteps      | 2820000   |
| train/                  |           |
|    approx_kl            | 0.5206097 |
|    clip_fraction        | 0.396     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.335    |
|    explained_variance   | 0.845     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0625   |
|    n_updates            | 3440      |
|    policy_gradient_loss | -0.0498   |
|    value_loss           | 0.00757   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 932      |
|    ep_rew_mean     | 7.87     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 345      |
|    time_elapsed    | 5171     |
|    total_timesteps | 2826240  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 910        |
|    ep_rew_mean          | 7.78       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 346        |
|    time_elapsed         | 5184       |
|    total_timesteps      | 2834432    |
| train/                  |            |
|    approx_kl            | 0.48260838 |
|    clip_fraction        | 0.396      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.345     |
|    explained_variance   | 0.865      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0684    |
|    n_updates            | 3450       |
|    policy_gradient_loss | -0.0538    |
|    value_loss           | 0.00781    |
----------------------------------------
Eval num_timesteps=2840000, episode_reward=7.60 +/- 1.53
Episode length: 999.80 +/- 393.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 7.6        |
| time/                   |            |
|    total_timesteps      | 2840000    |
| train/                  |            |
|    approx_kl            | 0.58445853 |
|    clip_fraction        | 0.406      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.333     |
|    explained_variance   | 0.883      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0498    |
|    n_updates            | 3460       |
|    policy_gradient_loss | -0.0575    |
|    value_loss           | 0.00749    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 892      |
|    ep_rew_mean     | 7.74     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 347      |
|    time_elapsed    | 5201     |
|    total_timesteps | 2842624  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 897        |
|    ep_rew_mean          | 7.76       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 348        |
|    time_elapsed         | 5214       |
|    total_timesteps      | 2850816    |
| train/                  |            |
|    approx_kl            | 0.47675204 |
|    clip_fraction        | 0.389      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.316     |
|    explained_variance   | 0.864      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0678    |
|    n_updates            | 3470       |
|    policy_gradient_loss | -0.0522    |
|    value_loss           | 0.00749    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 873        |
|    ep_rew_mean          | 7.67       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 349        |
|    time_elapsed         | 5227       |
|    total_timesteps      | 2859008    |
| train/                  |            |
|    approx_kl            | 0.55538744 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.301     |
|    explained_variance   | 0.85       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0433    |
|    n_updates            | 3480       |
|    policy_gradient_loss | -0.0474    |
|    value_loss           | 0.00816    |
----------------------------------------
Eval num_timesteps=2860000, episode_reward=8.30 +/- 1.03
Episode length: 976.20 +/- 185.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 976        |
|    mean_reward          | 8.3        |
| time/                   |            |
|    total_timesteps      | 2860000    |
| train/                  |            |
|    approx_kl            | 0.55268306 |
|    clip_fraction        | 0.403      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.332     |
|    explained_variance   | 0.867      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0618    |
|    n_updates            | 3490       |
|    policy_gradient_loss | -0.0535    |
|    value_loss           | 0.00695    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 852      |
|    ep_rew_mean     | 7.57     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 350      |
|    time_elapsed    | 5244     |
|    total_timesteps | 2867200  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 853        |
|    ep_rew_mean          | 7.57       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 351        |
|    time_elapsed         | 5257       |
|    total_timesteps      | 2875392    |
| train/                  |            |
|    approx_kl            | 0.63022137 |
|    clip_fraction        | 0.385      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.334     |
|    explained_variance   | 0.854      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0869    |
|    n_updates            | 3500       |
|    policy_gradient_loss | -0.0511    |
|    value_loss           | 0.00832    |
----------------------------------------
Eval num_timesteps=2880000, episode_reward=7.10 +/- 0.86
Episode length: 847.20 +/- 220.06
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 847        |
|    mean_reward          | 7.1        |
| time/                   |            |
|    total_timesteps      | 2880000    |
| train/                  |            |
|    approx_kl            | 0.42100623 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.351     |
|    explained_variance   | 0.845      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0753    |
|    n_updates            | 3510       |
|    policy_gradient_loss | -0.0466    |
|    value_loss           | 0.00892    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 885      |
|    ep_rew_mean     | 7.67     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 352      |
|    time_elapsed    | 5274     |
|    total_timesteps | 2883584  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 873       |
|    ep_rew_mean          | 7.63      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 353       |
|    time_elapsed         | 5287      |
|    total_timesteps      | 2891776   |
| train/                  |           |
|    approx_kl            | 0.4118349 |
|    clip_fraction        | 0.399     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.354    |
|    explained_variance   | 0.805     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0801   |
|    n_updates            | 3520      |
|    policy_gradient_loss | -0.0538   |
|    value_loss           | 0.00984   |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 889       |
|    ep_rew_mean          | 7.64      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 354       |
|    time_elapsed         | 5300      |
|    total_timesteps      | 2899968   |
| train/                  |           |
|    approx_kl            | 0.4874345 |
|    clip_fraction        | 0.361     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.303    |
|    explained_variance   | 0.851     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0572   |
|    n_updates            | 3530      |
|    policy_gradient_loss | -0.0477   |
|    value_loss           | 0.0099    |
---------------------------------------
Eval num_timesteps=2900000, episode_reward=6.80 +/- 2.20
Episode length: 802.20 +/- 290.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 802       |
|    mean_reward          | 6.8       |
| time/                   |           |
|    total_timesteps      | 2900000   |
| train/                  |           |
|    approx_kl            | 0.5967611 |
|    clip_fraction        | 0.405     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.349    |
|    explained_variance   | 0.818     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0874   |
|    n_updates            | 3540      |
|    policy_gradient_loss | -0.0529   |
|    value_loss           | 0.0115    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 870      |
|    ep_rew_mean     | 7.51     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 355      |
|    time_elapsed    | 5316     |
|    total_timesteps | 2908160  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 892        |
|    ep_rew_mean          | 7.54       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 356        |
|    time_elapsed         | 5329       |
|    total_timesteps      | 2916352    |
| train/                  |            |
|    approx_kl            | 0.47764552 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.319     |
|    explained_variance   | 0.787      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0799    |
|    n_updates            | 3550       |
|    policy_gradient_loss | -0.046     |
|    value_loss           | 0.0165     |
----------------------------------------
Eval num_timesteps=2920000, episode_reward=6.70 +/- 1.86
Episode length: 813.20 +/- 298.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 813        |
|    mean_reward          | 6.7        |
| time/                   |            |
|    total_timesteps      | 2920000    |
| train/                  |            |
|    approx_kl            | 0.53633344 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.343     |
|    explained_variance   | 0.831      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0925    |
|    n_updates            | 3560       |
|    policy_gradient_loss | -0.0523    |
|    value_loss           | 0.0093     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 896      |
|    ep_rew_mean     | 7.5      |
| time/              |          |
|    fps             | 546      |
|    iterations      | 357      |
|    time_elapsed    | 5346     |
|    total_timesteps | 2924544  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 912       |
|    ep_rew_mean          | 7.55      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 358       |
|    time_elapsed         | 5359      |
|    total_timesteps      | 2932736   |
| train/                  |           |
|    approx_kl            | 0.4962859 |
|    clip_fraction        | 0.391     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.34     |
|    explained_variance   | 0.852     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0549   |
|    n_updates            | 3570      |
|    policy_gradient_loss | -0.0532   |
|    value_loss           | 0.00966   |
---------------------------------------
Eval num_timesteps=2940000, episode_reward=8.80 +/- 1.40
Episode length: 1117.80 +/- 314.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.12e+03   |
|    mean_reward          | 8.8        |
| time/                   |            |
|    total_timesteps      | 2940000    |
| train/                  |            |
|    approx_kl            | 0.49929082 |
|    clip_fraction        | 0.39       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.338     |
|    explained_variance   | 0.843      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0647    |
|    n_updates            | 3580       |
|    policy_gradient_loss | -0.0566    |
|    value_loss           | 0.00842    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 933      |
|    ep_rew_mean     | 7.62     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 359      |
|    time_elapsed    | 5377     |
|    total_timesteps | 2940928  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 929        |
|    ep_rew_mean          | 7.57       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 360        |
|    time_elapsed         | 5391       |
|    total_timesteps      | 2949120    |
| train/                  |            |
|    approx_kl            | 0.46455324 |
|    clip_fraction        | 0.383      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.342     |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.059     |
|    n_updates            | 3590       |
|    policy_gradient_loss | -0.0526    |
|    value_loss           | 0.0105     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 929        |
|    ep_rew_mean          | 7.54       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 361        |
|    time_elapsed         | 5404       |
|    total_timesteps      | 2957312    |
| train/                  |            |
|    approx_kl            | 0.46392834 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.336     |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.056     |
|    n_updates            | 3600       |
|    policy_gradient_loss | -0.05      |
|    value_loss           | 0.0104     |
----------------------------------------
Eval num_timesteps=2960000, episode_reward=7.30 +/- 0.93
Episode length: 766.20 +/- 208.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 766        |
|    mean_reward          | 7.3        |
| time/                   |            |
|    total_timesteps      | 2960000    |
| train/                  |            |
|    approx_kl            | 0.47263336 |
|    clip_fraction        | 0.377      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.328     |
|    explained_variance   | 0.803      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0384    |
|    n_updates            | 3610       |
|    policy_gradient_loss | -0.0468    |
|    value_loss           | 0.0101     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 939      |
|    ep_rew_mean     | 7.56     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 362      |
|    time_elapsed    | 5420     |
|    total_timesteps | 2965504  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 941       |
|    ep_rew_mean          | 7.61      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 363       |
|    time_elapsed         | 5433      |
|    total_timesteps      | 2973696   |
| train/                  |           |
|    approx_kl            | 0.5404059 |
|    clip_fraction        | 0.384     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.321    |
|    explained_variance   | 0.838     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0801   |
|    n_updates            | 3620      |
|    policy_gradient_loss | -0.0496   |
|    value_loss           | 0.0102    |
---------------------------------------
Eval num_timesteps=2980000, episode_reward=6.30 +/- 0.51
Episode length: 763.40 +/- 154.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 763       |
|    mean_reward          | 6.3       |
| time/                   |           |
|    total_timesteps      | 2980000   |
| train/                  |           |
|    approx_kl            | 0.4844204 |
|    clip_fraction        | 0.395     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.338    |
|    explained_variance   | 0.861     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0702   |
|    n_updates            | 3630      |
|    policy_gradient_loss | -0.0539   |
|    value_loss           | 0.00891   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 912      |
|    ep_rew_mean     | 7.44     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 364      |
|    time_elapsed    | 5448     |
|    total_timesteps | 2981888  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 927        |
|    ep_rew_mean          | 7.47       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 365        |
|    time_elapsed         | 5461       |
|    total_timesteps      | 2990080    |
| train/                  |            |
|    approx_kl            | 0.43022466 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.34      |
|    explained_variance   | 0.812      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0741    |
|    n_updates            | 3640       |
|    policy_gradient_loss | -0.0513    |
|    value_loss           | 0.0138     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 926        |
|    ep_rew_mean          | 7.49       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 366        |
|    time_elapsed         | 5474       |
|    total_timesteps      | 2998272    |
| train/                  |            |
|    approx_kl            | 0.49847692 |
|    clip_fraction        | 0.397      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.338     |
|    explained_variance   | 0.826      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0773    |
|    n_updates            | 3650       |
|    policy_gradient_loss | -0.0524    |
|    value_loss           | 0.00928    |
----------------------------------------
Eval num_timesteps=3000000, episode_reward=9.60 +/- 3.22
Episode length: 1156.00 +/- 525.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.16e+03   |
|    mean_reward          | 9.6        |
| time/                   |            |
|    total_timesteps      | 3000000    |
| train/                  |            |
|    approx_kl            | 0.48653445 |
|    clip_fraction        | 0.391      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.353     |
|    explained_variance   | 0.834      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0814    |
|    n_updates            | 3660       |
|    policy_gradient_loss | -0.0548    |
|    value_loss           | 0.00975    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 925      |
|    ep_rew_mean     | 7.6      |
| time/              |          |
|    fps             | 547      |
|    iterations      | 367      |
|    time_elapsed    | 5494     |
|    total_timesteps | 3006464  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 928        |
|    ep_rew_mean          | 7.64       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 368        |
|    time_elapsed         | 5507       |
|    total_timesteps      | 3014656    |
| train/                  |            |
|    approx_kl            | 0.47535938 |
|    clip_fraction        | 0.379      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.333     |
|    explained_variance   | 0.819      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0558    |
|    n_updates            | 3670       |
|    policy_gradient_loss | -0.0487    |
|    value_loss           | 0.00999    |
----------------------------------------
Eval num_timesteps=3020000, episode_reward=10.10 +/- 2.52
Episode length: 1284.00 +/- 514.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.28e+03  |
|    mean_reward          | 10.1      |
| time/                   |           |
|    total_timesteps      | 3020000   |
| train/                  |           |
|    approx_kl            | 0.5833579 |
|    clip_fraction        | 0.374     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.322    |
|    explained_variance   | 0.82      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.078    |
|    n_updates            | 3680      |
|    policy_gradient_loss | -0.0464   |
|    value_loss           | 0.0128    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 918      |
|    ep_rew_mean     | 7.57     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 369      |
|    time_elapsed    | 5526     |
|    total_timesteps | 3022848  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 897       |
|    ep_rew_mean          | 7.39      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 370       |
|    time_elapsed         | 5539      |
|    total_timesteps      | 3031040   |
| train/                  |           |
|    approx_kl            | 0.4689594 |
|    clip_fraction        | 0.376     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.328    |
|    explained_variance   | 0.843     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0722   |
|    n_updates            | 3690      |
|    policy_gradient_loss | -0.0478   |
|    value_loss           | 0.0113    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 917        |
|    ep_rew_mean          | 7.43       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 371        |
|    time_elapsed         | 5552       |
|    total_timesteps      | 3039232    |
| train/                  |            |
|    approx_kl            | 0.45305258 |
|    clip_fraction        | 0.359      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.325     |
|    explained_variance   | 0.785      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0433    |
|    n_updates            | 3700       |
|    policy_gradient_loss | -0.0478    |
|    value_loss           | 0.016      |
----------------------------------------
Eval num_timesteps=3040000, episode_reward=8.40 +/- 1.85
Episode length: 1046.60 +/- 318.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.05e+03   |
|    mean_reward          | 8.4        |
| time/                   |            |
|    total_timesteps      | 3040000    |
| train/                  |            |
|    approx_kl            | 0.44280064 |
|    clip_fraction        | 0.409      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.381     |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0683    |
|    n_updates            | 3710       |
|    policy_gradient_loss | -0.0537    |
|    value_loss           | 0.0114     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 944      |
|    ep_rew_mean     | 7.58     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 372      |
|    time_elapsed    | 5570     |
|    total_timesteps | 3047424  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 954        |
|    ep_rew_mean          | 7.58       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 373        |
|    time_elapsed         | 5583       |
|    total_timesteps      | 3055616    |
| train/                  |            |
|    approx_kl            | 0.42866245 |
|    clip_fraction        | 0.365      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.318     |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0565    |
|    n_updates            | 3720       |
|    policy_gradient_loss | -0.0493    |
|    value_loss           | 0.0115     |
----------------------------------------
Eval num_timesteps=3060000, episode_reward=7.50 +/- 1.05
Episode length: 977.20 +/- 313.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 977       |
|    mean_reward          | 7.5       |
| time/                   |           |
|    total_timesteps      | 3060000   |
| train/                  |           |
|    approx_kl            | 0.5095409 |
|    clip_fraction        | 0.387     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.357    |
|    explained_variance   | 0.829     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0403   |
|    n_updates            | 3730      |
|    policy_gradient_loss | -0.0531   |
|    value_loss           | 0.0105    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 970      |
|    ep_rew_mean     | 7.65     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 374      |
|    time_elapsed    | 5599     |
|    total_timesteps | 3063808  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 996        |
|    ep_rew_mean          | 7.75       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 375        |
|    time_elapsed         | 5612       |
|    total_timesteps      | 3072000    |
| train/                  |            |
|    approx_kl            | 0.41096547 |
|    clip_fraction        | 0.365      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.327     |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0543    |
|    n_updates            | 3740       |
|    policy_gradient_loss | -0.0509    |
|    value_loss           | 0.0113     |
----------------------------------------
Eval num_timesteps=3080000, episode_reward=9.40 +/- 1.59
Episode length: 1283.80 +/- 501.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.28e+03   |
|    mean_reward          | 9.4        |
| time/                   |            |
|    total_timesteps      | 3080000    |
| train/                  |            |
|    approx_kl            | 0.47556692 |
|    clip_fraction        | 0.377      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.328     |
|    explained_variance   | 0.815      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0458    |
|    n_updates            | 3750       |
|    policy_gradient_loss | -0.0482    |
|    value_loss           | 0.0121     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 7.75     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 376      |
|    time_elapsed    | 5632     |
|    total_timesteps | 3080192  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 7.78       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 377        |
|    time_elapsed         | 5644       |
|    total_timesteps      | 3088384    |
| train/                  |            |
|    approx_kl            | 0.45670533 |
|    clip_fraction        | 0.379      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.327     |
|    explained_variance   | 0.846      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0562    |
|    n_updates            | 3760       |
|    policy_gradient_loss | -0.0531    |
|    value_loss           | 0.00932    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 7.9        |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 378        |
|    time_elapsed         | 5657       |
|    total_timesteps      | 3096576    |
| train/                  |            |
|    approx_kl            | 0.47936386 |
|    clip_fraction        | 0.389      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.345     |
|    explained_variance   | 0.884      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0944    |
|    n_updates            | 3770       |
|    policy_gradient_loss | -0.0588    |
|    value_loss           | 0.00588    |
----------------------------------------
Eval num_timesteps=3100000, episode_reward=7.80 +/- 1.03
Episode length: 881.40 +/- 272.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 881       |
|    mean_reward          | 7.8       |
| time/                   |           |
|    total_timesteps      | 3100000   |
| train/                  |           |
|    approx_kl            | 0.5225696 |
|    clip_fraction        | 0.372     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.312    |
|    explained_variance   | 0.864     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0813   |
|    n_updates            | 3780      |
|    policy_gradient_loss | -0.0536   |
|    value_loss           | 0.00795   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 7.88     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 379      |
|    time_elapsed    | 5674     |
|    total_timesteps | 3104768  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 7.83       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 380        |
|    time_elapsed         | 5688       |
|    total_timesteps      | 3112960    |
| train/                  |            |
|    approx_kl            | 0.43423057 |
|    clip_fraction        | 0.354      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.318     |
|    explained_variance   | 0.858      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.077     |
|    n_updates            | 3790       |
|    policy_gradient_loss | -0.0508    |
|    value_loss           | 0.00941    |
----------------------------------------
Eval num_timesteps=3120000, episode_reward=7.90 +/- 2.48
Episode length: 1211.60 +/- 273.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.21e+03  |
|    mean_reward          | 7.9       |
| time/                   |           |
|    total_timesteps      | 3120000   |
| train/                  |           |
|    approx_kl            | 0.5685196 |
|    clip_fraction        | 0.367     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.332    |
|    explained_variance   | 0.864     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0372   |
|    n_updates            | 3800      |
|    policy_gradient_loss | -0.0505   |
|    value_loss           | 0.0095    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 7.8      |
| time/              |          |
|    fps             | 546      |
|    iterations      | 381      |
|    time_elapsed    | 5707     |
|    total_timesteps | 3121152  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 7.88       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 382        |
|    time_elapsed         | 5721       |
|    total_timesteps      | 3129344    |
| train/                  |            |
|    approx_kl            | 0.51117563 |
|    clip_fraction        | 0.353      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.312     |
|    explained_variance   | 0.85       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0573    |
|    n_updates            | 3810       |
|    policy_gradient_loss | -0.0442    |
|    value_loss           | 0.0104     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.03e+03  |
|    ep_rew_mean          | 7.97      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 383       |
|    time_elapsed         | 5734      |
|    total_timesteps      | 3137536   |
| train/                  |           |
|    approx_kl            | 0.5305315 |
|    clip_fraction        | 0.36      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.31     |
|    explained_variance   | 0.833     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0943   |
|    n_updates            | 3820      |
|    policy_gradient_loss | -0.0456   |
|    value_loss           | 0.0093    |
---------------------------------------
Eval num_timesteps=3140000, episode_reward=6.10 +/- 2.03
Episode length: 745.20 +/- 188.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 745        |
|    mean_reward          | 6.1        |
| time/                   |            |
|    total_timesteps      | 3140000    |
| train/                  |            |
|    approx_kl            | 0.47698945 |
|    clip_fraction        | 0.359      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.327     |
|    explained_variance   | 0.802      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0512    |
|    n_updates            | 3830       |
|    policy_gradient_loss | -0.0468    |
|    value_loss           | 0.0124     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.03e+03 |
|    ep_rew_mean     | 7.92     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 384      |
|    time_elapsed    | 5751     |
|    total_timesteps | 3145728  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.04e+03   |
|    ep_rew_mean          | 7.89       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 385        |
|    time_elapsed         | 5764       |
|    total_timesteps      | 3153920    |
| train/                  |            |
|    approx_kl            | 0.53636235 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.285     |
|    explained_variance   | 0.805      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0591    |
|    n_updates            | 3840       |
|    policy_gradient_loss | -0.0452    |
|    value_loss           | 0.0159     |
----------------------------------------
Eval num_timesteps=3160000, episode_reward=9.80 +/- 3.06
Episode length: 1277.00 +/- 521.12
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1.28e+03 |
|    mean_reward          | 9.8      |
| time/                   |          |
|    total_timesteps      | 3160000  |
| train/                  |          |
|    approx_kl            | 0.502555 |
|    clip_fraction        | 0.348    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.292   |
|    explained_variance   | 0.785    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0768  |
|    n_updates            | 3850     |
|    policy_gradient_loss | -0.0424  |
|    value_loss           | 0.013    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.05e+03 |
|    ep_rew_mean     | 8.04     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 386      |
|    time_elapsed    | 5784     |
|    total_timesteps | 3162112  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 7.93      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 387       |
|    time_elapsed         | 5797      |
|    total_timesteps      | 3170304   |
| train/                  |           |
|    approx_kl            | 0.6517883 |
|    clip_fraction        | 0.366     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.304    |
|    explained_variance   | 0.832     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0889   |
|    n_updates            | 3860      |
|    policy_gradient_loss | -0.0479   |
|    value_loss           | 0.00879   |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.03e+03  |
|    ep_rew_mean          | 7.96      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 388       |
|    time_elapsed         | 5810      |
|    total_timesteps      | 3178496   |
| train/                  |           |
|    approx_kl            | 0.5373791 |
|    clip_fraction        | 0.364     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.317    |
|    explained_variance   | 0.827     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0376   |
|    n_updates            | 3870      |
|    policy_gradient_loss | -0.0502   |
|    value_loss           | 0.0106    |
---------------------------------------
Eval num_timesteps=3180000, episode_reward=7.50 +/- 2.17
Episode length: 1086.80 +/- 490.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.09e+03   |
|    mean_reward          | 7.5        |
| time/                   |            |
|    total_timesteps      | 3180000    |
| train/                  |            |
|    approx_kl            | 0.51294994 |
|    clip_fraction        | 0.376      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.311     |
|    explained_variance   | 0.856      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.047     |
|    n_updates            | 3880       |
|    policy_gradient_loss | -0.0507    |
|    value_loss           | 0.00661    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.03e+03 |
|    ep_rew_mean     | 8.05     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 389      |
|    time_elapsed    | 5827     |
|    total_timesteps | 3186688  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.03e+03   |
|    ep_rew_mean          | 7.93       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 390        |
|    time_elapsed         | 5840       |
|    total_timesteps      | 3194880    |
| train/                  |            |
|    approx_kl            | 0.52566385 |
|    clip_fraction        | 0.366      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.315     |
|    explained_variance   | 0.855      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0879    |
|    n_updates            | 3890       |
|    policy_gradient_loss | -0.0483    |
|    value_loss           | 0.00916    |
----------------------------------------
Eval num_timesteps=3200000, episode_reward=8.50 +/- 1.48
Episode length: 950.00 +/- 279.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 950        |
|    mean_reward          | 8.5        |
| time/                   |            |
|    total_timesteps      | 3200000    |
| train/                  |            |
|    approx_kl            | 0.46794927 |
|    clip_fraction        | 0.345      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.302     |
|    explained_variance   | 0.84       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0608    |
|    n_updates            | 3900       |
|    policy_gradient_loss | -0.043     |
|    value_loss           | 0.0132     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 7.84     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 391      |
|    time_elapsed    | 5857     |
|    total_timesteps | 3203072  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.01e+03  |
|    ep_rew_mean          | 7.79      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 392       |
|    time_elapsed         | 5870      |
|    total_timesteps      | 3211264   |
| train/                  |           |
|    approx_kl            | 0.5083246 |
|    clip_fraction        | 0.345     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.298    |
|    explained_variance   | 0.799     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0573   |
|    n_updates            | 3910      |
|    policy_gradient_loss | -0.0433   |
|    value_loss           | 0.0138    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.03e+03   |
|    ep_rew_mean          | 7.89       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 393        |
|    time_elapsed         | 5883       |
|    total_timesteps      | 3219456    |
| train/                  |            |
|    approx_kl            | 0.47216243 |
|    clip_fraction        | 0.359      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.308     |
|    explained_variance   | 0.866      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0618    |
|    n_updates            | 3920       |
|    policy_gradient_loss | -0.0511    |
|    value_loss           | 0.00993    |
----------------------------------------
Eval num_timesteps=3220000, episode_reward=7.00 +/- 1.30
Episode length: 704.20 +/- 217.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 704       |
|    mean_reward          | 7         |
| time/                   |           |
|    total_timesteps      | 3220000   |
| train/                  |           |
|    approx_kl            | 0.5036607 |
|    clip_fraction        | 0.358     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.309    |
|    explained_variance   | 0.843     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0596   |
|    n_updates            | 3930      |
|    policy_gradient_loss | -0.049    |
|    value_loss           | 0.0109    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 990      |
|    ep_rew_mean     | 7.72     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 394      |
|    time_elapsed    | 5899     |
|    total_timesteps | 3227648  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 982       |
|    ep_rew_mean          | 7.73      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 395       |
|    time_elapsed         | 5912      |
|    total_timesteps      | 3235840   |
| train/                  |           |
|    approx_kl            | 0.5553889 |
|    clip_fraction        | 0.351     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.287    |
|    explained_variance   | 0.838     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.077    |
|    n_updates            | 3940      |
|    policy_gradient_loss | -0.0486   |
|    value_loss           | 0.0103    |
---------------------------------------
Eval num_timesteps=3240000, episode_reward=7.70 +/- 1.44
Episode length: 1118.40 +/- 488.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.12e+03   |
|    mean_reward          | 7.7        |
| time/                   |            |
|    total_timesteps      | 3240000    |
| train/                  |            |
|    approx_kl            | 0.50377953 |
|    clip_fraction        | 0.354      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.294     |
|    explained_variance   | 0.827      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.061     |
|    n_updates            | 3950       |
|    policy_gradient_loss | -0.046     |
|    value_loss           | 0.00999    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | 7.88     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 396      |
|    time_elapsed    | 5932     |
|    total_timesteps | 3244032  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 955        |
|    ep_rew_mean          | 7.82       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 397        |
|    time_elapsed         | 5945       |
|    total_timesteps      | 3252224    |
| train/                  |            |
|    approx_kl            | 0.43920922 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.285     |
|    explained_variance   | 0.826      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0418    |
|    n_updates            | 3960       |
|    policy_gradient_loss | -0.0422    |
|    value_loss           | 0.0126     |
----------------------------------------
Eval num_timesteps=3260000, episode_reward=7.20 +/- 0.93
Episode length: 1005.60 +/- 183.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.01e+03   |
|    mean_reward          | 7.2        |
| time/                   |            |
|    total_timesteps      | 3260000    |
| train/                  |            |
|    approx_kl            | 0.49763256 |
|    clip_fraction        | 0.338      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.301     |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.077     |
|    n_updates            | 3970       |
|    policy_gradient_loss | -0.0469    |
|    value_loss           | 0.0113     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 943      |
|    ep_rew_mean     | 7.63     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 398      |
|    time_elapsed    | 5963     |
|    total_timesteps | 3260416  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 919      |
|    ep_rew_mean          | 7.44     |
| time/                   |          |
|    fps                  | 546      |
|    iterations           | 399      |
|    time_elapsed         | 5975     |
|    total_timesteps      | 3268608  |
| train/                  |          |
|    approx_kl            | 0.510501 |
|    clip_fraction        | 0.342    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.298   |
|    explained_variance   | 0.847    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0404  |
|    n_updates            | 3980     |
|    policy_gradient_loss | -0.0446  |
|    value_loss           | 0.0114   |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 894        |
|    ep_rew_mean          | 7.33       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 400        |
|    time_elapsed         | 5988       |
|    total_timesteps      | 3276800    |
| train/                  |            |
|    approx_kl            | 0.40907022 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.314     |
|    explained_variance   | 0.813      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0552    |
|    n_updates            | 3990       |
|    policy_gradient_loss | -0.0427    |
|    value_loss           | 0.0153     |
----------------------------------------
Eval num_timesteps=3280000, episode_reward=7.20 +/- 2.04
Episode length: 710.80 +/- 321.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 711       |
|    mean_reward          | 7.2       |
| time/                   |           |
|    total_timesteps      | 3280000   |
| train/                  |           |
|    approx_kl            | 0.5269578 |
|    clip_fraction        | 0.345     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.288    |
|    explained_variance   | 0.846     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0772   |
|    n_updates            | 4000      |
|    policy_gradient_loss | -0.0448   |
|    value_loss           | 0.0104    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 901      |
|    ep_rew_mean     | 7.4      |
| time/              |          |
|    fps             | 547      |
|    iterations      | 401      |
|    time_elapsed    | 6004     |
|    total_timesteps | 3284992  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 915        |
|    ep_rew_mean          | 7.49       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 402        |
|    time_elapsed         | 6017       |
|    total_timesteps      | 3293184    |
| train/                  |            |
|    approx_kl            | 0.55786824 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.297     |
|    explained_variance   | 0.841      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0514    |
|    n_updates            | 4010       |
|    policy_gradient_loss | -0.0438    |
|    value_loss           | 0.0111     |
----------------------------------------
Eval num_timesteps=3300000, episode_reward=7.30 +/- 1.03
Episode length: 965.80 +/- 307.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 966        |
|    mean_reward          | 7.3        |
| time/                   |            |
|    total_timesteps      | 3300000    |
| train/                  |            |
|    approx_kl            | 0.46790695 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.299     |
|    explained_variance   | 0.812      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0593    |
|    n_updates            | 4020       |
|    policy_gradient_loss | -0.0403    |
|    value_loss           | 0.0136     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 909      |
|    ep_rew_mean     | 7.45     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 403      |
|    time_elapsed    | 6034     |
|    total_timesteps | 3301376  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 880        |
|    ep_rew_mean          | 7.32       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 404        |
|    time_elapsed         | 6047       |
|    total_timesteps      | 3309568    |
| train/                  |            |
|    approx_kl            | 0.54842573 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.283     |
|    explained_variance   | 0.865      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0617    |
|    n_updates            | 4030       |
|    policy_gradient_loss | -0.042     |
|    value_loss           | 0.00951    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 899        |
|    ep_rew_mean          | 7.39       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 405        |
|    time_elapsed         | 6060       |
|    total_timesteps      | 3317760    |
| train/                  |            |
|    approx_kl            | 0.56793535 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.299     |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0459    |
|    n_updates            | 4040       |
|    policy_gradient_loss | -0.041     |
|    value_loss           | 0.0124     |
----------------------------------------
Eval num_timesteps=3320000, episode_reward=8.20 +/- 1.54
Episode length: 1022.20 +/- 383.87
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1.02e+03 |
|    mean_reward          | 8.2      |
| time/                   |          |
|    total_timesteps      | 3320000  |
| train/                  |          |
|    approx_kl            | 0.480291 |
|    clip_fraction        | 0.341    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.305   |
|    explained_variance   | 0.846    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0545  |
|    n_updates            | 4050     |
|    policy_gradient_loss | -0.0471  |
|    value_loss           | 0.0102   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 877      |
|    ep_rew_mean     | 7.25     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 406      |
|    time_elapsed    | 6077     |
|    total_timesteps | 3325952  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 891        |
|    ep_rew_mean          | 7.21       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 407        |
|    time_elapsed         | 6090       |
|    total_timesteps      | 3334144    |
| train/                  |            |
|    approx_kl            | 0.40344578 |
|    clip_fraction        | 0.348      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.332     |
|    explained_variance   | 0.817      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0478    |
|    n_updates            | 4060       |
|    policy_gradient_loss | -0.0438    |
|    value_loss           | 0.0125     |
----------------------------------------
Eval num_timesteps=3340000, episode_reward=6.60 +/- 0.86
Episode length: 768.20 +/- 246.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 768       |
|    mean_reward          | 6.6       |
| time/                   |           |
|    total_timesteps      | 3340000   |
| train/                  |           |
|    approx_kl            | 0.4888044 |
|    clip_fraction        | 0.332     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.319    |
|    explained_variance   | 0.8       |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0446   |
|    n_updates            | 4070      |
|    policy_gradient_loss | -0.0406   |
|    value_loss           | 0.0171    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 907      |
|    ep_rew_mean     | 7.24     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 408      |
|    time_elapsed    | 6106     |
|    total_timesteps | 3342336  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 911        |
|    ep_rew_mean          | 7.31       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 409        |
|    time_elapsed         | 6119       |
|    total_timesteps      | 3350528    |
| train/                  |            |
|    approx_kl            | 0.43531024 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.329     |
|    explained_variance   | 0.816      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0625    |
|    n_updates            | 4080       |
|    policy_gradient_loss | -0.0466    |
|    value_loss           | 0.0118     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 908        |
|    ep_rew_mean          | 7.29       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 410        |
|    time_elapsed         | 6132       |
|    total_timesteps      | 3358720    |
| train/                  |            |
|    approx_kl            | 0.53143185 |
|    clip_fraction        | 0.332      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.284     |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0779    |
|    n_updates            | 4090       |
|    policy_gradient_loss | -0.0428    |
|    value_loss           | 0.0109     |
----------------------------------------
Eval num_timesteps=3360000, episode_reward=7.10 +/- 0.97
Episode length: 756.40 +/- 239.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 756       |
|    mean_reward          | 7.1       |
| time/                   |           |
|    total_timesteps      | 3360000   |
| train/                  |           |
|    approx_kl            | 0.6102941 |
|    clip_fraction        | 0.334     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.289    |
|    explained_variance   | 0.807     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0553   |
|    n_updates            | 4100      |
|    policy_gradient_loss | -0.0367   |
|    value_loss           | 0.0113    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 900      |
|    ep_rew_mean     | 7.3      |
| time/              |          |
|    fps             | 547      |
|    iterations      | 411      |
|    time_elapsed    | 6149     |
|    total_timesteps | 3366912  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 900       |
|    ep_rew_mean          | 7.3       |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 412       |
|    time_elapsed         | 6162      |
|    total_timesteps      | 3375104   |
| train/                  |           |
|    approx_kl            | 0.5417857 |
|    clip_fraction        | 0.341     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.31     |
|    explained_variance   | 0.847     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0708   |
|    n_updates            | 4110      |
|    policy_gradient_loss | -0.0464   |
|    value_loss           | 0.0123    |
---------------------------------------
Eval num_timesteps=3380000, episode_reward=8.90 +/- 1.46
Episode length: 1149.00 +/- 388.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.15e+03   |
|    mean_reward          | 8.9        |
| time/                   |            |
|    total_timesteps      | 3380000    |
| train/                  |            |
|    approx_kl            | 0.43662938 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.339     |
|    explained_variance   | 0.837      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.064     |
|    n_updates            | 4120       |
|    policy_gradient_loss | -0.0417    |
|    value_loss           | 0.0106     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 948      |
|    ep_rew_mean     | 7.57     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 413      |
|    time_elapsed    | 6181     |
|    total_timesteps | 3383296  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 953        |
|    ep_rew_mean          | 7.55       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 414        |
|    time_elapsed         | 6193       |
|    total_timesteps      | 3391488    |
| train/                  |            |
|    approx_kl            | 0.42882085 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.324     |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0553    |
|    n_updates            | 4130       |
|    policy_gradient_loss | -0.0473    |
|    value_loss           | 0.0088     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 961        |
|    ep_rew_mean          | 7.49       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 415        |
|    time_elapsed         | 6206       |
|    total_timesteps      | 3399680    |
| train/                  |            |
|    approx_kl            | 0.41208866 |
|    clip_fraction        | 0.341      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.35      |
|    explained_variance   | 0.805      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0526    |
|    n_updates            | 4140       |
|    policy_gradient_loss | -0.0423    |
|    value_loss           | 0.0145     |
----------------------------------------
Eval num_timesteps=3400000, episode_reward=7.60 +/- 2.82
Episode length: 929.60 +/- 492.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 930        |
|    mean_reward          | 7.6        |
| time/                   |            |
|    total_timesteps      | 3400000    |
| train/                  |            |
|    approx_kl            | 0.38736093 |
|    clip_fraction        | 0.338      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.347     |
|    explained_variance   | 0.759      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0533    |
|    n_updates            | 4150       |
|    policy_gradient_loss | -0.0465    |
|    value_loss           | 0.0173     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 983      |
|    ep_rew_mean     | 7.49     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 416      |
|    time_elapsed    | 6224     |
|    total_timesteps | 3407872  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 975        |
|    ep_rew_mean          | 7.41       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 417        |
|    time_elapsed         | 6237       |
|    total_timesteps      | 3416064    |
| train/                  |            |
|    approx_kl            | 0.40297195 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.323     |
|    explained_variance   | 0.783      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0569    |
|    n_updates            | 4160       |
|    policy_gradient_loss | -0.0424    |
|    value_loss           | 0.0177     |
----------------------------------------
Eval num_timesteps=3420000, episode_reward=9.30 +/- 2.09
Episode length: 1165.80 +/- 360.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.17e+03  |
|    mean_reward          | 9.3       |
| time/                   |           |
|    total_timesteps      | 3420000   |
| train/                  |           |
|    approx_kl            | 0.3937178 |
|    clip_fraction        | 0.346     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.341    |
|    explained_variance   | 0.771     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0493   |
|    n_updates            | 4170      |
|    policy_gradient_loss | -0.0414   |
|    value_loss           | 0.0167    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | 7.42     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 418      |
|    time_elapsed    | 6257     |
|    total_timesteps | 3424256  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 994       |
|    ep_rew_mean          | 7.5       |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 419       |
|    time_elapsed         | 6270      |
|    total_timesteps      | 3432448   |
| train/                  |           |
|    approx_kl            | 0.3949289 |
|    clip_fraction        | 0.347     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.345    |
|    explained_variance   | 0.792     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.07     |
|    n_updates            | 4180      |
|    policy_gradient_loss | -0.0467   |
|    value_loss           | 0.0149    |
---------------------------------------
Eval num_timesteps=3440000, episode_reward=8.60 +/- 1.07
Episode length: 1038.20 +/- 215.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.04e+03   |
|    mean_reward          | 8.6        |
| time/                   |            |
|    total_timesteps      | 3440000    |
| train/                  |            |
|    approx_kl            | 0.38747478 |
|    clip_fraction        | 0.346      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.358     |
|    explained_variance   | 0.852      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0614    |
|    n_updates            | 4190       |
|    policy_gradient_loss | -0.0504    |
|    value_loss           | 0.0094     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 980      |
|    ep_rew_mean     | 7.51     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 420      |
|    time_elapsed    | 6289     |
|    total_timesteps | 3440640  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 961        |
|    ep_rew_mean          | 7.43       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 421        |
|    time_elapsed         | 6302       |
|    total_timesteps      | 3448832    |
| train/                  |            |
|    approx_kl            | 0.48962754 |
|    clip_fraction        | 0.343      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.334     |
|    explained_variance   | 0.854      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0809    |
|    n_updates            | 4200       |
|    policy_gradient_loss | -0.0506    |
|    value_loss           | 0.00827    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 977        |
|    ep_rew_mean          | 7.54       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 422        |
|    time_elapsed         | 6315       |
|    total_timesteps      | 3457024    |
| train/                  |            |
|    approx_kl            | 0.35140967 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.345     |
|    explained_variance   | 0.835      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.048     |
|    n_updates            | 4210       |
|    policy_gradient_loss | -0.0491    |
|    value_loss           | 0.0113     |
----------------------------------------
Eval num_timesteps=3460000, episode_reward=7.80 +/- 2.25
Episode length: 890.80 +/- 414.23
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 891        |
|    mean_reward          | 7.8        |
| time/                   |            |
|    total_timesteps      | 3460000    |
| train/                  |            |
|    approx_kl            | 0.38269567 |
|    clip_fraction        | 0.348      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.361     |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0485    |
|    n_updates            | 4220       |
|    policy_gradient_loss | -0.0487    |
|    value_loss           | 0.00963    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 999      |
|    ep_rew_mean     | 7.67     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 423      |
|    time_elapsed    | 6333     |
|    total_timesteps | 3465216  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 7.72       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 424        |
|    time_elapsed         | 6346       |
|    total_timesteps      | 3473408    |
| train/                  |            |
|    approx_kl            | 0.37500498 |
|    clip_fraction        | 0.342      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.337     |
|    explained_variance   | 0.846      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0501    |
|    n_updates            | 4230       |
|    policy_gradient_loss | -0.0514    |
|    value_loss           | 0.0112     |
----------------------------------------
Eval num_timesteps=3480000, episode_reward=7.10 +/- 1.02
Episode length: 777.80 +/- 238.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 778       |
|    mean_reward          | 7.1       |
| time/                   |           |
|    total_timesteps      | 3480000   |
| train/                  |           |
|    approx_kl            | 0.4457897 |
|    clip_fraction        | 0.346     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.337    |
|    explained_variance   | 0.856     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.061    |
|    n_updates            | 4240      |
|    policy_gradient_loss | -0.0465   |
|    value_loss           | 0.00901   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 978      |
|    ep_rew_mean     | 7.58     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 425      |
|    time_elapsed    | 6362     |
|    total_timesteps | 3481600  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 961       |
|    ep_rew_mean          | 7.54      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 426       |
|    time_elapsed         | 6375      |
|    total_timesteps      | 3489792   |
| train/                  |           |
|    approx_kl            | 0.4596284 |
|    clip_fraction        | 0.338     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.321    |
|    explained_variance   | 0.855     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0783   |
|    n_updates            | 4250      |
|    policy_gradient_loss | -0.0454   |
|    value_loss           | 0.00868   |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 950        |
|    ep_rew_mean          | 7.67       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 427        |
|    time_elapsed         | 6388       |
|    total_timesteps      | 3497984    |
| train/                  |            |
|    approx_kl            | 0.38929653 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.322     |
|    explained_variance   | 0.851      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0174    |
|    n_updates            | 4260       |
|    policy_gradient_loss | -0.0464    |
|    value_loss           | 0.00899    |
----------------------------------------
Eval num_timesteps=3500000, episode_reward=7.00 +/- 0.55
Episode length: 867.00 +/- 76.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 867        |
|    mean_reward          | 7          |
| time/                   |            |
|    total_timesteps      | 3500000    |
| train/                  |            |
|    approx_kl            | 0.42416123 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.296     |
|    explained_variance   | 0.802      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0598    |
|    n_updates            | 4270       |
|    policy_gradient_loss | -0.0377    |
|    value_loss           | 0.0124     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 935      |
|    ep_rew_mean     | 7.7      |
| time/              |          |
|    fps             | 547      |
|    iterations      | 428      |
|    time_elapsed    | 6405     |
|    total_timesteps | 3506176  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 948      |
|    ep_rew_mean          | 7.73     |
| time/                   |          |
|    fps                  | 547      |
|    iterations           | 429      |
|    time_elapsed         | 6418     |
|    total_timesteps      | 3514368  |
| train/                  |          |
|    approx_kl            | 0.587442 |
|    clip_fraction        | 0.302    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.29    |
|    explained_variance   | 0.822    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0396  |
|    n_updates            | 4280     |
|    policy_gradient_loss | -0.0402  |
|    value_loss           | 0.0143   |
--------------------------------------
Eval num_timesteps=3520000, episode_reward=8.10 +/- 0.66
Episode length: 932.80 +/- 80.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 933        |
|    mean_reward          | 8.1        |
| time/                   |            |
|    total_timesteps      | 3520000    |
| train/                  |            |
|    approx_kl            | 0.44549432 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.311     |
|    explained_variance   | 0.798      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0724    |
|    n_updates            | 4290       |
|    policy_gradient_loss | -0.0418    |
|    value_loss           | 0.0113     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 946      |
|    ep_rew_mean     | 7.77     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 430      |
|    time_elapsed    | 6435     |
|    total_timesteps | 3522560  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 953       |
|    ep_rew_mean          | 7.73      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 431       |
|    time_elapsed         | 6448      |
|    total_timesteps      | 3530752   |
| train/                  |           |
|    approx_kl            | 0.5213836 |
|    clip_fraction        | 0.314     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.302    |
|    explained_variance   | 0.796     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0384   |
|    n_updates            | 4300      |
|    policy_gradient_loss | -0.0402   |
|    value_loss           | 0.0119    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 970        |
|    ep_rew_mean          | 7.75       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 432        |
|    time_elapsed         | 6461       |
|    total_timesteps      | 3538944    |
| train/                  |            |
|    approx_kl            | 0.49199814 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.318     |
|    explained_variance   | 0.808      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0318    |
|    n_updates            | 4310       |
|    policy_gradient_loss | -0.0463    |
|    value_loss           | 0.0118     |
----------------------------------------
Eval num_timesteps=3540000, episode_reward=6.80 +/- 1.75
Episode length: 832.20 +/- 263.34
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 832        |
|    mean_reward          | 6.8        |
| time/                   |            |
|    total_timesteps      | 3540000    |
| train/                  |            |
|    approx_kl            | 0.45991546 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.289     |
|    explained_variance   | 0.785      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0724    |
|    n_updates            | 4320       |
|    policy_gradient_loss | -0.0375    |
|    value_loss           | 0.0145     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 970      |
|    ep_rew_mean     | 7.75     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 433      |
|    time_elapsed    | 6477     |
|    total_timesteps | 3547136  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 964      |
|    ep_rew_mean          | 7.72     |
| time/                   |          |
|    fps                  | 547      |
|    iterations           | 434      |
|    time_elapsed         | 6490     |
|    total_timesteps      | 3555328  |
| train/                  |          |
|    approx_kl            | 0.522557 |
|    clip_fraction        | 0.308    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.279   |
|    explained_variance   | 0.824    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0569  |
|    n_updates            | 4330     |
|    policy_gradient_loss | -0.041   |
|    value_loss           | 0.0127   |
--------------------------------------
Eval num_timesteps=3560000, episode_reward=9.00 +/- 2.21
Episode length: 1120.20 +/- 367.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.12e+03   |
|    mean_reward          | 9          |
| time/                   |            |
|    total_timesteps      | 3560000    |
| train/                  |            |
|    approx_kl            | 0.47942775 |
|    clip_fraction        | 0.322      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.295     |
|    explained_variance   | 0.853      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0614    |
|    n_updates            | 4340       |
|    policy_gradient_loss | -0.0411    |
|    value_loss           | 0.0103     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 953      |
|    ep_rew_mean     | 7.62     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 435      |
|    time_elapsed    | 6508     |
|    total_timesteps | 3563520  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 935        |
|    ep_rew_mean          | 7.46       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 436        |
|    time_elapsed         | 6521       |
|    total_timesteps      | 3571712    |
| train/                  |            |
|    approx_kl            | 0.62369573 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.299     |
|    explained_variance   | 0.817      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0585    |
|    n_updates            | 4350       |
|    policy_gradient_loss | -0.0418    |
|    value_loss           | 0.0131     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 954        |
|    ep_rew_mean          | 7.42       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 437        |
|    time_elapsed         | 6534       |
|    total_timesteps      | 3579904    |
| train/                  |            |
|    approx_kl            | 0.40425575 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.315     |
|    explained_variance   | 0.792      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0574    |
|    n_updates            | 4360       |
|    policy_gradient_loss | -0.0407    |
|    value_loss           | 0.0159     |
----------------------------------------
Eval num_timesteps=3580000, episode_reward=8.20 +/- 1.33
Episode length: 955.40 +/- 340.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 955        |
|    mean_reward          | 8.2        |
| time/                   |            |
|    total_timesteps      | 3580000    |
| train/                  |            |
|    approx_kl            | 0.34782678 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.315     |
|    explained_variance   | 0.811      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0614    |
|    n_updates            | 4370       |
|    policy_gradient_loss | -0.0406    |
|    value_loss           | 0.0177     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 945      |
|    ep_rew_mean     | 7.27     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 438      |
|    time_elapsed    | 6553     |
|    total_timesteps | 3588096  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 975        |
|    ep_rew_mean          | 7.34       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 439        |
|    time_elapsed         | 6565       |
|    total_timesteps      | 3596288    |
| train/                  |            |
|    approx_kl            | 0.37973216 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.31      |
|    explained_variance   | 0.8        |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0261    |
|    n_updates            | 4380       |
|    policy_gradient_loss | -0.0403    |
|    value_loss           | 0.0182     |
----------------------------------------
Eval num_timesteps=3600000, episode_reward=8.70 +/- 3.84
Episode length: 1224.60 +/- 1056.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.22e+03   |
|    mean_reward          | 8.7        |
| time/                   |            |
|    total_timesteps      | 3600000    |
| train/                  |            |
|    approx_kl            | 0.41097766 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.297     |
|    explained_variance   | 0.836      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.067     |
|    n_updates            | 4390       |
|    policy_gradient_loss | -0.0427    |
|    value_loss           | 0.0132     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 991      |
|    ep_rew_mean     | 7.39     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 440      |
|    time_elapsed    | 6588     |
|    total_timesteps | 3604480  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 994       |
|    ep_rew_mean          | 7.43      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 441       |
|    time_elapsed         | 6600      |
|    total_timesteps      | 3612672   |
| train/                  |           |
|    approx_kl            | 0.4223243 |
|    clip_fraction        | 0.315     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.318    |
|    explained_variance   | 0.84      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0502   |
|    n_updates            | 4400      |
|    policy_gradient_loss | -0.0462   |
|    value_loss           | 0.0115    |
---------------------------------------
Eval num_timesteps=3620000, episode_reward=9.00 +/- 2.10
Episode length: 1073.40 +/- 296.09
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.07e+03   |
|    mean_reward          | 9          |
| time/                   |            |
|    total_timesteps      | 3620000    |
| train/                  |            |
|    approx_kl            | 0.36370653 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.308     |
|    explained_variance   | 0.827      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.057     |
|    n_updates            | 4410       |
|    policy_gradient_loss | -0.0417    |
|    value_loss           | 0.0114     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 7.51     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 442      |
|    time_elapsed    | 6618     |
|    total_timesteps | 3620864  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 997        |
|    ep_rew_mean          | 7.45       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 443        |
|    time_elapsed         | 6631       |
|    total_timesteps      | 3629056    |
| train/                  |            |
|    approx_kl            | 0.47593066 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.348     |
|    explained_variance   | 0.782      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0547    |
|    n_updates            | 4420       |
|    policy_gradient_loss | -0.043     |
|    value_loss           | 0.00926    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 986        |
|    ep_rew_mean          | 7.48       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 444        |
|    time_elapsed         | 6644       |
|    total_timesteps      | 3637248    |
| train/                  |            |
|    approx_kl            | 0.41167104 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.312     |
|    explained_variance   | 0.837      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0406    |
|    n_updates            | 4430       |
|    policy_gradient_loss | -0.0423    |
|    value_loss           | 0.0114     |
----------------------------------------
Eval num_timesteps=3640000, episode_reward=7.60 +/- 0.80
Episode length: 879.20 +/- 157.06
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 879        |
|    mean_reward          | 7.6        |
| time/                   |            |
|    total_timesteps      | 3640000    |
| train/                  |            |
|    approx_kl            | 0.34979486 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.32      |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0482    |
|    n_updates            | 4440       |
|    policy_gradient_loss | -0.0446    |
|    value_loss           | 0.00844    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 980      |
|    ep_rew_mean     | 7.57     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 445      |
|    time_elapsed    | 6661     |
|    total_timesteps | 3645440  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 992        |
|    ep_rew_mean          | 7.64       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 446        |
|    time_elapsed         | 6674       |
|    total_timesteps      | 3653632    |
| train/                  |            |
|    approx_kl            | 0.33830684 |
|    clip_fraction        | 0.314      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.338     |
|    explained_variance   | 0.847      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.071     |
|    n_updates            | 4450       |
|    policy_gradient_loss | -0.0472    |
|    value_loss           | 0.00909    |
----------------------------------------
Eval num_timesteps=3660000, episode_reward=6.80 +/- 1.44
Episode length: 745.60 +/- 270.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 746       |
|    mean_reward          | 6.8       |
| time/                   |           |
|    total_timesteps      | 3660000   |
| train/                  |           |
|    approx_kl            | 0.4013729 |
|    clip_fraction        | 0.296     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.291    |
|    explained_variance   | 0.851     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0327   |
|    n_updates            | 4460      |
|    policy_gradient_loss | -0.0422   |
|    value_loss           | 0.0103    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 974      |
|    ep_rew_mean     | 7.58     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 447      |
|    time_elapsed    | 6691     |
|    total_timesteps | 3661824  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 993        |
|    ep_rew_mean          | 7.71       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 448        |
|    time_elapsed         | 6704       |
|    total_timesteps      | 3670016    |
| train/                  |            |
|    approx_kl            | 0.37607098 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.286     |
|    explained_variance   | 0.799      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0525    |
|    n_updates            | 4470       |
|    policy_gradient_loss | -0.0371    |
|    value_loss           | 0.0137     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 999        |
|    ep_rew_mean          | 7.82       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 449        |
|    time_elapsed         | 6717       |
|    total_timesteps      | 3678208    |
| train/                  |            |
|    approx_kl            | 0.45592338 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.304     |
|    explained_variance   | 0.837      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0424    |
|    n_updates            | 4480       |
|    policy_gradient_loss | -0.0404    |
|    value_loss           | 0.0111     |
----------------------------------------
Eval num_timesteps=3680000, episode_reward=10.00 +/- 2.90
Episode length: 1219.80 +/- 519.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.22e+03  |
|    mean_reward          | 10        |
| time/                   |           |
|    total_timesteps      | 3680000   |
| train/                  |           |
|    approx_kl            | 0.3934169 |
|    clip_fraction        | 0.307     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.308    |
|    explained_variance   | 0.842     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0361   |
|    n_updates            | 4490      |
|    policy_gradient_loss | -0.0405   |
|    value_loss           | 0.0106    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 7.95     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 450      |
|    time_elapsed    | 6738     |
|    total_timesteps | 3686400  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 997       |
|    ep_rew_mean          | 8.07      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 451       |
|    time_elapsed         | 6751      |
|    total_timesteps      | 3694592   |
| train/                  |           |
|    approx_kl            | 0.3570896 |
|    clip_fraction        | 0.316     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.326    |
|    explained_variance   | 0.855     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0692   |
|    n_updates            | 4500      |
|    policy_gradient_loss | -0.0431   |
|    value_loss           | 0.0105    |
---------------------------------------
Eval num_timesteps=3700000, episode_reward=7.10 +/- 1.28
Episode length: 806.20 +/- 412.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 806       |
|    mean_reward          | 7.1       |
| time/                   |           |
|    total_timesteps      | 3700000   |
| train/                  |           |
|    approx_kl            | 0.3855151 |
|    clip_fraction        | 0.309     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.321    |
|    explained_variance   | 0.821     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0363   |
|    n_updates            | 4510      |
|    policy_gradient_loss | -0.043    |
|    value_loss           | 0.0136    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 978      |
|    ep_rew_mean     | 8.01     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 452      |
|    time_elapsed    | 6768     |
|    total_timesteps | 3702784  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 990        |
|    ep_rew_mean          | 8.09       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 453        |
|    time_elapsed         | 6782       |
|    total_timesteps      | 3710976    |
| train/                  |            |
|    approx_kl            | 0.39777157 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.286     |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0593    |
|    n_updates            | 4520       |
|    policy_gradient_loss | -0.0391    |
|    value_loss           | 0.0109     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 951       |
|    ep_rew_mean          | 7.88      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 454       |
|    time_elapsed         | 6796      |
|    total_timesteps      | 3719168   |
| train/                  |           |
|    approx_kl            | 0.3827805 |
|    clip_fraction        | 0.288     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.294    |
|    explained_variance   | 0.783     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0324   |
|    n_updates            | 4530      |
|    policy_gradient_loss | -0.0362   |
|    value_loss           | 0.0132    |
---------------------------------------
Eval num_timesteps=3720000, episode_reward=7.90 +/- 0.80
Episode length: 955.40 +/- 271.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 955        |
|    mean_reward          | 7.9        |
| time/                   |            |
|    total_timesteps      | 3720000    |
| train/                  |            |
|    approx_kl            | 0.34820437 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.325     |
|    explained_variance   | 0.859      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0584    |
|    n_updates            | 4540       |
|    policy_gradient_loss | -0.0408    |
|    value_loss           | 0.00986    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 944      |
|    ep_rew_mean     | 7.87     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 455      |
|    time_elapsed    | 6814     |
|    total_timesteps | 3727360  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 936        |
|    ep_rew_mean          | 7.78       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 456        |
|    time_elapsed         | 6828       |
|    total_timesteps      | 3735552    |
| train/                  |            |
|    approx_kl            | 0.35732278 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.316     |
|    explained_variance   | 0.851      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.044     |
|    n_updates            | 4550       |
|    policy_gradient_loss | -0.0381    |
|    value_loss           | 0.00978    |
----------------------------------------
Eval num_timesteps=3740000, episode_reward=8.00 +/- 1.26
Episode length: 921.40 +/- 228.34
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 921        |
|    mean_reward          | 8          |
| time/                   |            |
|    total_timesteps      | 3740000    |
| train/                  |            |
|    approx_kl            | 0.34882784 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.343     |
|    explained_variance   | 0.857      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0478    |
|    n_updates            | 4560       |
|    policy_gradient_loss | -0.0443    |
|    value_loss           | 0.00768    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 931      |
|    ep_rew_mean     | 7.71     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 457      |
|    time_elapsed    | 6845     |
|    total_timesteps | 3743744  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 932        |
|    ep_rew_mean          | 7.58       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 458        |
|    time_elapsed         | 6858       |
|    total_timesteps      | 3751936    |
| train/                  |            |
|    approx_kl            | 0.36915064 |
|    clip_fraction        | 0.294      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.341     |
|    explained_variance   | 0.764      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0224    |
|    n_updates            | 4570       |
|    policy_gradient_loss | -0.0387    |
|    value_loss           | 0.0182     |
----------------------------------------
Eval num_timesteps=3760000, episode_reward=6.80 +/- 2.04
Episode length: 1099.40 +/- 298.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.1e+03    |
|    mean_reward          | 6.8        |
| time/                   |            |
|    total_timesteps      | 3760000    |
| train/                  |            |
|    approx_kl            | 0.39156327 |
|    clip_fraction        | 0.282      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.323     |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0421    |
|    n_updates            | 4580       |
|    policy_gradient_loss | -0.0365    |
|    value_loss           | 0.0177     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 946      |
|    ep_rew_mean     | 7.58     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 459      |
|    time_elapsed    | 6877     |
|    total_timesteps | 3760128  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 927        |
|    ep_rew_mean          | 7.51       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 460        |
|    time_elapsed         | 6890       |
|    total_timesteps      | 3768320    |
| train/                  |            |
|    approx_kl            | 0.38526553 |
|    clip_fraction        | 0.291      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.308     |
|    explained_variance   | 0.784      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0536    |
|    n_updates            | 4590       |
|    policy_gradient_loss | -0.036     |
|    value_loss           | 0.0166     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 931       |
|    ep_rew_mean          | 7.42      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 461       |
|    time_elapsed         | 6903      |
|    total_timesteps      | 3776512   |
| train/                  |           |
|    approx_kl            | 0.3716952 |
|    clip_fraction        | 0.322     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.348    |
|    explained_variance   | 0.867     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0347   |
|    n_updates            | 4600      |
|    policy_gradient_loss | -0.0454   |
|    value_loss           | 0.00781   |
---------------------------------------
Eval num_timesteps=3780000, episode_reward=8.50 +/- 1.97
Episode length: 1158.60 +/- 345.81
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.16e+03   |
|    mean_reward          | 8.5        |
| time/                   |            |
|    total_timesteps      | 3780000    |
| train/                  |            |
|    approx_kl            | 0.42966858 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.306     |
|    explained_variance   | 0.82       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0478    |
|    n_updates            | 4610       |
|    policy_gradient_loss | -0.0372    |
|    value_loss           | 0.0151     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 922      |
|    ep_rew_mean     | 7.38     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 462      |
|    time_elapsed    | 6921     |
|    total_timesteps | 3784704  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 924        |
|    ep_rew_mean          | 7.39       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 463        |
|    time_elapsed         | 6935       |
|    total_timesteps      | 3792896    |
| train/                  |            |
|    approx_kl            | 0.38338012 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.31      |
|    explained_variance   | 0.836      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0295    |
|    n_updates            | 4620       |
|    policy_gradient_loss | -0.0352    |
|    value_loss           | 0.0117     |
----------------------------------------
Eval num_timesteps=3800000, episode_reward=7.60 +/- 2.15
Episode length: 841.40 +/- 286.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 841       |
|    mean_reward          | 7.6       |
| time/                   |           |
|    total_timesteps      | 3800000   |
| train/                  |           |
|    approx_kl            | 0.3533693 |
|    clip_fraction        | 0.291     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.325    |
|    explained_variance   | 0.821     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0596   |
|    n_updates            | 4630      |
|    policy_gradient_loss | -0.0398   |
|    value_loss           | 0.0114    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 912      |
|    ep_rew_mean     | 7.36     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 464      |
|    time_elapsed    | 6951     |
|    total_timesteps | 3801088  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 908        |
|    ep_rew_mean          | 7.4        |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 465        |
|    time_elapsed         | 6964       |
|    total_timesteps      | 3809280    |
| train/                  |            |
|    approx_kl            | 0.41240242 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.342     |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0553    |
|    n_updates            | 4640       |
|    policy_gradient_loss | -0.0399    |
|    value_loss           | 0.00904    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 926        |
|    ep_rew_mean          | 7.51       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 466        |
|    time_elapsed         | 6977       |
|    total_timesteps      | 3817472    |
| train/                  |            |
|    approx_kl            | 0.37509727 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.34      |
|    explained_variance   | 0.837      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0522    |
|    n_updates            | 4650       |
|    policy_gradient_loss | -0.0362    |
|    value_loss           | 0.0106     |
----------------------------------------
Eval num_timesteps=3820000, episode_reward=8.00 +/- 0.89
Episode length: 998.00 +/- 77.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 998       |
|    mean_reward          | 8         |
| time/                   |           |
|    total_timesteps      | 3820000   |
| train/                  |           |
|    approx_kl            | 0.3436215 |
|    clip_fraction        | 0.31      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.339    |
|    explained_variance   | 0.861     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0468   |
|    n_updates            | 4660      |
|    policy_gradient_loss | -0.0393   |
|    value_loss           | 0.00723   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 930      |
|    ep_rew_mean     | 7.49     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 467      |
|    time_elapsed    | 6994     |
|    total_timesteps | 3825664  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 932       |
|    ep_rew_mean          | 7.51      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 468       |
|    time_elapsed         | 7008      |
|    total_timesteps      | 3833856   |
| train/                  |           |
|    approx_kl            | 0.3005486 |
|    clip_fraction        | 0.299     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.349    |
|    explained_variance   | 0.832     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.00168  |
|    n_updates            | 4670      |
|    policy_gradient_loss | -0.037    |
|    value_loss           | 0.0121    |
---------------------------------------
Eval num_timesteps=3840000, episode_reward=8.70 +/- 1.86
Episode length: 1079.60 +/- 289.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.08e+03   |
|    mean_reward          | 8.7        |
| time/                   |            |
|    total_timesteps      | 3840000    |
| train/                  |            |
|    approx_kl            | 0.31298232 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.35      |
|    explained_variance   | 0.801      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0599    |
|    n_updates            | 4680       |
|    policy_gradient_loss | -0.0364    |
|    value_loss           | 0.0127     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 941      |
|    ep_rew_mean     | 7.57     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 469      |
|    time_elapsed    | 7029     |
|    total_timesteps | 3842048  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 943        |
|    ep_rew_mean          | 7.67       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 470        |
|    time_elapsed         | 7043       |
|    total_timesteps      | 3850240    |
| train/                  |            |
|    approx_kl            | 0.29402786 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.337     |
|    explained_variance   | 0.82       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.063     |
|    n_updates            | 4690       |
|    policy_gradient_loss | -0.0377    |
|    value_loss           | 0.0116     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 948        |
|    ep_rew_mean          | 7.62       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 471        |
|    time_elapsed         | 7057       |
|    total_timesteps      | 3858432    |
| train/                  |            |
|    approx_kl            | 0.30289716 |
|    clip_fraction        | 0.295      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.357     |
|    explained_variance   | 0.837      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0709    |
|    n_updates            | 4700       |
|    policy_gradient_loss | -0.0405    |
|    value_loss           | 0.0121     |
----------------------------------------
Eval num_timesteps=3860000, episode_reward=7.40 +/- 2.20
Episode length: 861.40 +/- 358.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 861        |
|    mean_reward          | 7.4        |
| time/                   |            |
|    total_timesteps      | 3860000    |
| train/                  |            |
|    approx_kl            | 0.29472327 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.335     |
|    explained_variance   | 0.842      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0453    |
|    n_updates            | 4710       |
|    policy_gradient_loss | -0.0371    |
|    value_loss           | 0.0132     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 951      |
|    ep_rew_mean     | 7.62     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 472      |
|    time_elapsed    | 7075     |
|    total_timesteps | 3866624  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 953        |
|    ep_rew_mean          | 7.72       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 473        |
|    time_elapsed         | 7090       |
|    total_timesteps      | 3874816    |
| train/                  |            |
|    approx_kl            | 0.32946146 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.339     |
|    explained_variance   | 0.82       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0476    |
|    n_updates            | 4720       |
|    policy_gradient_loss | -0.037     |
|    value_loss           | 0.0132     |
----------------------------------------
Eval num_timesteps=3880000, episode_reward=6.40 +/- 0.37
Episode length: 635.40 +/- 128.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 635        |
|    mean_reward          | 6.4        |
| time/                   |            |
|    total_timesteps      | 3880000    |
| train/                  |            |
|    approx_kl            | 0.33017772 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.359     |
|    explained_variance   | 0.813      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0519    |
|    n_updates            | 4730       |
|    policy_gradient_loss | -0.0414    |
|    value_loss           | 0.011      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 953      |
|    ep_rew_mean     | 7.74     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 474      |
|    time_elapsed    | 7106     |
|    total_timesteps | 3883008  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 947        |
|    ep_rew_mean          | 7.75       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 475        |
|    time_elapsed         | 7120       |
|    total_timesteps      | 3891200    |
| train/                  |            |
|    approx_kl            | 0.39214793 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.317     |
|    explained_variance   | 0.847      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0482    |
|    n_updates            | 4740       |
|    policy_gradient_loss | -0.038     |
|    value_loss           | 0.0103     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 956        |
|    ep_rew_mean          | 7.68       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 476        |
|    time_elapsed         | 7133       |
|    total_timesteps      | 3899392    |
| train/                  |            |
|    approx_kl            | 0.35679173 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.309     |
|    explained_variance   | 0.867      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0553    |
|    n_updates            | 4750       |
|    policy_gradient_loss | -0.0402    |
|    value_loss           | 0.00863    |
----------------------------------------
Eval num_timesteps=3900000, episode_reward=8.10 +/- 1.11
Episode length: 952.40 +/- 261.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 952       |
|    mean_reward          | 8.1       |
| time/                   |           |
|    total_timesteps      | 3900000   |
| train/                  |           |
|    approx_kl            | 0.3615008 |
|    clip_fraction        | 0.298     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.336    |
|    explained_variance   | 0.818     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0465   |
|    n_updates            | 4760      |
|    policy_gradient_loss | -0.0357   |
|    value_loss           | 0.0107    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 986      |
|    ep_rew_mean     | 7.86     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 477      |
|    time_elapsed    | 7152     |
|    total_timesteps | 3907584  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 958        |
|    ep_rew_mean          | 7.74       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 478        |
|    time_elapsed         | 7165       |
|    total_timesteps      | 3915776    |
| train/                  |            |
|    approx_kl            | 0.36735496 |
|    clip_fraction        | 0.283      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.3       |
|    explained_variance   | 0.835      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0481    |
|    n_updates            | 4770       |
|    policy_gradient_loss | -0.0383    |
|    value_loss           | 0.0118     |
----------------------------------------
Eval num_timesteps=3920000, episode_reward=7.70 +/- 1.33
Episode length: 945.40 +/- 295.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 945        |
|    mean_reward          | 7.7        |
| time/                   |            |
|    total_timesteps      | 3920000    |
| train/                  |            |
|    approx_kl            | 0.32801354 |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.293     |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0415    |
|    n_updates            | 4780       |
|    policy_gradient_loss | -0.0349    |
|    value_loss           | 0.0111     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 7.89     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 479      |
|    time_elapsed    | 7184     |
|    total_timesteps | 3923968  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 7.93       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 480        |
|    time_elapsed         | 7198       |
|    total_timesteps      | 3932160    |
| train/                  |            |
|    approx_kl            | 0.32326204 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.351     |
|    explained_variance   | 0.799      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0744    |
|    n_updates            | 4790       |
|    policy_gradient_loss | -0.0366    |
|    value_loss           | 0.0145     |
----------------------------------------
Eval num_timesteps=3940000, episode_reward=8.50 +/- 4.34
Episode length: 1337.60 +/- 790.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.34e+03   |
|    mean_reward          | 8.5        |
| time/                   |            |
|    total_timesteps      | 3940000    |
| train/                  |            |
|    approx_kl            | 0.32993954 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.328     |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0534    |
|    n_updates            | 4800       |
|    policy_gradient_loss | -0.0386    |
|    value_loss           | 0.0129     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 7.83     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 481      |
|    time_elapsed    | 7219     |
|    total_timesteps | 3940352  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 995        |
|    ep_rew_mean          | 7.78       |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 482        |
|    time_elapsed         | 7232       |
|    total_timesteps      | 3948544    |
| train/                  |            |
|    approx_kl            | 0.36419883 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.339     |
|    explained_variance   | 0.792      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0366    |
|    n_updates            | 4810       |
|    policy_gradient_loss | -0.0371    |
|    value_loss           | 0.0162     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 995        |
|    ep_rew_mean          | 7.76       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 483        |
|    time_elapsed         | 7245       |
|    total_timesteps      | 3956736    |
| train/                  |            |
|    approx_kl            | 0.36846143 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.292     |
|    explained_variance   | 0.81       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0242    |
|    n_updates            | 4820       |
|    policy_gradient_loss | -0.0355    |
|    value_loss           | 0.0151     |
----------------------------------------
Eval num_timesteps=3960000, episode_reward=8.60 +/- 1.36
Episode length: 1265.20 +/- 369.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.27e+03  |
|    mean_reward          | 8.6       |
| time/                   |           |
|    total_timesteps      | 3960000   |
| train/                  |           |
|    approx_kl            | 0.3092262 |
|    clip_fraction        | 0.288     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.318    |
|    explained_variance   | 0.828     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0507   |
|    n_updates            | 4830      |
|    policy_gradient_loss | -0.0399   |
|    value_loss           | 0.0164    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 7.74     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 484      |
|    time_elapsed    | 7262     |
|    total_timesteps | 3964928  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 997        |
|    ep_rew_mean          | 7.7        |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 485        |
|    time_elapsed         | 7275       |
|    total_timesteps      | 3973120    |
| train/                  |            |
|    approx_kl            | 0.45735455 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.3       |
|    explained_variance   | 0.804      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0368    |
|    n_updates            | 4840       |
|    policy_gradient_loss | -0.0341    |
|    value_loss           | 0.0161     |
----------------------------------------
Eval num_timesteps=3980000, episode_reward=7.50 +/- 3.27
Episode length: 1133.60 +/- 622.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.13e+03   |
|    mean_reward          | 7.5        |
| time/                   |            |
|    total_timesteps      | 3980000    |
| train/                  |            |
|    approx_kl            | 0.43671572 |
|    clip_fraction        | 0.283      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.301     |
|    explained_variance   | 0.825      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0451    |
|    n_updates            | 4850       |
|    policy_gradient_loss | -0.0382    |
|    value_loss           | 0.0135     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 7.66     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 486      |
|    time_elapsed    | 7294     |
|    total_timesteps | 3981312  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 7.57       |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 487        |
|    time_elapsed         | 7308       |
|    total_timesteps      | 3989504    |
| train/                  |            |
|    approx_kl            | 0.34902725 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.267     |
|    explained_variance   | 0.824      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0424    |
|    n_updates            | 4860       |
|    policy_gradient_loss | -0.0357    |
|    value_loss           | 0.0142     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 989        |
|    ep_rew_mean          | 7.43       |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 488        |
|    time_elapsed         | 7322       |
|    total_timesteps      | 3997696    |
| train/                  |            |
|    approx_kl            | 0.32271597 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.271     |
|    explained_variance   | 0.846      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0392    |
|    n_updates            | 4870       |
|    policy_gradient_loss | -0.0377    |
|    value_loss           | 0.0139     |
----------------------------------------
Eval num_timesteps=4000000, episode_reward=6.50 +/- 0.95
Episode length: 864.00 +/- 487.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 864        |
|    mean_reward          | 6.5        |
| time/                   |            |
|    total_timesteps      | 4000000    |
| train/                  |            |
|    approx_kl            | 0.34071052 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.271     |
|    explained_variance   | 0.841      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0495    |
|    n_updates            | 4880       |
|    policy_gradient_loss | -0.0341    |
|    value_loss           | 0.015      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 973      |
|    ep_rew_mean     | 7.33     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 489      |
|    time_elapsed    | 7339     |
|    total_timesteps | 4005888  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 973        |
|    ep_rew_mean          | 7.26       |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 490        |
|    time_elapsed         | 7352       |
|    total_timesteps      | 4014080    |
| train/                  |            |
|    approx_kl            | 0.33201888 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.3       |
|    explained_variance   | 0.787      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0278    |
|    n_updates            | 4890       |
|    policy_gradient_loss | -0.0359    |
|    value_loss           | 0.0147     |
----------------------------------------
Eval num_timesteps=4020000, episode_reward=6.50 +/- 3.41
Episode length: 1267.40 +/- 712.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.27e+03   |
|    mean_reward          | 6.5        |
| time/                   |            |
|    total_timesteps      | 4020000    |
| train/                  |            |
|    approx_kl            | 0.32271162 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.317     |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0378    |
|    n_updates            | 4900       |
|    policy_gradient_loss | -0.0381    |
|    value_loss           | 0.0146     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 945      |
|    ep_rew_mean     | 7.13     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 491      |
|    time_elapsed    | 7371     |
|    total_timesteps | 4022272  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 912        |
|    ep_rew_mean          | 6.99       |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 492        |
|    time_elapsed         | 7385       |
|    total_timesteps      | 4030464    |
| train/                  |            |
|    approx_kl            | 0.33101875 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.259     |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0483    |
|    n_updates            | 4910       |
|    policy_gradient_loss | -0.0316    |
|    value_loss           | 0.0118     |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 930      |
|    ep_rew_mean          | 7.09     |
| time/                   |          |
|    fps                  | 545      |
|    iterations           | 493      |
|    time_elapsed         | 7399     |
|    total_timesteps      | 4038656  |
| train/                  |          |
|    approx_kl            | 0.286487 |
|    clip_fraction        | 0.276    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.316   |
|    explained_variance   | 0.831    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0524  |
|    n_updates            | 4920     |
|    policy_gradient_loss | -0.0344  |
|    value_loss           | 0.0141   |
--------------------------------------
Eval num_timesteps=4040000, episode_reward=6.40 +/- 1.50
Episode length: 650.00 +/- 215.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 650       |
|    mean_reward          | 6.4       |
| time/                   |           |
|    total_timesteps      | 4040000   |
| train/                  |           |
|    approx_kl            | 0.3356651 |
|    clip_fraction        | 0.287     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.33     |
|    explained_variance   | 0.804     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0557   |
|    n_updates            | 4930      |
|    policy_gradient_loss | -0.0383   |
|    value_loss           | 0.0107    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 938      |
|    ep_rew_mean     | 7.14     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 494      |
|    time_elapsed    | 7415     |
|    total_timesteps | 4046848  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 926        |
|    ep_rew_mean          | 7.22       |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 495        |
|    time_elapsed         | 7428       |
|    total_timesteps      | 4055040    |
| train/                  |            |
|    approx_kl            | 0.31283116 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.316     |
|    explained_variance   | 0.851      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0457    |
|    n_updates            | 4940       |
|    policy_gradient_loss | -0.0384    |
|    value_loss           | 0.011      |
----------------------------------------
Eval num_timesteps=4060000, episode_reward=8.10 +/- 2.08
Episode length: 991.40 +/- 403.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 991        |
|    mean_reward          | 8.1        |
| time/                   |            |
|    total_timesteps      | 4060000    |
| train/                  |            |
|    approx_kl            | 0.37663442 |
|    clip_fraction        | 0.291      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.316     |
|    explained_variance   | 0.84       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0458    |
|    n_updates            | 4950       |
|    policy_gradient_loss | -0.0389    |
|    value_loss           | 0.0123     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 932      |
|    ep_rew_mean     | 7.27     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 496      |
|    time_elapsed    | 7445     |
|    total_timesteps | 4063232  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 925        |
|    ep_rew_mean          | 7.23       |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 497        |
|    time_elapsed         | 7458       |
|    total_timesteps      | 4071424    |
| train/                  |            |
|    approx_kl            | 0.25432003 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.336     |
|    explained_variance   | 0.808      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0292    |
|    n_updates            | 4960       |
|    policy_gradient_loss | -0.0368    |
|    value_loss           | 0.0141     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 907       |
|    ep_rew_mean          | 7.13      |
| time/                   |           |
|    fps                  | 545       |
|    iterations           | 498       |
|    time_elapsed         | 7471      |
|    total_timesteps      | 4079616   |
| train/                  |           |
|    approx_kl            | 0.3106909 |
|    clip_fraction        | 0.268     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.299    |
|    explained_variance   | 0.829     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0246   |
|    n_updates            | 4970      |
|    policy_gradient_loss | -0.0355   |
|    value_loss           | 0.0149    |
---------------------------------------
Eval num_timesteps=4080000, episode_reward=7.40 +/- 1.39
Episode length: 886.80 +/- 369.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 887       |
|    mean_reward          | 7.4       |
| time/                   |           |
|    total_timesteps      | 4080000   |
| train/                  |           |
|    approx_kl            | 0.3008877 |
|    clip_fraction        | 0.286     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.325    |
|    explained_variance   | 0.802     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.038    |
|    n_updates            | 4980      |
|    policy_gradient_loss | -0.0355   |
|    value_loss           | 0.0162    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 908      |
|    ep_rew_mean     | 7.17     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 499      |
|    time_elapsed    | 7489     |
|    total_timesteps | 4087808  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 896      |
|    ep_rew_mean          | 7.12     |
| time/                   |          |
|    fps                  | 545      |
|    iterations           | 500      |
|    time_elapsed         | 7503     |
|    total_timesteps      | 4096000  |
| train/                  |          |
|    approx_kl            | 0.287849 |
|    clip_fraction        | 0.265    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.312   |
|    explained_variance   | 0.844    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.00528  |
|    n_updates            | 4990     |
|    policy_gradient_loss | -0.0329  |
|    value_loss           | 0.0125   |
--------------------------------------
Eval num_timesteps=4100000, episode_reward=6.70 +/- 0.87
Episode length: 913.00 +/- 303.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 913       |
|    mean_reward          | 6.7       |
| time/                   |           |
|    total_timesteps      | 4100000   |
| train/                  |           |
|    approx_kl            | 0.2891562 |
|    clip_fraction        | 0.258     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.295    |
|    explained_variance   | 0.844     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0629   |
|    n_updates            | 5000      |
|    policy_gradient_loss | -0.0376   |
|    value_loss           | 0.0133    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 893      |
|    ep_rew_mean     | 7.14     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 501      |
|    time_elapsed    | 7520     |
|    total_timesteps | 4104192  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 909       |
|    ep_rew_mean          | 7.21      |
| time/                   |           |
|    fps                  | 545       |
|    iterations           | 502       |
|    time_elapsed         | 7532      |
|    total_timesteps      | 4112384   |
| train/                  |           |
|    approx_kl            | 0.2855379 |
|    clip_fraction        | 0.272     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.327    |
|    explained_variance   | 0.829     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0377   |
|    n_updates            | 5010      |
|    policy_gradient_loss | -0.039    |
|    value_loss           | 0.0132    |
---------------------------------------
Eval num_timesteps=4120000, episode_reward=6.70 +/- 0.87
Episode length: 721.80 +/- 172.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 722        |
|    mean_reward          | 6.7        |
| time/                   |            |
|    total_timesteps      | 4120000    |
| train/                  |            |
|    approx_kl            | 0.33574587 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.311     |
|    explained_variance   | 0.832      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0399    |
|    n_updates            | 5020       |
|    policy_gradient_loss | -0.0435    |
|    value_loss           | 0.0134     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 916      |
|    ep_rew_mean     | 7.22     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 503      |
|    time_elapsed    | 7548     |
|    total_timesteps | 4120576  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 935        |
|    ep_rew_mean          | 7.36       |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 504        |
|    time_elapsed         | 7561       |
|    total_timesteps      | 4128768    |
| train/                  |            |
|    approx_kl            | 0.31113386 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.292     |
|    explained_variance   | 0.789      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0535    |
|    n_updates            | 5030       |
|    policy_gradient_loss | -0.0357    |
|    value_loss           | 0.0149     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 954        |
|    ep_rew_mean          | 7.43       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 505        |
|    time_elapsed         | 7574       |
|    total_timesteps      | 4136960    |
| train/                  |            |
|    approx_kl            | 0.33085322 |
|    clip_fraction        | 0.283      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.324     |
|    explained_variance   | 0.816      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0745    |
|    n_updates            | 5040       |
|    policy_gradient_loss | -0.0419    |
|    value_loss           | 0.0157     |
----------------------------------------
Eval num_timesteps=4140000, episode_reward=7.70 +/- 2.23
Episode length: 1105.60 +/- 550.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.11e+03   |
|    mean_reward          | 7.7        |
| time/                   |            |
|    total_timesteps      | 4140000    |
| train/                  |            |
|    approx_kl            | 0.33164707 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.3       |
|    explained_variance   | 0.759      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0211    |
|    n_updates            | 5050       |
|    policy_gradient_loss | -0.0344    |
|    value_loss           | 0.0151     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 955      |
|    ep_rew_mean     | 7.38     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 506      |
|    time_elapsed    | 7594     |
|    total_timesteps | 4145152  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 971        |
|    ep_rew_mean          | 7.4        |
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 507        |
|    time_elapsed         | 7607       |
|    total_timesteps      | 4153344    |
| train/                  |            |
|    approx_kl            | 0.31221655 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.29      |
|    explained_variance   | 0.796      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0147    |
|    n_updates            | 5060       |
|    policy_gradient_loss | -0.0364    |
|    value_loss           | 0.0142     |
----------------------------------------
Eval num_timesteps=4160000, episode_reward=6.70 +/- 1.29
Episode length: 705.00 +/- 175.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 705       |
|    mean_reward          | 6.7       |
| time/                   |           |
|    total_timesteps      | 4160000   |
| train/                  |           |
|    approx_kl            | 0.3507897 |
|    clip_fraction        | 0.246     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.243    |
|    explained_variance   | 0.824     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0481   |
|    n_updates            | 5070      |
|    policy_gradient_loss | -0.0297   |
|    value_loss           | 0.0163    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 955      |
|    ep_rew_mean     | 7.31     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 508      |
|    time_elapsed    | 7623     |
|    total_timesteps | 4161536  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 948        |
|    ep_rew_mean          | 7.32       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 509        |
|    time_elapsed         | 7636       |
|    total_timesteps      | 4169728    |
| train/                  |            |
|    approx_kl            | 0.32901835 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.28      |
|    explained_variance   | 0.829      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0279     |
|    n_updates            | 5080       |
|    policy_gradient_loss | -0.0305    |
|    value_loss           | 0.0151     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 955        |
|    ep_rew_mean          | 7.37       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 510        |
|    time_elapsed         | 7649       |
|    total_timesteps      | 4177920    |
| train/                  |            |
|    approx_kl            | 0.26729298 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.323     |
|    explained_variance   | 0.841      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0727    |
|    n_updates            | 5090       |
|    policy_gradient_loss | -0.0385    |
|    value_loss           | 0.011      |
----------------------------------------
Eval num_timesteps=4180000, episode_reward=8.00 +/- 1.41
Episode length: 1096.80 +/- 452.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.1e+03    |
|    mean_reward          | 8          |
| time/                   |            |
|    total_timesteps      | 4180000    |
| train/                  |            |
|    approx_kl            | 0.31329578 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.289     |
|    explained_variance   | 0.845      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0309    |
|    n_updates            | 5100       |
|    policy_gradient_loss | -0.0339    |
|    value_loss           | 0.0134     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 957      |
|    ep_rew_mean     | 7.44     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 511      |
|    time_elapsed    | 7665     |
|    total_timesteps | 4186112  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 961        |
|    ep_rew_mean          | 7.45       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 512        |
|    time_elapsed         | 7678       |
|    total_timesteps      | 4194304    |
| train/                  |            |
|    approx_kl            | 0.30319607 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.293     |
|    explained_variance   | 0.866      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0334    |
|    n_updates            | 5110       |
|    policy_gradient_loss | -0.034     |
|    value_loss           | 0.0118     |
----------------------------------------
Eval num_timesteps=4200000, episode_reward=7.30 +/- 1.66
Episode length: 824.00 +/- 440.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 824        |
|    mean_reward          | 7.3        |
| time/                   |            |
|    total_timesteps      | 4200000    |
| train/                  |            |
|    approx_kl            | 0.31257123 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.287     |
|    explained_variance   | 0.838      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0542    |
|    n_updates            | 5120       |
|    policy_gradient_loss | -0.0368    |
|    value_loss           | 0.0136     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 947      |
|    ep_rew_mean     | 7.42     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 513      |
|    time_elapsed    | 7695     |
|    total_timesteps | 4202496  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 941        |
|    ep_rew_mean          | 7.42       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 514        |
|    time_elapsed         | 7708       |
|    total_timesteps      | 4210688    |
| train/                  |            |
|    approx_kl            | 0.30794877 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.286     |
|    explained_variance   | 0.858      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0511    |
|    n_updates            | 5130       |
|    policy_gradient_loss | -0.0372    |
|    value_loss           | 0.0101     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 943       |
|    ep_rew_mean          | 7.5       |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 515       |
|    time_elapsed         | 7721      |
|    total_timesteps      | 4218880   |
| train/                  |           |
|    approx_kl            | 0.3214615 |
|    clip_fraction        | 0.262     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.304    |
|    explained_variance   | 0.864     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0352   |
|    n_updates            | 5140      |
|    policy_gradient_loss | -0.0373   |
|    value_loss           | 0.0109    |
---------------------------------------
Eval num_timesteps=4220000, episode_reward=5.40 +/- 1.28
Episode length: 579.80 +/- 182.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 580       |
|    mean_reward          | 5.4       |
| time/                   |           |
|    total_timesteps      | 4220000   |
| train/                  |           |
|    approx_kl            | 0.3135726 |
|    clip_fraction        | 0.273     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.307    |
|    explained_variance   | 0.869     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0607   |
|    n_updates            | 5150      |
|    policy_gradient_loss | -0.0405   |
|    value_loss           | 0.0101    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 925      |
|    ep_rew_mean     | 7.43     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 516      |
|    time_elapsed    | 7736     |
|    total_timesteps | 4227072  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 907        |
|    ep_rew_mean          | 7.31       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 517        |
|    time_elapsed         | 7749       |
|    total_timesteps      | 4235264    |
| train/                  |            |
|    approx_kl            | 0.28079706 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.323     |
|    explained_variance   | 0.809      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0384    |
|    n_updates            | 5160       |
|    policy_gradient_loss | -0.0374    |
|    value_loss           | 0.0141     |
----------------------------------------
Eval num_timesteps=4240000, episode_reward=7.80 +/- 1.75
Episode length: 793.60 +/- 280.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 794       |
|    mean_reward          | 7.8       |
| time/                   |           |
|    total_timesteps      | 4240000   |
| train/                  |           |
|    approx_kl            | 0.2907108 |
|    clip_fraction        | 0.262     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.31     |
|    explained_variance   | 0.813     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0375   |
|    n_updates            | 5170      |
|    policy_gradient_loss | -0.0338   |
|    value_loss           | 0.0158    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 890      |
|    ep_rew_mean     | 7.25     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 518      |
|    time_elapsed    | 7764     |
|    total_timesteps | 4243456  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 894        |
|    ep_rew_mean          | 7.3        |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 519        |
|    time_elapsed         | 7777       |
|    total_timesteps      | 4251648    |
| train/                  |            |
|    approx_kl            | 0.25765818 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.281     |
|    explained_variance   | 0.811      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0258    |
|    n_updates            | 5180       |
|    policy_gradient_loss | -0.033     |
|    value_loss           | 0.0167     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 903        |
|    ep_rew_mean          | 7.33       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 520        |
|    time_elapsed         | 7790       |
|    total_timesteps      | 4259840    |
| train/                  |            |
|    approx_kl            | 0.31823337 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.29      |
|    explained_variance   | 0.847      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.026     |
|    n_updates            | 5190       |
|    policy_gradient_loss | -0.0363    |
|    value_loss           | 0.0124     |
----------------------------------------
Eval num_timesteps=4260000, episode_reward=9.10 +/- 1.98
Episode length: 1168.60 +/- 366.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.17e+03  |
|    mean_reward          | 9.1       |
| time/                   |           |
|    total_timesteps      | 4260000   |
| train/                  |           |
|    approx_kl            | 0.3110556 |
|    clip_fraction        | 0.282     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.322    |
|    explained_variance   | 0.881     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0375   |
|    n_updates            | 5200      |
|    policy_gradient_loss | -0.0416   |
|    value_loss           | 0.0097    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 885      |
|    ep_rew_mean     | 7.21     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 521      |
|    time_elapsed    | 7809     |
|    total_timesteps | 4268032  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 897        |
|    ep_rew_mean          | 7.29       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 522        |
|    time_elapsed         | 7822       |
|    total_timesteps      | 4276224    |
| train/                  |            |
|    approx_kl            | 0.32337758 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.267     |
|    explained_variance   | 0.847      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00659   |
|    n_updates            | 5210       |
|    policy_gradient_loss | -0.0329    |
|    value_loss           | 0.0127     |
----------------------------------------
Eval num_timesteps=4280000, episode_reward=8.50 +/- 2.07
Episode length: 1014.20 +/- 427.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.01e+03  |
|    mean_reward          | 8.5       |
| time/                   |           |
|    total_timesteps      | 4280000   |
| train/                  |           |
|    approx_kl            | 0.3345961 |
|    clip_fraction        | 0.243     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.242    |
|    explained_variance   | 0.829     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0453   |
|    n_updates            | 5220      |
|    policy_gradient_loss | -0.0307   |
|    value_loss           | 0.0109    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 899      |
|    ep_rew_mean     | 7.33     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 523      |
|    time_elapsed    | 7839     |
|    total_timesteps | 4284416  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 895        |
|    ep_rew_mean          | 7.38       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 524        |
|    time_elapsed         | 7853       |
|    total_timesteps      | 4292608    |
| train/                  |            |
|    approx_kl            | 0.31428438 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.249     |
|    explained_variance   | 0.841      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.06      |
|    n_updates            | 5230       |
|    policy_gradient_loss | -0.0311    |
|    value_loss           | 0.0102     |
----------------------------------------
Eval num_timesteps=4300000, episode_reward=7.80 +/- 2.98
Episode length: 1283.20 +/- 112.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.28e+03   |
|    mean_reward          | 7.8        |
| time/                   |            |
|    total_timesteps      | 4300000    |
| train/                  |            |
|    approx_kl            | 0.36989915 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.295     |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.049     |
|    n_updates            | 5240       |
|    policy_gradient_loss | -0.0393    |
|    value_loss           | 0.00899    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 929      |
|    ep_rew_mean     | 7.47     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 525      |
|    time_elapsed    | 7872     |
|    total_timesteps | 4300800  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 943       |
|    ep_rew_mean          | 7.38      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 526       |
|    time_elapsed         | 7885      |
|    total_timesteps      | 4308992   |
| train/                  |           |
|    approx_kl            | 0.3311602 |
|    clip_fraction        | 0.248     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.256    |
|    explained_variance   | 0.776     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0237   |
|    n_updates            | 5250      |
|    policy_gradient_loss | -0.0332   |
|    value_loss           | 0.0136    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 932        |
|    ep_rew_mean          | 7.25       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 527        |
|    time_elapsed         | 7899       |
|    total_timesteps      | 4317184    |
| train/                  |            |
|    approx_kl            | 0.46777898 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.274     |
|    explained_variance   | 0.838      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0523    |
|    n_updates            | 5260       |
|    policy_gradient_loss | -0.034     |
|    value_loss           | 0.0114     |
----------------------------------------
Eval num_timesteps=4320000, episode_reward=6.90 +/- 1.32
Episode length: 832.60 +/- 285.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 833        |
|    mean_reward          | 6.9        |
| time/                   |            |
|    total_timesteps      | 4320000    |
| train/                  |            |
|    approx_kl            | 0.35356936 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.229     |
|    explained_variance   | 0.854      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0563    |
|    n_updates            | 5270       |
|    policy_gradient_loss | -0.0342    |
|    value_loss           | 0.0125     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 940      |
|    ep_rew_mean     | 7.33     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 528      |
|    time_elapsed    | 7915     |
|    total_timesteps | 4325376  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 941       |
|    ep_rew_mean          | 7.38      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 529       |
|    time_elapsed         | 7928      |
|    total_timesteps      | 4333568   |
| train/                  |           |
|    approx_kl            | 0.3036915 |
|    clip_fraction        | 0.236     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.247    |
|    explained_variance   | 0.825     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0361   |
|    n_updates            | 5280      |
|    policy_gradient_loss | -0.0291   |
|    value_loss           | 0.0202    |
---------------------------------------
Eval num_timesteps=4340000, episode_reward=7.50 +/- 0.71
Episode length: 857.60 +/- 135.93
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 858        |
|    mean_reward          | 7.5        |
| time/                   |            |
|    total_timesteps      | 4340000    |
| train/                  |            |
|    approx_kl            | 0.33015895 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.272     |
|    explained_variance   | 0.751      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0344    |
|    n_updates            | 5290       |
|    policy_gradient_loss | -0.0271    |
|    value_loss           | 0.0126     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 948      |
|    ep_rew_mean     | 7.39     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 530      |
|    time_elapsed    | 7945     |
|    total_timesteps | 4341760  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 962        |
|    ep_rew_mean          | 7.46       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 531        |
|    time_elapsed         | 7958       |
|    total_timesteps      | 4349952    |
| train/                  |            |
|    approx_kl            | 0.33492583 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.275     |
|    explained_variance   | 0.848      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0569    |
|    n_updates            | 5300       |
|    policy_gradient_loss | -0.0364    |
|    value_loss           | 0.00945    |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 965       |
|    ep_rew_mean          | 7.46      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 532       |
|    time_elapsed         | 7971      |
|    total_timesteps      | 4358144   |
| train/                  |           |
|    approx_kl            | 0.3381011 |
|    clip_fraction        | 0.24      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.243    |
|    explained_variance   | 0.812     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.027    |
|    n_updates            | 5310      |
|    policy_gradient_loss | -0.0313   |
|    value_loss           | 0.0113    |
---------------------------------------
Eval num_timesteps=4360000, episode_reward=8.00 +/- 2.55
Episode length: 1120.80 +/- 490.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.12e+03   |
|    mean_reward          | 8          |
| time/                   |            |
|    total_timesteps      | 4360000    |
| train/                  |            |
|    approx_kl            | 0.29381886 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.249     |
|    explained_variance   | 0.855      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0302    |
|    n_updates            | 5320       |
|    policy_gradient_loss | -0.0335    |
|    value_loss           | 0.0111     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 961      |
|    ep_rew_mean     | 7.5      |
| time/              |          |
|    fps             | 546      |
|    iterations      | 533      |
|    time_elapsed    | 7988     |
|    total_timesteps | 4366336  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 950        |
|    ep_rew_mean          | 7.42       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 534        |
|    time_elapsed         | 8001       |
|    total_timesteps      | 4374528    |
| train/                  |            |
|    approx_kl            | 0.30710727 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.274     |
|    explained_variance   | 0.844      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0344    |
|    n_updates            | 5330       |
|    policy_gradient_loss | -0.0353    |
|    value_loss           | 0.00852    |
----------------------------------------
Eval num_timesteps=4380000, episode_reward=7.70 +/- 2.36
Episode length: 1033.40 +/- 344.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.03e+03  |
|    mean_reward          | 7.7       |
| time/                   |           |
|    total_timesteps      | 4380000   |
| train/                  |           |
|    approx_kl            | 0.3101816 |
|    clip_fraction        | 0.256     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.264    |
|    explained_variance   | 0.867     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0559   |
|    n_updates            | 5340      |
|    policy_gradient_loss | -0.0346   |
|    value_loss           | 0.0111    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 959      |
|    ep_rew_mean     | 7.36     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 535      |
|    time_elapsed    | 8018     |
|    total_timesteps | 4382720  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 954        |
|    ep_rew_mean          | 7.36       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 536        |
|    time_elapsed         | 8031       |
|    total_timesteps      | 4390912    |
| train/                  |            |
|    approx_kl            | 0.27036762 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.23      |
|    explained_variance   | 0.809      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0359    |
|    n_updates            | 5350       |
|    policy_gradient_loss | -0.0293    |
|    value_loss           | 0.0127     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 940        |
|    ep_rew_mean          | 7.34       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 537        |
|    time_elapsed         | 8045       |
|    total_timesteps      | 4399104    |
| train/                  |            |
|    approx_kl            | 0.35589188 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.244     |
|    explained_variance   | 0.824      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0431    |
|    n_updates            | 5360       |
|    policy_gradient_loss | -0.0315    |
|    value_loss           | 0.00898    |
----------------------------------------
Eval num_timesteps=4400000, episode_reward=7.70 +/- 1.21
Episode length: 1020.40 +/- 453.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.02e+03  |
|    mean_reward          | 7.7       |
| time/                   |           |
|    total_timesteps      | 4400000   |
| train/                  |           |
|    approx_kl            | 0.2568059 |
|    clip_fraction        | 0.245     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.27     |
|    explained_variance   | 0.824     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0424   |
|    n_updates            | 5370      |
|    policy_gradient_loss | -0.0158   |
|    value_loss           | 0.0107    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 943      |
|    ep_rew_mean     | 7.42     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 538      |
|    time_elapsed    | 8064     |
|    total_timesteps | 4407296  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 945        |
|    ep_rew_mean          | 7.52       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 539        |
|    time_elapsed         | 8078       |
|    total_timesteps      | 4415488    |
| train/                  |            |
|    approx_kl            | 0.25873613 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.263     |
|    explained_variance   | 0.858      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0134    |
|    n_updates            | 5380       |
|    policy_gradient_loss | -0.0335    |
|    value_loss           | 0.0107     |
----------------------------------------
Eval num_timesteps=4420000, episode_reward=7.90 +/- 1.20
Episode length: 844.40 +/- 250.24
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 844        |
|    mean_reward          | 7.9        |
| time/                   |            |
|    total_timesteps      | 4420000    |
| train/                  |            |
|    approx_kl            | 0.33471173 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.238     |
|    explained_variance   | 0.846      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.054     |
|    n_updates            | 5390       |
|    policy_gradient_loss | -0.0312    |
|    value_loss           | 0.0108     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 952      |
|    ep_rew_mean     | 7.54     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 540      |
|    time_elapsed    | 8095     |
|    total_timesteps | 4423680  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 966       |
|    ep_rew_mean          | 7.67      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 541       |
|    time_elapsed         | 8108      |
|    total_timesteps      | 4431872   |
| train/                  |           |
|    approx_kl            | 0.2647797 |
|    clip_fraction        | 0.254     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.283    |
|    explained_variance   | 0.868     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0124   |
|    n_updates            | 5400      |
|    policy_gradient_loss | -0.0354   |
|    value_loss           | 0.011     |
---------------------------------------
Eval num_timesteps=4440000, episode_reward=8.80 +/- 2.42
Episode length: 996.40 +/- 398.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 996       |
|    mean_reward          | 8.8       |
| time/                   |           |
|    total_timesteps      | 4440000   |
| train/                  |           |
|    approx_kl            | 0.3497426 |
|    clip_fraction        | 0.256     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.264    |
|    explained_variance   | 0.849     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0421   |
|    n_updates            | 5410      |
|    policy_gradient_loss | -0.0348   |
|    value_loss           | 0.00957   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 973      |
|    ep_rew_mean     | 7.75     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 542      |
|    time_elapsed    | 8125     |
|    total_timesteps | 4440064  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 959        |
|    ep_rew_mean          | 7.7        |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 543        |
|    time_elapsed         | 8138       |
|    total_timesteps      | 4448256    |
| train/                  |            |
|    approx_kl            | 0.29020172 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.28      |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00394    |
|    n_updates            | 5420       |
|    policy_gradient_loss | -0.0362    |
|    value_loss           | 0.0139     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 944        |
|    ep_rew_mean          | 7.63       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 544        |
|    time_elapsed         | 8151       |
|    total_timesteps      | 4456448    |
| train/                  |            |
|    approx_kl            | 0.30267495 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.274     |
|    explained_variance   | 0.838      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0456    |
|    n_updates            | 5430       |
|    policy_gradient_loss | -0.0345    |
|    value_loss           | 0.0128     |
----------------------------------------
Eval num_timesteps=4460000, episode_reward=7.70 +/- 1.17
Episode length: 856.20 +/- 315.21
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 856        |
|    mean_reward          | 7.7        |
| time/                   |            |
|    total_timesteps      | 4460000    |
| train/                  |            |
|    approx_kl            | 0.27045107 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.262     |
|    explained_variance   | 0.832      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0367    |
|    n_updates            | 5440       |
|    policy_gradient_loss | -0.0327    |
|    value_loss           | 0.0138     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 952      |
|    ep_rew_mean     | 7.72     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 545      |
|    time_elapsed    | 8168     |
|    total_timesteps | 4464640  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 940       |
|    ep_rew_mean          | 7.72      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 546       |
|    time_elapsed         | 8181      |
|    total_timesteps      | 4472832   |
| train/                  |           |
|    approx_kl            | 0.2900384 |
|    clip_fraction        | 0.249     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.244    |
|    explained_variance   | 0.855     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0522   |
|    n_updates            | 5450      |
|    policy_gradient_loss | -0.0325   |
|    value_loss           | 0.00972   |
---------------------------------------
Eval num_timesteps=4480000, episode_reward=9.20 +/- 2.38
Episode length: 1392.20 +/- 377.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.39e+03   |
|    mean_reward          | 9.2        |
| time/                   |            |
|    total_timesteps      | 4480000    |
| train/                  |            |
|    approx_kl            | 0.30717644 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.268     |
|    explained_variance   | 0.853      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0509    |
|    n_updates            | 5460       |
|    policy_gradient_loss | -0.0348    |
|    value_loss           | 0.0118     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 965      |
|    ep_rew_mean     | 7.86     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 547      |
|    time_elapsed    | 8200     |
|    total_timesteps | 4481024  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 962       |
|    ep_rew_mean          | 7.81      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 548       |
|    time_elapsed         | 8213      |
|    total_timesteps      | 4489216   |
| train/                  |           |
|    approx_kl            | 0.2886694 |
|    clip_fraction        | 0.251     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.272    |
|    explained_variance   | 0.8       |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0406   |
|    n_updates            | 5470      |
|    policy_gradient_loss | -0.0352   |
|    value_loss           | 0.0154    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 946        |
|    ep_rew_mean          | 7.76       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 549        |
|    time_elapsed         | 8226       |
|    total_timesteps      | 4497408    |
| train/                  |            |
|    approx_kl            | 0.29698277 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.27      |
|    explained_variance   | 0.829      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0569    |
|    n_updates            | 5480       |
|    policy_gradient_loss | -0.0283    |
|    value_loss           | 0.0113     |
----------------------------------------
Eval num_timesteps=4500000, episode_reward=8.30 +/- 1.96
Episode length: 927.20 +/- 327.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 927        |
|    mean_reward          | 8.3        |
| time/                   |            |
|    total_timesteps      | 4500000    |
| train/                  |            |
|    approx_kl            | 0.30649132 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.269     |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00522    |
|    n_updates            | 5490       |
|    policy_gradient_loss | -0.0297    |
|    value_loss           | 0.0162     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 942      |
|    ep_rew_mean     | 7.77     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 550      |
|    time_elapsed    | 8242     |
|    total_timesteps | 4505600  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 949       |
|    ep_rew_mean          | 7.82      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 551       |
|    time_elapsed         | 8255      |
|    total_timesteps      | 4513792   |
| train/                  |           |
|    approx_kl            | 0.3022161 |
|    clip_fraction        | 0.26      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.275    |
|    explained_variance   | 0.884     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0364   |
|    n_updates            | 5500      |
|    policy_gradient_loss | -0.0333   |
|    value_loss           | 0.00699   |
---------------------------------------
Eval num_timesteps=4520000, episode_reward=8.20 +/- 2.36
Episode length: 1136.00 +/- 531.81
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.14e+03   |
|    mean_reward          | 8.2        |
| time/                   |            |
|    total_timesteps      | 4520000    |
| train/                  |            |
|    approx_kl            | 0.25232965 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.306     |
|    explained_variance   | 0.831      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0635    |
|    n_updates            | 5510       |
|    policy_gradient_loss | -0.0421    |
|    value_loss           | 0.0106     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 940      |
|    ep_rew_mean     | 7.71     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 552      |
|    time_elapsed    | 8274     |
|    total_timesteps | 4521984  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 909        |
|    ep_rew_mean          | 7.51       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 553        |
|    time_elapsed         | 8287       |
|    total_timesteps      | 4530176    |
| train/                  |            |
|    approx_kl            | 0.23422475 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.313     |
|    explained_variance   | 0.788      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0488    |
|    n_updates            | 5520       |
|    policy_gradient_loss | -0.0355    |
|    value_loss           | 0.0185     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 920        |
|    ep_rew_mean          | 7.49       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 554        |
|    time_elapsed         | 8300       |
|    total_timesteps      | 4538368    |
| train/                  |            |
|    approx_kl            | 0.26776242 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.256     |
|    explained_variance   | 0.843      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0487    |
|    n_updates            | 5530       |
|    policy_gradient_loss | -0.0353    |
|    value_loss           | 0.0134     |
----------------------------------------
Eval num_timesteps=4540000, episode_reward=7.30 +/- 1.29
Episode length: 954.00 +/- 299.06
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 954        |
|    mean_reward          | 7.3        |
| time/                   |            |
|    total_timesteps      | 4540000    |
| train/                  |            |
|    approx_kl            | 0.25445855 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.295     |
|    explained_variance   | 0.791      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0486    |
|    n_updates            | 5540       |
|    policy_gradient_loss | -0.0339    |
|    value_loss           | 0.0183     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 915      |
|    ep_rew_mean     | 7.5      |
| time/              |          |
|    fps             | 546      |
|    iterations      | 555      |
|    time_elapsed    | 8316     |
|    total_timesteps | 4546560  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 923       |
|    ep_rew_mean          | 7.49      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 556       |
|    time_elapsed         | 8329      |
|    total_timesteps      | 4554752   |
| train/                  |           |
|    approx_kl            | 0.3285382 |
|    clip_fraction        | 0.264     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.294    |
|    explained_variance   | 0.846     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0536   |
|    n_updates            | 5550      |
|    policy_gradient_loss | -0.0387   |
|    value_loss           | 0.0134    |
---------------------------------------
Eval num_timesteps=4560000, episode_reward=7.50 +/- 1.61
Episode length: 825.20 +/- 294.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 825        |
|    mean_reward          | 7.5        |
| time/                   |            |
|    total_timesteps      | 4560000    |
| train/                  |            |
|    approx_kl            | 0.35885957 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.259     |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0414    |
|    n_updates            | 5560       |
|    policy_gradient_loss | -0.0309    |
|    value_loss           | 0.0147     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 907      |
|    ep_rew_mean     | 7.36     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 557      |
|    time_elapsed    | 8345     |
|    total_timesteps | 4562944  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 919       |
|    ep_rew_mean          | 7.3       |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 558       |
|    time_elapsed         | 8358      |
|    total_timesteps      | 4571136   |
| train/                  |           |
|    approx_kl            | 0.2588924 |
|    clip_fraction        | 0.241     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.253    |
|    explained_variance   | 0.824     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0212   |
|    n_updates            | 5570      |
|    policy_gradient_loss | -0.034    |
|    value_loss           | 0.014     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 944        |
|    ep_rew_mean          | 7.49       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 559        |
|    time_elapsed         | 8372       |
|    total_timesteps      | 4579328    |
| train/                  |            |
|    approx_kl            | 0.29476172 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.256     |
|    explained_variance   | 0.848      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0564    |
|    n_updates            | 5580       |
|    policy_gradient_loss | -0.0307    |
|    value_loss           | 0.0106     |
----------------------------------------
Eval num_timesteps=4580000, episode_reward=8.80 +/- 1.57
Episode length: 1262.80 +/- 684.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.26e+03  |
|    mean_reward          | 8.8       |
| time/                   |           |
|    total_timesteps      | 4580000   |
| train/                  |           |
|    approx_kl            | 0.2816123 |
|    clip_fraction        | 0.26      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.29     |
|    explained_variance   | 0.845     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.058    |
|    n_updates            | 5590      |
|    policy_gradient_loss | -0.0387   |
|    value_loss           | 0.0103    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 950      |
|    ep_rew_mean     | 7.55     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 560      |
|    time_elapsed    | 8390     |
|    total_timesteps | 4587520  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 967        |
|    ep_rew_mean          | 7.59       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 561        |
|    time_elapsed         | 8403       |
|    total_timesteps      | 4595712    |
| train/                  |            |
|    approx_kl            | 0.30254468 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.271     |
|    explained_variance   | 0.835      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0367    |
|    n_updates            | 5600       |
|    policy_gradient_loss | -0.0353    |
|    value_loss           | 0.0139     |
----------------------------------------
Eval num_timesteps=4600000, episode_reward=6.10 +/- 1.36
Episode length: 930.60 +/- 264.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 931        |
|    mean_reward          | 6.1        |
| time/                   |            |
|    total_timesteps      | 4600000    |
| train/                  |            |
|    approx_kl            | 0.29392302 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.28      |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.052     |
|    n_updates            | 5610       |
|    policy_gradient_loss | -0.0375    |
|    value_loss           | 0.0124     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 970      |
|    ep_rew_mean     | 7.6      |
| time/              |          |
|    fps             | 546      |
|    iterations      | 562      |
|    time_elapsed    | 8420     |
|    total_timesteps | 4603904  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 976        |
|    ep_rew_mean          | 7.58       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 563        |
|    time_elapsed         | 8432       |
|    total_timesteps      | 4612096    |
| train/                  |            |
|    approx_kl            | 0.28942475 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.258     |
|    explained_variance   | 0.852      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0396    |
|    n_updates            | 5620       |
|    policy_gradient_loss | -0.0347    |
|    value_loss           | 0.0123     |
----------------------------------------
Eval num_timesteps=4620000, episode_reward=6.70 +/- 2.44
Episode length: 1052.40 +/- 700.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.05e+03   |
|    mean_reward          | 6.7        |
| time/                   |            |
|    total_timesteps      | 4620000    |
| train/                  |            |
|    approx_kl            | 0.29743364 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.259     |
|    explained_variance   | 0.847      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.046     |
|    n_updates            | 5630       |
|    policy_gradient_loss | -0.0354    |
|    value_loss           | 0.0125     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 975      |
|    ep_rew_mean     | 7.57     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 564      |
|    time_elapsed    | 8452     |
|    total_timesteps | 4620288  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 984        |
|    ep_rew_mean          | 7.61       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 565        |
|    time_elapsed         | 8465       |
|    total_timesteps      | 4628480    |
| train/                  |            |
|    approx_kl            | 0.26729435 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.272     |
|    explained_variance   | 0.797      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0533    |
|    n_updates            | 5640       |
|    policy_gradient_loss | -0.0353    |
|    value_loss           | 0.0127     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 987        |
|    ep_rew_mean          | 7.67       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 566        |
|    time_elapsed         | 8477       |
|    total_timesteps      | 4636672    |
| train/                  |            |
|    approx_kl            | 0.61795545 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.256     |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0477    |
|    n_updates            | 5650       |
|    policy_gradient_loss | -0.0289    |
|    value_loss           | 0.0127     |
----------------------------------------
Eval num_timesteps=4640000, episode_reward=6.50 +/- 0.32
Episode length: 740.40 +/- 133.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 740        |
|    mean_reward          | 6.5        |
| time/                   |            |
|    total_timesteps      | 4640000    |
| train/                  |            |
|    approx_kl            | 0.27770516 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.231     |
|    explained_variance   | 0.766      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0141    |
|    n_updates            | 5660       |
|    policy_gradient_loss | -0.0248    |
|    value_loss           | 0.0125     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 975      |
|    ep_rew_mean     | 7.6      |
| time/              |          |
|    fps             | 546      |
|    iterations      | 567      |
|    time_elapsed    | 8493     |
|    total_timesteps | 4644864  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 984        |
|    ep_rew_mean          | 7.67       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 568        |
|    time_elapsed         | 8506       |
|    total_timesteps      | 4653056    |
| train/                  |            |
|    approx_kl            | 0.25014636 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.29      |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0562    |
|    n_updates            | 5670       |
|    policy_gradient_loss | -0.0341    |
|    value_loss           | 0.0148     |
----------------------------------------
Eval num_timesteps=4660000, episode_reward=7.80 +/- 1.66
Episode length: 919.80 +/- 501.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 920        |
|    mean_reward          | 7.8        |
| time/                   |            |
|    total_timesteps      | 4660000    |
| train/                  |            |
|    approx_kl            | 0.31144178 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.27      |
|    explained_variance   | 0.812      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0484    |
|    n_updates            | 5680       |
|    policy_gradient_loss | -0.0348    |
|    value_loss           | 0.0123     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 7.75     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 569      |
|    time_elapsed    | 8524     |
|    total_timesteps | 4661248  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.01e+03  |
|    ep_rew_mean          | 7.8       |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 570       |
|    time_elapsed         | 8538      |
|    total_timesteps      | 4669440   |
| train/                  |           |
|    approx_kl            | 0.2670942 |
|    clip_fraction        | 0.26      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.298    |
|    explained_variance   | 0.828     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0583   |
|    n_updates            | 5690      |
|    policy_gradient_loss | -0.0391   |
|    value_loss           | 0.0138    |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 992       |
|    ep_rew_mean          | 7.67      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 571       |
|    time_elapsed         | 8551      |
|    total_timesteps      | 4677632   |
| train/                  |           |
|    approx_kl            | 0.2587787 |
|    clip_fraction        | 0.242     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.276    |
|    explained_variance   | 0.813     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0501   |
|    n_updates            | 5700      |
|    policy_gradient_loss | -0.0362   |
|    value_loss           | 0.0142    |
---------------------------------------
Eval num_timesteps=4680000, episode_reward=6.50 +/- 2.30
Episode length: 842.60 +/- 389.78
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 843        |
|    mean_reward          | 6.5        |
| time/                   |            |
|    total_timesteps      | 4680000    |
| train/                  |            |
|    approx_kl            | 0.24807864 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.228     |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0385    |
|    n_updates            | 5710       |
|    policy_gradient_loss | -0.0298    |
|    value_loss           | 0.0178     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 967      |
|    ep_rew_mean     | 7.53     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 572      |
|    time_elapsed    | 8570     |
|    total_timesteps | 4685824  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.01e+03  |
|    ep_rew_mean          | 7.67      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 573       |
|    time_elapsed         | 8582      |
|    total_timesteps      | 4694016   |
| train/                  |           |
|    approx_kl            | 0.2580018 |
|    clip_fraction        | 0.252     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.262    |
|    explained_variance   | 0.872     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0477   |
|    n_updates            | 5720      |
|    policy_gradient_loss | -0.0325   |
|    value_loss           | 0.0113    |
---------------------------------------
Eval num_timesteps=4700000, episode_reward=8.80 +/- 3.16
Episode length: 1150.00 +/- 735.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.15e+03   |
|    mean_reward          | 8.8        |
| time/                   |            |
|    total_timesteps      | 4700000    |
| train/                  |            |
|    approx_kl            | 0.26930216 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.281     |
|    explained_variance   | 0.842      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0361    |
|    n_updates            | 5730       |
|    policy_gradient_loss | -0.0396    |
|    value_loss           | 0.012      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 991      |
|    ep_rew_mean     | 7.53     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 574      |
|    time_elapsed    | 8603     |
|    total_timesteps | 4702208  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 7.58       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 575        |
|    time_elapsed         | 8615       |
|    total_timesteps      | 4710400    |
| train/                  |            |
|    approx_kl            | 0.24832492 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.26      |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0469    |
|    n_updates            | 5740       |
|    policy_gradient_loss | -0.0356    |
|    value_loss           | 0.0184     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 999        |
|    ep_rew_mean          | 7.51       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 576        |
|    time_elapsed         | 8628       |
|    total_timesteps      | 4718592    |
| train/                  |            |
|    approx_kl            | 0.25875932 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.243     |
|    explained_variance   | 0.844      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0587    |
|    n_updates            | 5750       |
|    policy_gradient_loss | -0.0334    |
|    value_loss           | 0.0146     |
----------------------------------------
Eval num_timesteps=4720000, episode_reward=7.20 +/- 0.87
Episode length: 910.80 +/- 190.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 911        |
|    mean_reward          | 7.2        |
| time/                   |            |
|    total_timesteps      | 4720000    |
| train/                  |            |
|    approx_kl            | 0.29274043 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.253     |
|    explained_variance   | 0.855      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0735    |
|    n_updates            | 5760       |
|    policy_gradient_loss | -0.0296    |
|    value_loss           | 0.0133     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 971      |
|    ep_rew_mean     | 7.39     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 577      |
|    time_elapsed    | 8644     |
|    total_timesteps | 4726784  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 978        |
|    ep_rew_mean          | 7.37       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 578        |
|    time_elapsed         | 8657       |
|    total_timesteps      | 4734976    |
| train/                  |            |
|    approx_kl            | 0.25947428 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.227     |
|    explained_variance   | 0.846      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0503    |
|    n_updates            | 5770       |
|    policy_gradient_loss | -0.0309    |
|    value_loss           | 0.0171     |
----------------------------------------
Eval num_timesteps=4740000, episode_reward=6.30 +/- 1.72
Episode length: 1139.60 +/- 208.75
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1.14e+03 |
|    mean_reward          | 6.3      |
| time/                   |          |
|    total_timesteps      | 4740000  |
| train/                  |          |
|    approx_kl            | 0.284203 |
|    clip_fraction        | 0.232    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.243   |
|    explained_variance   | 0.852    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0266  |
|    n_updates            | 5780     |
|    policy_gradient_loss | -0.0327  |
|    value_loss           | 0.0157   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 989      |
|    ep_rew_mean     | 7.32     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 579      |
|    time_elapsed    | 8674     |
|    total_timesteps | 4743168  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 970        |
|    ep_rew_mean          | 7.11       |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 580        |
|    time_elapsed         | 8687       |
|    total_timesteps      | 4751360    |
| train/                  |            |
|    approx_kl            | 0.27725095 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.201     |
|    explained_variance   | 0.808      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0579    |
|    n_updates            | 5790       |
|    policy_gradient_loss | -0.028     |
|    value_loss           | 0.0171     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 942       |
|    ep_rew_mean          | 6.73      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 581       |
|    time_elapsed         | 8700      |
|    total_timesteps      | 4759552   |
| train/                  |           |
|    approx_kl            | 0.2533473 |
|    clip_fraction        | 0.224     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.26     |
|    explained_variance   | 0.79      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0455   |
|    n_updates            | 5800      |
|    policy_gradient_loss | -0.0326   |
|    value_loss           | 0.0223    |
---------------------------------------
Eval num_timesteps=4760000, episode_reward=4.80 +/- 1.44
Episode length: 775.80 +/- 68.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 776        |
|    mean_reward          | 4.8        |
| time/                   |            |
|    total_timesteps      | 4760000    |
| train/                  |            |
|    approx_kl            | 0.22763212 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.225     |
|    explained_variance   | 0.837      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0498    |
|    n_updates            | 5810       |
|    policy_gradient_loss | -0.0287    |
|    value_loss           | 0.0247     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 930      |
|    ep_rew_mean     | 6.63     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 582      |
|    time_elapsed    | 8716     |
|    total_timesteps | 4767744  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 908       |
|    ep_rew_mean          | 6.42      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 583       |
|    time_elapsed         | 8728      |
|    total_timesteps      | 4775936   |
| train/                  |           |
|    approx_kl            | 0.2080379 |
|    clip_fraction        | 0.223     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.259    |
|    explained_variance   | 0.858     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0526   |
|    n_updates            | 5820      |
|    policy_gradient_loss | -0.0364   |
|    value_loss           | 0.0214    |
---------------------------------------
Eval num_timesteps=4780000, episode_reward=6.10 +/- 1.98
Episode length: 1081.00 +/- 474.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.08e+03   |
|    mean_reward          | 6.1        |
| time/                   |            |
|    total_timesteps      | 4780000    |
| train/                  |            |
|    approx_kl            | 0.25125176 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.233     |
|    explained_variance   | 0.864      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0421    |
|    n_updates            | 5830       |
|    policy_gradient_loss | -0.0317    |
|    value_loss           | 0.0211     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 858      |
|    ep_rew_mean     | 6.24     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 584      |
|    time_elapsed    | 8746     |
|    total_timesteps | 4784128  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 864        |
|    ep_rew_mean          | 6.22       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 585        |
|    time_elapsed         | 8759       |
|    total_timesteps      | 4792320    |
| train/                  |            |
|    approx_kl            | 0.29152602 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.256     |
|    explained_variance   | 0.852      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0338    |
|    n_updates            | 5840       |
|    policy_gradient_loss | -0.0367    |
|    value_loss           | 0.0201     |
----------------------------------------
Eval num_timesteps=4800000, episode_reward=6.80 +/- 1.40
Episode length: 649.00 +/- 256.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 649        |
|    mean_reward          | 6.8        |
| time/                   |            |
|    total_timesteps      | 4800000    |
| train/                  |            |
|    approx_kl            | 0.28512162 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.262     |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0319    |
|    n_updates            | 5850       |
|    policy_gradient_loss | -0.0388    |
|    value_loss           | 0.0168     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 857      |
|    ep_rew_mean     | 6.13     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 586      |
|    time_elapsed    | 8774     |
|    total_timesteps | 4800512  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 863       |
|    ep_rew_mean          | 6.05      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 587       |
|    time_elapsed         | 8787      |
|    total_timesteps      | 4808704   |
| train/                  |           |
|    approx_kl            | 0.3174405 |
|    clip_fraction        | 0.235     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.252    |
|    explained_variance   | 0.855     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.047    |
|    n_updates            | 5860      |
|    policy_gradient_loss | -0.04     |
|    value_loss           | 0.021     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 865        |
|    ep_rew_mean          | 5.95       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 588        |
|    time_elapsed         | 8800       |
|    total_timesteps      | 4816896    |
| train/                  |            |
|    approx_kl            | 0.26427656 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.294     |
|    explained_variance   | 0.86       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0496    |
|    n_updates            | 5870       |
|    policy_gradient_loss | -0.0365    |
|    value_loss           | 0.0181     |
----------------------------------------
Eval num_timesteps=4820000, episode_reward=5.80 +/- 2.11
Episode length: 889.60 +/- 263.31
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 890        |
|    mean_reward          | 5.8        |
| time/                   |            |
|    total_timesteps      | 4820000    |
| train/                  |            |
|    approx_kl            | 0.22518694 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.242     |
|    explained_variance   | 0.857      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0421    |
|    n_updates            | 5880       |
|    policy_gradient_loss | -0.0336    |
|    value_loss           | 0.0229     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 863      |
|    ep_rew_mean     | 5.89     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 589      |
|    time_elapsed    | 8817     |
|    total_timesteps | 4825088  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 871        |
|    ep_rew_mean          | 5.87       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 590        |
|    time_elapsed         | 8830       |
|    total_timesteps      | 4833280    |
| train/                  |            |
|    approx_kl            | 0.23794265 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.266     |
|    explained_variance   | 0.87       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0494    |
|    n_updates            | 5890       |
|    policy_gradient_loss | -0.0365    |
|    value_loss           | 0.0208     |
----------------------------------------
Eval num_timesteps=4840000, episode_reward=8.10 +/- 1.62
Episode length: 1150.20 +/- 149.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.15e+03   |
|    mean_reward          | 8.1        |
| time/                   |            |
|    total_timesteps      | 4840000    |
| train/                  |            |
|    approx_kl            | 0.20938233 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.251     |
|    explained_variance   | 0.887      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0421    |
|    n_updates            | 5900       |
|    policy_gradient_loss | -0.0359    |
|    value_loss           | 0.0189     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 856      |
|    ep_rew_mean     | 5.96     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 591      |
|    time_elapsed    | 8848     |
|    total_timesteps | 4841472  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 856        |
|    ep_rew_mean          | 6.04       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 592        |
|    time_elapsed         | 8861       |
|    total_timesteps      | 4849664    |
| train/                  |            |
|    approx_kl            | 0.26018855 |
|    clip_fraction        | 0.222      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.23      |
|    explained_variance   | 0.877      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0328    |
|    n_updates            | 5910       |
|    policy_gradient_loss | -0.0349    |
|    value_loss           | 0.0158     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 869        |
|    ep_rew_mean          | 6.15       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 593        |
|    time_elapsed         | 8874       |
|    total_timesteps      | 4857856    |
| train/                  |            |
|    approx_kl            | 0.26049605 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.235     |
|    explained_variance   | 0.879      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0264    |
|    n_updates            | 5920       |
|    policy_gradient_loss | -0.0331    |
|    value_loss           | 0.0147     |
----------------------------------------
Eval num_timesteps=4860000, episode_reward=6.60 +/- 2.20
Episode length: 927.20 +/- 274.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 927        |
|    mean_reward          | 6.6        |
| time/                   |            |
|    total_timesteps      | 4860000    |
| train/                  |            |
|    approx_kl            | 0.24681379 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.229     |
|    explained_variance   | 0.867      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0388    |
|    n_updates            | 5930       |
|    policy_gradient_loss | -0.0341    |
|    value_loss           | 0.0176     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 893      |
|    ep_rew_mean     | 6.29     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 594      |
|    time_elapsed    | 8892     |
|    total_timesteps | 4866048  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 903        |
|    ep_rew_mean          | 6.33       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 595        |
|    time_elapsed         | 8906       |
|    total_timesteps      | 4874240    |
| train/                  |            |
|    approx_kl            | 0.24178597 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.257     |
|    explained_variance   | 0.858      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.011     |
|    n_updates            | 5940       |
|    policy_gradient_loss | -0.0376    |
|    value_loss           | 0.0184     |
----------------------------------------
Eval num_timesteps=4880000, episode_reward=6.10 +/- 0.80
Episode length: 591.40 +/- 140.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 591        |
|    mean_reward          | 6.1        |
| time/                   |            |
|    total_timesteps      | 4880000    |
| train/                  |            |
|    approx_kl            | 0.28107032 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.237     |
|    explained_variance   | 0.853      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.062     |
|    n_updates            | 5950       |
|    policy_gradient_loss | -0.0384    |
|    value_loss           | 0.0153     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 919      |
|    ep_rew_mean     | 6.28     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 596      |
|    time_elapsed    | 8922     |
|    total_timesteps | 4882432  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 900        |
|    ep_rew_mean          | 6.18       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 597        |
|    time_elapsed         | 8935       |
|    total_timesteps      | 4890624    |
| train/                  |            |
|    approx_kl            | 0.29213667 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.244     |
|    explained_variance   | 0.862      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0461    |
|    n_updates            | 5960       |
|    policy_gradient_loss | -0.0308    |
|    value_loss           | 0.0177     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 903       |
|    ep_rew_mean          | 6.1       |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 598       |
|    time_elapsed         | 8948      |
|    total_timesteps      | 4898816   |
| train/                  |           |
|    approx_kl            | 0.3187499 |
|    clip_fraction        | 0.223     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.235    |
|    explained_variance   | 0.858     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0654   |
|    n_updates            | 5970      |
|    policy_gradient_loss | -0.0308   |
|    value_loss           | 0.0203    |
---------------------------------------
Eval num_timesteps=4900000, episode_reward=7.10 +/- 1.24
Episode length: 859.20 +/- 218.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 859        |
|    mean_reward          | 7.1        |
| time/                   |            |
|    total_timesteps      | 4900000    |
| train/                  |            |
|    approx_kl            | 0.26807433 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.23      |
|    explained_variance   | 0.872      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0439    |
|    n_updates            | 5980       |
|    policy_gradient_loss | -0.0309    |
|    value_loss           | 0.0185     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 894      |
|    ep_rew_mean     | 6.09     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 599      |
|    time_elapsed    | 8966     |
|    total_timesteps | 4907008  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 897        |
|    ep_rew_mean          | 6.21       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 600        |
|    time_elapsed         | 8979       |
|    total_timesteps      | 4915200    |
| train/                  |            |
|    approx_kl            | 0.24737363 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.236     |
|    explained_variance   | 0.879      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0335    |
|    n_updates            | 5990       |
|    policy_gradient_loss | -0.0323    |
|    value_loss           | 0.0191     |
----------------------------------------
Eval num_timesteps=4920000, episode_reward=6.40 +/- 0.92
Episode length: 937.40 +/- 277.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 937        |
|    mean_reward          | 6.4        |
| time/                   |            |
|    total_timesteps      | 4920000    |
| train/                  |            |
|    approx_kl            | 0.28785533 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.244     |
|    explained_variance   | 0.834      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0387    |
|    n_updates            | 6000       |
|    policy_gradient_loss | -0.0338    |
|    value_loss           | 0.0159     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 877      |
|    ep_rew_mean     | 6.3      |
| time/              |          |
|    fps             | 547      |
|    iterations      | 601      |
|    time_elapsed    | 8996     |
|    total_timesteps | 4923392  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 895        |
|    ep_rew_mean          | 6.41       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 602        |
|    time_elapsed         | 9009       |
|    total_timesteps      | 4931584    |
| train/                  |            |
|    approx_kl            | 0.29017565 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.226     |
|    explained_variance   | 0.856      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0381    |
|    n_updates            | 6010       |
|    policy_gradient_loss | -0.0335    |
|    value_loss           | 0.015      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 915        |
|    ep_rew_mean          | 6.35       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 603        |
|    time_elapsed         | 9021       |
|    total_timesteps      | 4939776    |
| train/                  |            |
|    approx_kl            | 0.24548021 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.273     |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0551    |
|    n_updates            | 6020       |
|    policy_gradient_loss | -0.0391    |
|    value_loss           | 0.0165     |
----------------------------------------
Eval num_timesteps=4940000, episode_reward=5.60 +/- 1.74
Episode length: 952.80 +/- 244.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 953       |
|    mean_reward          | 5.6       |
| time/                   |           |
|    total_timesteps      | 4940000   |
| train/                  |           |
|    approx_kl            | 0.2461599 |
|    clip_fraction        | 0.25      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.29     |
|    explained_variance   | 0.886     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0406   |
|    n_updates            | 6030      |
|    policy_gradient_loss | -0.041    |
|    value_loss           | 0.0172    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 914      |
|    ep_rew_mean     | 6.34     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 604      |
|    time_elapsed    | 9038     |
|    total_timesteps | 4947968  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 916        |
|    ep_rew_mean          | 6.33       |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 605        |
|    time_elapsed         | 9051       |
|    total_timesteps      | 4956160    |
| train/                  |            |
|    approx_kl            | 0.20254238 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.246     |
|    explained_variance   | 0.874      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0399    |
|    n_updates            | 6040       |
|    policy_gradient_loss | -0.0318    |
|    value_loss           | 0.0194     |
----------------------------------------
Eval num_timesteps=4960000, episode_reward=4.80 +/- 1.60
Episode length: 895.60 +/- 242.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 896        |
|    mean_reward          | 4.8        |
| time/                   |            |
|    total_timesteps      | 4960000    |
| train/                  |            |
|    approx_kl            | 0.22520366 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.23      |
|    explained_variance   | 0.862      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0235    |
|    n_updates            | 6050       |
|    policy_gradient_loss | -0.0331    |
|    value_loss           | 0.0163     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 901      |
|    ep_rew_mean     | 6.17     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 606      |
|    time_elapsed    | 9067     |
|    total_timesteps | 4964352  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 895       |
|    ep_rew_mean          | 6.09      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 607       |
|    time_elapsed         | 9080      |
|    total_timesteps      | 4972544   |
| train/                  |           |
|    approx_kl            | 0.2329215 |
|    clip_fraction        | 0.22      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.236    |
|    explained_variance   | 0.826     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0351   |
|    n_updates            | 6060      |
|    policy_gradient_loss | -0.0309   |
|    value_loss           | 0.0216    |
---------------------------------------
Eval num_timesteps=4980000, episode_reward=5.70 +/- 1.25
Episode length: 1071.20 +/- 303.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.07e+03  |
|    mean_reward          | 5.7       |
| time/                   |           |
|    total_timesteps      | 4980000   |
| train/                  |           |
|    approx_kl            | 0.1941555 |
|    clip_fraction        | 0.241     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.287    |
|    explained_variance   | 0.842     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0412   |
|    n_updates            | 6070      |
|    policy_gradient_loss | -0.0346   |
|    value_loss           | 0.0239    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 900      |
|    ep_rew_mean     | 6.12     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 608      |
|    time_elapsed    | 9096     |
|    total_timesteps | 4980736  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 910        |
|    ep_rew_mean          | 6.2        |
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 609        |
|    time_elapsed         | 9109       |
|    total_timesteps      | 4988928    |
| train/                  |            |
|    approx_kl            | 0.20538373 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.226     |
|    explained_variance   | 0.856      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0543    |
|    n_updates            | 6080       |
|    policy_gradient_loss | -0.0318    |
|    value_loss           | 0.0177     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 901       |
|    ep_rew_mean          | 6.19      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 610       |
|    time_elapsed         | 9122      |
|    total_timesteps      | 4997120   |
| train/                  |           |
|    approx_kl            | 0.1974344 |
|    clip_fraction        | 0.228     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.259    |
|    explained_variance   | 0.879     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0145   |
|    n_updates            | 6090      |
|    policy_gradient_loss | -0.0326   |
|    value_loss           | 0.0194    |
---------------------------------------
Eval num_timesteps=5000000, episode_reward=7.90 +/- 1.91
Episode length: 1127.00 +/- 485.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.13e+03   |
|    mean_reward          | 7.9        |
| time/                   |            |
|    total_timesteps      | 5000000    |
| train/                  |            |
|    approx_kl            | 0.23650743 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.23      |
|    explained_variance   | 0.869      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0276    |
|    n_updates            | 6100       |
|    policy_gradient_loss | -0.0288    |
|    value_loss           | 0.0191     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 906      |
|    ep_rew_mean     | 6.28     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 611      |
|    time_elapsed    | 9139     |
|    total_timesteps | 5005312  |
---------------------------------