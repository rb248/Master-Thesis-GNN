
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -5.62    |
| time/              |          |
|    fps             | 316      |
|    iterations      | 1        |
|    time_elapsed    | 25       |
|    total_timesteps | 8192     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.48e+03     |
|    ep_rew_mean          | 2.56         |
| time/                   |              |
|    fps                  | 372          |
|    iterations           | 2            |
|    time_elapsed         | 43           |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 0.0010191705 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.16         |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.000516    |
|    value_loss           | 9.07         |
------------------------------------------
Eval num_timesteps=20000, episode_reward=-12.90 +/- 34.92
Episode length: 1513.20 +/- 190.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.51e+03     |
|    mean_reward          | -12.9        |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0002944714 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.09         |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.000116    |
|    value_loss           | 8.57         |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | 2.19     |
| time/              |          |
|    fps             | 378      |
|    iterations      | 3        |
|    time_elapsed    | 65       |
|    total_timesteps | 24576    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.67e+03     |
|    ep_rew_mean          | -0.147       |
| time/                   |              |
|    fps                  | 412          |
|    iterations           | 4            |
|    time_elapsed         | 79           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0006334414 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.33         |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.000272    |
|    value_loss           | 8.21         |
------------------------------------------
Eval num_timesteps=40000, episode_reward=-19.40 +/- 23.08
Episode length: 1892.40 +/- 182.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.89e+03     |
|    mean_reward          | -19.4        |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0006374143 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 4.93         |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.000288    |
|    value_loss           | 8.52         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -8.4     |
| time/              |          |
|    fps             | 406      |
|    iterations      | 5        |
|    time_elapsed    | 100      |
|    total_timesteps | 40960    |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.69e+03      |
|    ep_rew_mean          | -10.9         |
| time/                   |               |
|    fps                  | 425           |
|    iterations           | 6             |
|    time_elapsed         | 115           |
|    total_timesteps      | 49152         |
| train/                  |               |
|    approx_kl            | 1.4873702e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.39          |
|    n_updates            | 50            |
|    policy_gradient_loss | 1.67e-05      |
|    value_loss           | 8.41          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.69e+03     |
|    ep_rew_mean          | -13.2        |
| time/                   |              |
|    fps                  | 442          |
|    iterations           | 7            |
|    time_elapsed         | 129          |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 6.507241e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.3          |
|    n_updates            | 60           |
|    policy_gradient_loss | -7.44e-06    |
|    value_loss           | 7.44         |
------------------------------------------
Eval num_timesteps=60000, episode_reward=-0.10 +/- 46.63
Episode length: 1583.40 +/- 349.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.58e+03     |
|    mean_reward          | -0.1         |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0003312674 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 4.9          |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.000138    |
|    value_loss           | 7.45         |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -9.27    |
| time/              |          |
|    fps             | 431      |
|    iterations      | 8        |
|    time_elapsed    | 151      |
|    total_timesteps | 65536    |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.71e+03      |
|    ep_rew_mean          | -9.55         |
| time/                   |               |
|    fps                  | 443           |
|    iterations           | 9             |
|    time_elapsed         | 166           |
|    total_timesteps      | 73728         |
| train/                  |               |
|    approx_kl            | 0.00023473373 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.1          |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 6.55          |
|    n_updates            | 80            |
|    policy_gradient_loss | -0.000104     |
|    value_loss           | 8.44          |
-------------------------------------------
Eval num_timesteps=80000, episode_reward=11.40 +/- 42.86
Episode length: 1579.80 +/- 334.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.58e+03      |
|    mean_reward          | 11.4          |
| time/                   |               |
|    total_timesteps      | 80000         |
| train/                  |               |
|    approx_kl            | 0.00055796513 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.41          |
|    n_updates            | 90            |
|    policy_gradient_loss | -0.000286     |
|    value_loss           | 8.09          |
-------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.73e+03 |
|    ep_rew_mean     | -8.4     |
| time/              |          |
|    fps             | 434      |
|    iterations      | 10       |
|    time_elapsed    | 188      |
|    total_timesteps | 81920    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.72e+03     |
|    ep_rew_mean          | -5.4         |
| time/                   |              |
|    fps                  | 444          |
|    iterations           | 11           |
|    time_elapsed         | 202          |
|    total_timesteps      | 90112        |
| train/                  |              |
|    approx_kl            | 0.0011196547 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.96         |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.000543    |
|    value_loss           | 8.93         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.71e+03     |
|    ep_rew_mean          | -7.39        |
| time/                   |              |
|    fps                  | 452          |
|    iterations           | 12           |
|    time_elapsed         | 217          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0011550833 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.04         |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.000554    |
|    value_loss           | 8.76         |
------------------------------------------
Eval num_timesteps=100000, episode_reward=7.80 +/- 45.27
Episode length: 1770.60 +/- 303.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.77e+03     |
|    mean_reward          | 7.8          |
| time/                   |              |
|    total_timesteps      | 100000       |
| train/                  |              |
|    approx_kl            | 0.0007238351 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.17         |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.000355    |
|    value_loss           | 8.29         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.72e+03 |
|    ep_rew_mean     | -9.25    |
| time/              |          |
|    fps             | 446      |
|    iterations      | 13       |
|    time_elapsed    | 238      |
|    total_timesteps | 106496   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.73e+03     |
|    ep_rew_mean          | -10.1        |
| time/                   |              |
|    fps                  | 453          |
|    iterations           | 14           |
|    time_elapsed         | 252          |
|    total_timesteps      | 114688       |
| train/                  |              |
|    approx_kl            | 0.0012405936 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.36         |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.000629    |
|    value_loss           | 8.21         |
------------------------------------------
Eval num_timesteps=120000, episode_reward=4.10 +/- 45.12
Episode length: 1979.20 +/- 488.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.98e+03      |
|    mean_reward          | 4.1           |
| time/                   |               |
|    total_timesteps      | 120000        |
| train/                  |               |
|    approx_kl            | 0.00028984743 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 1.75          |
|    n_updates            | 140           |
|    policy_gradient_loss | -0.000128     |
|    value_loss           | 8.28          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.74e+03 |
|    ep_rew_mean     | -10.1    |
| time/              |          |
|    fps             | 444      |
|    iterations      | 15       |
|    time_elapsed    | 276      |
|    total_timesteps | 122880   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.75e+03     |
|    ep_rew_mean          | -9.33        |
| time/                   |              |
|    fps                  | 451          |
|    iterations           | 16           |
|    time_elapsed         | 290          |
|    total_timesteps      | 131072       |
| train/                  |              |
|    approx_kl            | 0.0002644877 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.83         |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.000114    |
|    value_loss           | 8.11         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.74e+03      |
|    ep_rew_mean          | -9.3          |
| time/                   |               |
|    fps                  | 457           |
|    iterations           | 17            |
|    time_elapsed         | 304           |
|    total_timesteps      | 139264        |
| train/                  |               |
|    approx_kl            | 0.00057518075 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.2           |
|    n_updates            | 160           |
|    policy_gradient_loss | -0.000283     |
|    value_loss           | 7.71          |
-------------------------------------------
Eval num_timesteps=140000, episode_reward=-26.60 +/- 41.10
Episode length: 1435.40 +/- 140.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.44e+03      |
|    mean_reward          | -26.6         |
| time/                   |               |
|    total_timesteps      | 140000        |
| train/                  |               |
|    approx_kl            | 0.00019000571 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.46          |
|    n_updates            | 170           |
|    policy_gradient_loss | -8.9e-05      |
|    value_loss           | 8.56          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.72e+03 |
|    ep_rew_mean     | -9.49    |
| time/              |          |
|    fps             | 453      |
|    iterations      | 18       |
|    time_elapsed    | 325      |
|    total_timesteps | 147456   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.71e+03      |
|    ep_rew_mean          | -11.7         |
| time/                   |               |
|    fps                  | 457           |
|    iterations           | 19            |
|    time_elapsed         | 340           |
|    total_timesteps      | 155648        |
| train/                  |               |
|    approx_kl            | 3.2388016e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.1          |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 6.72          |
|    n_updates            | 180           |
|    policy_gradient_loss | 6.56e-06      |
|    value_loss           | 8.77          |
-------------------------------------------
Eval num_timesteps=160000, episode_reward=-34.20 +/- 38.06
Episode length: 1590.60 +/- 358.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.59e+03      |
|    mean_reward          | -34.2         |
| time/                   |               |
|    total_timesteps      | 160000        |
| train/                  |               |
|    approx_kl            | 0.00044246795 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.79          |
|    n_updates            | 190           |
|    policy_gradient_loss | -0.000167     |
|    value_loss           | 8.32          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -12.2    |
| time/              |          |
|    fps             | 452      |
|    iterations      | 20       |
|    time_elapsed    | 362      |
|    total_timesteps | 163840   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.69e+03     |
|    ep_rew_mean          | -12.7        |
| time/                   |              |
|    fps                  | 457          |
|    iterations           | 21           |
|    time_elapsed         | 375          |
|    total_timesteps      | 172032       |
| train/                  |              |
|    approx_kl            | 0.0006602385 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 6.8          |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.000291    |
|    value_loss           | 9.19         |
------------------------------------------
Eval num_timesteps=180000, episode_reward=-20.90 +/- 34.91
Episode length: 1939.00 +/- 647.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.94e+03      |
|    mean_reward          | -20.9         |
| time/                   |               |
|    total_timesteps      | 180000        |
| train/                  |               |
|    approx_kl            | 1.8915714e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.1          |
|    explained_variance   | 2.38e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 4.69          |
|    n_updates            | 210           |
|    policy_gradient_loss | 1.7e-05       |
|    value_loss           | 8.95          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -12.4    |
| time/              |          |
|    fps             | 449      |
|    iterations      | 22       |
|    time_elapsed    | 401      |
|    total_timesteps | 180224   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.7e+03      |
|    ep_rew_mean          | -13.8        |
| time/                   |              |
|    fps                  | 453          |
|    iterations           | 23           |
|    time_elapsed         | 415          |
|    total_timesteps      | 188416       |
| train/                  |              |
|    approx_kl            | 0.0027761357 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.03         |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00143     |
|    value_loss           | 8.26         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.71e+03      |
|    ep_rew_mean          | -14.4         |
| time/                   |               |
|    fps                  | 458           |
|    iterations           | 24            |
|    time_elapsed         | 428           |
|    total_timesteps      | 196608        |
| train/                  |               |
|    approx_kl            | 1.3864228e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.1          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 4.19          |
|    n_updates            | 230           |
|    policy_gradient_loss | 1.24e-05      |
|    value_loss           | 8.78          |
-------------------------------------------
Eval num_timesteps=200000, episode_reward=4.30 +/- 56.05
Episode length: 1670.80 +/- 380.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.67e+03      |
|    mean_reward          | 4.3           |
| time/                   |               |
|    total_timesteps      | 200000        |
| train/                  |               |
|    approx_kl            | 0.00027148632 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.19          |
|    n_updates            | 240           |
|    policy_gradient_loss | -0.000111     |
|    value_loss           | 9.21          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -14.1    |
| time/              |          |
|    fps             | 452      |
|    iterations      | 25       |
|    time_elapsed    | 452      |
|    total_timesteps | 204800   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.69e+03     |
|    ep_rew_mean          | -12.1        |
| time/                   |              |
|    fps                  | 455          |
|    iterations           | 26           |
|    time_elapsed         | 467          |
|    total_timesteps      | 212992       |
| train/                  |              |
|    approx_kl            | 0.0005280853 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.94         |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.000242    |
|    value_loss           | 8.11         |
------------------------------------------
Eval num_timesteps=220000, episode_reward=8.20 +/- 26.95
Episode length: 1786.20 +/- 184.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.79e+03      |
|    mean_reward          | 8.2           |
| time/                   |               |
|    total_timesteps      | 220000        |
| train/                  |               |
|    approx_kl            | 0.00038929057 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5             |
|    n_updates            | 260           |
|    policy_gradient_loss | -0.00021      |
|    value_loss           | 8.93          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -9.46    |
| time/              |          |
|    fps             | 449      |
|    iterations      | 27       |
|    time_elapsed    | 492      |
|    total_timesteps | 221184   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.67e+03      |
|    ep_rew_mean          | -8.12         |
| time/                   |               |
|    fps                  | 452           |
|    iterations           | 28            |
|    time_elapsed         | 506           |
|    total_timesteps      | 229376        |
| train/                  |               |
|    approx_kl            | 0.00013705684 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.24          |
|    n_updates            | 270           |
|    policy_gradient_loss | -4.39e-05     |
|    value_loss           | 8.93          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.66e+03      |
|    ep_rew_mean          | -7.58         |
| time/                   |               |
|    fps                  | 455           |
|    iterations           | 29            |
|    time_elapsed         | 521           |
|    total_timesteps      | 237568        |
| train/                  |               |
|    approx_kl            | 0.00021887038 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.2           |
|    n_updates            | 280           |
|    policy_gradient_loss | -7.09e-05     |
|    value_loss           | 9.06          |
-------------------------------------------
Eval num_timesteps=240000, episode_reward=20.40 +/- 40.13
Episode length: 2035.80 +/- 345.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.04e+03     |
|    mean_reward          | 20.4         |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0012652124 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.08         |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.000665    |
|    value_loss           | 8.52         |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -8.59    |
| time/              |          |
|    fps             | 450      |
|    iterations      | 30       |
|    time_elapsed    | 545      |
|    total_timesteps | 245760   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.64e+03      |
|    ep_rew_mean          | -10.3         |
| time/                   |               |
|    fps                  | 453           |
|    iterations           | 31            |
|    time_elapsed         | 559           |
|    total_timesteps      | 253952        |
| train/                  |               |
|    approx_kl            | 0.00055542006 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.09         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.52          |
|    n_updates            | 300           |
|    policy_gradient_loss | -0.00025      |
|    value_loss           | 8.21          |
-------------------------------------------
Eval num_timesteps=260000, episode_reward=49.60 +/- 9.45
Episode length: 1842.60 +/- 376.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.84e+03      |
|    mean_reward          | 49.6          |
| time/                   |               |
|    total_timesteps      | 260000        |
| train/                  |               |
|    approx_kl            | 0.00047784724 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.09         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.96          |
|    n_updates            | 310           |
|    policy_gradient_loss | -0.000231     |
|    value_loss           | 8.41          |
-------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -9.92    |
| time/              |          |
|    fps             | 450      |
|    iterations      | 32       |
|    time_elapsed    | 581      |
|    total_timesteps | 262144   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.64e+03      |
|    ep_rew_mean          | -8.32         |
| time/                   |               |
|    fps                  | 453           |
|    iterations           | 33            |
|    time_elapsed         | 595           |
|    total_timesteps      | 270336        |
| train/                  |               |
|    approx_kl            | 2.5268135e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.09         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.91          |
|    n_updates            | 320           |
|    policy_gradient_loss | 1.33e-05      |
|    value_loss           | 8.33          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.62e+03      |
|    ep_rew_mean          | -6.85         |
| time/                   |               |
|    fps                  | 456           |
|    iterations           | 34            |
|    time_elapsed         | 610           |
|    total_timesteps      | 278528        |
| train/                  |               |
|    approx_kl            | 0.00021400623 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.09         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.27          |
|    n_updates            | 330           |
|    policy_gradient_loss | -9.12e-05     |
|    value_loss           | 9.22          |
-------------------------------------------
Eval num_timesteps=280000, episode_reward=8.00 +/- 25.50
Episode length: 2024.20 +/- 177.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 2.02e+03      |
|    mean_reward          | 8             |
| time/                   |               |
|    total_timesteps      | 280000        |
| train/                  |               |
|    approx_kl            | 0.00090329885 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.09         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.88          |
|    n_updates            | 340           |
|    policy_gradient_loss | -0.000409     |
|    value_loss           | 9.68          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | -5.68    |
| time/              |          |
|    fps             | 451      |
|    iterations      | 35       |
|    time_elapsed    | 634      |
|    total_timesteps | 286720   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.59e+03      |
|    ep_rew_mean          | -3.75         |
| time/                   |               |
|    fps                  | 454           |
|    iterations           | 36            |
|    time_elapsed         | 648           |
|    total_timesteps      | 294912        |
| train/                  |               |
|    approx_kl            | 0.00011312849 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.09         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 6.92          |
|    n_updates            | 350           |
|    policy_gradient_loss | -3.77e-05     |
|    value_loss           | 8.96          |
-------------------------------------------
Eval num_timesteps=300000, episode_reward=24.60 +/- 27.21
Episode length: 2161.60 +/- 365.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 2.16e+03      |
|    mean_reward          | 24.6          |
| time/                   |               |
|    total_timesteps      | 300000        |
| train/                  |               |
|    approx_kl            | 0.00020202922 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.09         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.62          |
|    n_updates            | 360           |
|    policy_gradient_loss | -8.34e-05     |
|    value_loss           | 8.92          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | -2.04    |
| time/              |          |
|    fps             | 449      |
|    iterations      | 37       |
|    time_elapsed    | 674      |
|    total_timesteps | 303104   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.62e+03      |
|    ep_rew_mean          | -0.925        |
| time/                   |               |
|    fps                  | 451           |
|    iterations           | 38            |
|    time_elapsed         | 689           |
|    total_timesteps      | 311296        |
| train/                  |               |
|    approx_kl            | 0.00097440125 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.09         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 1.32          |
|    n_updates            | 370           |
|    policy_gradient_loss | -0.000469     |
|    value_loss           | 7.43          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.63e+03      |
|    ep_rew_mean          | -0.905        |
| time/                   |               |
|    fps                  | 453           |
|    iterations           | 39            |
|    time_elapsed         | 703           |
|    total_timesteps      | 319488        |
| train/                  |               |
|    approx_kl            | 0.00037855518 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.09         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.52          |
|    n_updates            | 380           |
|    policy_gradient_loss | -0.000157     |
|    value_loss           | 9.15          |
-------------------------------------------
Eval num_timesteps=320000, episode_reward=-24.10 +/- 47.70
Episode length: 1406.20 +/- 174.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.41e+03     |
|    mean_reward          | -24.1        |
| time/                   |              |
|    total_timesteps      | 320000       |
| train/                  |              |
|    approx_kl            | 0.0001727214 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.87         |
|    n_updates            | 390          |
|    policy_gradient_loss | -6.87e-05    |
|    value_loss           | 8.18         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | 1.6      |
| time/              |          |
|    fps             | 452      |
|    iterations      | 40       |
|    time_elapsed    | 724      |
|    total_timesteps | 327680   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.63e+03      |
|    ep_rew_mean          | 2             |
| time/                   |               |
|    fps                  | 454           |
|    iterations           | 41            |
|    time_elapsed         | 739           |
|    total_timesteps      | 335872        |
| train/                  |               |
|    approx_kl            | 0.00081185787 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.09         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 5.66          |
|    n_updates            | 400           |
|    policy_gradient_loss | -0.000405     |
|    value_loss           | 8.38          |
-------------------------------------------
Eval num_timesteps=340000, episode_reward=-4.80 +/- 32.60
Episode length: 1544.00 +/- 158.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.54e+03      |
|    mean_reward          | -4.8          |
| time/                   |               |
|    total_timesteps      | 340000        |
| train/                  |               |
|    approx_kl            | 5.8804304e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.08         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.12          |
|    n_updates            | 410           |
|    policy_gradient_loss | -8.34e-06     |
|    value_loss           | 10.1          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | 2.29     |
| time/              |          |
|    fps             | 452      |
|    iterations      | 42       |
|    time_elapsed    | 760      |
|    total_timesteps | 344064   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.6e+03     |
|    ep_rew_mean          | 1.26        |
| time/                   |             |
|    fps                  | 454         |
|    iterations           | 43          |
|    time_elapsed         | 774         |
|    total_timesteps      | 352256      |
| train/                  |             |
|    approx_kl            | 0.000542747 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.82        |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.000238   |
|    value_loss           | 9.29        |
-----------------------------------------
Eval num_timesteps=360000, episode_reward=-19.50 +/- 50.63
Episode length: 1709.00 +/- 297.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.71e+03     |
|    mean_reward          | -19.5        |
| time/                   |              |
|    total_timesteps      | 360000       |
| train/                  |              |
|    approx_kl            | 0.0010411185 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.34         |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.000524    |
|    value_loss           | 7.88         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | 2.99     |
| time/              |          |
|    fps             | 451      |
|    iterations      | 44       |
|    time_elapsed    | 797      |
|    total_timesteps | 360448   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.61e+03     |
|    ep_rew_mean          | 2.88         |
| time/                   |              |
|    fps                  | 454          |
|    iterations           | 45           |
|    time_elapsed         | 811          |
|    total_timesteps      | 368640       |
| train/                  |              |
|    approx_kl            | 7.519964e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.61         |
|    n_updates            | 440          |
|    policy_gradient_loss | 1.86e-05     |
|    value_loss           | 8.87         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.61e+03     |
|    ep_rew_mean          | 1.21         |
| time/                   |              |
|    fps                  | 456          |
|    iterations           | 46           |
|    time_elapsed         | 825          |
|    total_timesteps      | 376832       |
| train/                  |              |
|    approx_kl            | 0.0007112388 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.01         |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.000365    |
|    value_loss           | 8.8          |
------------------------------------------
Eval num_timesteps=380000, episode_reward=-2.80 +/- 25.12
Episode length: 1932.20 +/- 451.06
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.93e+03   |
|    mean_reward          | -2.8       |
| time/                   |            |
|    total_timesteps      | 380000     |
| train/                  |            |
|    approx_kl            | 0.00130783 |
|    clip_fraction        | 0          |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.07      |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 4.41       |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.000721  |
|    value_loss           | 9.26       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.6e+03  |
|    ep_rew_mean     | -0.74    |
| time/              |          |
|    fps             | 454      |
|    iterations      | 47       |
|    time_elapsed    | 846      |
|    total_timesteps | 385024   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.61e+03      |
|    ep_rew_mean          | -1.83         |
| time/                   |               |
|    fps                  | 457           |
|    iterations           | 48            |
|    time_elapsed         | 860           |
|    total_timesteps      | 393216        |
| train/                  |               |
|    approx_kl            | 0.00021246164 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.57          |
|    n_updates            | 470           |
|    policy_gradient_loss | -9.5e-05      |
|    value_loss           | 9.94          |
-------------------------------------------
Eval num_timesteps=400000, episode_reward=-46.30 +/- 23.53
Episode length: 1440.40 +/- 263.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.44e+03     |
|    mean_reward          | -46.3        |
| time/                   |              |
|    total_timesteps      | 400000       |
| train/                  |              |
|    approx_kl            | 0.0009022475 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.97         |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.000432    |
|    value_loss           | 7.32         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | -4.13    |
| time/              |          |
|    fps             | 455      |
|    iterations      | 49       |
|    time_elapsed    | 880      |
|    total_timesteps | 401408   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.62e+03      |
|    ep_rew_mean          | -5.78         |
| time/                   |               |
|    fps                  | 457           |
|    iterations           | 50            |
|    time_elapsed         | 894           |
|    total_timesteps      | 409600        |
| train/                  |               |
|    approx_kl            | 0.00043499027 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.05         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.3           |
|    n_updates            | 490           |
|    policy_gradient_loss | -0.000207     |
|    value_loss           | 8.76          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.61e+03      |
|    ep_rew_mean          | -6.32         |
| time/                   |               |
|    fps                  | 459           |
|    iterations           | 51            |
|    time_elapsed         | 908           |
|    total_timesteps      | 417792        |
| train/                  |               |
|    approx_kl            | 0.00012785176 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.35          |
|    n_updates            | 500           |
|    policy_gradient_loss | -4.89e-05     |
|    value_loss           | 8.85          |
-------------------------------------------
Eval num_timesteps=420000, episode_reward=45.50 +/- 31.65
Episode length: 1858.40 +/- 387.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.86e+03     |
|    mean_reward          | 45.5         |
| time/                   |              |
|    total_timesteps      | 420000       |
| train/                  |              |
|    approx_kl            | 0.0013731183 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.27         |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.000583    |
|    value_loss           | 8.56         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -7.07    |
| time/              |          |
|    fps             | 456      |
|    iterations      | 52       |
|    time_elapsed    | 932      |
|    total_timesteps | 425984   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.62e+03     |
|    ep_rew_mean          | -7.39        |
| time/                   |              |
|    fps                  | 458          |
|    iterations           | 53           |
|    time_elapsed         | 946          |
|    total_timesteps      | 434176       |
| train/                  |              |
|    approx_kl            | 0.0008602997 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.82         |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.000471    |
|    value_loss           | 8.3          |
------------------------------------------
Eval num_timesteps=440000, episode_reward=22.70 +/- 32.55
Episode length: 1780.00 +/- 452.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.78e+03      |
|    mean_reward          | 22.7          |
| time/                   |               |
|    total_timesteps      | 440000        |
| train/                  |               |
|    approx_kl            | 0.00011935633 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.05         |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 4.41          |
|    n_updates            | 530           |
|    policy_gradient_loss | -3.41e-05     |
|    value_loss           | 8.53          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -9.13    |
| time/              |          |
|    fps             | 457      |
|    iterations      | 54       |
|    time_elapsed    | 966      |
|    total_timesteps | 442368   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.63e+03     |
|    ep_rew_mean          | -7.6         |
| time/                   |              |
|    fps                  | 459          |
|    iterations           | 55           |
|    time_elapsed         | 980          |
|    total_timesteps      | 450560       |
| train/                  |              |
|    approx_kl            | 3.121439e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.41         |
|    n_updates            | 540          |
|    policy_gradient_loss | 5.77e-06     |
|    value_loss           | 8.74         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.62e+03     |
|    ep_rew_mean          | -12.1        |
| time/                   |              |
|    fps                  | 461          |
|    iterations           | 56           |
|    time_elapsed         | 994          |
|    total_timesteps      | 458752       |
| train/                  |              |
|    approx_kl            | 0.0003651646 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.01         |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.000153    |
|    value_loss           | 9.09         |
------------------------------------------
Eval num_timesteps=460000, episode_reward=13.70 +/- 53.08
Episode length: 1773.80 +/- 393.76
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 1.77e+03       |
|    mean_reward          | 13.7           |
| time/                   |                |
|    total_timesteps      | 460000         |
| train/                  |                |
|    approx_kl            | 0.000101257014 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.06          |
|    explained_variance   | 0              |
|    learning_rate        | 0.0003         |
|    loss                 | 6.76           |
|    n_updates            | 560            |
|    policy_gradient_loss | -2.43e-05      |
|    value_loss           | 8.87           |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -12.4    |
| time/              |          |
|    fps             | 459      |
|    iterations      | 57       |
|    time_elapsed    | 1016     |
|    total_timesteps | 466944   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.6e+03       |
|    ep_rew_mean          | -15.4         |
| time/                   |               |
|    fps                  | 460           |
|    iterations           | 58            |
|    time_elapsed         | 1030          |
|    total_timesteps      | 475136        |
| train/                  |               |
|    approx_kl            | 2.0820044e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.55          |
|    n_updates            | 570           |
|    policy_gradient_loss | 8.6e-06       |
|    value_loss           | 8.75          |
-------------------------------------------
Eval num_timesteps=480000, episode_reward=-27.40 +/- 17.37
Episode length: 1864.20 +/- 281.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.86e+03      |
|    mean_reward          | -27.4         |
| time/                   |               |
|    total_timesteps      | 480000        |
| train/                  |               |
|    approx_kl            | 0.00065357675 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.05         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.68          |
|    n_updates            | 580           |
|    policy_gradient_loss | -0.000335     |
|    value_loss           | 8.1           |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.6e+03  |
|    ep_rew_mean     | -16.6    |
| time/              |          |
|    fps             | 458      |
|    iterations      | 59       |
|    time_elapsed    | 1053     |
|    total_timesteps | 483328   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.59e+03     |
|    ep_rew_mean          | -17.5        |
| time/                   |              |
|    fps                  | 460          |
|    iterations           | 60           |
|    time_elapsed         | 1067         |
|    total_timesteps      | 491520       |
| train/                  |              |
|    approx_kl            | 0.0004795086 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.05        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 3.77         |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.000213    |
|    value_loss           | 8.76         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.59e+03      |
|    ep_rew_mean          | -18.1         |
| time/                   |               |
|    fps                  | 462           |
|    iterations           | 61            |
|    time_elapsed         | 1081          |
|    total_timesteps      | 499712        |
| train/                  |               |
|    approx_kl            | 0.00059717393 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.69          |
|    n_updates            | 600           |
|    policy_gradient_loss | -0.000262     |
|    value_loss           | 8.41          |
-------------------------------------------
Eval num_timesteps=500000, episode_reward=2.70 +/- 37.94
Episode length: 1789.20 +/- 383.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.79e+03     |
|    mean_reward          | 2.7          |
| time/                   |              |
|    total_timesteps      | 500000       |
| train/                  |              |
|    approx_kl            | 3.383412e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.96         |
|    n_updates            | 610          |
|    policy_gradient_loss | 7.86e-06     |
|    value_loss           | 9.19         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.58e+03 |
|    ep_rew_mean     | -18.2    |
| time/              |          |
|    fps             | 460      |
|    iterations      | 62       |
|    time_elapsed    | 1102     |
|    total_timesteps | 507904   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.58e+03      |
|    ep_rew_mean          | -18.4         |
| time/                   |               |
|    fps                  | 462           |
|    iterations           | 63            |
|    time_elapsed         | 1116          |
|    total_timesteps      | 516096        |
| train/                  |               |
|    approx_kl            | 0.00035083585 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.38          |
|    n_updates            | 620           |
|    policy_gradient_loss | -0.000161     |
|    value_loss           | 8.42          |
-------------------------------------------
Eval num_timesteps=520000, episode_reward=-3.20 +/- 49.07
Episode length: 1649.80 +/- 397.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.65e+03      |
|    mean_reward          | -3.2          |
| time/                   |               |
|    total_timesteps      | 520000        |
| train/                  |               |
|    approx_kl            | 1.2013421e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.07         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 6.57          |
|    n_updates            | 630           |
|    policy_gradient_loss | 1.68e-05      |
|    value_loss           | 8.6           |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.58e+03 |
|    ep_rew_mean     | -17.6    |
| time/              |          |
|    fps             | 461      |
|    iterations      | 64       |
|    time_elapsed    | 1136     |
|    total_timesteps | 524288   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.59e+03      |
|    ep_rew_mean          | -15.7         |
| time/                   |               |
|    fps                  | 462           |
|    iterations           | 65            |
|    time_elapsed         | 1150          |
|    total_timesteps      | 532480        |
| train/                  |               |
|    approx_kl            | 0.00018341403 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.03          |
|    n_updates            | 640           |
|    policy_gradient_loss | -5.63e-05     |
|    value_loss           | 8.5           |
-------------------------------------------
Eval num_timesteps=540000, episode_reward=43.00 +/- 43.42
Episode length: 1723.40 +/- 588.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.72e+03      |
|    mean_reward          | 43            |
| time/                   |               |
|    total_timesteps      | 540000        |
| train/                  |               |
|    approx_kl            | 0.00022513092 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.39          |
|    n_updates            | 650           |
|    policy_gradient_loss | -8.6e-05      |
|    value_loss           | 8.28          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.59e+03 |
|    ep_rew_mean     | -17.1    |
| time/              |          |
|    fps             | 459      |
|    iterations      | 66       |
|    time_elapsed    | 1176     |
|    total_timesteps | 540672   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.6e+03       |
|    ep_rew_mean          | -17.1         |
| time/                   |               |
|    fps                  | 461           |
|    iterations           | 67            |
|    time_elapsed         | 1189          |
|    total_timesteps      | 548864        |
| train/                  |               |
|    approx_kl            | 0.00026018912 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.7           |
|    n_updates            | 660           |
|    policy_gradient_loss | -0.000114     |
|    value_loss           | 8.2           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.62e+03      |
|    ep_rew_mean          | -14.4         |
| time/                   |               |
|    fps                  | 462           |
|    iterations           | 68            |
|    time_elapsed         | 1204          |
|    total_timesteps      | 557056        |
| train/                  |               |
|    approx_kl            | 0.00025784998 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.95          |
|    n_updates            | 670           |
|    policy_gradient_loss | -9.19e-05     |
|    value_loss           | 7.88          |
-------------------------------------------
Eval num_timesteps=560000, episode_reward=-2.20 +/- 30.96
Episode length: 1978.20 +/- 517.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.98e+03      |
|    mean_reward          | -2.2          |
| time/                   |               |
|    total_timesteps      | 560000        |
| train/                  |               |
|    approx_kl            | 0.00072542013 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.65          |
|    n_updates            | 680           |
|    policy_gradient_loss | -0.000305     |
|    value_loss           | 7.91          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -14.4    |
| time/              |          |
|    fps             | 459      |
|    iterations      | 69       |
|    time_elapsed    | 1229     |
|    total_timesteps | 565248   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.62e+03     |
|    ep_rew_mean          | -14.3        |
| time/                   |              |
|    fps                  | 461          |
|    iterations           | 70           |
|    time_elapsed         | 1243         |
|    total_timesteps      | 573440       |
| train/                  |              |
|    approx_kl            | 8.715128e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.67         |
|    n_updates            | 690          |
|    policy_gradient_loss | -3.72e-05    |
|    value_loss           | 8.88         |
------------------------------------------
Eval num_timesteps=580000, episode_reward=47.20 +/- 23.40
Episode length: 1687.20 +/- 231.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.69e+03      |
|    mean_reward          | 47.2          |
| time/                   |               |
|    total_timesteps      | 580000        |
| train/                  |               |
|    approx_kl            | 0.00038130366 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.64          |
|    n_updates            | 700           |
|    policy_gradient_loss | -0.000195     |
|    value_loss           | 8.81          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -13.1    |
| time/              |          |
|    fps             | 459      |
|    iterations      | 71       |
|    time_elapsed    | 1266     |
|    total_timesteps | 581632   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.62e+03     |
|    ep_rew_mean          | -14.3        |
| time/                   |              |
|    fps                  | 460          |
|    iterations           | 72           |
|    time_elapsed         | 1280         |
|    total_timesteps      | 589824       |
| train/                  |              |
|    approx_kl            | 0.0006998921 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.95         |
|    n_updates            | 710          |
|    policy_gradient_loss | -0.000289    |
|    value_loss           | 8.92         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.62e+03     |
|    ep_rew_mean          | -14.7        |
| time/                   |              |
|    fps                  | 461          |
|    iterations           | 73           |
|    time_elapsed         | 1295         |
|    total_timesteps      | 598016       |
| train/                  |              |
|    approx_kl            | 0.0011194949 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.09         |
|    n_updates            | 720          |
|    policy_gradient_loss | -0.000575    |
|    value_loss           | 8.46         |
------------------------------------------
Eval num_timesteps=600000, episode_reward=14.40 +/- 44.57
Episode length: 2090.20 +/- 332.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 2.09e+03      |
|    mean_reward          | 14.4          |
| time/                   |               |
|    total_timesteps      | 600000        |
| train/                  |               |
|    approx_kl            | 0.00032157797 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.11          |
|    n_updates            | 730           |
|    policy_gradient_loss | -0.000168     |
|    value_loss           | 8.6           |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.6e+03  |
|    ep_rew_mean     | -14.3    |
| time/              |          |
|    fps             | 459      |
|    iterations      | 74       |
|    time_elapsed    | 1319     |
|    total_timesteps | 606208   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.6e+03       |
|    ep_rew_mean          | -15.8         |
| time/                   |               |
|    fps                  | 460           |
|    iterations           | 75            |
|    time_elapsed         | 1333          |
|    total_timesteps      | 614400        |
| train/                  |               |
|    approx_kl            | 2.8120776e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 6.21          |
|    n_updates            | 740           |
|    policy_gradient_loss | 8.96e-06      |
|    value_loss           | 10            |
-------------------------------------------
Eval num_timesteps=620000, episode_reward=10.50 +/- 36.56
Episode length: 1642.80 +/- 395.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.64e+03     |
|    mean_reward          | 10.5         |
| time/                   |              |
|    total_timesteps      | 620000       |
| train/                  |              |
|    approx_kl            | 0.0003733497 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.01         |
|    n_updates            | 750          |
|    policy_gradient_loss | -0.00017     |
|    value_loss           | 9.16         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -12.4    |
| time/              |          |
|    fps             | 458      |
|    iterations      | 76       |
|    time_elapsed    | 1357     |
|    total_timesteps | 622592   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.6e+03       |
|    ep_rew_mean          | -15.6         |
| time/                   |               |
|    fps                  | 459           |
|    iterations           | 77            |
|    time_elapsed         | 1371          |
|    total_timesteps      | 630784        |
| train/                  |               |
|    approx_kl            | 1.4560217e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.07         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.09          |
|    n_updates            | 760           |
|    policy_gradient_loss | 1.52e-05      |
|    value_loss           | 8.73          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.61e+03      |
|    ep_rew_mean          | -13.9         |
| time/                   |               |
|    fps                  | 460           |
|    iterations           | 78            |
|    time_elapsed         | 1388          |
|    total_timesteps      | 638976        |
| train/                  |               |
|    approx_kl            | 3.9457504e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.07         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.34          |
|    n_updates            | 770           |
|    policy_gradient_loss | 3.69e-06      |
|    value_loss           | 8.62          |
-------------------------------------------
Eval num_timesteps=640000, episode_reward=17.60 +/- 32.23
Episode length: 1973.40 +/- 461.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.97e+03      |
|    mean_reward          | 17.6          |
| time/                   |               |
|    total_timesteps      | 640000        |
| train/                  |               |
|    approx_kl            | 0.00026551308 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.49          |
|    n_updates            | 780           |
|    policy_gradient_loss | -0.000115     |
|    value_loss           | 8.79          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -13.3    |
| time/              |          |
|    fps             | 457      |
|    iterations      | 79       |
|    time_elapsed    | 1413     |
|    total_timesteps | 647168   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.62e+03      |
|    ep_rew_mean          | -14.3         |
| time/                   |               |
|    fps                  | 458           |
|    iterations           | 80            |
|    time_elapsed         | 1428          |
|    total_timesteps      | 655360        |
| train/                  |               |
|    approx_kl            | 1.1589807e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.04          |
|    n_updates            | 790           |
|    policy_gradient_loss | 1.24e-05      |
|    value_loss           | 7.91          |
-------------------------------------------
Eval num_timesteps=660000, episode_reward=16.40 +/- 35.06
Episode length: 2140.80 +/- 417.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 2.14e+03      |
|    mean_reward          | 16.4          |
| time/                   |               |
|    total_timesteps      | 660000        |
| train/                  |               |
|    approx_kl            | 0.00016060175 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.67          |
|    n_updates            | 800           |
|    policy_gradient_loss | -4.23e-05     |
|    value_loss           | 9.83          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -14.1    |
| time/              |          |
|    fps             | 456      |
|    iterations      | 81       |
|    time_elapsed    | 1452     |
|    total_timesteps | 663552   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.64e+03      |
|    ep_rew_mean          | -13.5         |
| time/                   |               |
|    fps                  | 458           |
|    iterations           | 82            |
|    time_elapsed         | 1465          |
|    total_timesteps      | 671744        |
| train/                  |               |
|    approx_kl            | 0.00014392752 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.72          |
|    n_updates            | 810           |
|    policy_gradient_loss | -4.31e-05     |
|    value_loss           | 8.25          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.64e+03     |
|    ep_rew_mean          | -15.9        |
| time/                   |              |
|    fps                  | 459          |
|    iterations           | 83           |
|    time_elapsed         | 1478         |
|    total_timesteps      | 679936       |
| train/                  |              |
|    approx_kl            | 0.0002984131 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.07        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 3.62         |
|    n_updates            | 820          |
|    policy_gradient_loss | -0.000123    |
|    value_loss           | 8.7          |
------------------------------------------
Eval num_timesteps=680000, episode_reward=41.80 +/- 7.45
Episode length: 2006.80 +/- 331.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.01e+03     |
|    mean_reward          | 41.8         |
| time/                   |              |
|    total_timesteps      | 680000       |
| train/                  |              |
|    approx_kl            | 5.440569e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.07        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.91         |
|    n_updates            | 830          |
|    policy_gradient_loss | 5.24e-06     |
|    value_loss           | 8.26         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -17.8    |
| time/              |          |
|    fps             | 457      |
|    iterations      | 84       |
|    time_elapsed    | 1502     |
|    total_timesteps | 688128   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.62e+03      |
|    ep_rew_mean          | -20.1         |
| time/                   |               |
|    fps                  | 459           |
|    iterations           | 85            |
|    time_elapsed         | 1516          |
|    total_timesteps      | 696320        |
| train/                  |               |
|    approx_kl            | 0.00038845325 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.07         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.09          |
|    n_updates            | 840           |
|    policy_gradient_loss | -0.000229     |
|    value_loss           | 9.16          |
-------------------------------------------
Eval num_timesteps=700000, episode_reward=36.70 +/- 27.63
Episode length: 1732.80 +/- 282.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.73e+03      |
|    mean_reward          | 36.7          |
| time/                   |               |
|    total_timesteps      | 700000        |
| train/                  |               |
|    approx_kl            | 1.2061741e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 3.57          |
|    n_updates            | 850           |
|    policy_gradient_loss | 1.29e-05      |
|    value_loss           | 8.39          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -20      |
| time/              |          |
|    fps             | 458      |
|    iterations      | 86       |
|    time_elapsed    | 1537     |
|    total_timesteps | 704512   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.66e+03     |
|    ep_rew_mean          | -17.7        |
| time/                   |              |
|    fps                  | 459          |
|    iterations           | 87           |
|    time_elapsed         | 1551         |
|    total_timesteps      | 712704       |
| train/                  |              |
|    approx_kl            | 3.635396e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.25         |
|    n_updates            | 860          |
|    policy_gradient_loss | 8.92e-06     |
|    value_loss           | 7.31         |
------------------------------------------
Eval num_timesteps=720000, episode_reward=5.40 +/- 26.67
Episode length: 1935.00 +/- 251.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.94e+03      |
|    mean_reward          | 5.4           |
| time/                   |               |
|    total_timesteps      | 720000        |
| train/                  |               |
|    approx_kl            | 0.00062962354 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 4.49          |
|    n_updates            | 870           |
|    policy_gradient_loss | -0.000244     |
|    value_loss           | 7.96          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -19.1    |
| time/              |          |
|    fps             | 458      |
|    iterations      | 88       |
|    time_elapsed    | 1572     |
|    total_timesteps | 720896   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.64e+03      |
|    ep_rew_mean          | -19.9         |
| time/                   |               |
|    fps                  | 459           |
|    iterations           | 89            |
|    time_elapsed         | 1586          |
|    total_timesteps      | 729088        |
| train/                  |               |
|    approx_kl            | 1.6826736e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.97          |
|    n_updates            | 880           |
|    policy_gradient_loss | 1.9e-05       |
|    value_loss           | 8             |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.63e+03     |
|    ep_rew_mean          | -20.4        |
| time/                   |              |
|    fps                  | 460          |
|    iterations           | 90           |
|    time_elapsed         | 1599         |
|    total_timesteps      | 737280       |
| train/                  |              |
|    approx_kl            | 7.314121e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.56         |
|    n_updates            | 890          |
|    policy_gradient_loss | -2.06e-05    |
|    value_loss           | 8.86         |
------------------------------------------
Eval num_timesteps=740000, episode_reward=-12.50 +/- 22.61
Episode length: 1991.60 +/- 308.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.99e+03      |
|    mean_reward          | -12.5         |
| time/                   |               |
|    total_timesteps      | 740000        |
| train/                  |               |
|    approx_kl            | 0.00069141283 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.94          |
|    n_updates            | 900           |
|    policy_gradient_loss | -0.000325     |
|    value_loss           | 8.4           |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -21.6    |
| time/              |          |
|    fps             | 459      |
|    iterations      | 91       |
|    time_elapsed    | 1622     |
|    total_timesteps | 745472   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.63e+03      |
|    ep_rew_mean          | -22           |
| time/                   |               |
|    fps                  | 460           |
|    iterations           | 92            |
|    time_elapsed         | 1636          |
|    total_timesteps      | 753664        |
| train/                  |               |
|    approx_kl            | 0.00014846143 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.66          |
|    n_updates            | 910           |
|    policy_gradient_loss | -6.76e-05     |
|    value_loss           | 8.67          |
-------------------------------------------
Eval num_timesteps=760000, episode_reward=-42.30 +/- 21.98
Episode length: 1560.60 +/- 205.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.56e+03      |
|    mean_reward          | -42.3         |
| time/                   |               |
|    total_timesteps      | 760000        |
| train/                  |               |
|    approx_kl            | 0.00021698797 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.01          |
|    n_updates            | 920           |
|    policy_gradient_loss | -0.000101     |
|    value_loss           | 8.45          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -20.1    |
| time/              |          |
|    fps             | 459      |
|    iterations      | 93       |
|    time_elapsed    | 1658     |
|    total_timesteps | 761856   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.63e+03      |
|    ep_rew_mean          | -20.1         |
| time/                   |               |
|    fps                  | 460           |
|    iterations           | 94            |
|    time_elapsed         | 1671          |
|    total_timesteps      | 770048        |
| train/                  |               |
|    approx_kl            | 0.00022103768 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.54          |
|    n_updates            | 930           |
|    policy_gradient_loss | -0.000122     |
|    value_loss           | 9.51          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.64e+03      |
|    ep_rew_mean          | -20           |
| time/                   |               |
|    fps                  | 461           |
|    iterations           | 95            |
|    time_elapsed         | 1685          |
|    total_timesteps      | 778240        |
| train/                  |               |
|    approx_kl            | 1.8124454e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 2.04          |
|    n_updates            | 940           |
|    policy_gradient_loss | 2.93e-06      |
|    value_loss           | 7.69          |
-------------------------------------------
Eval num_timesteps=780000, episode_reward=-8.20 +/- 65.39
Episode length: 1440.60 +/- 531.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.44e+03      |
|    mean_reward          | -8.2          |
| time/                   |               |
|    total_timesteps      | 780000        |
| train/                  |               |
|    approx_kl            | 0.00031232432 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 4.33          |
|    n_updates            | 950           |
|    policy_gradient_loss | -0.000134     |
|    value_loss           | 9.35          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -18.2    |
| time/              |          |
|    fps             | 461      |
|    iterations      | 96       |
|    time_elapsed    | 1704     |
|    total_timesteps | 786432   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.64e+03     |
|    ep_rew_mean          | -19.5        |
| time/                   |              |
|    fps                  | 462          |
|    iterations           | 97           |
|    time_elapsed         | 1718         |
|    total_timesteps      | 794624       |
| train/                  |              |
|    approx_kl            | 0.0005407473 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.15         |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.000278    |
|    value_loss           | 8.33         |
------------------------------------------
Eval num_timesteps=800000, episode_reward=-0.60 +/- 34.56
Episode length: 1489.40 +/- 255.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.49e+03     |
|    mean_reward          | -0.6         |
| time/                   |              |
|    total_timesteps      | 800000       |
| train/                  |              |
|    approx_kl            | 9.268197e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.79         |
|    n_updates            | 970          |
|    policy_gradient_loss | -4.23e-05    |
|    value_loss           | 8.44         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -16.1    |
| time/              |          |
|    fps             | 461      |
|    iterations      | 98       |
|    time_elapsed    | 1740     |
|    total_timesteps | 802816   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.64e+03      |
|    ep_rew_mean          | -17.5         |
| time/                   |               |
|    fps                  | 462           |
|    iterations           | 99            |
|    time_elapsed         | 1754          |
|    total_timesteps      | 811008        |
| train/                  |               |
|    approx_kl            | 4.8410147e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.22          |
|    n_updates            | 980           |
|    policy_gradient_loss | -4.74e-06     |
|    value_loss           | 8.76          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.62e+03      |
|    ep_rew_mean          | -18.5         |
| time/                   |               |
|    fps                  | 463           |
|    iterations           | 100           |
|    time_elapsed         | 1768          |
|    total_timesteps      | 819200        |
| train/                  |               |
|    approx_kl            | 0.00015005688 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.42          |
|    n_updates            | 990           |
|    policy_gradient_loss | -4.42e-05     |
|    value_loss           | 7.87          |
-------------------------------------------
Eval num_timesteps=820000, episode_reward=-3.80 +/- 50.66
Episode length: 1629.00 +/- 488.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.63e+03      |
|    mean_reward          | -3.8          |
| time/                   |               |
|    total_timesteps      | 820000        |
| train/                  |               |
|    approx_kl            | 3.6073565e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.6           |
|    n_updates            | 1000          |
|    policy_gradient_loss | -1.88e-06     |
|    value_loss           | 8.63          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -17.5    |
| time/              |          |
|    fps             | 462      |
|    iterations      | 101      |
|    time_elapsed    | 1788     |
|    total_timesteps | 827392   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.61e+03      |
|    ep_rew_mean          | -16.5         |
| time/                   |               |
|    fps                  | 463           |
|    iterations           | 102           |
|    time_elapsed         | 1802          |
|    total_timesteps      | 835584        |
| train/                  |               |
|    approx_kl            | 2.3774533e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.73          |
|    n_updates            | 1010          |
|    policy_gradient_loss | 1.26e-05      |
|    value_loss           | 9.9           |
-------------------------------------------
Eval num_timesteps=840000, episode_reward=-38.30 +/- 21.94
Episode length: 1586.60 +/- 396.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.59e+03     |
|    mean_reward          | -38.3        |
| time/                   |              |
|    total_timesteps      | 840000       |
| train/                  |              |
|    approx_kl            | 0.0016231984 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.38         |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.000787    |
|    value_loss           | 8.21         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -14.5    |
| time/              |          |
|    fps             | 462      |
|    iterations      | 103      |
|    time_elapsed    | 1823     |
|    total_timesteps | 843776   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.62e+03      |
|    ep_rew_mean          | -14.1         |
| time/                   |               |
|    fps                  | 463           |
|    iterations           | 104           |
|    time_elapsed         | 1837          |
|    total_timesteps      | 851968        |
| train/                  |               |
|    approx_kl            | 0.00023668082 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.05         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.13          |
|    n_updates            | 1030          |
|    policy_gradient_loss | -0.000116     |
|    value_loss           | 8.48          |
-------------------------------------------
Eval num_timesteps=860000, episode_reward=-1.80 +/- 32.05
Episode length: 1777.20 +/- 311.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.78e+03     |
|    mean_reward          | -1.8         |
| time/                   |              |
|    total_timesteps      | 860000       |
| train/                  |              |
|    approx_kl            | 5.325638e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.62         |
|    n_updates            | 1040         |
|    policy_gradient_loss | -1.4e-05     |
|    value_loss           | 8.92         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -13.8    |
| time/              |          |
|    fps             | 462      |
|    iterations      | 105      |
|    time_elapsed    | 1859     |
|    total_timesteps | 860160   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.63e+03      |
|    ep_rew_mean          | -10.8         |
| time/                   |               |
|    fps                  | 463           |
|    iterations           | 106           |
|    time_elapsed         | 1872          |
|    total_timesteps      | 868352        |
| train/                  |               |
|    approx_kl            | 0.00081378117 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.04         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.17          |
|    n_updates            | 1050          |
|    policy_gradient_loss | -0.000329     |
|    value_loss           | 8.67          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.59e+03     |
|    ep_rew_mean          | -10.9        |
| time/                   |              |
|    fps                  | 464          |
|    iterations           | 107          |
|    time_elapsed         | 1888         |
|    total_timesteps      | 876544       |
| train/                  |              |
|    approx_kl            | 0.0018526202 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.76         |
|    n_updates            | 1060         |
|    policy_gradient_loss | -0.000815    |
|    value_loss           | 9.2          |
------------------------------------------
Eval num_timesteps=880000, episode_reward=-36.20 +/- 40.63
Episode length: 1420.40 +/- 269.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.42e+03     |
|    mean_reward          | -36.2        |
| time/                   |              |
|    total_timesteps      | 880000       |
| train/                  |              |
|    approx_kl            | 0.0001566569 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.1          |
|    n_updates            | 1070         |
|    policy_gradient_loss | -7.19e-05    |
|    value_loss           | 8.45         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.59e+03 |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    fps             | 463      |
|    iterations      | 108      |
|    time_elapsed    | 1910     |
|    total_timesteps | 884736   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.59e+03      |
|    ep_rew_mean          | -10.9         |
| time/                   |               |
|    fps                  | 463           |
|    iterations           | 109           |
|    time_elapsed         | 1924          |
|    total_timesteps      | 892928        |
| train/                  |               |
|    approx_kl            | 0.00013425646 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.04         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 2.78          |
|    n_updates            | 1080          |
|    policy_gradient_loss | -6.08e-05     |
|    value_loss           | 8.74          |
-------------------------------------------
Eval num_timesteps=900000, episode_reward=-9.80 +/- 37.07
Episode length: 1900.60 +/- 275.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.9e+03      |
|    mean_reward          | -9.8         |
| time/                   |              |
|    total_timesteps      | 900000       |
| train/                  |              |
|    approx_kl            | 0.0003945122 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.01         |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.000222    |
|    value_loss           | 9.1          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.59e+03 |
|    ep_rew_mean     | -10.5    |
| time/              |          |
|    fps             | 462      |
|    iterations      | 110      |
|    time_elapsed    | 1946     |
|    total_timesteps | 901120   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.59e+03     |
|    ep_rew_mean          | -9.45        |
| time/                   |              |
|    fps                  | 463          |
|    iterations           | 111          |
|    time_elapsed         | 1960         |
|    total_timesteps      | 909312       |
| train/                  |              |
|    approx_kl            | 0.0011709491 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.27         |
|    n_updates            | 1100         |
|    policy_gradient_loss | -0.000507    |
|    value_loss           | 9.05         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.59e+03     |
|    ep_rew_mean          | -7.74        |
| time/                   |              |
|    fps                  | 464          |
|    iterations           | 112          |
|    time_elapsed         | 1974         |
|    total_timesteps      | 917504       |
| train/                  |              |
|    approx_kl            | 0.0001745033 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.03         |
|    n_updates            | 1110         |
|    policy_gradient_loss | -7.52e-05    |
|    value_loss           | 8.24         |
------------------------------------------
Eval num_timesteps=920000, episode_reward=-0.50 +/- 24.53
Episode length: 1728.80 +/- 261.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.73e+03     |
|    mean_reward          | -0.5         |
| time/                   |              |
|    total_timesteps      | 920000       |
| train/                  |              |
|    approx_kl            | 0.0018365374 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.5          |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.000812    |
|    value_loss           | 9.77         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.6e+03  |
|    ep_rew_mean     | -9.35    |
| time/              |          |
|    fps             | 463      |
|    iterations      | 113      |
|    time_elapsed    | 1996     |
|    total_timesteps | 925696   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.58e+03     |
|    ep_rew_mean          | -9.28        |
| time/                   |              |
|    fps                  | 464          |
|    iterations           | 114          |
|    time_elapsed         | 2011         |
|    total_timesteps      | 933888       |
| train/                  |              |
|    approx_kl            | 0.0007387102 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.9          |
|    n_updates            | 1130         |
|    policy_gradient_loss | -0.000322    |
|    value_loss           | 7.79         |
------------------------------------------
Eval num_timesteps=940000, episode_reward=-1.60 +/- 30.07
Episode length: 1866.60 +/- 268.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.87e+03     |
|    mean_reward          | -1.6         |
| time/                   |              |
|    total_timesteps      | 940000       |
| train/                  |              |
|    approx_kl            | 8.595709e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.15         |
|    n_updates            | 1140         |
|    policy_gradient_loss | -3.26e-05    |
|    value_loss           | 9.38         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.58e+03 |
|    ep_rew_mean     | -10.3    |
| time/              |          |
|    fps             | 463      |
|    iterations      | 115      |
|    time_elapsed    | 2033     |
|    total_timesteps | 942080   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.58e+03      |
|    ep_rew_mean          | -9.86         |
| time/                   |               |
|    fps                  | 464           |
|    iterations           | 116           |
|    time_elapsed         | 2047          |
|    total_timesteps      | 950272        |
| train/                  |               |
|    approx_kl            | 0.00083867146 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.03         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.18          |
|    n_updates            | 1150          |
|    policy_gradient_loss | -0.000353     |
|    value_loss           | 8.6           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.58e+03     |
|    ep_rew_mean          | -12.1        |
| time/                   |              |
|    fps                  | 465          |
|    iterations           | 117          |
|    time_elapsed         | 2060         |
|    total_timesteps      | 958464       |
| train/                  |              |
|    approx_kl            | 6.469111e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.36         |
|    n_updates            | 1160         |
|    policy_gradient_loss | -1.41e-05    |
|    value_loss           | 8.19         |
------------------------------------------
Eval num_timesteps=960000, episode_reward=-31.90 +/- 10.67
Episode length: 1633.60 +/- 281.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.63e+03     |
|    mean_reward          | -31.9        |
| time/                   |              |
|    total_timesteps      | 960000       |
| train/                  |              |
|    approx_kl            | 0.0007897785 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 7.24         |
|    n_updates            | 1170         |
|    policy_gradient_loss | -0.000321    |
|    value_loss           | 8.95         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.59e+03 |
|    ep_rew_mean     | -11.9    |
| time/              |          |
|    fps             | 464      |
|    iterations      | 118      |
|    time_elapsed    | 2083     |
|    total_timesteps | 966656   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.59e+03      |
|    ep_rew_mean          | -12.3         |
| time/                   |               |
|    fps                  | 464           |
|    iterations           | 119           |
|    time_elapsed         | 2096          |
|    total_timesteps      | 974848        |
| train/                  |               |
|    approx_kl            | 0.00062082626 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.02         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 6.2           |
|    n_updates            | 1180          |
|    policy_gradient_loss | -0.000359     |
|    value_loss           | 8.74          |
-------------------------------------------
Eval num_timesteps=980000, episode_reward=-26.40 +/- 31.55
Episode length: 1821.40 +/- 731.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.82e+03      |
|    mean_reward          | -26.4         |
| time/                   |               |
|    total_timesteps      | 980000        |
| train/                  |               |
|    approx_kl            | 9.7870965e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.02         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 5.46          |
|    n_updates            | 1190          |
|    policy_gradient_loss | -2.55e-05     |
|    value_loss           | 8.32          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.59e+03 |
|    ep_rew_mean     | -13      |
| time/              |          |
|    fps             | 464      |
|    iterations      | 120      |
|    time_elapsed    | 2117     |
|    total_timesteps | 983040   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.61e+03     |
|    ep_rew_mean          | -11.9        |
| time/                   |              |
|    fps                  | 465          |
|    iterations           | 121          |
|    time_elapsed         | 2131         |
|    total_timesteps      | 991232       |
| train/                  |              |
|    approx_kl            | 0.0012368449 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.01        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.53         |
|    n_updates            | 1200         |
|    policy_gradient_loss | -0.000632    |
|    value_loss           | 7.84         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.61e+03      |
|    ep_rew_mean          | -13.2         |
| time/                   |               |
|    fps                  | 465           |
|    iterations           | 122           |
|    time_elapsed         | 2148          |
|    total_timesteps      | 999424        |
| train/                  |               |
|    approx_kl            | 1.7041086e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.01         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.51          |
|    n_updates            | 1210          |
|    policy_gradient_loss | 1.31e-05      |
|    value_loss           | 9.22          |
-------------------------------------------
Eval num_timesteps=1000000, episode_reward=-24.70 +/- 24.35
Episode length: 1578.20 +/- 179.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.58e+03      |
|    mean_reward          | -24.7         |
| time/                   |               |
|    total_timesteps      | 1000000       |
| train/                  |               |
|    approx_kl            | 0.00019496093 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.01         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.52          |
|    n_updates            | 1220          |
|    policy_gradient_loss | -9.05e-05     |
|    value_loss           | 7.84          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | -13.4    |
| time/              |          |
|    fps             | 464      |
|    iterations      | 123      |
|    time_elapsed    | 2171     |
|    total_timesteps | 1007616  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.62e+03     |
|    ep_rew_mean          | -14.5        |
| time/                   |              |
|    fps                  | 464          |
|    iterations           | 124          |
|    time_elapsed         | 2185         |
|    total_timesteps      | 1015808      |
| train/                  |              |
|    approx_kl            | 0.0006465317 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1           |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.72         |
|    n_updates            | 1230         |
|    policy_gradient_loss | -0.000264    |
|    value_loss           | 8.56         |
------------------------------------------
Eval num_timesteps=1020000, episode_reward=-44.80 +/- 44.35
Episode length: 1291.80 +/- 122.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.29e+03      |
|    mean_reward          | -44.8         |
| time/                   |               |
|    total_timesteps      | 1020000       |
| train/                  |               |
|    approx_kl            | 0.00079339347 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.993        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 2.33          |
|    n_updates            | 1240          |
|    policy_gradient_loss | -0.000322     |
|    value_loss           | 7.07          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -14.8    |
| time/              |          |
|    fps             | 464      |
|    iterations      | 125      |
|    time_elapsed    | 2206     |
|    total_timesteps | 1024000  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.63e+03     |
|    ep_rew_mean          | -16.6        |
| time/                   |              |
|    fps                  | 464          |
|    iterations           | 126          |
|    time_elapsed         | 2220         |
|    total_timesteps      | 1032192      |
| train/                  |              |
|    approx_kl            | 9.945863e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.989       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.88         |
|    n_updates            | 1250         |
|    policy_gradient_loss | -5.21e-05    |
|    value_loss           | 8.28         |
------------------------------------------
Eval num_timesteps=1040000, episode_reward=-19.20 +/- 35.76
Episode length: 1784.60 +/- 137.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.78e+03     |
|    mean_reward          | -19.2        |
| time/                   |              |
|    total_timesteps      | 1040000      |
| train/                  |              |
|    approx_kl            | 0.0014314998 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.984       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.41         |
|    n_updates            | 1260         |
|    policy_gradient_loss | -0.000588    |
|    value_loss           | 8.39         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -17.5    |
| time/              |          |
|    fps             | 463      |
|    iterations      | 127      |
|    time_elapsed    | 2242     |
|    total_timesteps | 1040384  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.64e+03     |
|    ep_rew_mean          | -18.1        |
| time/                   |              |
|    fps                  | 464          |
|    iterations           | 128          |
|    time_elapsed         | 2257         |
|    total_timesteps      | 1048576      |
| train/                  |              |
|    approx_kl            | 0.0003768178 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.971       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 2.68         |
|    n_updates            | 1270         |
|    policy_gradient_loss | -0.000271    |
|    value_loss           | 7.25         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.65e+03      |
|    ep_rew_mean          | -18.7         |
| time/                   |               |
|    fps                  | 465           |
|    iterations           | 129           |
|    time_elapsed         | 2271          |
|    total_timesteps      | 1056768       |
| train/                  |               |
|    approx_kl            | 0.00025795028 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.966        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.2           |
|    n_updates            | 1280          |
|    policy_gradient_loss | -0.000144     |
|    value_loss           | 8.83          |
-------------------------------------------
Eval num_timesteps=1060000, episode_reward=-32.30 +/- 24.01
Episode length: 1655.20 +/- 352.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.66e+03   |
|    mean_reward          | -32.3      |
| time/                   |            |
|    total_timesteps      | 1060000    |
| train/                  |            |
|    approx_kl            | 0.00218783 |
|    clip_fraction        | 0          |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.978     |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 4.36       |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.000975  |
|    value_loss           | 8.67       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -20.8    |
| time/              |          |
|    fps             | 464      |
|    iterations      | 130      |
|    time_elapsed    | 2294     |
|    total_timesteps | 1064960  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.63e+03     |
|    ep_rew_mean          | -21.5        |
| time/                   |              |
|    fps                  | 464          |
|    iterations           | 131          |
|    time_elapsed         | 2309         |
|    total_timesteps      | 1073152      |
| train/                  |              |
|    approx_kl            | 0.0007981596 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.995       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.33         |
|    n_updates            | 1300         |
|    policy_gradient_loss | -0.000341    |
|    value_loss           | 9.5          |
------------------------------------------
Eval num_timesteps=1080000, episode_reward=38.40 +/- 50.26
Episode length: 1631.60 +/- 377.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.63e+03     |
|    mean_reward          | 38.4         |
| time/                   |              |
|    total_timesteps      | 1080000      |
| train/                  |              |
|    approx_kl            | 8.931154e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.998       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.71         |
|    n_updates            | 1310         |
|    policy_gradient_loss | -2.17e-05    |
|    value_loss           | 9.16         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -21      |
| time/              |          |
|    fps             | 464      |
|    iterations      | 132      |
|    time_elapsed    | 2329     |
|    total_timesteps | 1081344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.64e+03    |
|    ep_rew_mean          | -20.4       |
| time/                   |             |
|    fps                  | 464         |
|    iterations           | 133         |
|    time_elapsed         | 2344        |
|    total_timesteps      | 1089536     |
| train/                  |             |
|    approx_kl            | 0.003297797 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.999      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 5.03        |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.00146    |
|    value_loss           | 7.84        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.64e+03    |
|    ep_rew_mean          | -20.1       |
| time/                   |             |
|    fps                  | 465         |
|    iterations           | 134         |
|    time_elapsed         | 2358        |
|    total_timesteps      | 1097728     |
| train/                  |             |
|    approx_kl            | 0.012186667 |
|    clip_fraction        | 0.0237      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.991      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 6.02        |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.00149    |
|    value_loss           | 8.15        |
-----------------------------------------
Eval num_timesteps=1100000, episode_reward=-25.90 +/- 40.06
Episode length: 1698.80 +/- 286.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.7e+03      |
|    mean_reward          | -25.9        |
| time/                   |              |
|    total_timesteps      | 1100000      |
| train/                  |              |
|    approx_kl            | 0.0076101683 |
|    clip_fraction        | 0.0133       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.94        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.16         |
|    n_updates            | 1340         |
|    policy_gradient_loss | -0.000726    |
|    value_loss           | 8.66         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -19.7    |
| time/              |          |
|    fps             | 464      |
|    iterations      | 135      |
|    time_elapsed    | 2381     |
|    total_timesteps | 1105920  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.64e+03    |
|    ep_rew_mean          | -18         |
| time/                   |             |
|    fps                  | 465         |
|    iterations           | 136         |
|    time_elapsed         | 2395        |
|    total_timesteps      | 1114112     |
| train/                  |             |
|    approx_kl            | 0.010993085 |
|    clip_fraction        | 0.0543      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.862      |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 3.98        |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.00263    |
|    value_loss           | 9.42        |
-----------------------------------------
Eval num_timesteps=1120000, episode_reward=-11.70 +/- 34.41
Episode length: 1736.20 +/- 258.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.74e+03     |
|    mean_reward          | -11.7        |
| time/                   |              |
|    total_timesteps      | 1120000      |
| train/                  |              |
|    approx_kl            | 0.0051368913 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.851       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.56         |
|    n_updates            | 1360         |
|    policy_gradient_loss | -0.000423    |
|    value_loss           | 8.55         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -20.7    |
| time/              |          |
|    fps             | 463      |
|    iterations      | 137      |
|    time_elapsed    | 2419     |
|    total_timesteps | 1122304  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.65e+03     |
|    ep_rew_mean          | -21          |
| time/                   |              |
|    fps                  | 464          |
|    iterations           | 138          |
|    time_elapsed         | 2433         |
|    total_timesteps      | 1130496      |
| train/                  |              |
|    approx_kl            | 0.0041861217 |
|    clip_fraction        | 0.00852      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.931       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.4          |
|    n_updates            | 1370         |
|    policy_gradient_loss | -0.000143    |
|    value_loss           | 8.48         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.63e+03    |
|    ep_rew_mean          | -22.1       |
| time/                   |             |
|    fps                  | 465         |
|    iterations           | 139         |
|    time_elapsed         | 2447        |
|    total_timesteps      | 1138688     |
| train/                  |             |
|    approx_kl            | 0.012864019 |
|    clip_fraction        | 0.0311      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.991      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.18        |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.00194    |
|    value_loss           | 8.84        |
-----------------------------------------
Eval num_timesteps=1140000, episode_reward=-10.80 +/- 39.23
Episode length: 1461.80 +/- 115.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.46e+03     |
|    mean_reward          | -10.8        |
| time/                   |              |
|    total_timesteps      | 1140000      |
| train/                  |              |
|    approx_kl            | 0.0024089697 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.01        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.6          |
|    n_updates            | 1390         |
|    policy_gradient_loss | -0.000197    |
|    value_loss           | 8.1          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -21.9    |
| time/              |          |
|    fps             | 464      |
|    iterations      | 140      |
|    time_elapsed    | 2468     |
|    total_timesteps | 1146880  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.62e+03     |
|    ep_rew_mean          | -21.5        |
| time/                   |              |
|    fps                  | 465          |
|    iterations           | 141          |
|    time_elapsed         | 2482         |
|    total_timesteps      | 1155072      |
| train/                  |              |
|    approx_kl            | 0.0031326057 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 7.16         |
|    n_updates            | 1400         |
|    policy_gradient_loss | -0.000118    |
|    value_loss           | 8.71         |
------------------------------------------
Eval num_timesteps=1160000, episode_reward=-12.20 +/- 43.00
Episode length: 1626.20 +/- 255.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.63e+03    |
|    mean_reward          | -12.2       |
| time/                   |             |
|    total_timesteps      | 1160000     |
| train/                  |             |
|    approx_kl            | 0.011092334 |
|    clip_fraction        | 0.0404      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.74        |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.00253    |
|    value_loss           | 9.29        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -20.3    |
| time/              |          |
|    fps             | 464      |
|    iterations      | 142      |
|    time_elapsed    | 2506     |
|    total_timesteps | 1163264  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.62e+03     |
|    ep_rew_mean          | -20.2        |
| time/                   |              |
|    fps                  | 464          |
|    iterations           | 143          |
|    time_elapsed         | 2520         |
|    total_timesteps      | 1171456      |
| train/                  |              |
|    approx_kl            | 0.0023773843 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.12         |
|    n_updates            | 1420         |
|    policy_gradient_loss | 0.00031      |
|    value_loss           | 8.98         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.6e+03      |
|    ep_rew_mean          | -20.7        |
| time/                   |              |
|    fps                  | 465          |
|    iterations           | 144          |
|    time_elapsed         | 2534         |
|    total_timesteps      | 1179648      |
| train/                  |              |
|    approx_kl            | 0.0101205185 |
|    clip_fraction        | 0.0362       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.27         |
|    n_updates            | 1430         |
|    policy_gradient_loss | -0.00256     |
|    value_loss           | 8.49         |
------------------------------------------
Eval num_timesteps=1180000, episode_reward=-27.20 +/- 25.93
Episode length: 1791.20 +/- 600.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.79e+03    |
|    mean_reward          | -27.2       |
| time/                   |             |
|    total_timesteps      | 1180000     |
| train/                  |             |
|    approx_kl            | 0.013734911 |
|    clip_fraction        | 0.0231      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.44        |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.00191    |
|    value_loss           | 9.02        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.59e+03 |
|    ep_rew_mean     | -19.5    |
| time/              |          |
|    fps             | 464      |
|    iterations      | 145      |
|    time_elapsed    | 2556     |
|    total_timesteps | 1187840  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.58e+03    |
|    ep_rew_mean          | -17         |
| time/                   |             |
|    fps                  | 465         |
|    iterations           | 146         |
|    time_elapsed         | 2570        |
|    total_timesteps      | 1196032     |
| train/                  |             |
|    approx_kl            | 0.008650734 |
|    clip_fraction        | 0.0284      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.59        |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.00109    |
|    value_loss           | 8.62        |
-----------------------------------------
Eval num_timesteps=1200000, episode_reward=5.40 +/- 28.31
Episode length: 1657.20 +/- 241.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.66e+03    |
|    mean_reward          | 5.4         |
| time/                   |             |
|    total_timesteps      | 1200000     |
| train/                  |             |
|    approx_kl            | 0.013723873 |
|    clip_fraction        | 0.00576     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.53        |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.000481   |
|    value_loss           | 8.37        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.59e+03 |
|    ep_rew_mean     | -15.7    |
| time/              |          |
|    fps             | 464      |
|    iterations      | 147      |
|    time_elapsed    | 2591     |
|    total_timesteps | 1204224  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.59e+03    |
|    ep_rew_mean          | -15.1       |
| time/                   |             |
|    fps                  | 465         |
|    iterations           | 148         |
|    time_elapsed         | 2604        |
|    total_timesteps      | 1212416     |
| train/                  |             |
|    approx_kl            | 0.012513596 |
|    clip_fraction        | 0.0129      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.79        |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.00101    |
|    value_loss           | 7.82        |
-----------------------------------------
Eval num_timesteps=1220000, episode_reward=-51.80 +/- 39.56
Episode length: 1467.60 +/- 314.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.47e+03    |
|    mean_reward          | -51.8       |
| time/                   |             |
|    total_timesteps      | 1220000     |
| train/                  |             |
|    approx_kl            | 0.008740719 |
|    clip_fraction        | 0.0247      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.62        |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.00179    |
|    value_loss           | 8.17        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.59e+03 |
|    ep_rew_mean     | -14.6    |
| time/              |          |
|    fps             | 464      |
|    iterations      | 149      |
|    time_elapsed    | 2625     |
|    total_timesteps | 1220608  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.59e+03     |
|    ep_rew_mean          | -13.6        |
| time/                   |              |
|    fps                  | 465          |
|    iterations           | 150          |
|    time_elapsed         | 2640         |
|    total_timesteps      | 1228800      |
| train/                  |              |
|    approx_kl            | 0.0040890137 |
|    clip_fraction        | 0.00138      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.28         |
|    n_updates            | 1490         |
|    policy_gradient_loss | -9.53e-05    |
|    value_loss           | 9.25         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.61e+03    |
|    ep_rew_mean          | -9.98       |
| time/                   |             |
|    fps                  | 466         |
|    iterations           | 151         |
|    time_elapsed         | 2653        |
|    total_timesteps      | 1236992     |
| train/                  |             |
|    approx_kl            | 0.008046714 |
|    clip_fraction        | 0.00134     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.999      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.19        |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.000108   |
|    value_loss           | 7.93        |
-----------------------------------------
Eval num_timesteps=1240000, episode_reward=-30.10 +/- 33.95
Episode length: 1535.40 +/- 113.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.54e+03   |
|    mean_reward          | -30.1      |
| time/                   |            |
|    total_timesteps      | 1240000    |
| train/                  |            |
|    approx_kl            | 0.01080236 |
|    clip_fraction        | 0.0186     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.994     |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 4.73       |
|    n_updates            | 1510       |
|    policy_gradient_loss | -0.0014    |
|    value_loss           | 9.76       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | -11.4    |
| time/              |          |
|    fps             | 465      |
|    iterations      | 152      |
|    time_elapsed    | 2675     |
|    total_timesteps | 1245184  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.61e+03     |
|    ep_rew_mean          | -11.7        |
| time/                   |              |
|    fps                  | 466          |
|    iterations           | 153          |
|    time_elapsed         | 2689         |
|    total_timesteps      | 1253376      |
| train/                  |              |
|    approx_kl            | 0.0021157758 |
|    clip_fraction        | 0.00238      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.999       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.91         |
|    n_updates            | 1520         |
|    policy_gradient_loss | -0.000155    |
|    value_loss           | 8.95         |
------------------------------------------
Eval num_timesteps=1260000, episode_reward=8.80 +/- 34.12
Episode length: 1797.60 +/- 471.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.8e+03     |
|    mean_reward          | 8.8         |
| time/                   |             |
|    total_timesteps      | 1260000     |
| train/                  |             |
|    approx_kl            | 0.008734517 |
|    clip_fraction        | 0.0115      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 2.57        |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.00104    |
|    value_loss           | 8.26        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | -12      |
| time/              |          |
|    fps             | 465      |
|    iterations      | 154      |
|    time_elapsed    | 2710     |
|    total_timesteps | 1261568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.61e+03    |
|    ep_rew_mean          | -11.5       |
| time/                   |             |
|    fps                  | 466         |
|    iterations           | 155         |
|    time_elapsed         | 2724        |
|    total_timesteps      | 1269760     |
| train/                  |             |
|    approx_kl            | 0.006373954 |
|    clip_fraction        | 0.0123      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.96       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 7           |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.000793   |
|    value_loss           | 9.63        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.61e+03    |
|    ep_rew_mean          | -13.6       |
| time/                   |             |
|    fps                  | 466         |
|    iterations           | 156         |
|    time_elapsed         | 2738        |
|    total_timesteps      | 1277952     |
| train/                  |             |
|    approx_kl            | 0.012257004 |
|    clip_fraction        | 0.0414      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.859      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.77        |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.00261    |
|    value_loss           | 9.39        |
-----------------------------------------
Eval num_timesteps=1280000, episode_reward=18.60 +/- 38.98
Episode length: 1739.80 +/- 244.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.74e+03  |
|    mean_reward          | 18.6      |
| time/                   |           |
|    total_timesteps      | 1280000   |
| train/                  |           |
|    approx_kl            | 0.00452   |
|    clip_fraction        | 0.0062    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.902    |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 3.67      |
|    n_updates            | 1560      |
|    policy_gradient_loss | -0.000295 |
|    value_loss           | 8.6       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.6e+03  |
|    ep_rew_mean     | -12.4    |
| time/              |          |
|    fps             | 465      |
|    iterations      | 157      |
|    time_elapsed    | 2761     |
|    total_timesteps | 1286144  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.61e+03     |
|    ep_rew_mean          | -11.3        |
| time/                   |              |
|    fps                  | 466          |
|    iterations           | 158          |
|    time_elapsed         | 2775         |
|    total_timesteps      | 1294336      |
| train/                  |              |
|    approx_kl            | 0.0067734774 |
|    clip_fraction        | 0.00657      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.898       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.18         |
|    n_updates            | 1570         |
|    policy_gradient_loss | -0.000598    |
|    value_loss           | 9.12         |
------------------------------------------
Eval num_timesteps=1300000, episode_reward=-26.00 +/- 27.08
Episode length: 1710.00 +/- 312.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.71e+03    |
|    mean_reward          | -26         |
| time/                   |             |
|    total_timesteps      | 1300000     |
| train/                  |             |
|    approx_kl            | 0.004648722 |
|    clip_fraction        | 0.00271     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.861      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 5.21        |
|    n_updates            | 1580        |
|    policy_gradient_loss | -0.000161   |
|    value_loss           | 8.8         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -9.61    |
| time/              |          |
|    fps             | 465      |
|    iterations      | 159      |
|    time_elapsed    | 2798     |
|    total_timesteps | 1302528  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.61e+03    |
|    ep_rew_mean          | -7.54       |
| time/                   |             |
|    fps                  | 466         |
|    iterations           | 160         |
|    time_elapsed         | 2812        |
|    total_timesteps      | 1310720     |
| train/                  |             |
|    approx_kl            | 0.002845897 |
|    clip_fraction        | 0.00126     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.861      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.55        |
|    n_updates            | 1590        |
|    policy_gradient_loss | -4.77e-05   |
|    value_loss           | 8.59        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.61e+03    |
|    ep_rew_mean          | -8.04       |
| time/                   |             |
|    fps                  | 466         |
|    iterations           | 161         |
|    time_elapsed         | 2826        |
|    total_timesteps      | 1318912     |
| train/                  |             |
|    approx_kl            | 0.009768295 |
|    clip_fraction        | 0.0298      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.78       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.17        |
|    n_updates            | 1600        |
|    policy_gradient_loss | -0.00188    |
|    value_loss           | 8.08        |
-----------------------------------------
Eval num_timesteps=1320000, episode_reward=-37.70 +/- 16.93
Episode length: 1683.00 +/- 338.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.68e+03     |
|    mean_reward          | -37.7        |
| time/                   |              |
|    total_timesteps      | 1320000      |
| train/                  |              |
|    approx_kl            | 0.0067698145 |
|    clip_fraction        | 0.039        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.68        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.29         |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.00282     |
|    value_loss           | 9.2          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | -9.07    |
| time/              |          |
|    fps             | 465      |
|    iterations      | 162      |
|    time_elapsed    | 2848     |
|    total_timesteps | 1327104  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.6e+03      |
|    ep_rew_mean          | -10.3        |
| time/                   |              |
|    fps                  | 466          |
|    iterations           | 163          |
|    time_elapsed         | 2862         |
|    total_timesteps      | 1335296      |
| train/                  |              |
|    approx_kl            | 0.0015613849 |
|    clip_fraction        | 0.0183       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.714       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.09         |
|    n_updates            | 1620         |
|    policy_gradient_loss | -0.000832    |
|    value_loss           | 9.09         |
------------------------------------------
Eval num_timesteps=1340000, episode_reward=-20.30 +/- 28.32
Episode length: 1636.40 +/- 221.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.64e+03     |
|    mean_reward          | -20.3        |
| time/                   |              |
|    total_timesteps      | 1340000      |
| train/                  |              |
|    approx_kl            | 0.0042059235 |
|    clip_fraction        | 0.0196       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.779       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.01         |
|    n_updates            | 1630         |
|    policy_gradient_loss | -0.00102     |
|    value_loss           | 8.02         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -10.4    |
| time/              |          |
|    fps             | 465      |
|    iterations      | 164      |
|    time_elapsed    | 2885     |
|    total_timesteps | 1343488  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.63e+03    |
|    ep_rew_mean          | -9.47       |
| time/                   |             |
|    fps                  | 466         |
|    iterations           | 165         |
|    time_elapsed         | 2898        |
|    total_timesteps      | 1351680     |
| train/                  |             |
|    approx_kl            | 0.002913617 |
|    clip_fraction        | 0.0136      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.814      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.92        |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.000872   |
|    value_loss           | 8.57        |
-----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.63e+03      |
|    ep_rew_mean          | -11.3         |
| time/                   |               |
|    fps                  | 466           |
|    iterations           | 166           |
|    time_elapsed         | 2912          |
|    total_timesteps      | 1359872       |
| train/                  |               |
|    approx_kl            | 0.00018173279 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.774        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 7.3           |
|    n_updates            | 1650          |
|    policy_gradient_loss | 0.000291      |
|    value_loss           | 8.09          |
-------------------------------------------
Eval num_timesteps=1360000, episode_reward=-8.30 +/- 55.45
Episode length: 1396.00 +/- 185.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.4e+03      |
|    mean_reward          | -8.3         |
| time/                   |              |
|    total_timesteps      | 1360000      |
| train/                  |              |
|    approx_kl            | 0.0028771586 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.851       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.87         |
|    n_updates            | 1660         |
|    policy_gradient_loss | -0.000219    |
|    value_loss           | 7.89         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -11.8    |
| time/              |          |
|    fps             | 466      |
|    iterations      | 167      |
|    time_elapsed    | 2932     |
|    total_timesteps | 1368064  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.62e+03     |
|    ep_rew_mean          | -12.1        |
| time/                   |              |
|    fps                  | 467          |
|    iterations           | 168          |
|    time_elapsed         | 2945         |
|    total_timesteps      | 1376256      |
| train/                  |              |
|    approx_kl            | 0.0057793674 |
|    clip_fraction        | 0.00398      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.773       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.34         |
|    n_updates            | 1670         |
|    policy_gradient_loss | 0.000137     |
|    value_loss           | 7.39         |
------------------------------------------
Eval num_timesteps=1380000, episode_reward=-23.90 +/- 32.71
Episode length: 1685.20 +/- 133.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.69e+03     |
|    mean_reward          | -23.9        |
| time/                   |              |
|    total_timesteps      | 1380000      |
| train/                  |              |
|    approx_kl            | 0.0014996149 |
|    clip_fraction        | 0.0024       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.769       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.16         |
|    n_updates            | 1680         |
|    policy_gradient_loss | -7.28e-05    |
|    value_loss           | 7.04         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -11.4    |
| time/              |          |
|    fps             | 466      |
|    iterations      | 169      |
|    time_elapsed    | 2967     |
|    total_timesteps | 1384448  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.64e+03     |
|    ep_rew_mean          | -13          |
| time/                   |              |
|    fps                  | 467          |
|    iterations           | 170          |
|    time_elapsed         | 2981         |
|    total_timesteps      | 1392640      |
| train/                  |              |
|    approx_kl            | 0.0037156504 |
|    clip_fraction        | 0.000806     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.822       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.62         |
|    n_updates            | 1690         |
|    policy_gradient_loss | -0.000173    |
|    value_loss           | 8.65         |
------------------------------------------
Eval num_timesteps=1400000, episode_reward=-15.80 +/- 39.55
Episode length: 1725.60 +/- 334.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.73e+03    |
|    mean_reward          | -15.8       |
| time/                   |             |
|    total_timesteps      | 1400000     |
| train/                  |             |
|    approx_kl            | 0.003713749 |
|    clip_fraction        | 0.0308      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.884      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 2.15        |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.00129    |
|    value_loss           | 7.84        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -15.3    |
| time/              |          |
|    fps             | 465      |
|    iterations      | 171      |
|    time_elapsed    | 3006     |
|    total_timesteps | 1400832  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.66e+03     |
|    ep_rew_mean          | -14.2        |
| time/                   |              |
|    fps                  | 466          |
|    iterations           | 172          |
|    time_elapsed         | 3020         |
|    total_timesteps      | 1409024      |
| train/                  |              |
|    approx_kl            | 0.0040264456 |
|    clip_fraction        | 0.0025       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.844       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.32         |
|    n_updates            | 1710         |
|    policy_gradient_loss | -0.000311    |
|    value_loss           | 8.58         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.66e+03    |
|    ep_rew_mean          | -13.8       |
| time/                   |             |
|    fps                  | 467         |
|    iterations           | 173         |
|    time_elapsed         | 3034        |
|    total_timesteps      | 1417216     |
| train/                  |             |
|    approx_kl            | 0.007947726 |
|    clip_fraction        | 0.0204      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.862      |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 3.49        |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.00146    |
|    value_loss           | 7.32        |
-----------------------------------------
Eval num_timesteps=1420000, episode_reward=-30.10 +/- 16.89
Episode length: 1614.80 +/- 270.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.61e+03    |
|    mean_reward          | -30.1       |
| time/                   |             |
|    total_timesteps      | 1420000     |
| train/                  |             |
|    approx_kl            | 0.011012778 |
|    clip_fraction        | 0.0418      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.957      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 5.58        |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.00327    |
|    value_loss           | 8.91        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -13.8    |
| time/              |          |
|    fps             | 466      |
|    iterations      | 174      |
|    time_elapsed    | 3056     |
|    total_timesteps | 1425408  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.68e+03    |
|    ep_rew_mean          | -12         |
| time/                   |             |
|    fps                  | 466         |
|    iterations           | 175         |
|    time_elapsed         | 3069        |
|    total_timesteps      | 1433600     |
| train/                  |             |
|    approx_kl            | 0.007601826 |
|    clip_fraction        | 0.0206      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.972      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.3         |
|    n_updates            | 1740        |
|    policy_gradient_loss | -0.00133    |
|    value_loss           | 8.69        |
-----------------------------------------
Eval num_timesteps=1440000, episode_reward=-13.60 +/- 42.65
Episode length: 1640.20 +/- 439.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.64e+03     |
|    mean_reward          | -13.6        |
| time/                   |              |
|    total_timesteps      | 1440000      |
| train/                  |              |
|    approx_kl            | 0.0066507338 |
|    clip_fraction        | 0.0509       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.99        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.49         |
|    n_updates            | 1750         |
|    policy_gradient_loss | -0.00353     |
|    value_loss           | 8.23         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -13.1    |
| time/              |          |
|    fps             | 466      |
|    iterations      | 176      |
|    time_elapsed    | 3090     |
|    total_timesteps | 1441792  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.68e+03    |
|    ep_rew_mean          | -13.7       |
| time/                   |             |
|    fps                  | 467         |
|    iterations           | 177         |
|    time_elapsed         | 3104        |
|    total_timesteps      | 1449984     |
| train/                  |             |
|    approx_kl            | 0.009521683 |
|    clip_fraction        | 0.0291      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.963      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 5.95        |
|    n_updates            | 1760        |
|    policy_gradient_loss | -0.00209    |
|    value_loss           | 8.16        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.67e+03     |
|    ep_rew_mean          | -13.1        |
| time/                   |              |
|    fps                  | 467          |
|    iterations           | 178          |
|    time_elapsed         | 3117         |
|    total_timesteps      | 1458176      |
| train/                  |              |
|    approx_kl            | 0.0039874474 |
|    clip_fraction        | 0.00557      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.924       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 3.07         |
|    n_updates            | 1770         |
|    policy_gradient_loss | -0.000499    |
|    value_loss           | 9.55         |
------------------------------------------
Eval num_timesteps=1460000, episode_reward=-25.10 +/- 49.81
Episode length: 1287.40 +/- 96.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.29e+03     |
|    mean_reward          | -25.1        |
| time/                   |              |
|    total_timesteps      | 1460000      |
| train/                  |              |
|    approx_kl            | 0.0044506304 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.909       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.54         |
|    n_updates            | 1780         |
|    policy_gradient_loss | -0.000192    |
|    value_loss           | 7.77         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -13.4    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 179      |
|    time_elapsed    | 3136     |
|    total_timesteps | 1466368  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.67e+03     |
|    ep_rew_mean          | -14          |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 180          |
|    time_elapsed         | 3150         |
|    total_timesteps      | 1474560      |
| train/                  |              |
|    approx_kl            | 0.0041209585 |
|    clip_fraction        | 0.000818     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.863       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.01         |
|    n_updates            | 1790         |
|    policy_gradient_loss | -0.00026     |
|    value_loss           | 8.51         |
------------------------------------------
Eval num_timesteps=1480000, episode_reward=-24.30 +/- 12.87
Episode length: 1742.00 +/- 175.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.74e+03     |
|    mean_reward          | -24.3        |
| time/                   |              |
|    total_timesteps      | 1480000      |
| train/                  |              |
|    approx_kl            | 0.0043841307 |
|    clip_fraction        | 0.00588      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.869       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 5.04         |
|    n_updates            | 1800         |
|    policy_gradient_loss | -0.00046     |
|    value_loss           | 9.01         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -14.9    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 181      |
|    time_elapsed    | 3172     |
|    total_timesteps | 1482752  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.7e+03     |
|    ep_rew_mean          | -16.6       |
| time/                   |             |
|    fps                  | 467         |
|    iterations           | 182         |
|    time_elapsed         | 3186        |
|    total_timesteps      | 1490944     |
| train/                  |             |
|    approx_kl            | 0.009388708 |
|    clip_fraction        | 0.0202      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.933      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 2.52        |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.00171    |
|    value_loss           | 7.67        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.7e+03      |
|    ep_rew_mean          | -17          |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 183          |
|    time_elapsed         | 3201         |
|    total_timesteps      | 1499136      |
| train/                  |              |
|    approx_kl            | 0.0028992856 |
|    clip_fraction        | 0.0131       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.982       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.15         |
|    n_updates            | 1820         |
|    policy_gradient_loss | -0.00111     |
|    value_loss           | 8.74         |
------------------------------------------
Eval num_timesteps=1500000, episode_reward=-12.30 +/- 53.53
Episode length: 1570.20 +/- 278.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.57e+03    |
|    mean_reward          | -12.3       |
| time/                   |             |
|    total_timesteps      | 1500000     |
| train/                  |             |
|    approx_kl            | 0.009979261 |
|    clip_fraction        | 0.0169      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.949      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 2.27        |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.0016     |
|    value_loss           | 7.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -16.5    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 184      |
|    time_elapsed    | 3222     |
|    total_timesteps | 1507328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.71e+03    |
|    ep_rew_mean          | -15.5       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 185         |
|    time_elapsed         | 3236        |
|    total_timesteps      | 1515520     |
| train/                  |             |
|    approx_kl            | 0.010722528 |
|    clip_fraction        | 0.0227      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.861      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.25        |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.00158    |
|    value_loss           | 8.45        |
-----------------------------------------
Eval num_timesteps=1520000, episode_reward=-42.70 +/- 31.95
Episode length: 1425.60 +/- 259.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.43e+03     |
|    mean_reward          | -42.7        |
| time/                   |              |
|    total_timesteps      | 1520000      |
| train/                  |              |
|    approx_kl            | 0.0045348564 |
|    clip_fraction        | 0.00634      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.821       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.39         |
|    n_updates            | 1850         |
|    policy_gradient_loss | -0.000342    |
|    value_loss           | 9.21         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -15      |
| time/              |          |
|    fps             | 468      |
|    iterations      | 186      |
|    time_elapsed    | 3255     |
|    total_timesteps | 1523712  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.68e+03     |
|    ep_rew_mean          | -16.9        |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 187          |
|    time_elapsed         | 3269         |
|    total_timesteps      | 1531904      |
| train/                  |              |
|    approx_kl            | 0.0046548643 |
|    clip_fraction        | 0.00597      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.759       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.33         |
|    n_updates            | 1860         |
|    policy_gradient_loss | -0.000102    |
|    value_loss           | 8.47         |
------------------------------------------
Eval num_timesteps=1540000, episode_reward=-22.10 +/- 55.23
Episode length: 1539.20 +/- 336.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.54e+03    |
|    mean_reward          | -22.1       |
| time/                   |             |
|    total_timesteps      | 1540000     |
| train/                  |             |
|    approx_kl            | 0.004035428 |
|    clip_fraction        | 0.00812     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.704      |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 6.1         |
|    n_updates            | 1870        |
|    policy_gradient_loss | -0.00095    |
|    value_loss           | 9.47        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -15.7    |
| time/              |          |
|    fps             | 468      |
|    iterations      | 188      |
|    time_elapsed    | 3290     |
|    total_timesteps | 1540096  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.68e+03     |
|    ep_rew_mean          | -16.5        |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 189          |
|    time_elapsed         | 3305         |
|    total_timesteps      | 1548288      |
| train/                  |              |
|    approx_kl            | 0.0038865192 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.714       |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 4.26         |
|    n_updates            | 1880         |
|    policy_gradient_loss | -0.000983    |
|    value_loss           | 7.87         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.66e+03    |
|    ep_rew_mean          | -17.9       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 190         |
|    time_elapsed         | 3319        |
|    total_timesteps      | 1556480     |
| train/                  |             |
|    approx_kl            | 0.005491308 |
|    clip_fraction        | 0.0248      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.682      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.04        |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.00159    |
|    value_loss           | 8.8         |
-----------------------------------------
Eval num_timesteps=1560000, episode_reward=-26.20 +/- 31.40
Episode length: 1605.60 +/- 312.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.61e+03    |
|    mean_reward          | -26.2       |
| time/                   |             |
|    total_timesteps      | 1560000     |
| train/                  |             |
|    approx_kl            | 0.002974932 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.761      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 2.37        |
|    n_updates            | 1900        |
|    policy_gradient_loss | -0.000558   |
|    value_loss           | 8.62        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -16.7    |
| time/              |          |
|    fps             | 468      |
|    iterations      | 191      |
|    time_elapsed    | 3339     |
|    total_timesteps | 1564672  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.65e+03     |
|    ep_rew_mean          | -15.5        |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 192          |
|    time_elapsed         | 3353         |
|    total_timesteps      | 1572864      |
| train/                  |              |
|    approx_kl            | 0.0056924867 |
|    clip_fraction        | 0.0207       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.806       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.92         |
|    n_updates            | 1910         |
|    policy_gradient_loss | -0.00153     |
|    value_loss           | 9.25         |
------------------------------------------
Eval num_timesteps=1580000, episode_reward=12.30 +/- 34.99
Episode length: 1671.20 +/- 200.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.67e+03    |
|    mean_reward          | 12.3        |
| time/                   |             |
|    total_timesteps      | 1580000     |
| train/                  |             |
|    approx_kl            | 0.004877547 |
|    clip_fraction        | 0.0105      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.82       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.91        |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.000645   |
|    value_loss           | 7.99        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -15.8    |
| time/              |          |
|    fps             | 468      |
|    iterations      | 193      |
|    time_elapsed    | 3375     |
|    total_timesteps | 1581056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.64e+03    |
|    ep_rew_mean          | -15.5       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 194         |
|    time_elapsed         | 3389        |
|    total_timesteps      | 1589248     |
| train/                  |             |
|    approx_kl            | 0.004596507 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.829      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 4.35        |
|    n_updates            | 1930        |
|    policy_gradient_loss | -0.000226   |
|    value_loss           | 7.84        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.65e+03     |
|    ep_rew_mean          | -15.2        |
| time/                   |              |
|    fps                  | 469          |
|    iterations           | 195          |
|    time_elapsed         | 3404         |
|    total_timesteps      | 1597440      |
| train/                  |              |
|    approx_kl            | 0.0043153143 |
|    clip_fraction        | 0.0171       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.781       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.18         |
|    n_updates            | 1940         |
|    policy_gradient_loss | -0.000646    |
|    value_loss           | 8.37         |
------------------------------------------
Eval num_timesteps=1600000, episode_reward=-1.60 +/- 31.31
Episode length: 1826.80 +/- 365.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.83e+03     |
|    mean_reward          | -1.6         |
| time/                   |              |
|    total_timesteps      | 1600000      |
| train/                  |              |
|    approx_kl            | 0.0063598175 |
|    clip_fraction        | 0.00167      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.82        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.81         |
|    n_updates            | 1950         |
|    policy_gradient_loss | -0.000396    |
|    value_loss           | 8.7          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -16.3    |
| time/              |          |
|    fps             | 468      |
|    iterations      | 196      |
|    time_elapsed    | 3427     |
|    total_timesteps | 1605632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.63e+03    |
|    ep_rew_mean          | -15.6       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 197         |
|    time_elapsed         | 3442        |
|    total_timesteps      | 1613824     |
| train/                  |             |
|    approx_kl            | 0.003292207 |
|    clip_fraction        | 0.00415     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.857      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.96        |
|    n_updates            | 1960        |
|    policy_gradient_loss | 0.000152    |
|    value_loss           | 8.3         |
-----------------------------------------
Eval num_timesteps=1620000, episode_reward=2.20 +/- 46.44
Episode length: 1725.00 +/- 515.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.72e+03    |
|    mean_reward          | 2.2         |
| time/                   |             |
|    total_timesteps      | 1620000     |
| train/                  |             |
|    approx_kl            | 0.008813955 |
|    clip_fraction        | 0.0014      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.841      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.97        |
|    n_updates            | 1970        |
|    policy_gradient_loss | -0.000472   |
|    value_loss           | 8.44        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | -17.5    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 198      |
|    time_elapsed    | 3466     |
|    total_timesteps | 1622016  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.61e+03     |
|    ep_rew_mean          | -18.9        |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 199          |
|    time_elapsed         | 3481         |
|    total_timesteps      | 1630208      |
| train/                  |              |
|    approx_kl            | 0.0011910971 |
|    clip_fraction        | 0.00372      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.854       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.6          |
|    n_updates            | 1980         |
|    policy_gradient_loss | -4.77e-05    |
|    value_loss           | 8.24         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.62e+03     |
|    ep_rew_mean          | -19          |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 200          |
|    time_elapsed         | 3498         |
|    total_timesteps      | 1638400      |
| train/                  |              |
|    approx_kl            | 0.0014862169 |
|    clip_fraction        | 0.00936      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.882       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 3.75         |
|    n_updates            | 1990         |
|    policy_gradient_loss | -0.000649    |
|    value_loss           | 8.83         |
------------------------------------------
Eval num_timesteps=1640000, episode_reward=-34.10 +/- 38.64
Episode length: 1614.60 +/- 505.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.61e+03    |
|    mean_reward          | -34.1       |
| time/                   |             |
|    total_timesteps      | 1640000     |
| train/                  |             |
|    approx_kl            | 0.010267237 |
|    clip_fraction        | 0.023       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.78       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.66        |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.00128    |
|    value_loss           | 7.69        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | -18.8    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 201      |
|    time_elapsed    | 3520     |
|    total_timesteps | 1646592  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.61e+03     |
|    ep_rew_mean          | -16.7        |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 202          |
|    time_elapsed         | 3534         |
|    total_timesteps      | 1654784      |
| train/                  |              |
|    approx_kl            | 0.0033698012 |
|    clip_fraction        | 0.00784      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.819       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 5.09         |
|    n_updates            | 2010         |
|    policy_gradient_loss | -0.000486    |
|    value_loss           | 8.7          |
------------------------------------------
Eval num_timesteps=1660000, episode_reward=-52.10 +/- 33.46
Episode length: 1528.80 +/- 152.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.53e+03     |
|    mean_reward          | -52.1        |
| time/                   |              |
|    total_timesteps      | 1660000      |
| train/                  |              |
|    approx_kl            | 0.0067926785 |
|    clip_fraction        | 0.015        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.759       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.68         |
|    n_updates            | 2020         |
|    policy_gradient_loss | -0.000964    |
|    value_loss           | 7.72         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -17      |
| time/              |          |
|    fps             | 467      |
|    iterations      | 203      |
|    time_elapsed    | 3555     |
|    total_timesteps | 1662976  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.61e+03      |
|    ep_rew_mean          | -17.1         |
| time/                   |               |
|    fps                  | 468           |
|    iterations           | 204           |
|    time_elapsed         | 3570          |
|    total_timesteps      | 1671168       |
| train/                  |               |
|    approx_kl            | 3.0244984e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.732        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.94          |
|    n_updates            | 2030          |
|    policy_gradient_loss | 0.000175      |
|    value_loss           | 8.02          |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.62e+03    |
|    ep_rew_mean          | -15.9       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 205         |
|    time_elapsed         | 3584        |
|    total_timesteps      | 1679360     |
| train/                  |             |
|    approx_kl            | 0.004489001 |
|    clip_fraction        | 0.0075      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.741      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.91        |
|    n_updates            | 2040        |
|    policy_gradient_loss | -0.000437   |
|    value_loss           | 8.38        |
-----------------------------------------
Eval num_timesteps=1680000, episode_reward=14.50 +/- 47.28
Episode length: 1723.40 +/- 362.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.72e+03     |
|    mean_reward          | 14.5         |
| time/                   |              |
|    total_timesteps      | 1680000      |
| train/                  |              |
|    approx_kl            | 0.0053958097 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.784       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.15         |
|    n_updates            | 2050         |
|    policy_gradient_loss | -0.000285    |
|    value_loss           | 8.37         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -14.9    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 206      |
|    time_elapsed    | 3606     |
|    total_timesteps | 1687552  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.63e+03    |
|    ep_rew_mean          | -14.8       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 207         |
|    time_elapsed         | 3620        |
|    total_timesteps      | 1695744     |
| train/                  |             |
|    approx_kl            | 0.003824993 |
|    clip_fraction        | 0.00662     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.682      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.78        |
|    n_updates            | 2060        |
|    policy_gradient_loss | 3.54e-05    |
|    value_loss           | 9.8         |
-----------------------------------------
Eval num_timesteps=1700000, episode_reward=-14.30 +/- 30.43
Episode length: 1689.00 +/- 256.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.69e+03     |
|    mean_reward          | -14.3        |
| time/                   |              |
|    total_timesteps      | 1700000      |
| train/                  |              |
|    approx_kl            | 0.0016644546 |
|    clip_fraction        | 0.00552      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.738       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.3          |
|    n_updates            | 2070         |
|    policy_gradient_loss | -8.39e-05    |
|    value_loss           | 7.57         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -15.2    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 208      |
|    time_elapsed    | 3642     |
|    total_timesteps | 1703936  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.63e+03    |
|    ep_rew_mean          | -16.4       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 209         |
|    time_elapsed         | 3657        |
|    total_timesteps      | 1712128     |
| train/                  |             |
|    approx_kl            | 0.003602793 |
|    clip_fraction        | 0.0116      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.668      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 5.61        |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.000747   |
|    value_loss           | 8.89        |
-----------------------------------------
Eval num_timesteps=1720000, episode_reward=9.70 +/- 25.85
Episode length: 1732.40 +/- 348.09
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.73e+03   |
|    mean_reward          | 9.7        |
| time/                   |            |
|    total_timesteps      | 1720000    |
| train/                  |            |
|    approx_kl            | 0.00499421 |
|    clip_fraction        | 0.0153     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.587     |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 4.21       |
|    n_updates            | 2090       |
|    policy_gradient_loss | -0.00082   |
|    value_loss           | 8.42       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -13.9    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 210      |
|    time_elapsed    | 3679     |
|    total_timesteps | 1720320  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.64e+03    |
|    ep_rew_mean          | -13.6       |
| time/                   |             |
|    fps                  | 467         |
|    iterations           | 211         |
|    time_elapsed         | 3693        |
|    total_timesteps      | 1728512     |
| train/                  |             |
|    approx_kl            | 0.002778926 |
|    clip_fraction        | 0.0236      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.628      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.28        |
|    n_updates            | 2100        |
|    policy_gradient_loss | -0.00139    |
|    value_loss           | 9.35        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.63e+03     |
|    ep_rew_mean          | -15.6        |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 212          |
|    time_elapsed         | 3707         |
|    total_timesteps      | 1736704      |
| train/                  |              |
|    approx_kl            | 0.0031991315 |
|    clip_fraction        | 0.00903      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.676       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.92         |
|    n_updates            | 2110         |
|    policy_gradient_loss | -0.000549    |
|    value_loss           | 9.8          |
------------------------------------------
Eval num_timesteps=1740000, episode_reward=18.10 +/- 50.46
Episode length: 1602.40 +/- 373.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.6e+03      |
|    mean_reward          | 18.1         |
| time/                   |              |
|    total_timesteps      | 1740000      |
| train/                  |              |
|    approx_kl            | 0.0023087244 |
|    clip_fraction        | 0.00905      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.76        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.88         |
|    n_updates            | 2120         |
|    policy_gradient_loss | -0.000319    |
|    value_loss           | 8.43         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -16.9    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 213      |
|    time_elapsed    | 3729     |
|    total_timesteps | 1744896  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.62e+03     |
|    ep_rew_mean          | -16.9        |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 214          |
|    time_elapsed         | 3744         |
|    total_timesteps      | 1753088      |
| train/                  |              |
|    approx_kl            | 0.0067867544 |
|    clip_fraction        | 0.0207       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.664       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.73         |
|    n_updates            | 2130         |
|    policy_gradient_loss | -0.00128     |
|    value_loss           | 8.81         |
------------------------------------------
Eval num_timesteps=1760000, episode_reward=-1.30 +/- 38.62
Episode length: 1787.60 +/- 240.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.79e+03     |
|    mean_reward          | -1.3         |
| time/                   |              |
|    total_timesteps      | 1760000      |
| train/                  |              |
|    approx_kl            | 0.0022317837 |
|    clip_fraction        | 0.016        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.584       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.99         |
|    n_updates            | 2140         |
|    policy_gradient_loss | -0.000629    |
|    value_loss           | 8.96         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.6e+03  |
|    ep_rew_mean     | -17.1    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 215      |
|    time_elapsed    | 3765     |
|    total_timesteps | 1761280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.6e+03     |
|    ep_rew_mean          | -16.7       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 216         |
|    time_elapsed         | 3778        |
|    total_timesteps      | 1769472     |
| train/                  |             |
|    approx_kl            | 0.002124365 |
|    clip_fraction        | 0.00737     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.629      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.38        |
|    n_updates            | 2150        |
|    policy_gradient_loss | -0.000422   |
|    value_loss           | 9.49        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.6e+03      |
|    ep_rew_mean          | -15.9        |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 217          |
|    time_elapsed         | 3792         |
|    total_timesteps      | 1777664      |
| train/                  |              |
|    approx_kl            | 0.0009033113 |
|    clip_fraction        | 0.0024       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.61        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.25         |
|    n_updates            | 2160         |
|    policy_gradient_loss | 9.04e-05     |
|    value_loss           | 9.12         |
------------------------------------------
Eval num_timesteps=1780000, episode_reward=5.20 +/- 30.67
Episode length: 1624.80 +/- 248.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.62e+03     |
|    mean_reward          | 5.2          |
| time/                   |              |
|    total_timesteps      | 1780000      |
| train/                  |              |
|    approx_kl            | 0.0029739132 |
|    clip_fraction        | 0.00237      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.676       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.16         |
|    n_updates            | 2170         |
|    policy_gradient_loss | -5.83e-05    |
|    value_loss           | 9.4          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.6e+03  |
|    ep_rew_mean     | -14.4    |
| time/              |          |
|    fps             | 468      |
|    iterations      | 218      |
|    time_elapsed    | 3814     |
|    total_timesteps | 1785856  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.63e+03     |
|    ep_rew_mean          | -11.2        |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 219          |
|    time_elapsed         | 3828         |
|    total_timesteps      | 1794048      |
| train/                  |              |
|    approx_kl            | 0.0018075325 |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.631       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.49         |
|    n_updates            | 2180         |
|    policy_gradient_loss | -0.000678    |
|    value_loss           | 8.21         |
------------------------------------------
Eval num_timesteps=1800000, episode_reward=-8.80 +/- 32.66
Episode length: 1531.20 +/- 146.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.53e+03      |
|    mean_reward          | -8.8          |
| time/                   |               |
|    total_timesteps      | 1800000       |
| train/                  |               |
|    approx_kl            | 0.00097435835 |
|    clip_fraction        | 0.00155       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.629        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.41          |
|    n_updates            | 2190          |
|    policy_gradient_loss | 6.1e-05       |
|    value_loss           | 7.56          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -9.49    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 220      |
|    time_elapsed    | 3851     |
|    total_timesteps | 1802240  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.63e+03     |
|    ep_rew_mean          | -8.54        |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 221          |
|    time_elapsed         | 3865         |
|    total_timesteps      | 1810432      |
| train/                  |              |
|    approx_kl            | 0.0031464845 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.73        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 5.63         |
|    n_updates            | 2200         |
|    policy_gradient_loss | -0.000611    |
|    value_loss           | 9.36         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.65e+03    |
|    ep_rew_mean          | -9.27       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 222         |
|    time_elapsed         | 3879        |
|    total_timesteps      | 1818624     |
| train/                  |             |
|    approx_kl            | 0.004204342 |
|    clip_fraction        | 0.00454     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.8        |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 3.12        |
|    n_updates            | 2210        |
|    policy_gradient_loss | -0.000382   |
|    value_loss           | 7.11        |
-----------------------------------------
Eval num_timesteps=1820000, episode_reward=-10.00 +/- 35.87
Episode length: 1653.20 +/- 198.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.65e+03     |
|    mean_reward          | -10          |
| time/                   |              |
|    total_timesteps      | 1820000      |
| train/                  |              |
|    approx_kl            | 0.0025806543 |
|    clip_fraction        | 0.0115       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.78        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.83         |
|    n_updates            | 2220         |
|    policy_gradient_loss | -0.000828    |
|    value_loss           | 7.96         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -9.71    |
| time/              |          |
|    fps             | 468      |
|    iterations      | 223      |
|    time_elapsed    | 3901     |
|    total_timesteps | 1826816  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.66e+03    |
|    ep_rew_mean          | -7.54       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 224         |
|    time_elapsed         | 3915        |
|    total_timesteps      | 1835008     |
| train/                  |             |
|    approx_kl            | 0.007379456 |
|    clip_fraction        | 0.047       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.857      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.91        |
|    n_updates            | 2230        |
|    policy_gradient_loss | -0.00288    |
|    value_loss           | 8.01        |
-----------------------------------------
Eval num_timesteps=1840000, episode_reward=9.30 +/- 37.38
Episode length: 1525.00 +/- 156.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.52e+03    |
|    mean_reward          | 9.3         |
| time/                   |             |
|    total_timesteps      | 1840000     |
| train/                  |             |
|    approx_kl            | 0.006222382 |
|    clip_fraction        | 0.00955     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.879      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 2.11        |
|    n_updates            | 2240        |
|    policy_gradient_loss | -0.000955   |
|    value_loss           | 8.59        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -7.84    |
| time/              |          |
|    fps             | 468      |
|    iterations      | 225      |
|    time_elapsed    | 3935     |
|    total_timesteps | 1843200  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.65e+03    |
|    ep_rew_mean          | -8.08       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 226         |
|    time_elapsed         | 3950        |
|    total_timesteps      | 1851392     |
| train/                  |             |
|    approx_kl            | 0.006862688 |
|    clip_fraction        | 0.00415     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.868      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.67        |
|    n_updates            | 2250        |
|    policy_gradient_loss | -0.000481   |
|    value_loss           | 8.26        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.67e+03    |
|    ep_rew_mean          | -6.6        |
| time/                   |             |
|    fps                  | 469         |
|    iterations           | 227         |
|    time_elapsed         | 3964        |
|    total_timesteps      | 1859584     |
| train/                  |             |
|    approx_kl            | 0.011882706 |
|    clip_fraction        | 0.00876     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.853      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.2         |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.000829   |
|    value_loss           | 8.92        |
-----------------------------------------
Eval num_timesteps=1860000, episode_reward=-48.00 +/- 43.21
Episode length: 1508.20 +/- 418.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.51e+03    |
|    mean_reward          | -48         |
| time/                   |             |
|    total_timesteps      | 1860000     |
| train/                  |             |
|    approx_kl            | 0.008251897 |
|    clip_fraction        | 0.0122      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.821      |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 5.11        |
|    n_updates            | 2270        |
|    policy_gradient_loss | -0.00079    |
|    value_loss           | 7.77        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -5.99    |
| time/              |          |
|    fps             | 468      |
|    iterations      | 228      |
|    time_elapsed    | 3986     |
|    total_timesteps | 1867776  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.68e+03    |
|    ep_rew_mean          | -6.66       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 229         |
|    time_elapsed         | 4000        |
|    total_timesteps      | 1875968     |
| train/                  |             |
|    approx_kl            | 0.006280481 |
|    clip_fraction        | 0.00885     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.825      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.68        |
|    n_updates            | 2280        |
|    policy_gradient_loss | -0.00063    |
|    value_loss           | 7.67        |
-----------------------------------------
Eval num_timesteps=1880000, episode_reward=17.80 +/- 43.99
Episode length: 1938.40 +/- 495.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.94e+03    |
|    mean_reward          | 17.8        |
| time/                   |             |
|    total_timesteps      | 1880000     |
| train/                  |             |
|    approx_kl            | 0.005701129 |
|    clip_fraction        | 0.00302     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.788      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.1         |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.000305   |
|    value_loss           | 8.11        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -5.61    |
| time/              |          |
|    fps             | 468      |
|    iterations      | 230      |
|    time_elapsed    | 4021     |
|    total_timesteps | 1884160  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.69e+03     |
|    ep_rew_mean          | -4.75        |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 231          |
|    time_elapsed         | 4035         |
|    total_timesteps      | 1892352      |
| train/                  |              |
|    approx_kl            | 0.0068110595 |
|    clip_fraction        | 0.00411      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.78        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 4.15         |
|    n_updates            | 2300         |
|    policy_gradient_loss | -0.00024     |
|    value_loss           | 7.83         |
------------------------------------------
Eval num_timesteps=1900000, episode_reward=-18.60 +/- 30.31
Episode length: 1579.40 +/- 141.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.58e+03    |
|    mean_reward          | -18.6       |
| time/                   |             |
|    total_timesteps      | 1900000     |
| train/                  |             |
|    approx_kl            | 0.008067494 |
|    clip_fraction        | 0.0271      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.798      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 5.6         |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.00189    |
|    value_loss           | 8.71        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -6.21    |
| time/              |          |
|    fps             | 468      |
|    iterations      | 232      |
|    time_elapsed    | 4057     |
|    total_timesteps | 1900544  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.68e+03     |
|    ep_rew_mean          | -3.88        |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 233          |
|    time_elapsed         | 4075         |
|    total_timesteps      | 1908736      |
| train/                  |              |
|    approx_kl            | 0.0017003568 |
|    clip_fraction        | 0.00415      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.829       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.73         |
|    n_updates            | 2320         |
|    policy_gradient_loss | -0.000119    |
|    value_loss           | 8.64         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.68e+03     |
|    ep_rew_mean          | -3.72        |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 234          |
|    time_elapsed         | 4089         |
|    total_timesteps      | 1916928      |
| train/                  |              |
|    approx_kl            | 0.0023566438 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.835       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.06         |
|    n_updates            | 2330         |
|    policy_gradient_loss | -0.000318    |
|    value_loss           | 8.72         |
------------------------------------------
Eval num_timesteps=1920000, episode_reward=-23.00 +/- 26.01
Episode length: 1486.00 +/- 152.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.49e+03    |
|    mean_reward          | -23         |
| time/                   |             |
|    total_timesteps      | 1920000     |
| train/                  |             |
|    approx_kl            | 0.002583065 |
|    clip_fraction        | 0.00217     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.835      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.59        |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.000324   |
|    value_loss           | 8.4         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -3.48    |
| time/              |          |
|    fps             | 468      |
|    iterations      | 235      |
|    time_elapsed    | 4110     |
|    total_timesteps | 1925120  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.72e+03     |
|    ep_rew_mean          | -3           |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 236          |
|    time_elapsed         | 4125         |
|    total_timesteps      | 1933312      |
| train/                  |              |
|    approx_kl            | 0.0036581587 |
|    clip_fraction        | 0.019        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.832       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.72         |
|    n_updates            | 2350         |
|    policy_gradient_loss | -0.00128     |
|    value_loss           | 7.18         |
------------------------------------------
Eval num_timesteps=1940000, episode_reward=-12.20 +/- 25.43
Episode length: 1695.40 +/- 161.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.7e+03      |
|    mean_reward          | -12.2        |
| time/                   |              |
|    total_timesteps      | 1940000      |
| train/                  |              |
|    approx_kl            | 0.0025467046 |
|    clip_fraction        | 0.00999      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.855       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.16         |
|    n_updates            | 2360         |
|    policy_gradient_loss | -0.000639    |
|    value_loss           | 8.37         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.72e+03 |
|    ep_rew_mean     | -1.49    |
| time/              |          |
|    fps             | 468      |
|    iterations      | 237      |
|    time_elapsed    | 4147     |
|    total_timesteps | 1941504  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.71e+03     |
|    ep_rew_mean          | -2.55        |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 238          |
|    time_elapsed         | 4164         |
|    total_timesteps      | 1949696      |
| train/                  |              |
|    approx_kl            | 0.0036194348 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.836       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 3.06         |
|    n_updates            | 2370         |
|    policy_gradient_loss | 1.59e-05     |
|    value_loss           | 9.43         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.72e+03    |
|    ep_rew_mean          | -2.31       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 239         |
|    time_elapsed         | 4177        |
|    total_timesteps      | 1957888     |
| train/                  |             |
|    approx_kl            | 0.005317121 |
|    clip_fraction        | 0.0256      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.862      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.16        |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.00153    |
|    value_loss           | 7.73        |
-----------------------------------------
Eval num_timesteps=1960000, episode_reward=-5.90 +/- 24.05
Episode length: 1782.40 +/- 244.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.78e+03    |
|    mean_reward          | -5.9        |
| time/                   |             |
|    total_timesteps      | 1960000     |
| train/                  |             |
|    approx_kl            | 0.004977696 |
|    clip_fraction        | 0.00717     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.918      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 6.77        |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.000391   |
|    value_loss           | 8.37        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.71e+03 |
|    ep_rew_mean     | -4.75    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 240      |
|    time_elapsed    | 4202     |
|    total_timesteps | 1966080  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.71e+03    |
|    ep_rew_mean          | -5.34       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 241         |
|    time_elapsed         | 4216        |
|    total_timesteps      | 1974272     |
| train/                  |             |
|    approx_kl            | 0.005559028 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.959      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.33        |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.000146   |
|    value_loss           | 9.29        |
-----------------------------------------
Eval num_timesteps=1980000, episode_reward=-27.40 +/- 27.66
Episode length: 1778.00 +/- 143.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.78e+03    |
|    mean_reward          | -27.4       |
| time/                   |             |
|    total_timesteps      | 1980000     |
| train/                  |             |
|    approx_kl            | 0.009101836 |
|    clip_fraction        | 0.0223      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.97       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 6.39        |
|    n_updates            | 2410        |
|    policy_gradient_loss | -0.00165    |
|    value_loss           | 8.53        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.71e+03 |
|    ep_rew_mean     | -5.42    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 242      |
|    time_elapsed    | 4239     |
|    total_timesteps | 1982464  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.68e+03     |
|    ep_rew_mean          | -5.6         |
| time/                   |              |
|    fps                  | 467          |
|    iterations           | 243          |
|    time_elapsed         | 4254         |
|    total_timesteps      | 1990656      |
| train/                  |              |
|    approx_kl            | 0.0060484125 |
|    clip_fraction        | 0.00723      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.998       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.78         |
|    n_updates            | 2420         |
|    policy_gradient_loss | -0.000486    |
|    value_loss           | 8.58         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.66e+03     |
|    ep_rew_mean          | -5.75        |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 244          |
|    time_elapsed         | 4268         |
|    total_timesteps      | 1998848      |
| train/                  |              |
|    approx_kl            | 7.227113e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.02        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.92         |
|    n_updates            | 2430         |
|    policy_gradient_loss | 0.000185     |
|    value_loss           | 8.81         |
------------------------------------------
Eval num_timesteps=2000000, episode_reward=6.60 +/- 40.21
Episode length: 1437.00 +/- 306.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.44e+03    |
|    mean_reward          | 6.6         |
| time/                   |             |
|    total_timesteps      | 2000000     |
| train/                  |             |
|    approx_kl            | 0.014339829 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.74        |
|    n_updates            | 2440        |
|    policy_gradient_loss | -0.000611   |
|    value_loss           | 9.16        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -7.1     |
| time/              |          |
|    fps             | 467      |
|    iterations      | 245      |
|    time_elapsed    | 4290     |
|    total_timesteps | 2007040  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.67e+03  |
|    ep_rew_mean          | -6.68     |
| time/                   |           |
|    fps                  | 468       |
|    iterations           | 246       |
|    time_elapsed         | 4304      |
|    total_timesteps      | 2015232   |
| train/                  |           |
|    approx_kl            | 0.0156275 |
|    clip_fraction        | 0.0513    |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.03     |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 4.53      |
|    n_updates            | 2450      |
|    policy_gradient_loss | -0.00354  |
|    value_loss           | 8.28      |
---------------------------------------
Eval num_timesteps=2020000, episode_reward=1.20 +/- 32.61
Episode length: 1941.20 +/- 270.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.94e+03    |
|    mean_reward          | 1.2         |
| time/                   |             |
|    total_timesteps      | 2020000     |
| train/                  |             |
|    approx_kl            | 0.007928689 |
|    clip_fraction        | 0.00823     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.995      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 6.19        |
|    n_updates            | 2460        |
|    policy_gradient_loss | -0.000935   |
|    value_loss           | 8.48        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -6.29    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 247      |
|    time_elapsed    | 4328     |
|    total_timesteps | 2023424  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.66e+03    |
|    ep_rew_mean          | -7.63       |
| time/                   |             |
|    fps                  | 467         |
|    iterations           | 248         |
|    time_elapsed         | 4342        |
|    total_timesteps      | 2031616     |
| train/                  |             |
|    approx_kl            | 0.008010073 |
|    clip_fraction        | 0.00232     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.29        |
|    n_updates            | 2470        |
|    policy_gradient_loss | -0.000576   |
|    value_loss           | 8.61        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.65e+03    |
|    ep_rew_mean          | -8.58       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 249         |
|    time_elapsed         | 4356        |
|    total_timesteps      | 2039808     |
| train/                  |             |
|    approx_kl            | 0.007863758 |
|    clip_fraction        | 0.016       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.25        |
|    n_updates            | 2480        |
|    policy_gradient_loss | -0.00117    |
|    value_loss           | 8.53        |
-----------------------------------------
Eval num_timesteps=2040000, episode_reward=-26.50 +/- 36.06
Episode length: 1507.00 +/- 309.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.51e+03    |
|    mean_reward          | -26.5       |
| time/                   |             |
|    total_timesteps      | 2040000     |
| train/                  |             |
|    approx_kl            | 0.008679595 |
|    clip_fraction        | 0.00851     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.996      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.25        |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.000811   |
|    value_loss           | 9.65        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -8.91    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 250      |
|    time_elapsed    | 4378     |
|    total_timesteps | 2048000  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.64e+03    |
|    ep_rew_mean          | -10.6       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 251         |
|    time_elapsed         | 4393        |
|    total_timesteps      | 2056192     |
| train/                  |             |
|    approx_kl            | 0.011764355 |
|    clip_fraction        | 0.00854     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.55        |
|    n_updates            | 2500        |
|    policy_gradient_loss | -0.0015     |
|    value_loss           | 8.42        |
-----------------------------------------
Eval num_timesteps=2060000, episode_reward=-12.60 +/- 24.17
Episode length: 1686.20 +/- 291.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.69e+03    |
|    mean_reward          | -12.6       |
| time/                   |             |
|    total_timesteps      | 2060000     |
| train/                  |             |
|    approx_kl            | 0.003714084 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.49        |
|    n_updates            | 2510        |
|    policy_gradient_loss | -0.000209   |
|    value_loss           | 8.68        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -11.8    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 252      |
|    time_elapsed    | 4417     |
|    total_timesteps | 2064384  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.63e+03    |
|    ep_rew_mean          | -10.7       |
| time/                   |             |
|    fps                  | 467         |
|    iterations           | 253         |
|    time_elapsed         | 4431        |
|    total_timesteps      | 2072576     |
| train/                  |             |
|    approx_kl            | 0.005046226 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.998      |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 4.35        |
|    n_updates            | 2520        |
|    policy_gradient_loss | -0.000403   |
|    value_loss           | 8.31        |
-----------------------------------------
Eval num_timesteps=2080000, episode_reward=-36.00 +/- 26.90
Episode length: 1629.80 +/- 406.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.63e+03     |
|    mean_reward          | -36          |
| time/                   |              |
|    total_timesteps      | 2080000      |
| train/                  |              |
|    approx_kl            | 0.0015077483 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.982       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.98         |
|    n_updates            | 2530         |
|    policy_gradient_loss | 9.45e-05     |
|    value_loss           | 9.2          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -13.5    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 254      |
|    time_elapsed    | 4453     |
|    total_timesteps | 2080768  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.61e+03     |
|    ep_rew_mean          | -15.1        |
| time/                   |              |
|    fps                  | 467          |
|    iterations           | 255          |
|    time_elapsed         | 4467         |
|    total_timesteps      | 2088960      |
| train/                  |              |
|    approx_kl            | 0.0058322307 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.998       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.73         |
|    n_updates            | 2540         |
|    policy_gradient_loss | -0.000423    |
|    value_loss           | 8.76         |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.59e+03   |
|    ep_rew_mean          | -14.7      |
| time/                   |            |
|    fps                  | 468        |
|    iterations           | 256        |
|    time_elapsed         | 4480       |
|    total_timesteps      | 2097152    |
| train/                  |            |
|    approx_kl            | 0.00452116 |
|    clip_fraction        | 0          |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.968     |
|    explained_variance   | 1.19e-07   |
|    learning_rate        | 0.0003     |
|    loss                 | 3.61       |
|    n_updates            | 2550       |
|    policy_gradient_loss | -0.000285  |
|    value_loss           | 10         |
----------------------------------------
Eval num_timesteps=2100000, episode_reward=-2.30 +/- 45.35
Episode length: 1618.80 +/- 395.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.62e+03     |
|    mean_reward          | -2.3         |
| time/                   |              |
|    total_timesteps      | 2100000      |
| train/                  |              |
|    approx_kl            | 0.0050919764 |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.936       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.04         |
|    n_updates            | 2560         |
|    policy_gradient_loss | -0.000419    |
|    value_loss           | 8.17         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.58e+03 |
|    ep_rew_mean     | -14.5    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 257      |
|    time_elapsed    | 4503     |
|    total_timesteps | 2105344  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.59e+03    |
|    ep_rew_mean          | -15.1       |
| time/                   |             |
|    fps                  | 467         |
|    iterations           | 258         |
|    time_elapsed         | 4517        |
|    total_timesteps      | 2113536     |
| train/                  |             |
|    approx_kl            | 0.006154032 |
|    clip_fraction        | 0.0193      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.938      |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 4.08        |
|    n_updates            | 2570        |
|    policy_gradient_loss | -0.000895   |
|    value_loss           | 7.96        |
-----------------------------------------
Eval num_timesteps=2120000, episode_reward=-10.20 +/- 53.01
Episode length: 1572.60 +/- 106.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.57e+03     |
|    mean_reward          | -10.2        |
| time/                   |              |
|    total_timesteps      | 2120000      |
| train/                  |              |
|    approx_kl            | 0.0026874822 |
|    clip_fraction        | 0.00759      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.917       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 2.27         |
|    n_updates            | 2580         |
|    policy_gradient_loss | -0.000397    |
|    value_loss           | 8.4          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.6e+03  |
|    ep_rew_mean     | -16.8    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 259      |
|    time_elapsed    | 4538     |
|    total_timesteps | 2121728  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.6e+03     |
|    ep_rew_mean          | -16         |
| time/                   |             |
|    fps                  | 467         |
|    iterations           | 260         |
|    time_elapsed         | 4552        |
|    total_timesteps      | 2129920     |
| train/                  |             |
|    approx_kl            | 0.007251709 |
|    clip_fraction        | 0.00542     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.911      |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 4.98        |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.000519   |
|    value_loss           | 8.02        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.59e+03    |
|    ep_rew_mean          | -15.7       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 261         |
|    time_elapsed         | 4567        |
|    total_timesteps      | 2138112     |
| train/                  |             |
|    approx_kl            | 0.011783823 |
|    clip_fraction        | 0.000623    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.878      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.55        |
|    n_updates            | 2600        |
|    policy_gradient_loss | -0.000454   |
|    value_loss           | 9.15        |
-----------------------------------------
Eval num_timesteps=2140000, episode_reward=-11.10 +/- 14.15
Episode length: 2024.00 +/- 242.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.02e+03     |
|    mean_reward          | -11.1        |
| time/                   |              |
|    total_timesteps      | 2140000      |
| train/                  |              |
|    approx_kl            | 0.0057624327 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.812       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 4.41         |
|    n_updates            | 2610         |
|    policy_gradient_loss | -0.000144    |
|    value_loss           | 9.37         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.58e+03 |
|    ep_rew_mean     | -16.3    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 262      |
|    time_elapsed    | 4591     |
|    total_timesteps | 2146304  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.59e+03     |
|    ep_rew_mean          | -15.1        |
| time/                   |              |
|    fps                  | 467          |
|    iterations           | 263          |
|    time_elapsed         | 4605         |
|    total_timesteps      | 2154496      |
| train/                  |              |
|    approx_kl            | 0.0067252726 |
|    clip_fraction        | 0.0222       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.739       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.48         |
|    n_updates            | 2620         |
|    policy_gradient_loss | -0.00166     |
|    value_loss           | 8.74         |
------------------------------------------
Eval num_timesteps=2160000, episode_reward=-10.00 +/- 50.01
Episode length: 1541.60 +/- 251.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.54e+03     |
|    mean_reward          | -10          |
| time/                   |              |
|    total_timesteps      | 2160000      |
| train/                  |              |
|    approx_kl            | 0.0048319213 |
|    clip_fraction        | 0.00854      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.685       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.7          |
|    n_updates            | 2630         |
|    policy_gradient_loss | -0.000383    |
|    value_loss           | 8.08         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | -14.3    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 264      |
|    time_elapsed    | 4628     |
|    total_timesteps | 2162688  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.59e+03     |
|    ep_rew_mean          | -13          |
| time/                   |              |
|    fps                  | 467          |
|    iterations           | 265          |
|    time_elapsed         | 4641         |
|    total_timesteps      | 2170880      |
| train/                  |              |
|    approx_kl            | 0.0014628706 |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.651       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.04         |
|    n_updates            | 2640         |
|    policy_gradient_loss | -0.000567    |
|    value_loss           | 9.18         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.6e+03     |
|    ep_rew_mean          | -12.2       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 266         |
|    time_elapsed         | 4655        |
|    total_timesteps      | 2179072     |
| train/                  |             |
|    approx_kl            | 0.004822217 |
|    clip_fraction        | 0.0407      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.63       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 5.91        |
|    n_updates            | 2650        |
|    policy_gradient_loss | -0.00289    |
|    value_loss           | 8.29        |
-----------------------------------------
Eval num_timesteps=2180000, episode_reward=-31.50 +/- 5.68
Episode length: 1705.60 +/- 410.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.71e+03    |
|    mean_reward          | -31.5       |
| time/                   |             |
|    total_timesteps      | 2180000     |
| train/                  |             |
|    approx_kl            | 0.004325975 |
|    clip_fraction        | 0.0153      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.582      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.46        |
|    n_updates            | 2660        |
|    policy_gradient_loss | -0.00104    |
|    value_loss           | 8.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.6e+03  |
|    ep_rew_mean     | -11.9    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 267      |
|    time_elapsed    | 4677     |
|    total_timesteps | 2187264  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.6e+03      |
|    ep_rew_mean          | -11.3        |
| time/                   |              |
|    fps                  | 467          |
|    iterations           | 268          |
|    time_elapsed         | 4691         |
|    total_timesteps      | 2195456      |
| train/                  |              |
|    approx_kl            | 0.0020099338 |
|    clip_fraction        | 0.00536      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.605       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.91         |
|    n_updates            | 2670         |
|    policy_gradient_loss | -0.000151    |
|    value_loss           | 9.32         |
------------------------------------------
Eval num_timesteps=2200000, episode_reward=-5.80 +/- 39.95
Episode length: 1636.40 +/- 266.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.64e+03     |
|    mean_reward          | -5.8         |
| time/                   |              |
|    total_timesteps      | 2200000      |
| train/                  |              |
|    approx_kl            | 0.0027591868 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.582       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5            |
|    n_updates            | 2680         |
|    policy_gradient_loss | -7.57e-05    |
|    value_loss           | 8.82         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -10.4    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 269      |
|    time_elapsed    | 4712     |
|    total_timesteps | 2203648  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.61e+03      |
|    ep_rew_mean          | -9.59         |
| time/                   |               |
|    fps                  | 467           |
|    iterations           | 270           |
|    time_elapsed         | 4727          |
|    total_timesteps      | 2211840       |
| train/                  |               |
|    approx_kl            | 0.00039355943 |
|    clip_fraction        | 0.00245       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.575        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.31          |
|    n_updates            | 2690          |
|    policy_gradient_loss | 0.000163      |
|    value_loss           | 8.14          |
-------------------------------------------
Eval num_timesteps=2220000, episode_reward=-18.10 +/- 27.15
Episode length: 1650.00 +/- 157.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.65e+03     |
|    mean_reward          | -18.1        |
| time/                   |              |
|    total_timesteps      | 2220000      |
| train/                  |              |
|    approx_kl            | 0.0010276728 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.527       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.09         |
|    n_updates            | 2700         |
|    policy_gradient_loss | 5.13e-05     |
|    value_loss           | 7.87         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -9.34    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 271      |
|    time_elapsed    | 4748     |
|    total_timesteps | 2220032  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.63e+03     |
|    ep_rew_mean          | -8.82        |
| time/                   |              |
|    fps                  | 467          |
|    iterations           | 272          |
|    time_elapsed         | 4762         |
|    total_timesteps      | 2228224      |
| train/                  |              |
|    approx_kl            | 0.0017269852 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.546       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.24         |
|    n_updates            | 2710         |
|    policy_gradient_loss | -0.000956    |
|    value_loss           | 8.49         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.63e+03    |
|    ep_rew_mean          | -9.61       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 273         |
|    time_elapsed         | 4777        |
|    total_timesteps      | 2236416     |
| train/                  |             |
|    approx_kl            | 0.002639803 |
|    clip_fraction        | 0.0104      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.504      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.52        |
|    n_updates            | 2720        |
|    policy_gradient_loss | -0.000872   |
|    value_loss           | 9.18        |
-----------------------------------------
Eval num_timesteps=2240000, episode_reward=-24.20 +/- 29.62
Episode length: 1575.00 +/- 199.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.58e+03     |
|    mean_reward          | -24.2        |
| time/                   |              |
|    total_timesteps      | 2240000      |
| train/                  |              |
|    approx_kl            | 0.0007675111 |
|    clip_fraction        | 0.00656      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.505       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.45         |
|    n_updates            | 2730         |
|    policy_gradient_loss | -0.000381    |
|    value_loss           | 9.65         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -8.49    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 274      |
|    time_elapsed    | 4798     |
|    total_timesteps | 2244608  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.65e+03     |
|    ep_rew_mean          | -7.32        |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 275          |
|    time_elapsed         | 4812         |
|    total_timesteps      | 2252800      |
| train/                  |              |
|    approx_kl            | 0.0019809785 |
|    clip_fraction        | 0.00791      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.554       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.21         |
|    n_updates            | 2740         |
|    policy_gradient_loss | -0.000409    |
|    value_loss           | 7.5          |
------------------------------------------
Eval num_timesteps=2260000, episode_reward=-33.50 +/- 29.61
Episode length: 1666.60 +/- 202.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.67e+03     |
|    mean_reward          | -33.5        |
| time/                   |              |
|    total_timesteps      | 2260000      |
| train/                  |              |
|    approx_kl            | 0.0038530845 |
|    clip_fraction        | 0.00696      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.519       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.68         |
|    n_updates            | 2750         |
|    policy_gradient_loss | -0.000416    |
|    value_loss           | 8            |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -7.83    |
| time/              |          |
|    fps             | 467      |
|    iterations      | 276      |
|    time_elapsed    | 4835     |
|    total_timesteps | 2260992  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.66e+03     |
|    ep_rew_mean          | -7.54        |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 277          |
|    time_elapsed         | 4847         |
|    total_timesteps      | 2269184      |
| train/                  |              |
|    approx_kl            | 0.0017062542 |
|    clip_fraction        | 0.0149       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.531       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.02         |
|    n_updates            | 2760         |
|    policy_gradient_loss | -0.000987    |
|    value_loss           | 9.68         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.65e+03    |
|    ep_rew_mean          | -8.36       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 278         |
|    time_elapsed         | 4860        |
|    total_timesteps      | 2277376     |
| train/                  |             |
|    approx_kl            | 0.002772324 |
|    clip_fraction        | 0.00854     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.559      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 2.47        |
|    n_updates            | 2770        |
|    policy_gradient_loss | -0.000454   |
|    value_loss           | 7.82        |
-----------------------------------------
Eval num_timesteps=2280000, episode_reward=-7.80 +/- 43.07
Episode length: 1586.60 +/- 316.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.59e+03     |
|    mean_reward          | -7.8         |
| time/                   |              |
|    total_timesteps      | 2280000      |
| train/                  |              |
|    approx_kl            | 0.0019577336 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.606       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.29         |
|    n_updates            | 2780         |
|    policy_gradient_loss | -5.23e-05    |
|    value_loss           | 8.81         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -6.63    |
| time/              |          |
|    fps             | 468      |
|    iterations      | 279      |
|    time_elapsed    | 4881     |
|    total_timesteps | 2285568  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.65e+03     |
|    ep_rew_mean          | -7.02        |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 280          |
|    time_elapsed         | 4896         |
|    total_timesteps      | 2293760      |
| train/                  |              |
|    approx_kl            | 0.0011828562 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.638       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.84         |
|    n_updates            | 2790         |
|    policy_gradient_loss | -2.59e-05    |
|    value_loss           | 8.38         |
------------------------------------------
Eval num_timesteps=2300000, episode_reward=-26.30 +/- 32.07
Episode length: 1631.40 +/- 202.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.63e+03     |
|    mean_reward          | -26.3        |
| time/                   |              |
|    total_timesteps      | 2300000      |
| train/                  |              |
|    approx_kl            | 0.0031857293 |
|    clip_fraction        | 0.0069       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.648       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 5.43         |
|    n_updates            | 2800         |
|    policy_gradient_loss | -0.000507    |
|    value_loss           | 8.87         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -5.57    |
| time/              |          |
|    fps             | 468      |
|    iterations      | 281      |
|    time_elapsed    | 4918     |
|    total_timesteps | 2301952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.67e+03    |
|    ep_rew_mean          | -4.62       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 282         |
|    time_elapsed         | 4932        |
|    total_timesteps      | 2310144     |
| train/                  |             |
|    approx_kl            | 0.004139774 |
|    clip_fraction        | 0.0167      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.606      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.38        |
|    n_updates            | 2810        |
|    policy_gradient_loss | -5.24e-05   |
|    value_loss           | 8.35        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.66e+03     |
|    ep_rew_mean          | -4.78        |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 283          |
|    time_elapsed         | 4947         |
|    total_timesteps      | 2318336      |
| train/                  |              |
|    approx_kl            | 0.0030843548 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.625       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4            |
|    n_updates            | 2820         |
|    policy_gradient_loss | -0.000692    |
|    value_loss           | 8.9          |
------------------------------------------
Eval num_timesteps=2320000, episode_reward=-14.30 +/- 19.16
Episode length: 1668.40 +/- 209.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.67e+03     |
|    mean_reward          | -14.3        |
| time/                   |              |
|    total_timesteps      | 2320000      |
| train/                  |              |
|    approx_kl            | 0.0033452145 |
|    clip_fraction        | 0.0142       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.684       |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 2.94         |
|    n_updates            | 2830         |
|    policy_gradient_loss | -0.000686    |
|    value_loss           | 6.8          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -5       |
| time/              |          |
|    fps             | 468      |
|    iterations      | 284      |
|    time_elapsed    | 4970     |
|    total_timesteps | 2326528  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.67e+03    |
|    ep_rew_mean          | -5.86       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 285         |
|    time_elapsed         | 4983        |
|    total_timesteps      | 2334720     |
| train/                  |             |
|    approx_kl            | 0.005369815 |
|    clip_fraction        | 0.0147      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.736      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.49        |
|    n_updates            | 2840        |
|    policy_gradient_loss | -0.000783   |
|    value_loss           | 8.61        |
-----------------------------------------
Eval num_timesteps=2340000, episode_reward=-30.70 +/- 33.07
Episode length: 1517.00 +/- 160.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.52e+03    |
|    mean_reward          | -30.7       |
| time/                   |             |
|    total_timesteps      | 2340000     |
| train/                  |             |
|    approx_kl            | 0.007334166 |
|    clip_fraction        | 0.0397      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.809      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.37        |
|    n_updates            | 2850        |
|    policy_gradient_loss | -0.0022     |
|    value_loss           | 8.26        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -6.61    |
| time/              |          |
|    fps             | 468      |
|    iterations      | 286      |
|    time_elapsed    | 5003     |
|    total_timesteps | 2342912  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.68e+03    |
|    ep_rew_mean          | -5.42       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 287         |
|    time_elapsed         | 5017        |
|    total_timesteps      | 2351104     |
| train/                  |             |
|    approx_kl            | 0.007452893 |
|    clip_fraction        | 0.014       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.82       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.57        |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.000779   |
|    value_loss           | 8           |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.68e+03    |
|    ep_rew_mean          | -7.42       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 288         |
|    time_elapsed         | 5031        |
|    total_timesteps      | 2359296     |
| train/                  |             |
|    approx_kl            | 0.003150692 |
|    clip_fraction        | 0.00317     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.801      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.66        |
|    n_updates            | 2870        |
|    policy_gradient_loss | -0.000113   |
|    value_loss           | 9.01        |
-----------------------------------------
Eval num_timesteps=2360000, episode_reward=-18.40 +/- 38.76
Episode length: 1540.20 +/- 195.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.54e+03    |
|    mean_reward          | -18.4       |
| time/                   |             |
|    total_timesteps      | 2360000     |
| train/                  |             |
|    approx_kl            | 0.011894188 |
|    clip_fraction        | 0.0501      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.732      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 2.31        |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.00339    |
|    value_loss           | 8.58        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -7.06    |
| time/              |          |
|    fps             | 468      |
|    iterations      | 289      |
|    time_elapsed    | 5051     |
|    total_timesteps | 2367488  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.69e+03    |
|    ep_rew_mean          | -6.36       |
| time/                   |             |
|    fps                  | 469         |
|    iterations           | 290         |
|    time_elapsed         | 5065        |
|    total_timesteps      | 2375680     |
| train/                  |             |
|    approx_kl            | 0.007777267 |
|    clip_fraction        | 0.00978     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.659      |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 4.43        |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.000736   |
|    value_loss           | 7.64        |
-----------------------------------------
Eval num_timesteps=2380000, episode_reward=-29.30 +/- 9.14
Episode length: 1801.40 +/- 294.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.8e+03       |
|    mean_reward          | -29.3         |
| time/                   |               |
|    total_timesteps      | 2380000       |
| train/                  |               |
|    approx_kl            | 0.00018027384 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.637        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.61          |
|    n_updates            | 2900          |
|    policy_gradient_loss | 0.000141      |
|    value_loss           | 9.32          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -5.41    |
| time/              |          |
|    fps             | 468      |
|    iterations      | 291      |
|    time_elapsed    | 5086     |
|    total_timesteps | 2383872  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.67e+03      |
|    ep_rew_mean          | -5.62         |
| time/                   |               |
|    fps                  | 469           |
|    iterations           | 292           |
|    time_elapsed         | 5100          |
|    total_timesteps      | 2392064       |
| train/                  |               |
|    approx_kl            | 4.0314415e-05 |
|    clip_fraction        | 7.32e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.621        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 6.59          |
|    n_updates            | 2910          |
|    policy_gradient_loss | 0.000234      |
|    value_loss           | 9.89          |
-------------------------------------------
Eval num_timesteps=2400000, episode_reward=-28.50 +/- 42.15
Episode length: 1438.40 +/- 335.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.44e+03     |
|    mean_reward          | -28.5        |
| time/                   |              |
|    total_timesteps      | 2400000      |
| train/                  |              |
|    approx_kl            | 0.0039110836 |
|    clip_fraction        | 0.0207       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.668       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.61         |
|    n_updates            | 2920         |
|    policy_gradient_loss | -0.00127     |
|    value_loss           | 8.42         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -3.89    |
| time/              |          |
|    fps             | 468      |
|    iterations      | 293      |
|    time_elapsed    | 5119     |
|    total_timesteps | 2400256  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.66e+03     |
|    ep_rew_mean          | -4.56        |
| time/                   |              |
|    fps                  | 469          |
|    iterations           | 294          |
|    time_elapsed         | 5132         |
|    total_timesteps      | 2408448      |
| train/                  |              |
|    approx_kl            | 0.0059675155 |
|    clip_fraction        | 0.00504      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.632       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.42         |
|    n_updates            | 2930         |
|    policy_gradient_loss | -0.000564    |
|    value_loss           | 8.99         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.66e+03     |
|    ep_rew_mean          | -4.56        |
| time/                   |              |
|    fps                  | 469          |
|    iterations           | 295          |
|    time_elapsed         | 5146         |
|    total_timesteps      | 2416640      |
| train/                  |              |
|    approx_kl            | 0.0027239127 |
|    clip_fraction        | 0.00197      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.611       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.53         |
|    n_updates            | 2940         |
|    policy_gradient_loss | 4.19e-05     |
|    value_loss           | 7.92         |
------------------------------------------
Eval num_timesteps=2420000, episode_reward=-33.20 +/- 23.48
Episode length: 1852.20 +/- 524.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.85e+03     |
|    mean_reward          | -33.2        |
| time/                   |              |
|    total_timesteps      | 2420000      |
| train/                  |              |
|    approx_kl            | 0.0043350775 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.554       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.22         |
|    n_updates            | 2950         |
|    policy_gradient_loss | -0.00102     |
|    value_loss           | 8.17         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -4.59    |
| time/              |          |
|    fps             | 469      |
|    iterations      | 296      |
|    time_elapsed    | 5166     |
|    total_timesteps | 2424832  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.68e+03    |
|    ep_rew_mean          | -5.98       |
| time/                   |             |
|    fps                  | 469         |
|    iterations           | 297         |
|    time_elapsed         | 5180        |
|    total_timesteps      | 2433024     |
| train/                  |             |
|    approx_kl            | 0.004599133 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.489      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 2.29        |
|    n_updates            | 2960        |
|    policy_gradient_loss | -0.000999   |
|    value_loss           | 8.44        |
-----------------------------------------
Eval num_timesteps=2440000, episode_reward=-26.40 +/- 8.36
Episode length: 1705.20 +/- 218.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.71e+03      |
|    mean_reward          | -26.4         |
| time/                   |               |
|    total_timesteps      | 2440000       |
| train/                  |               |
|    approx_kl            | 0.00042050707 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.484        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.34          |
|    n_updates            | 2970          |
|    policy_gradient_loss | 4.81e-05      |
|    value_loss           | 7.75          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -5.79    |
| time/              |          |
|    fps             | 469      |
|    iterations      | 298      |
|    time_elapsed    | 5202     |
|    total_timesteps | 2441216  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.69e+03     |
|    ep_rew_mean          | -6.5         |
| time/                   |              |
|    fps                  | 469          |
|    iterations           | 299          |
|    time_elapsed         | 5217         |
|    total_timesteps      | 2449408      |
| train/                  |              |
|    approx_kl            | 0.0023503047 |
|    clip_fraction        | 0.0176       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.516       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 7.56         |
|    n_updates            | 2980         |
|    policy_gradient_loss | -0.0011      |
|    value_loss           | 8.71         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.7e+03      |
|    ep_rew_mean          | -6.84        |
| time/                   |              |
|    fps                  | 469          |
|    iterations           | 300          |
|    time_elapsed         | 5230         |
|    total_timesteps      | 2457600      |
| train/                  |              |
|    approx_kl            | 0.0010553086 |
|    clip_fraction        | 0.00167      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.524       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 4.72         |
|    n_updates            | 2990         |
|    policy_gradient_loss | -8.98e-05    |
|    value_loss           | 8.57         |
------------------------------------------
Eval num_timesteps=2460000, episode_reward=-14.50 +/- 29.75
Episode length: 1545.00 +/- 232.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.54e+03     |
|    mean_reward          | -14.5        |
| time/                   |              |
|    total_timesteps      | 2460000      |
| train/                  |              |
|    approx_kl            | 0.0029691593 |
|    clip_fraction        | 0.0209       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.559       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.63         |
|    n_updates            | 3000         |
|    policy_gradient_loss | -0.0012      |
|    value_loss           | 7.24         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -8.08    |
| time/              |          |
|    fps             | 469      |
|    iterations      | 301      |
|    time_elapsed    | 5253     |
|    total_timesteps | 2465792  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.68e+03     |
|    ep_rew_mean          | -9.78        |
| time/                   |              |
|    fps                  | 469          |
|    iterations           | 302          |
|    time_elapsed         | 5267         |
|    total_timesteps      | 2473984      |
| train/                  |              |
|    approx_kl            | 0.0017759292 |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.574       |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 3.89         |
|    n_updates            | 3010         |
|    policy_gradient_loss | -0.00096     |
|    value_loss           | 7.95         |
------------------------------------------
Eval num_timesteps=2480000, episode_reward=-35.80 +/- 20.87
Episode length: 1648.20 +/- 284.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.65e+03     |
|    mean_reward          | -35.8        |
| time/                   |              |
|    total_timesteps      | 2480000      |
| train/                  |              |
|    approx_kl            | 0.0006487356 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.547       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.38         |
|    n_updates            | 3020         |
|    policy_gradient_loss | 5.06e-05     |
|    value_loss           | 8.64         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -11.8    |
| time/              |          |
|    fps             | 469      |
|    iterations      | 303      |
|    time_elapsed    | 5289     |
|    total_timesteps | 2482176  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.67e+03     |
|    ep_rew_mean          | -13.1        |
| time/                   |              |
|    fps                  | 469          |
|    iterations           | 304          |
|    time_elapsed         | 5302         |
|    total_timesteps      | 2490368      |
| train/                  |              |
|    approx_kl            | 0.0041548456 |
|    clip_fraction        | 0.0252       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.464       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.91         |
|    n_updates            | 3030         |
|    policy_gradient_loss | -0.00157     |
|    value_loss           | 7.6          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.66e+03     |
|    ep_rew_mean          | -11.9        |
| time/                   |              |
|    fps                  | 469          |
|    iterations           | 305          |
|    time_elapsed         | 5316         |
|    total_timesteps      | 2498560      |
| train/                  |              |
|    approx_kl            | 0.0024743713 |
|    clip_fraction        | 0.00902      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.425       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.42         |
|    n_updates            | 3040         |
|    policy_gradient_loss | -0.000817    |
|    value_loss           | 9.09         |
------------------------------------------
Eval num_timesteps=2500000, episode_reward=-9.40 +/- 18.27
Episode length: 1802.40 +/- 241.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.8e+03      |
|    mean_reward          | -9.4         |
| time/                   |              |
|    total_timesteps      | 2500000      |
| train/                  |              |
|    approx_kl            | 0.0015957833 |
|    clip_fraction        | 0.00184      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.432       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 2.17         |
|    n_updates            | 3050         |
|    policy_gradient_loss | 0.000104     |
|    value_loss           | 8.67         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -10.4    |
| time/              |          |
|    fps             | 469      |
|    iterations      | 306      |
|    time_elapsed    | 5338     |
|    total_timesteps | 2506752  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.66e+03     |
|    ep_rew_mean          | -13.2        |
| time/                   |              |
|    fps                  | 469          |
|    iterations           | 307          |
|    time_elapsed         | 5351         |
|    total_timesteps      | 2514944      |
| train/                  |              |
|    approx_kl            | 0.0015402099 |
|    clip_fraction        | 0.00175      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.473       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.56         |
|    n_updates            | 3060         |
|    policy_gradient_loss | -0.000218    |
|    value_loss           | 7.5          |
------------------------------------------
Eval num_timesteps=2520000, episode_reward=7.30 +/- 28.14
Episode length: 1981.00 +/- 485.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.98e+03      |
|    mean_reward          | 7.3           |
| time/                   |               |
|    total_timesteps      | 2520000       |
| train/                  |               |
|    approx_kl            | 0.00069599797 |
|    clip_fraction        | 0.00178       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.471        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.36          |
|    n_updates            | 3070          |
|    policy_gradient_loss | -5.62e-05     |
|    value_loss           | 8.97          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -13.4    |
| time/              |          |
|    fps             | 469      |
|    iterations      | 308      |
|    time_elapsed    | 5372     |
|    total_timesteps | 2523136  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.64e+03    |
|    ep_rew_mean          | -13.7       |
| time/                   |             |
|    fps                  | 469         |
|    iterations           | 309         |
|    time_elapsed         | 5386        |
|    total_timesteps      | 2531328     |
| train/                  |             |
|    approx_kl            | 0.002132453 |
|    clip_fraction        | 0.0124      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.52       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 2.01        |
|    n_updates            | 3080        |
|    policy_gradient_loss | -0.000737   |
|    value_loss           | 7.27        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.65e+03     |
|    ep_rew_mean          | -12          |
| time/                   |              |
|    fps                  | 470          |
|    iterations           | 310          |
|    time_elapsed         | 5399         |
|    total_timesteps      | 2539520      |
| train/                  |              |
|    approx_kl            | 0.0016396794 |
|    clip_fraction        | 0.00355      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.503       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.01         |
|    n_updates            | 3090         |
|    policy_gradient_loss | -0.000135    |
|    value_loss           | 8.12         |
------------------------------------------
Eval num_timesteps=2540000, episode_reward=-15.60 +/- 17.79
Episode length: 1852.00 +/- 227.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.85e+03      |
|    mean_reward          | -15.6         |
| time/                   |               |
|    total_timesteps      | 2540000       |
| train/                  |               |
|    approx_kl            | 0.00080863526 |
|    clip_fraction        | 0.00293       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.513        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.81          |
|    n_updates            | 3100          |
|    policy_gradient_loss | -0.00012      |
|    value_loss           | 7.4           |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    fps             | 469      |
|    iterations      | 311      |
|    time_elapsed    | 5421     |
|    total_timesteps | 2547712  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.63e+03     |
|    ep_rew_mean          | -12          |
| time/                   |              |
|    fps                  | 470          |
|    iterations           | 312          |
|    time_elapsed         | 5435         |
|    total_timesteps      | 2555904      |
| train/                  |              |
|    approx_kl            | 0.0013785098 |
|    clip_fraction        | 0.00618      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.514       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.31         |
|    n_updates            | 3110         |
|    policy_gradient_loss | -0.000322    |
|    value_loss           | 9.15         |
------------------------------------------
Eval num_timesteps=2560000, episode_reward=-10.60 +/- 36.26
Episode length: 1566.40 +/- 156.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.57e+03     |
|    mean_reward          | -10.6        |
| time/                   |              |
|    total_timesteps      | 2560000      |
| train/                  |              |
|    approx_kl            | 0.0020162668 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.568       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.26         |
|    n_updates            | 3120         |
|    policy_gradient_loss | -0.000611    |
|    value_loss           | 8.2          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -13.7    |
| time/              |          |
|    fps             | 469      |
|    iterations      | 313      |
|    time_elapsed    | 5457     |
|    total_timesteps | 2564096  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.64e+03      |
|    ep_rew_mean          | -15.3         |
| time/                   |               |
|    fps                  | 470           |
|    iterations           | 314           |
|    time_elapsed         | 5471          |
|    total_timesteps      | 2572288       |
| train/                  |               |
|    approx_kl            | 0.00086044805 |
|    clip_fraction        | 0.00182       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.549        |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 5.75          |
|    n_updates            | 3130          |
|    policy_gradient_loss | -1.82e-05     |
|    value_loss           | 8.4           |
-------------------------------------------
Eval num_timesteps=2580000, episode_reward=17.30 +/- 45.07
Episode length: 1465.80 +/- 250.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.47e+03      |
|    mean_reward          | 17.3          |
| time/                   |               |
|    total_timesteps      | 2580000       |
| train/                  |               |
|    approx_kl            | 0.00091420044 |
|    clip_fraction        | 0.00463       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.6          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.07          |
|    n_updates            | 3140          |
|    policy_gradient_loss | -2.44e-05     |
|    value_loss           | 7.81          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -16.3    |
| time/              |          |
|    fps             | 469      |
|    iterations      | 315      |
|    time_elapsed    | 5491     |
|    total_timesteps | 2580480  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.65e+03      |
|    ep_rew_mean          | -16.1         |
| time/                   |               |
|    fps                  | 470           |
|    iterations           | 316           |
|    time_elapsed         | 5504          |
|    total_timesteps      | 2588672       |
| train/                  |               |
|    approx_kl            | 0.00015303266 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.606        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.86          |
|    n_updates            | 3150          |
|    policy_gradient_loss | 0.00019       |
|    value_loss           | 8.24          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.66e+03     |
|    ep_rew_mean          | -15.8        |
| time/                   |              |
|    fps                  | 470          |
|    iterations           | 317          |
|    time_elapsed         | 5517         |
|    total_timesteps      | 2596864      |
| train/                  |              |
|    approx_kl            | 0.0033676308 |
|    clip_fraction        | 0.000476     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.642       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.18         |
|    n_updates            | 3160         |
|    policy_gradient_loss | -5.53e-05    |
|    value_loss           | 8.03         |
------------------------------------------
Eval num_timesteps=2600000, episode_reward=21.40 +/- 26.88
Episode length: 1559.00 +/- 411.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.56e+03    |
|    mean_reward          | 21.4        |
| time/                   |             |
|    total_timesteps      | 2600000     |
| train/                  |             |
|    approx_kl            | 0.005251859 |
|    clip_fraction        | 0.0107      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.602      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.26        |
|    n_updates            | 3170        |
|    policy_gradient_loss | -0.000687   |
|    value_loss           | 7.57        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -16.8    |
| time/              |          |
|    fps             | 470      |
|    iterations      | 318      |
|    time_elapsed    | 5537     |
|    total_timesteps | 2605056  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.64e+03     |
|    ep_rew_mean          | -14.8        |
| time/                   |              |
|    fps                  | 470          |
|    iterations           | 319          |
|    time_elapsed         | 5551         |
|    total_timesteps      | 2613248      |
| train/                  |              |
|    approx_kl            | 0.0014845843 |
|    clip_fraction        | 0.00338      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.621       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.49         |
|    n_updates            | 3180         |
|    policy_gradient_loss | -0.000198    |
|    value_loss           | 8.86         |
------------------------------------------
Eval num_timesteps=2620000, episode_reward=-29.20 +/- 21.12
Episode length: 1793.20 +/- 320.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.79e+03     |
|    mean_reward          | -29.2        |
| time/                   |              |
|    total_timesteps      | 2620000      |
| train/                  |              |
|    approx_kl            | 0.0010237586 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.629       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.32         |
|    n_updates            | 3190         |
|    policy_gradient_loss | -7.1e-06     |
|    value_loss           | 9.72         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -14.3    |
| time/              |          |
|    fps             | 470      |
|    iterations      | 320      |
|    time_elapsed    | 5572     |
|    total_timesteps | 2621440  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.63e+03     |
|    ep_rew_mean          | -12.7        |
| time/                   |              |
|    fps                  | 470          |
|    iterations           | 321          |
|    time_elapsed         | 5586         |
|    total_timesteps      | 2629632      |
| train/                  |              |
|    approx_kl            | 0.0038270822 |
|    clip_fraction        | 0.0222       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.624       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.01         |
|    n_updates            | 3200         |
|    policy_gradient_loss | -0.00128     |
|    value_loss           | 9.24         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.63e+03     |
|    ep_rew_mean          | -10.9        |
| time/                   |              |
|    fps                  | 471          |
|    iterations           | 322          |
|    time_elapsed         | 5599         |
|    total_timesteps      | 2637824      |
| train/                  |              |
|    approx_kl            | 0.0041164528 |
|    clip_fraction        | 0.0273       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.565       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.82         |
|    n_updates            | 3210         |
|    policy_gradient_loss | -0.0012      |
|    value_loss           | 8.7          |
------------------------------------------
Eval num_timesteps=2640000, episode_reward=-26.40 +/- 16.14
Episode length: 1621.00 +/- 269.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.62e+03     |
|    mean_reward          | -26.4        |
| time/                   |              |
|    total_timesteps      | 2640000      |
| train/                  |              |
|    approx_kl            | 0.0042144135 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.521       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.5          |
|    n_updates            | 3220         |
|    policy_gradient_loss | -0.000913    |
|    value_loss           | 7.42         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    fps             | 470      |
|    iterations      | 323      |
|    time_elapsed    | 5619     |
|    total_timesteps | 2646016  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.63e+03     |
|    ep_rew_mean          | -11          |
| time/                   |              |
|    fps                  | 471          |
|    iterations           | 324          |
|    time_elapsed         | 5633         |
|    total_timesteps      | 2654208      |
| train/                  |              |
|    approx_kl            | 0.0010886095 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.518       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 3.4          |
|    n_updates            | 3230         |
|    policy_gradient_loss | 8.16e-06     |
|    value_loss           | 8            |
------------------------------------------
Eval num_timesteps=2660000, episode_reward=-22.10 +/- 39.37
Episode length: 1577.60 +/- 168.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.58e+03     |
|    mean_reward          | -22.1        |
| time/                   |              |
|    total_timesteps      | 2660000      |
| train/                  |              |
|    approx_kl            | 0.0021373623 |
|    clip_fraction        | 0.0158       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.53        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.29         |
|    n_updates            | 3240         |
|    policy_gradient_loss | -0.000975    |
|    value_loss           | 8.09         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    fps             | 470      |
|    iterations      | 325      |
|    time_elapsed    | 5653     |
|    total_timesteps | 2662400  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.64e+03     |
|    ep_rew_mean          | -13.7        |
| time/                   |              |
|    fps                  | 471          |
|    iterations           | 326          |
|    time_elapsed         | 5666         |
|    total_timesteps      | 2670592      |
| train/                  |              |
|    approx_kl            | 0.0011990309 |
|    clip_fraction        | 0.00848      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.59        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3            |
|    n_updates            | 3250         |
|    policy_gradient_loss | -0.0003      |
|    value_loss           | 7.92         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.65e+03     |
|    ep_rew_mean          | -13.7        |
| time/                   |              |
|    fps                  | 471          |
|    iterations           | 327          |
|    time_elapsed         | 5680         |
|    total_timesteps      | 2678784      |
| train/                  |              |
|    approx_kl            | 0.0026293932 |
|    clip_fraction        | 0.00227      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.539       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2            |
|    n_updates            | 3260         |
|    policy_gradient_loss | -0.000159    |
|    value_loss           | 8.37         |
------------------------------------------
Eval num_timesteps=2680000, episode_reward=-8.70 +/- 26.02
Episode length: 2016.00 +/- 285.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.02e+03     |
|    mean_reward          | -8.7         |
| time/                   |              |
|    total_timesteps      | 2680000      |
| train/                  |              |
|    approx_kl            | 0.0026181564 |
|    clip_fraction        | 0.00413      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.58        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.27         |
|    n_updates            | 3270         |
|    policy_gradient_loss | -0.000188    |
|    value_loss           | 7.52         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -13.8    |
| time/              |          |
|    fps             | 471      |
|    iterations      | 328      |
|    time_elapsed    | 5701     |
|    total_timesteps | 2686976  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.64e+03    |
|    ep_rew_mean          | -13.1       |
| time/                   |             |
|    fps                  | 471         |
|    iterations           | 329         |
|    time_elapsed         | 5714        |
|    total_timesteps      | 2695168     |
| train/                  |             |
|    approx_kl            | 0.002825299 |
|    clip_fraction        | 0.0163      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.554      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.14        |
|    n_updates            | 3280        |
|    policy_gradient_loss | -0.00104    |
|    value_loss           | 8.7         |
-----------------------------------------
Eval num_timesteps=2700000, episode_reward=-20.90 +/- 51.78
Episode length: 1353.80 +/- 326.14
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 1.35e+03       |
|    mean_reward          | -20.9          |
| time/                   |                |
|    total_timesteps      | 2700000        |
| train/                  |                |
|    approx_kl            | 0.000109287495 |
|    clip_fraction        | 0.00461        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.504         |
|    explained_variance   | 0              |
|    learning_rate        | 0.0003         |
|    loss                 | 3.22           |
|    n_updates            | 3290           |
|    policy_gradient_loss | 0.000201       |
|    value_loss           | 8.88           |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -12.8    |
| time/              |          |
|    fps             | 471      |
|    iterations      | 330      |
|    time_elapsed    | 5732     |
|    total_timesteps | 2703360  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.64e+03     |
|    ep_rew_mean          | -14.2        |
| time/                   |              |
|    fps                  | 472          |
|    iterations           | 331          |
|    time_elapsed         | 5744         |
|    total_timesteps      | 2711552      |
| train/                  |              |
|    approx_kl            | 0.0010899026 |
|    clip_fraction        | 0.00319      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.56        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.7          |
|    n_updates            | 3300         |
|    policy_gradient_loss | -7.9e-05     |
|    value_loss           | 8.38         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.64e+03      |
|    ep_rew_mean          | -13.3         |
| time/                   |               |
|    fps                  | 472           |
|    iterations           | 332           |
|    time_elapsed         | 5756          |
|    total_timesteps      | 2719744       |
| train/                  |               |
|    approx_kl            | 0.00016710369 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.538        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 6.86          |
|    n_updates            | 3310          |
|    policy_gradient_loss | 0.000149      |
|    value_loss           | 9.61          |
-------------------------------------------
Eval num_timesteps=2720000, episode_reward=13.80 +/- 37.83
Episode length: 1640.20 +/- 280.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.64e+03     |
|    mean_reward          | 13.8         |
| time/                   |              |
|    total_timesteps      | 2720000      |
| train/                  |              |
|    approx_kl            | 0.0021604248 |
|    clip_fraction        | 0.00621      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.505       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.18         |
|    n_updates            | 3320         |
|    policy_gradient_loss | -0.000474    |
|    value_loss           | 8.6          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -13.7    |
| time/              |          |
|    fps             | 472      |
|    iterations      | 333      |
|    time_elapsed    | 5776     |
|    total_timesteps | 2727936  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.65e+03     |
|    ep_rew_mean          | -14          |
| time/                   |              |
|    fps                  | 472          |
|    iterations           | 334          |
|    time_elapsed         | 5789         |
|    total_timesteps      | 2736128      |
| train/                  |              |
|    approx_kl            | 0.0018005342 |
|    clip_fraction        | 0.0122       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.479       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 2.97         |
|    n_updates            | 3330         |
|    policy_gradient_loss | -0.001       |
|    value_loss           | 7.44         |
------------------------------------------
Eval num_timesteps=2740000, episode_reward=-14.00 +/- 38.34
Episode length: 1670.80 +/- 291.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.67e+03      |
|    mean_reward          | -14           |
| time/                   |               |
|    total_timesteps      | 2740000       |
| train/                  |               |
|    approx_kl            | 0.00020918183 |
|    clip_fraction        | 0.00361       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.491        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.26          |
|    n_updates            | 3340          |
|    policy_gradient_loss | -7.03e-05     |
|    value_loss           | 8.32          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -12.8    |
| time/              |          |
|    fps             | 472      |
|    iterations      | 335      |
|    time_elapsed    | 5810     |
|    total_timesteps | 2744320  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.65e+03     |
|    ep_rew_mean          | -11.7        |
| time/                   |              |
|    fps                  | 472          |
|    iterations           | 336          |
|    time_elapsed         | 5824         |
|    total_timesteps      | 2752512      |
| train/                  |              |
|    approx_kl            | 0.0010942561 |
|    clip_fraction        | 0.00574      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.521       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.71         |
|    n_updates            | 3350         |
|    policy_gradient_loss | -0.000137    |
|    value_loss           | 7.52         |
------------------------------------------
Eval num_timesteps=2760000, episode_reward=-50.00 +/- 8.71
Episode length: 1551.20 +/- 285.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.55e+03     |
|    mean_reward          | -50          |
| time/                   |              |
|    total_timesteps      | 2760000      |
| train/                  |              |
|    approx_kl            | 0.0018268737 |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.516       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.05         |
|    n_updates            | 3360         |
|    policy_gradient_loss | -0.000974    |
|    value_loss           | 8.35         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    fps             | 472      |
|    iterations      | 337      |
|    time_elapsed    | 5843     |
|    total_timesteps | 2760704  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.66e+03     |
|    ep_rew_mean          | -12          |
| time/                   |              |
|    fps                  | 472          |
|    iterations           | 338          |
|    time_elapsed         | 5856         |
|    total_timesteps      | 2768896      |
| train/                  |              |
|    approx_kl            | 0.0010514355 |
|    clip_fraction        | 0.00192      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.526       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.07         |
|    n_updates            | 3370         |
|    policy_gradient_loss | -7.34e-05    |
|    value_loss           | 8.22         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.65e+03     |
|    ep_rew_mean          | -11.6        |
| time/                   |              |
|    fps                  | 473          |
|    iterations           | 339          |
|    time_elapsed         | 5870         |
|    total_timesteps      | 2777088      |
| train/                  |              |
|    approx_kl            | 0.0032477519 |
|    clip_fraction        | 0.0135       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.513       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 3.44         |
|    n_updates            | 3380         |
|    policy_gradient_loss | -0.000829    |
|    value_loss           | 8.27         |
------------------------------------------
Eval num_timesteps=2780000, episode_reward=-30.60 +/- 29.77
Episode length: 1557.40 +/- 253.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.56e+03     |
|    mean_reward          | -30.6        |
| time/                   |              |
|    total_timesteps      | 2780000      |
| train/                  |              |
|    approx_kl            | 0.0022613825 |
|    clip_fraction        | 0.00836      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.507       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 4.05         |
|    n_updates            | 3390         |
|    policy_gradient_loss | -0.00053     |
|    value_loss           | 8.83         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -11.3    |
| time/              |          |
|    fps             | 472      |
|    iterations      | 340      |
|    time_elapsed    | 5890     |
|    total_timesteps | 2785280  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.67e+03     |
|    ep_rew_mean          | -11.4        |
| time/                   |              |
|    fps                  | 473          |
|    iterations           | 341          |
|    time_elapsed         | 5905         |
|    total_timesteps      | 2793472      |
| train/                  |              |
|    approx_kl            | 0.0013429272 |
|    clip_fraction        | 0.00452      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.54        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 8.06         |
|    n_updates            | 3400         |
|    policy_gradient_loss | -0.000222    |
|    value_loss           | 8.82         |
------------------------------------------
Eval num_timesteps=2800000, episode_reward=-44.60 +/- 17.08
Episode length: 1458.80 +/- 160.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.46e+03     |
|    mean_reward          | -44.6        |
| time/                   |              |
|    total_timesteps      | 2800000      |
| train/                  |              |
|    approx_kl            | 0.0034964965 |
|    clip_fraction        | 0.031        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.464       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.05         |
|    n_updates            | 3410         |
|    policy_gradient_loss | -0.00181     |
|    value_loss           | 8.7          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -10.6    |
| time/              |          |
|    fps             | 472      |
|    iterations      | 342      |
|    time_elapsed    | 5924     |
|    total_timesteps | 2801664  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.67e+03    |
|    ep_rew_mean          | -13.8       |
| time/                   |             |
|    fps                  | 473         |
|    iterations           | 343         |
|    time_elapsed         | 5939        |
|    total_timesteps      | 2809856     |
| train/                  |             |
|    approx_kl            | 0.002238743 |
|    clip_fraction        | 0.0309      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.519      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 6.8         |
|    n_updates            | 3420        |
|    policy_gradient_loss | -0.0019     |
|    value_loss           | 8.78        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.67e+03     |
|    ep_rew_mean          | -12.6        |
| time/                   |              |
|    fps                  | 473          |
|    iterations           | 344          |
|    time_elapsed         | 5951         |
|    total_timesteps      | 2818048      |
| train/                  |              |
|    approx_kl            | 0.0005940732 |
|    clip_fraction        | 0.00231      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.538       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.05         |
|    n_updates            | 3430         |
|    policy_gradient_loss | 5.37e-05     |
|    value_loss           | 8.85         |
------------------------------------------
Eval num_timesteps=2820000, episode_reward=12.30 +/- 33.58
Episode length: 1708.00 +/- 463.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.71e+03     |
|    mean_reward          | 12.3         |
| time/                   |              |
|    total_timesteps      | 2820000      |
| train/                  |              |
|    approx_kl            | 0.0016834296 |
|    clip_fraction        | 0.00249      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.499       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.06         |
|    n_updates            | 3440         |
|    policy_gradient_loss | -0.000264    |
|    value_loss           | 7.7          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -12.9    |
| time/              |          |
|    fps             | 473      |
|    iterations      | 345      |
|    time_elapsed    | 5969     |
|    total_timesteps | 2826240  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.68e+03     |
|    ep_rew_mean          | -12.2        |
| time/                   |              |
|    fps                  | 473          |
|    iterations           | 346          |
|    time_elapsed         | 5980         |
|    total_timesteps      | 2834432      |
| train/                  |              |
|    approx_kl            | 0.0008845596 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.526       |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 4.53         |
|    n_updates            | 3450         |
|    policy_gradient_loss | -1.47e-05    |
|    value_loss           | 7.52         |
------------------------------------------
Eval num_timesteps=2840000, episode_reward=2.80 +/- 36.40
Episode length: 1813.00 +/- 272.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.81e+03     |
|    mean_reward          | 2.8          |
| time/                   |              |
|    total_timesteps      | 2840000      |
| train/                  |              |
|    approx_kl            | 0.0031670867 |
|    clip_fraction        | 0.00731      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.484       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 3.51         |
|    n_updates            | 3460         |
|    policy_gradient_loss | -0.000449    |
|    value_loss           | 8.28         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -12.9    |
| time/              |          |
|    fps             | 473      |
|    iterations      | 347      |
|    time_elapsed    | 6001     |
|    total_timesteps | 2842624  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.68e+03    |
|    ep_rew_mean          | -12         |
| time/                   |             |
|    fps                  | 474         |
|    iterations           | 348         |
|    time_elapsed         | 6012        |
|    total_timesteps      | 2850816     |
| train/                  |             |
|    approx_kl            | 0.002223222 |
|    clip_fraction        | 0.00649     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.474      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.45        |
|    n_updates            | 3470        |
|    policy_gradient_loss | -0.000588   |
|    value_loss           | 6.92        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.67e+03     |
|    ep_rew_mean          | -12.1        |
| time/                   |              |
|    fps                  | 474          |
|    iterations           | 349          |
|    time_elapsed         | 6024         |
|    total_timesteps      | 2859008      |
| train/                  |              |
|    approx_kl            | 0.0022992927 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.406       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.24         |
|    n_updates            | 3480         |
|    policy_gradient_loss | -0.000531    |
|    value_loss           | 7.72         |
------------------------------------------
Eval num_timesteps=2860000, episode_reward=-18.90 +/- 39.06
Episode length: 1825.60 +/- 271.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.83e+03     |
|    mean_reward          | -18.9        |
| time/                   |              |
|    total_timesteps      | 2860000      |
| train/                  |              |
|    approx_kl            | 0.0017670316 |
|    clip_fraction        | 0.00546      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.365       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.46         |
|    n_updates            | 3490         |
|    policy_gradient_loss | -6.26e-05    |
|    value_loss           | 8.89         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -12.6    |
| time/              |          |
|    fps             | 474      |
|    iterations      | 350      |
|    time_elapsed    | 6045     |
|    total_timesteps | 2867200  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.68e+03    |
|    ep_rew_mean          | -12.4       |
| time/                   |             |
|    fps                  | 474         |
|    iterations           | 351         |
|    time_elapsed         | 6057        |
|    total_timesteps      | 2875392     |
| train/                  |             |
|    approx_kl            | 0.001636452 |
|    clip_fraction        | 0.00692     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.357      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 6.73        |
|    n_updates            | 3500        |
|    policy_gradient_loss | -0.000578   |
|    value_loss           | 9.77        |
-----------------------------------------
Eval num_timesteps=2880000, episode_reward=-5.30 +/- 38.74
Episode length: 1818.00 +/- 277.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.82e+03     |
|    mean_reward          | -5.3         |
| time/                   |              |
|    total_timesteps      | 2880000      |
| train/                  |              |
|    approx_kl            | 0.0017394042 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.38        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.57         |
|    n_updates            | 3510         |
|    policy_gradient_loss | -0.000827    |
|    value_loss           | 9.54         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -11.9    |
| time/              |          |
|    fps             | 474      |
|    iterations      | 352      |
|    time_elapsed    | 6076     |
|    total_timesteps | 2883584  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.68e+03     |
|    ep_rew_mean          | -11.7        |
| time/                   |              |
|    fps                  | 474          |
|    iterations           | 353          |
|    time_elapsed         | 6088         |
|    total_timesteps      | 2891776      |
| train/                  |              |
|    approx_kl            | 0.0021394035 |
|    clip_fraction        | 0.0108       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.341       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 7.84         |
|    n_updates            | 3520         |
|    policy_gradient_loss | -0.000767    |
|    value_loss           | 9.66         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.7e+03      |
|    ep_rew_mean          | -10.5        |
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 354          |
|    time_elapsed         | 6100         |
|    total_timesteps      | 2899968      |
| train/                  |              |
|    approx_kl            | 0.0013212678 |
|    clip_fraction        | 0.00933      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.3         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 2.96         |
|    n_updates            | 3530         |
|    policy_gradient_loss | -0.000392    |
|    value_loss           | 7.33         |
------------------------------------------
Eval num_timesteps=2900000, episode_reward=-17.00 +/- 32.47
Episode length: 1435.20 +/- 141.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.44e+03     |
|    mean_reward          | -17          |
| time/                   |              |
|    total_timesteps      | 2900000      |
| train/                  |              |
|    approx_kl            | 0.0013946695 |
|    clip_fraction        | 0.00947      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.26        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.3          |
|    n_updates            | 3540         |
|    policy_gradient_loss | -0.00017     |
|    value_loss           | 8.37         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -9.57    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 355      |
|    time_elapsed    | 6119     |
|    total_timesteps | 2908160  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.69e+03      |
|    ep_rew_mean          | -8.89         |
| time/                   |               |
|    fps                  | 475           |
|    iterations           | 356           |
|    time_elapsed         | 6130          |
|    total_timesteps      | 2916352       |
| train/                  |               |
|    approx_kl            | 0.00023343318 |
|    clip_fraction        | 0.000427      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.27         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.76          |
|    n_updates            | 3550          |
|    policy_gradient_loss | 0.000107      |
|    value_loss           | 8.22          |
-------------------------------------------
Eval num_timesteps=2920000, episode_reward=13.00 +/- 43.73
Episode length: 1474.80 +/- 244.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.47e+03     |
|    mean_reward          | 13           |
| time/                   |              |
|    total_timesteps      | 2920000      |
| train/                  |              |
|    approx_kl            | 0.0012824279 |
|    clip_fraction        | 0.00294      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.22        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.97         |
|    n_updates            | 3560         |
|    policy_gradient_loss | -0.00012     |
|    value_loss           | 9.44         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -8.28    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 357      |
|    time_elapsed    | 6152     |
|    total_timesteps | 2924544  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.67e+03     |
|    ep_rew_mean          | -9.32        |
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 358          |
|    time_elapsed         | 6165         |
|    total_timesteps      | 2932736      |
| train/                  |              |
|    approx_kl            | 0.0006485309 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.242       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.4          |
|    n_updates            | 3570         |
|    policy_gradient_loss | -0.000463    |
|    value_loss           | 7.89         |
------------------------------------------
Eval num_timesteps=2940000, episode_reward=-5.50 +/- 37.91
Episode length: 1782.40 +/- 97.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.78e+03     |
|    mean_reward          | -5.5         |
| time/                   |              |
|    total_timesteps      | 2940000      |
| train/                  |              |
|    approx_kl            | 0.0009230358 |
|    clip_fraction        | 0.00632      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.207       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.83         |
|    n_updates            | 3580         |
|    policy_gradient_loss | -0.000224    |
|    value_loss           | 8.4          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -7.02    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 359      |
|    time_elapsed    | 6186     |
|    total_timesteps | 2940928  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.68e+03      |
|    ep_rew_mean          | -6.39         |
| time/                   |               |
|    fps                  | 475           |
|    iterations           | 360           |
|    time_elapsed         | 6200          |
|    total_timesteps      | 2949120       |
| train/                  |               |
|    approx_kl            | 0.00072226947 |
|    clip_fraction        | 0.00284       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.219        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.52          |
|    n_updates            | 3590          |
|    policy_gradient_loss | -8.94e-05     |
|    value_loss           | 8.83          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.69e+03     |
|    ep_rew_mean          | -8.51        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 361          |
|    time_elapsed         | 6212         |
|    total_timesteps      | 2957312      |
| train/                  |              |
|    approx_kl            | 0.0007360605 |
|    clip_fraction        | 0.00251      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.227       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.58         |
|    n_updates            | 3600         |
|    policy_gradient_loss | -0.00017     |
|    value_loss           | 8.33         |
------------------------------------------
Eval num_timesteps=2960000, episode_reward=-1.60 +/- 37.21
Episode length: 1774.60 +/- 292.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.77e+03     |
|    mean_reward          | -1.6         |
| time/                   |              |
|    total_timesteps      | 2960000      |
| train/                  |              |
|    approx_kl            | 0.0002101098 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.241       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.89         |
|    n_updates            | 3610         |
|    policy_gradient_loss | 0.00019      |
|    value_loss           | 7.05         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -10.7    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 362      |
|    time_elapsed    | 6232     |
|    total_timesteps | 2965504  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.69e+03     |
|    ep_rew_mean          | -9.67        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 363          |
|    time_elapsed         | 6246         |
|    total_timesteps      | 2973696      |
| train/                  |              |
|    approx_kl            | 0.0001614997 |
|    clip_fraction        | 0.00123      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.204       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.41         |
|    n_updates            | 3620         |
|    policy_gradient_loss | 1.17e-05     |
|    value_loss           | 7.97         |
------------------------------------------
Eval num_timesteps=2980000, episode_reward=-31.50 +/- 29.76
Episode length: 1807.00 +/- 347.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.81e+03     |
|    mean_reward          | -31.5        |
| time/                   |              |
|    total_timesteps      | 2980000      |
| train/                  |              |
|    approx_kl            | 0.0010716794 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.193       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.48         |
|    n_updates            | 3630         |
|    policy_gradient_loss | -0.000596    |
|    value_loss           | 8.93         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -9.15    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 364      |
|    time_elapsed    | 6268     |
|    total_timesteps | 2981888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.69e+03    |
|    ep_rew_mean          | -9.5        |
| time/                   |             |
|    fps                  | 475         |
|    iterations           | 365         |
|    time_elapsed         | 6281        |
|    total_timesteps      | 2990080     |
| train/                  |             |
|    approx_kl            | 0.000563335 |
|    clip_fraction        | 0.00771     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.213      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.46        |
|    n_updates            | 3640        |
|    policy_gradient_loss | -0.000274   |
|    value_loss           | 8.53        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.7e+03      |
|    ep_rew_mean          | -9.03        |
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 366          |
|    time_elapsed         | 6299         |
|    total_timesteps      | 2998272      |
| train/                  |              |
|    approx_kl            | 0.0008177976 |
|    clip_fraction        | 0.00525      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.189       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.27         |
|    n_updates            | 3650         |
|    policy_gradient_loss | -0.000411    |
|    value_loss           | 8.49         |
------------------------------------------
Eval num_timesteps=3000000, episode_reward=-5.10 +/- 34.05
Episode length: 1865.00 +/- 299.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.86e+03     |
|    mean_reward          | -5.1         |
| time/                   |              |
|    total_timesteps      | 3000000      |
| train/                  |              |
|    approx_kl            | 0.0007518467 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.209       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.07         |
|    n_updates            | 3660         |
|    policy_gradient_loss | -0.000617    |
|    value_loss           | 8.23         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -9.47    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 367      |
|    time_elapsed    | 6320     |
|    total_timesteps | 3006464  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.7e+03      |
|    ep_rew_mean          | -8.64        |
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 368          |
|    time_elapsed         | 6334         |
|    total_timesteps      | 3014656      |
| train/                  |              |
|    approx_kl            | 0.0003993877 |
|    clip_fraction        | 0.000183     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.195       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.64         |
|    n_updates            | 3670         |
|    policy_gradient_loss | 7.25e-05     |
|    value_loss           | 7.91         |
------------------------------------------
Eval num_timesteps=3020000, episode_reward=15.50 +/- 36.59
Episode length: 1617.80 +/- 158.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.62e+03      |
|    mean_reward          | 15.5          |
| time/                   |               |
|    total_timesteps      | 3020000       |
| train/                  |               |
|    approx_kl            | 0.00055681844 |
|    clip_fraction        | 0.00291       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.216        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.24          |
|    n_updates            | 3680          |
|    policy_gradient_loss | -9.16e-05     |
|    value_loss           | 8.3           |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -8.92    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 369      |
|    time_elapsed    | 6356     |
|    total_timesteps | 3022848  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.68e+03     |
|    ep_rew_mean          | -7.26        |
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 370          |
|    time_elapsed         | 6369         |
|    total_timesteps      | 3031040      |
| train/                  |              |
|    approx_kl            | 0.0011119931 |
|    clip_fraction        | 0.00419      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.195       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.84         |
|    n_updates            | 3690         |
|    policy_gradient_loss | -0.00024     |
|    value_loss           | 8.63         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.67e+03      |
|    ep_rew_mean          | -10.1         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 371           |
|    time_elapsed         | 6383          |
|    total_timesteps      | 3039232       |
| train/                  |               |
|    approx_kl            | 0.00014675409 |
|    clip_fraction        | 0.00132       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.198        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.95          |
|    n_updates            | 3700          |
|    policy_gradient_loss | -0.000121     |
|    value_loss           | 9.15          |
-------------------------------------------
Eval num_timesteps=3040000, episode_reward=-9.80 +/- 49.30
Episode length: 1559.40 +/- 249.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.56e+03     |
|    mean_reward          | -9.8         |
| time/                   |              |
|    total_timesteps      | 3040000      |
| train/                  |              |
|    approx_kl            | 0.0009773533 |
|    clip_fraction        | 0.0033       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.187       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.82         |
|    n_updates            | 3710         |
|    policy_gradient_loss | -7.99e-05    |
|    value_loss           | 8.97         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -11.3    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 372      |
|    time_elapsed    | 6405     |
|    total_timesteps | 3047424  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.68e+03      |
|    ep_rew_mean          | -10.7         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 373           |
|    time_elapsed         | 6419          |
|    total_timesteps      | 3055616       |
| train/                  |               |
|    approx_kl            | 0.00062215957 |
|    clip_fraction        | 0.00641       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.186        |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 3.61          |
|    n_updates            | 3720          |
|    policy_gradient_loss | -0.000446     |
|    value_loss           | 9.62          |
-------------------------------------------
Eval num_timesteps=3060000, episode_reward=-27.90 +/- 11.78
Episode length: 1707.20 +/- 323.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.71e+03     |
|    mean_reward          | -27.9        |
| time/                   |              |
|    total_timesteps      | 3060000      |
| train/                  |              |
|    approx_kl            | 0.0007864663 |
|    clip_fraction        | 0.0115       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.209       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.44         |
|    n_updates            | 3730         |
|    policy_gradient_loss | -0.000547    |
|    value_loss           | 7.87         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -10.7    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 374      |
|    time_elapsed    | 6440     |
|    total_timesteps | 3063808  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.65e+03     |
|    ep_rew_mean          | -13.3        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 375          |
|    time_elapsed         | 6453         |
|    total_timesteps      | 3072000      |
| train/                  |              |
|    approx_kl            | 0.0004021668 |
|    clip_fraction        | 0.00107      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.23        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.33         |
|    n_updates            | 3740         |
|    policy_gradient_loss | 1.13e-05     |
|    value_loss           | 8.96         |
------------------------------------------
Eval num_timesteps=3080000, episode_reward=1.20 +/- 40.77
Episode length: 1813.60 +/- 146.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.81e+03     |
|    mean_reward          | 1.2          |
| time/                   |              |
|    total_timesteps      | 3080000      |
| train/                  |              |
|    approx_kl            | 0.0011841657 |
|    clip_fraction        | 0.00596      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.21        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.81         |
|    n_updates            | 3750         |
|    policy_gradient_loss | -0.000328    |
|    value_loss           | 8.66         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -11.8    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 376      |
|    time_elapsed    | 6475     |
|    total_timesteps | 3080192  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.65e+03      |
|    ep_rew_mean          | -14.5         |
| time/                   |               |
|    fps                  | 475           |
|    iterations           | 377           |
|    time_elapsed         | 6488          |
|    total_timesteps      | 3088384       |
| train/                  |               |
|    approx_kl            | 0.00097142323 |
|    clip_fraction        | 0.00914       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.175        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.81          |
|    n_updates            | 3760          |
|    policy_gradient_loss | -0.000539     |
|    value_loss           | 8.5           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.64e+03      |
|    ep_rew_mean          | -14.4         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 378           |
|    time_elapsed         | 6501          |
|    total_timesteps      | 3096576       |
| train/                  |               |
|    approx_kl            | 0.00017671971 |
|    clip_fraction        | 0.000854      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.183        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.38          |
|    n_updates            | 3770          |
|    policy_gradient_loss | 3.62e-05      |
|    value_loss           | 9.22          |
-------------------------------------------
Eval num_timesteps=3100000, episode_reward=3.70 +/- 31.46
Episode length: 1671.80 +/- 95.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.67e+03      |
|    mean_reward          | 3.7           |
| time/                   |               |
|    total_timesteps      | 3100000       |
| train/                  |               |
|    approx_kl            | 0.00065368647 |
|    clip_fraction        | 0.00311       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.152        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.56          |
|    n_updates            | 3780          |
|    policy_gradient_loss | -0.000175     |
|    value_loss           | 9.24          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -14.8    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 379      |
|    time_elapsed    | 6522     |
|    total_timesteps | 3104768  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.64e+03      |
|    ep_rew_mean          | -14.6         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 380           |
|    time_elapsed         | 6537          |
|    total_timesteps      | 3112960       |
| train/                  |               |
|    approx_kl            | 0.00078684266 |
|    clip_fraction        | 0.00991       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.126        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.85          |
|    n_updates            | 3790          |
|    policy_gradient_loss | -0.000694     |
|    value_loss           | 9.13          |
-------------------------------------------
Eval num_timesteps=3120000, episode_reward=-24.30 +/- 30.52
Episode length: 1604.40 +/- 290.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.6e+03      |
|    mean_reward          | -24.3        |
| time/                   |              |
|    total_timesteps      | 3120000      |
| train/                  |              |
|    approx_kl            | 7.058633e-05 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.135       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.69         |
|    n_updates            | 3800         |
|    policy_gradient_loss | 5.21e-05     |
|    value_loss           | 7.5          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -15.1    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 381      |
|    time_elapsed    | 6558     |
|    total_timesteps | 3121152  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.64e+03    |
|    ep_rew_mean          | -14.5       |
| time/                   |             |
|    fps                  | 476         |
|    iterations           | 382         |
|    time_elapsed         | 6572        |
|    total_timesteps      | 3129344     |
| train/                  |             |
|    approx_kl            | 0.000513691 |
|    clip_fraction        | 0.00338     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.114      |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 4.52        |
|    n_updates            | 3810        |
|    policy_gradient_loss | -0.000189   |
|    value_loss           | 7.54        |
-----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.63e+03      |
|    ep_rew_mean          | -14           |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 383           |
|    time_elapsed         | 6586          |
|    total_timesteps      | 3137536       |
| train/                  |               |
|    approx_kl            | 0.00020874142 |
|    clip_fraction        | 0.00167       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.12         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.79          |
|    n_updates            | 3820          |
|    policy_gradient_loss | -2.67e-06     |
|    value_loss           | 8.08          |
-------------------------------------------
Eval num_timesteps=3140000, episode_reward=7.40 +/- 47.75
Episode length: 1370.80 +/- 134.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.37e+03     |
|    mean_reward          | 7.4          |
| time/                   |              |
|    total_timesteps      | 3140000      |
| train/                  |              |
|    approx_kl            | 0.0004828016 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.136       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.82         |
|    n_updates            | 3830         |
|    policy_gradient_loss | -0.000511    |
|    value_loss           | 8.88         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -15.3    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 384      |
|    time_elapsed    | 6607     |
|    total_timesteps | 3145728  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.64e+03      |
|    ep_rew_mean          | -17.3         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 385           |
|    time_elapsed         | 6621          |
|    total_timesteps      | 3153920       |
| train/                  |               |
|    approx_kl            | 0.00071862986 |
|    clip_fraction        | 0.0101        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.121        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.47          |
|    n_updates            | 3840          |
|    policy_gradient_loss | -0.000689     |
|    value_loss           | 8.08          |
-------------------------------------------
Eval num_timesteps=3160000, episode_reward=21.80 +/- 36.49
Episode length: 1527.00 +/- 262.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.53e+03      |
|    mean_reward          | 21.8          |
| time/                   |               |
|    total_timesteps      | 3160000       |
| train/                  |               |
|    approx_kl            | 6.6427674e-05 |
|    clip_fraction        | 0.00083       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.13         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.64          |
|    n_updates            | 3850          |
|    policy_gradient_loss | 8e-05         |
|    value_loss           | 8.27          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -17.1    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 386      |
|    time_elapsed    | 6642     |
|    total_timesteps | 3162112  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.62e+03      |
|    ep_rew_mean          | -16.4         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 387           |
|    time_elapsed         | 6656          |
|    total_timesteps      | 3170304       |
| train/                  |               |
|    approx_kl            | 1.8059414e-05 |
|    clip_fraction        | 0.00104       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.113        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.29          |
|    n_updates            | 3860          |
|    policy_gradient_loss | 0.000124      |
|    value_loss           | 9.23          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.61e+03     |
|    ep_rew_mean          | -17          |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 388          |
|    time_elapsed         | 6670         |
|    total_timesteps      | 3178496      |
| train/                  |              |
|    approx_kl            | 0.0005458031 |
|    clip_fraction        | 0.00244      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.11        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.87         |
|    n_updates            | 3870         |
|    policy_gradient_loss | -0.000156    |
|    value_loss           | 8.84         |
------------------------------------------
Eval num_timesteps=3180000, episode_reward=-14.20 +/- 31.31
Episode length: 1663.60 +/- 140.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.66e+03     |
|    mean_reward          | -14.2        |
| time/                   |              |
|    total_timesteps      | 3180000      |
| train/                  |              |
|    approx_kl            | 0.0003520217 |
|    clip_fraction        | 0.00398      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0883      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.1          |
|    n_updates            | 3880         |
|    policy_gradient_loss | -0.000245    |
|    value_loss           | 8.49         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -14.3    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 389      |
|    time_elapsed    | 6692     |
|    total_timesteps | 3186688  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.62e+03     |
|    ep_rew_mean          | -16.3        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 390          |
|    time_elapsed         | 6706         |
|    total_timesteps      | 3194880      |
| train/                  |              |
|    approx_kl            | 0.0003614264 |
|    clip_fraction        | 0.00341      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0756      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.16         |
|    n_updates            | 3890         |
|    policy_gradient_loss | 9.38e-05     |
|    value_loss           | 8.62         |
------------------------------------------
Eval num_timesteps=3200000, episode_reward=-33.40 +/- 31.52
Episode length: 1703.20 +/- 349.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.7e+03       |
|    mean_reward          | -33.4         |
| time/                   |               |
|    total_timesteps      | 3200000       |
| train/                  |               |
|    approx_kl            | 0.00030680298 |
|    clip_fraction        | 0.00309       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0704       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.18          |
|    n_updates            | 3900          |
|    policy_gradient_loss | -0.000109     |
|    value_loss           | 8.72          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -17.4    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 391      |
|    time_elapsed    | 6729     |
|    total_timesteps | 3203072  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.62e+03      |
|    ep_rew_mean          | -16.1         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 392           |
|    time_elapsed         | 6743          |
|    total_timesteps      | 3211264       |
| train/                  |               |
|    approx_kl            | 0.00026571867 |
|    clip_fraction        | 0.000989      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0596       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 6.38          |
|    n_updates            | 3910          |
|    policy_gradient_loss | -0.00013      |
|    value_loss           | 8.1           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.63e+03      |
|    ep_rew_mean          | -15.5         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 393           |
|    time_elapsed         | 6757          |
|    total_timesteps      | 3219456       |
| train/                  |               |
|    approx_kl            | 0.00013405345 |
|    clip_fraction        | 0.000574      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0613       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 6.31          |
|    n_updates            | 3920          |
|    policy_gradient_loss | 2.93e-05      |
|    value_loss           | 9.33          |
-------------------------------------------
Eval num_timesteps=3220000, episode_reward=-22.40 +/- 27.25
Episode length: 1583.60 +/- 340.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.58e+03      |
|    mean_reward          | -22.4         |
| time/                   |               |
|    total_timesteps      | 3220000       |
| train/                  |               |
|    approx_kl            | 1.9289953e-05 |
|    clip_fraction        | 0.00166       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0564       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.2           |
|    n_updates            | 3930          |
|    policy_gradient_loss | 9.23e-05      |
|    value_loss           | 8.99          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -18.6    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 394      |
|    time_elapsed    | 6776     |
|    total_timesteps | 3227648  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.64e+03      |
|    ep_rew_mean          | -18.1         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 395           |
|    time_elapsed         | 6790          |
|    total_timesteps      | 3235840       |
| train/                  |               |
|    approx_kl            | 0.00025874842 |
|    clip_fraction        | 0.00663       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0764       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.55          |
|    n_updates            | 3940          |
|    policy_gradient_loss | -0.000388     |
|    value_loss           | 8.09          |
-------------------------------------------
Eval num_timesteps=3240000, episode_reward=11.60 +/- 41.03
Episode length: 1557.40 +/- 393.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.56e+03     |
|    mean_reward          | 11.6         |
| time/                   |              |
|    total_timesteps      | 3240000      |
| train/                  |              |
|    approx_kl            | 0.0001847891 |
|    clip_fraction        | 0.00477      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0871      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.16         |
|    n_updates            | 3950         |
|    policy_gradient_loss | -0.000287    |
|    value_loss           | 8.19         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -16.6    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 396      |
|    time_elapsed    | 6812     |
|    total_timesteps | 3244032  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.65e+03      |
|    ep_rew_mean          | -17.7         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 397           |
|    time_elapsed         | 6827          |
|    total_timesteps      | 3252224       |
| train/                  |               |
|    approx_kl            | 0.00015305722 |
|    clip_fraction        | 0.003         |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0755       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.28          |
|    n_updates            | 3960          |
|    policy_gradient_loss | -5.71e-05     |
|    value_loss           | 7.46          |
-------------------------------------------
Eval num_timesteps=3260000, episode_reward=5.40 +/- 46.62
Episode length: 1501.00 +/- 236.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.5e+03       |
|    mean_reward          | 5.4           |
| time/                   |               |
|    total_timesteps      | 3260000       |
| train/                  |               |
|    approx_kl            | 0.00035767455 |
|    clip_fraction        | 0.00283       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0752       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.94          |
|    n_updates            | 3970          |
|    policy_gradient_loss | -0.000221     |
|    value_loss           | 8.1           |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -14.5    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 398      |
|    time_elapsed    | 6847     |
|    total_timesteps | 3260416  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.66e+03      |
|    ep_rew_mean          | -15.5         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 399           |
|    time_elapsed         | 6862          |
|    total_timesteps      | 3268608       |
| train/                  |               |
|    approx_kl            | 0.00028114708 |
|    clip_fraction        | 0.00266       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0662       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.16          |
|    n_updates            | 3980          |
|    policy_gradient_loss | -0.000153     |
|    value_loss           | 7.91          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.65e+03      |
|    ep_rew_mean          | -17.3         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 400           |
|    time_elapsed         | 6876          |
|    total_timesteps      | 3276800       |
| train/                  |               |
|    approx_kl            | 5.3398253e-05 |
|    clip_fraction        | 0.000696      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0646       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.34          |
|    n_updates            | 3990          |
|    policy_gradient_loss | 2.72e-05      |
|    value_loss           | 9.07          |
-------------------------------------------
Eval num_timesteps=3280000, episode_reward=-29.60 +/- 57.10
Episode length: 1474.40 +/- 376.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.47e+03      |
|    mean_reward          | -29.6         |
| time/                   |               |
|    total_timesteps      | 3280000       |
| train/                  |               |
|    approx_kl            | 0.00023721793 |
|    clip_fraction        | 0.0038        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.057        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.61          |
|    n_updates            | 4000          |
|    policy_gradient_loss | -0.000161     |
|    value_loss           | 8.46          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -17.2    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 401      |
|    time_elapsed    | 6895     |
|    total_timesteps | 3284992  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.63e+03      |
|    ep_rew_mean          | -19.3         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 402           |
|    time_elapsed         | 6909          |
|    total_timesteps      | 3293184       |
| train/                  |               |
|    approx_kl            | 5.1171934e-05 |
|    clip_fraction        | 0.000989      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0529       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 5.11          |
|    n_updates            | 4010          |
|    policy_gradient_loss | -5.02e-05     |
|    value_loss           | 8.52          |
-------------------------------------------
Eval num_timesteps=3300000, episode_reward=12.90 +/- 30.11
Episode length: 1834.40 +/- 331.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.83e+03      |
|    mean_reward          | 12.9          |
| time/                   |               |
|    total_timesteps      | 3300000       |
| train/                  |               |
|    approx_kl            | 9.1963244e-05 |
|    clip_fraction        | 0.00139       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0563       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.13          |
|    n_updates            | 4020          |
|    policy_gradient_loss | -5.41e-05     |
|    value_loss           | 7.84          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -18.2    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 403      |
|    time_elapsed    | 6932     |
|    total_timesteps | 3301376  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.65e+03      |
|    ep_rew_mean          | -18.4         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 404           |
|    time_elapsed         | 6946          |
|    total_timesteps      | 3309568       |
| train/                  |               |
|    approx_kl            | 0.00020749183 |
|    clip_fraction        | 0.00269       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0511       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.19          |
|    n_updates            | 4030          |
|    policy_gradient_loss | -0.000128     |
|    value_loss           | 8.24          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.65e+03      |
|    ep_rew_mean          | -18.4         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 405           |
|    time_elapsed         | 6961          |
|    total_timesteps      | 3317760       |
| train/                  |               |
|    approx_kl            | 0.00019838638 |
|    clip_fraction        | 0.0041        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0441       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.16          |
|    n_updates            | 4040          |
|    policy_gradient_loss | -0.000246     |
|    value_loss           | 8.9           |
-------------------------------------------
Eval num_timesteps=3320000, episode_reward=-26.30 +/- 38.22
Episode length: 1570.00 +/- 316.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.57e+03     |
|    mean_reward          | -26.3        |
| time/                   |              |
|    total_timesteps      | 3320000      |
| train/                  |              |
|    approx_kl            | 0.0001708061 |
|    clip_fraction        | 0.00475      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0514      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.63         |
|    n_updates            | 4050         |
|    policy_gradient_loss | -0.000459    |
|    value_loss           | 7.46         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -18.5    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 406      |
|    time_elapsed    | 6983     |
|    total_timesteps | 3325952  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.65e+03      |
|    ep_rew_mean          | -17.7         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 407           |
|    time_elapsed         | 6998          |
|    total_timesteps      | 3334144       |
| train/                  |               |
|    approx_kl            | 0.00019169862 |
|    clip_fraction        | 0.00345       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0442       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.63          |
|    n_updates            | 4060          |
|    policy_gradient_loss | -0.000149     |
|    value_loss           | 8.22          |
-------------------------------------------
Eval num_timesteps=3340000, episode_reward=0.10 +/- 50.56
Episode length: 1577.00 +/- 303.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.58e+03      |
|    mean_reward          | 0.1           |
| time/                   |               |
|    total_timesteps      | 3340000       |
| train/                  |               |
|    approx_kl            | 0.00015415766 |
|    clip_fraction        | 0.0012        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0514       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.96          |
|    n_updates            | 4070          |
|    policy_gradient_loss | -6.9e-05      |
|    value_loss           | 8.38          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -18.8    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 408      |
|    time_elapsed    | 7020     |
|    total_timesteps | 3342336  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.65e+03      |
|    ep_rew_mean          | -18.5         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 409           |
|    time_elapsed         | 7034          |
|    total_timesteps      | 3350528       |
| train/                  |               |
|    approx_kl            | 0.00011340878 |
|    clip_fraction        | 0.00338       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0564       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 5.94          |
|    n_updates            | 4080          |
|    policy_gradient_loss | -0.000212     |
|    value_loss           | 8.95          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.64e+03     |
|    ep_rew_mean          | -20.4        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 410          |
|    time_elapsed         | 7048         |
|    total_timesteps      | 3358720      |
| train/                  |              |
|    approx_kl            | 0.0002772425 |
|    clip_fraction        | 0.00546      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0456      |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 6.23         |
|    n_updates            | 4090         |
|    policy_gradient_loss | -0.000308    |
|    value_loss           | 8.88         |
------------------------------------------
Eval num_timesteps=3360000, episode_reward=-4.90 +/- 50.13
Episode length: 1359.40 +/- 97.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.36e+03     |
|    mean_reward          | -4.9         |
| time/                   |              |
|    total_timesteps      | 3360000      |
| train/                  |              |
|    approx_kl            | 7.304045e-05 |
|    clip_fraction        | 0.00216      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0478      |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 4.34         |
|    n_updates            | 4100         |
|    policy_gradient_loss | -0.000118    |
|    value_loss           | 9.26         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -22.4    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 411      |
|    time_elapsed    | 7069     |
|    total_timesteps | 3366912  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.65e+03      |
|    ep_rew_mean          | -21           |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 412           |
|    time_elapsed         | 7083          |
|    total_timesteps      | 3375104       |
| train/                  |               |
|    approx_kl            | 0.00013534402 |
|    clip_fraction        | 0.00111       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0416       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.79          |
|    n_updates            | 4110          |
|    policy_gradient_loss | -7.74e-05     |
|    value_loss           | 7.1           |
-------------------------------------------
Eval num_timesteps=3380000, episode_reward=9.60 +/- 32.44
Episode length: 1708.20 +/- 305.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.71e+03      |
|    mean_reward          | 9.6           |
| time/                   |               |
|    total_timesteps      | 3380000       |
| train/                  |               |
|    approx_kl            | 0.00017038718 |
|    clip_fraction        | 0.00283       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0306       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.32          |
|    n_updates            | 4120          |
|    policy_gradient_loss | -0.000184     |
|    value_loss           | 8.45          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -21.2    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 413      |
|    time_elapsed    | 7104     |
|    total_timesteps | 3383296  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.67e+03      |
|    ep_rew_mean          | -21.2         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 414           |
|    time_elapsed         | 7118          |
|    total_timesteps      | 3391488       |
| train/                  |               |
|    approx_kl            | 3.1969015e-05 |
|    clip_fraction        | 0.00061       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0329       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.4           |
|    n_updates            | 4130          |
|    policy_gradient_loss | 4.06e-05      |
|    value_loss           | 8.4           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.66e+03     |
|    ep_rew_mean          | -21.3        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 415          |
|    time_elapsed         | 7133         |
|    total_timesteps      | 3399680      |
| train/                  |              |
|    approx_kl            | 9.368874e-05 |
|    clip_fraction        | 0.00183      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0361      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.12         |
|    n_updates            | 4140         |
|    policy_gradient_loss | -0.000151    |
|    value_loss           | 8.18         |
------------------------------------------
Eval num_timesteps=3400000, episode_reward=1.20 +/- 42.22
Episode length: 1491.40 +/- 272.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.49e+03      |
|    mean_reward          | 1.2           |
| time/                   |               |
|    total_timesteps      | 3400000       |
| train/                  |               |
|    approx_kl            | 0.00011279593 |
|    clip_fraction        | 0.0014        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0334       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.78          |
|    n_updates            | 4150          |
|    policy_gradient_loss | -2.87e-05     |
|    value_loss           | 9.13          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -20.9    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 416      |
|    time_elapsed    | 7154     |
|    total_timesteps | 3407872  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.64e+03     |
|    ep_rew_mean          | -19.7        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 417          |
|    time_elapsed         | 7167         |
|    total_timesteps      | 3416064      |
| train/                  |              |
|    approx_kl            | 9.491955e-05 |
|    clip_fraction        | 0.000952     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0274      |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 4.89         |
|    n_updates            | 4160         |
|    policy_gradient_loss | -8.17e-05    |
|    value_loss           | 9.19         |
------------------------------------------
Eval num_timesteps=3420000, episode_reward=-23.10 +/- 37.90
Episode length: 1763.60 +/- 306.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.76e+03     |
|    mean_reward          | -23.1        |
| time/                   |              |
|    total_timesteps      | 3420000      |
| train/                  |              |
|    approx_kl            | 3.205722e-05 |
|    clip_fraction        | 0.00105      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0225      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.7          |
|    n_updates            | 4170         |
|    policy_gradient_loss | -2.96e-05    |
|    value_loss           | 9.27         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -20.6    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 418      |
|    time_elapsed    | 7187     |
|    total_timesteps | 3424256  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.64e+03      |
|    ep_rew_mean          | -21.3         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 419           |
|    time_elapsed         | 7201          |
|    total_timesteps      | 3432448       |
| train/                  |               |
|    approx_kl            | 5.9574755e-05 |
|    clip_fraction        | 0.000916      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0287       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.27          |
|    n_updates            | 4180          |
|    policy_gradient_loss | -3.42e-05     |
|    value_loss           | 8.61          |
-------------------------------------------
Eval num_timesteps=3440000, episode_reward=-36.40 +/- 38.61
Episode length: 1509.00 +/- 139.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.51e+03      |
|    mean_reward          | -36.4         |
| time/                   |               |
|    total_timesteps      | 3440000       |
| train/                  |               |
|    approx_kl            | 4.6312372e-05 |
|    clip_fraction        | 0.000928      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0316       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.08          |
|    n_updates            | 4190          |
|    policy_gradient_loss | -5.13e-05     |
|    value_loss           | 8.66          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -22.2    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 420      |
|    time_elapsed    | 7221     |
|    total_timesteps | 3440640  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.64e+03      |
|    ep_rew_mean          | -23.1         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 421           |
|    time_elapsed         | 7236          |
|    total_timesteps      | 3448832       |
| train/                  |               |
|    approx_kl            | 5.8434453e-05 |
|    clip_fraction        | 0.000696      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.028        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.54          |
|    n_updates            | 4200          |
|    policy_gradient_loss | -1.33e-05     |
|    value_loss           | 9.57          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.65e+03      |
|    ep_rew_mean          | -22.7         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 422           |
|    time_elapsed         | 7250          |
|    total_timesteps      | 3457024       |
| train/                  |               |
|    approx_kl            | 2.4164641e-05 |
|    clip_fraction        | 0.00105       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0336       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.3           |
|    n_updates            | 4210          |
|    policy_gradient_loss | 1.1e-05       |
|    value_loss           | 7.28          |
-------------------------------------------
Eval num_timesteps=3460000, episode_reward=-15.30 +/- 29.51
Episode length: 1807.40 +/- 384.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.81e+03     |
|    mean_reward          | -15.3        |
| time/                   |              |
|    total_timesteps      | 3460000      |
| train/                  |              |
|    approx_kl            | 7.358662e-05 |
|    clip_fraction        | 0.000647     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0277      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.88         |
|    n_updates            | 4220         |
|    policy_gradient_loss | -5.82e-05    |
|    value_loss           | 7.99         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -21.2    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 423      |
|    time_elapsed    | 7271     |
|    total_timesteps | 3465216  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.66e+03     |
|    ep_rew_mean          | -21.1        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 424          |
|    time_elapsed         | 7285         |
|    total_timesteps      | 3473408      |
| train/                  |              |
|    approx_kl            | 5.578364e-05 |
|    clip_fraction        | 0.000269     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0299      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.38         |
|    n_updates            | 4230         |
|    policy_gradient_loss | 1.18e-05     |
|    value_loss           | 8.99         |
------------------------------------------
Eval num_timesteps=3480000, episode_reward=-25.90 +/- 36.31
Episode length: 1589.20 +/- 253.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.59e+03     |
|    mean_reward          | -25.9        |
| time/                   |              |
|    total_timesteps      | 3480000      |
| train/                  |              |
|    approx_kl            | 6.416633e-05 |
|    clip_fraction        | 0.00139      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0359      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.19         |
|    n_updates            | 4240         |
|    policy_gradient_loss | -6.81e-05    |
|    value_loss           | 8.77         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -20.7    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 425      |
|    time_elapsed    | 7306     |
|    total_timesteps | 3481600  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.65e+03      |
|    ep_rew_mean          | -20.3         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 426           |
|    time_elapsed         | 7322          |
|    total_timesteps      | 3489792       |
| train/                  |               |
|    approx_kl            | 7.9807745e-05 |
|    clip_fraction        | 0.00173       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0312       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.07          |
|    n_updates            | 4250          |
|    policy_gradient_loss | -2.13e-05     |
|    value_loss           | 9.3           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.65e+03      |
|    ep_rew_mean          | -19.5         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 427           |
|    time_elapsed         | 7336          |
|    total_timesteps      | 3497984       |
| train/                  |               |
|    approx_kl            | 4.7490154e-05 |
|    clip_fraction        | 0.000623      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0277       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.2           |
|    n_updates            | 4260          |
|    policy_gradient_loss | -2.62e-05     |
|    value_loss           | 8.52          |
-------------------------------------------
Eval num_timesteps=3500000, episode_reward=-45.10 +/- 15.28
Episode length: 1402.60 +/- 275.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.4e+03       |
|    mean_reward          | -45.1         |
| time/                   |               |
|    total_timesteps      | 3500000       |
| train/                  |               |
|    approx_kl            | 4.7320224e-05 |
|    clip_fraction        | 0.000806      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0308       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.73          |
|    n_updates            | 4270          |
|    policy_gradient_loss | -5.06e-05     |
|    value_loss           | 8.54          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -19.7    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 428      |
|    time_elapsed    | 7357     |
|    total_timesteps | 3506176  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.65e+03      |
|    ep_rew_mean          | -19.6         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 429           |
|    time_elapsed         | 7371          |
|    total_timesteps      | 3514368       |
| train/                  |               |
|    approx_kl            | 2.2530076e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0311       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.88          |
|    n_updates            | 4280          |
|    policy_gradient_loss | 2.24e-05      |
|    value_loss           | 8.65          |
-------------------------------------------
Eval num_timesteps=3520000, episode_reward=19.90 +/- 32.89
Episode length: 1660.00 +/- 358.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.66e+03     |
|    mean_reward          | 19.9         |
| time/                   |              |
|    total_timesteps      | 3520000      |
| train/                  |              |
|    approx_kl            | 9.911594e-05 |
|    clip_fraction        | 0.00193      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0259      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.73         |
|    n_updates            | 4290         |
|    policy_gradient_loss | -6.09e-05    |
|    value_loss           | 8.59         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -16.7    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 430      |
|    time_elapsed    | 7397     |
|    total_timesteps | 3522560  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.65e+03     |
|    ep_rew_mean          | -17.5        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 431          |
|    time_elapsed         | 7412         |
|    total_timesteps      | 3530752      |
| train/                  |              |
|    approx_kl            | 6.226496e-05 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0238      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.77         |
|    n_updates            | 4300         |
|    policy_gradient_loss | 0.000226     |
|    value_loss           | 8.81         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.63e+03     |
|    ep_rew_mean          | -16.6        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 432          |
|    time_elapsed         | 7427         |
|    total_timesteps      | 3538944      |
| train/                  |              |
|    approx_kl            | 2.523006e-05 |
|    clip_fraction        | 0.00011      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0303      |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 3.14         |
|    n_updates            | 4310         |
|    policy_gradient_loss | 5.31e-05     |
|    value_loss           | 7.88         |
------------------------------------------
Eval num_timesteps=3540000, episode_reward=6.10 +/- 26.55
Episode length: 1793.00 +/- 479.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.79e+03      |
|    mean_reward          | 6.1           |
| time/                   |               |
|    total_timesteps      | 3540000       |
| train/                  |               |
|    approx_kl            | 1.0448581e-05 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.028        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.61          |
|    n_updates            | 4320          |
|    policy_gradient_loss | 2.3e-05       |
|    value_loss           | 9.06          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -14.8    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 433      |
|    time_elapsed    | 7449     |
|    total_timesteps | 3547136  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.63e+03     |
|    ep_rew_mean          | -15.3        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 434          |
|    time_elapsed         | 7464         |
|    total_timesteps      | 3555328      |
| train/                  |              |
|    approx_kl            | 7.948669e-05 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0236      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.5          |
|    n_updates            | 4330         |
|    policy_gradient_loss | -8.22e-05    |
|    value_loss           | 8.32         |
------------------------------------------
Eval num_timesteps=3560000, episode_reward=-1.40 +/- 16.88
Episode length: 1827.60 +/- 281.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.83e+03     |
|    mean_reward          | -1.4         |
| time/                   |              |
|    total_timesteps      | 3560000      |
| train/                  |              |
|    approx_kl            | 4.112399e-05 |
|    clip_fraction        | 0.000696     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0245      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.24         |
|    n_updates            | 4340         |
|    policy_gradient_loss | -0.000119    |
|    value_loss           | 8.41         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -16.6    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 435      |
|    time_elapsed    | 7487     |
|    total_timesteps | 3563520  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.64e+03     |
|    ep_rew_mean          | -15.5        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 436          |
|    time_elapsed         | 7501         |
|    total_timesteps      | 3571712      |
| train/                  |              |
|    approx_kl            | 9.293358e-05 |
|    clip_fraction        | 0.0017       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0227      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.31         |
|    n_updates            | 4350         |
|    policy_gradient_loss | -0.000145    |
|    value_loss           | 8.47         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.65e+03     |
|    ep_rew_mean          | -15.1        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 437          |
|    time_elapsed         | 7515         |
|    total_timesteps      | 3579904      |
| train/                  |              |
|    approx_kl            | 4.012722e-05 |
|    clip_fraction        | 0.000842     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0203      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.46         |
|    n_updates            | 4360         |
|    policy_gradient_loss | -4.01e-05    |
|    value_loss           | 7.6          |
------------------------------------------
Eval num_timesteps=3580000, episode_reward=-8.10 +/- 49.44
Episode length: 1490.00 +/- 166.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.49e+03      |
|    mean_reward          | -8.1          |
| time/                   |               |
|    total_timesteps      | 3580000       |
| train/                  |               |
|    approx_kl            | 5.2954434e-05 |
|    clip_fraction        | 0.000623      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.023        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.86          |
|    n_updates            | 4370          |
|    policy_gradient_loss | -1.7e-05      |
|    value_loss           | 8.62          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -15.9    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 438      |
|    time_elapsed    | 7535     |
|    total_timesteps | 3588096  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.63e+03    |
|    ep_rew_mean          | -16.4       |
| time/                   |             |
|    fps                  | 476         |
|    iterations           | 439         |
|    time_elapsed         | 7549        |
|    total_timesteps      | 3596288     |
| train/                  |             |
|    approx_kl            | 3.86506e-05 |
|    clip_fraction        | 0.00105     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0281     |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.91        |
|    n_updates            | 4380        |
|    policy_gradient_loss | -3.83e-05   |
|    value_loss           | 9.85        |
-----------------------------------------
Eval num_timesteps=3600000, episode_reward=-10.60 +/- 37.88
Episode length: 1658.80 +/- 311.19
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 1.66e+03       |
|    mean_reward          | -10.6          |
| time/                   |                |
|    total_timesteps      | 3600000        |
| train/                  |                |
|    approx_kl            | 0.000110696215 |
|    clip_fraction        | 0.0025         |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0241        |
|    explained_variance   | 0              |
|    learning_rate        | 0.0003         |
|    loss                 | 4.3            |
|    n_updates            | 4390           |
|    policy_gradient_loss | -0.000225      |
|    value_loss           | 9.14           |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -16.7    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 440      |
|    time_elapsed    | 7570     |
|    total_timesteps | 3604480  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.66e+03     |
|    ep_rew_mean          | -15.3        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 441          |
|    time_elapsed         | 7585         |
|    total_timesteps      | 3612672      |
| train/                  |              |
|    approx_kl            | 8.014079e-05 |
|    clip_fraction        | 0.00121      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.021       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.73         |
|    n_updates            | 4400         |
|    policy_gradient_loss | -9.04e-05    |
|    value_loss           | 8.96         |
------------------------------------------
Eval num_timesteps=3620000, episode_reward=-12.50 +/- 27.93
Episode length: 1644.00 +/- 116.54
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 1.64e+03       |
|    mean_reward          | -12.5          |
| time/                   |                |
|    total_timesteps      | 3620000        |
| train/                  |                |
|    approx_kl            | 0.000107166365 |
|    clip_fraction        | 0.0022         |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0174        |
|    explained_variance   | 5.96e-08       |
|    learning_rate        | 0.0003         |
|    loss                 | 2.06           |
|    n_updates            | 4410           |
|    policy_gradient_loss | -0.000156      |
|    value_loss           | 7.29           |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -14.1    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 442      |
|    time_elapsed    | 7607     |
|    total_timesteps | 3620864  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.65e+03     |
|    ep_rew_mean          | -13          |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 443          |
|    time_elapsed         | 7621         |
|    total_timesteps      | 3629056      |
| train/                  |              |
|    approx_kl            | 1.539765e-05 |
|    clip_fraction        | 0.000317     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.015       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.65         |
|    n_updates            | 4420         |
|    policy_gradient_loss | -1.82e-05    |
|    value_loss           | 8.68         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.64e+03     |
|    ep_rew_mean          | -13          |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 444          |
|    time_elapsed         | 7635         |
|    total_timesteps      | 3637248      |
| train/                  |              |
|    approx_kl            | 4.720084e-05 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0167      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.52         |
|    n_updates            | 4430         |
|    policy_gradient_loss | -2.29e-05    |
|    value_loss           | 8.78         |
------------------------------------------
Eval num_timesteps=3640000, episode_reward=-1.60 +/- 47.28
Episode length: 1685.40 +/- 289.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.69e+03      |
|    mean_reward          | -1.6          |
| time/                   |               |
|    total_timesteps      | 3640000       |
| train/                  |               |
|    approx_kl            | 3.9605868e-05 |
|    clip_fraction        | 0.00072       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0195       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 6             |
|    n_updates            | 4440          |
|    policy_gradient_loss | -5.37e-05     |
|    value_loss           | 7.76          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -12.8    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 445      |
|    time_elapsed    | 7656     |
|    total_timesteps | 3645440  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.64e+03     |
|    ep_rew_mean          | -11.4        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 446          |
|    time_elapsed         | 7669         |
|    total_timesteps      | 3653632      |
| train/                  |              |
|    approx_kl            | 4.196643e-05 |
|    clip_fraction        | 0.000806     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0182      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.35         |
|    n_updates            | 4450         |
|    policy_gradient_loss | -3.79e-05    |
|    value_loss           | 8.82         |
------------------------------------------
Eval num_timesteps=3660000, episode_reward=-16.00 +/- 44.17
Episode length: 1573.40 +/- 424.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.57e+03     |
|    mean_reward          | -16          |
| time/                   |              |
|    total_timesteps      | 3660000      |
| train/                  |              |
|    approx_kl            | 4.360976e-05 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0202      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.79         |
|    n_updates            | 4460         |
|    policy_gradient_loss | -6.91e-05    |
|    value_loss           | 9.16         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | -12.6    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 447      |
|    time_elapsed    | 7690     |
|    total_timesteps | 3661824  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.61e+03      |
|    ep_rew_mean          | -11.3         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 448           |
|    time_elapsed         | 7704          |
|    total_timesteps      | 3670016       |
| train/                  |               |
|    approx_kl            | 4.6854853e-05 |
|    clip_fraction        | 0.000452      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0178       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.04          |
|    n_updates            | 4470          |
|    policy_gradient_loss | -8.33e-06     |
|    value_loss           | 7.53          |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.62e+03    |
|    ep_rew_mean          | -12         |
| time/                   |             |
|    fps                  | 476         |
|    iterations           | 449         |
|    time_elapsed         | 7718        |
|    total_timesteps      | 3678208     |
| train/                  |             |
|    approx_kl            | 8.69433e-05 |
|    clip_fraction        | 0.000623    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0166     |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 5.68        |
|    n_updates            | 4480        |
|    policy_gradient_loss | 8.45e-06    |
|    value_loss           | 8.82        |
-----------------------------------------
Eval num_timesteps=3680000, episode_reward=22.40 +/- 27.39
Episode length: 1799.20 +/- 337.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.8e+03      |
|    mean_reward          | 22.4         |
| time/                   |              |
|    total_timesteps      | 3680000      |
| train/                  |              |
|    approx_kl            | 5.405247e-05 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.012       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.56         |
|    n_updates            | 4490         |
|    policy_gradient_loss | -8.77e-05    |
|    value_loss           | 8.98         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | -12.3    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 450      |
|    time_elapsed    | 7741     |
|    total_timesteps | 3686400  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.61e+03      |
|    ep_rew_mean          | -13.9         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 451           |
|    time_elapsed         | 7756          |
|    total_timesteps      | 3694592       |
| train/                  |               |
|    approx_kl            | 1.2039331e-05 |
|    clip_fraction        | 0.000159      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0127       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.12          |
|    n_updates            | 4500          |
|    policy_gradient_loss | 7.76e-06      |
|    value_loss           | 8.29          |
-------------------------------------------
Eval num_timesteps=3700000, episode_reward=-37.60 +/- 10.15
Episode length: 1729.40 +/- 73.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.73e+03      |
|    mean_reward          | -37.6         |
| time/                   |               |
|    total_timesteps      | 3700000       |
| train/                  |               |
|    approx_kl            | 7.0048496e-05 |
|    clip_fraction        | 0.000732      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0119       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.54          |
|    n_updates            | 4510          |
|    policy_gradient_loss | -1.71e-05     |
|    value_loss           | 7.46          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | -11.9    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 452      |
|    time_elapsed    | 7779     |
|    total_timesteps | 3702784  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.63e+03      |
|    ep_rew_mean          | -12.1         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 453           |
|    time_elapsed         | 7794          |
|    total_timesteps      | 3710976       |
| train/                  |               |
|    approx_kl            | 4.5823333e-05 |
|    clip_fraction        | 0.000574      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.01         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.66          |
|    n_updates            | 4520          |
|    policy_gradient_loss | -3.04e-05     |
|    value_loss           | 8.04          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.64e+03      |
|    ep_rew_mean          | -14.5         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 454           |
|    time_elapsed         | 7809          |
|    total_timesteps      | 3719168       |
| train/                  |               |
|    approx_kl            | 3.5076504e-05 |
|    clip_fraction        | 0.000464      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00904      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.05          |
|    n_updates            | 4530          |
|    policy_gradient_loss | -1.85e-05     |
|    value_loss           | 8.1           |
-------------------------------------------
Eval num_timesteps=3720000, episode_reward=-7.10 +/- 47.77
Episode length: 1338.00 +/- 297.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.34e+03      |
|    mean_reward          | -7.1          |
| time/                   |               |
|    total_timesteps      | 3720000       |
| train/                  |               |
|    approx_kl            | 4.1190528e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0113       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.87          |
|    n_updates            | 4540          |
|    policy_gradient_loss | -1.03e-05     |
|    value_loss           | 8.34          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -13.5    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 455      |
|    time_elapsed    | 7830     |
|    total_timesteps | 3727360  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.63e+03      |
|    ep_rew_mean          | -13.5         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 456           |
|    time_elapsed         | 7845          |
|    total_timesteps      | 3735552       |
| train/                  |               |
|    approx_kl            | 8.6261694e-05 |
|    clip_fraction        | 0.00187       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00923      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.71          |
|    n_updates            | 4550          |
|    policy_gradient_loss | -0.000117     |
|    value_loss           | 9.44          |
-------------------------------------------
Eval num_timesteps=3740000, episode_reward=-13.20 +/- 24.70
Episode length: 1873.60 +/- 319.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.87e+03      |
|    mean_reward          | -13.2         |
| time/                   |               |
|    total_timesteps      | 3740000       |
| train/                  |               |
|    approx_kl            | 2.1559346e-05 |
|    clip_fraction        | 0.000574      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00709      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.49          |
|    n_updates            | 4560          |
|    policy_gradient_loss | -7.78e-05     |
|    value_loss           | 8.36          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -12      |
| time/              |          |
|    fps             | 475      |
|    iterations      | 457      |
|    time_elapsed    | 7867     |
|    total_timesteps | 3743744  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.62e+03      |
|    ep_rew_mean          | -11.7         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 458           |
|    time_elapsed         | 7881          |
|    total_timesteps      | 3751936       |
| train/                  |               |
|    approx_kl            | 4.8185248e-06 |
|    clip_fraction        | 8.54e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00863      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.94          |
|    n_updates            | 4570          |
|    policy_gradient_loss | 6.19e-06      |
|    value_loss           | 9.3           |
-------------------------------------------
Eval num_timesteps=3760000, episode_reward=-26.70 +/- 45.18
Episode length: 1436.20 +/- 208.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.44e+03     |
|    mean_reward          | -26.7        |
| time/                   |              |
|    total_timesteps      | 3760000      |
| train/                  |              |
|    approx_kl            | 2.580363e-05 |
|    clip_fraction        | 0.000806     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00704     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.56         |
|    n_updates            | 4580         |
|    policy_gradient_loss | -3.09e-05    |
|    value_loss           | 7.32         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 459      |
|    time_elapsed    | 7902     |
|    total_timesteps | 3760128  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.64e+03     |
|    ep_rew_mean          | -10          |
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 460          |
|    time_elapsed         | 7916         |
|    total_timesteps      | 3768320      |
| train/                  |              |
|    approx_kl            | 2.264735e-05 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00655     |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 4.65         |
|    n_updates            | 4590         |
|    policy_gradient_loss | -5.09e-05    |
|    value_loss           | 8.89         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.65e+03      |
|    ep_rew_mean          | -7            |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 461           |
|    time_elapsed         | 7931          |
|    total_timesteps      | 3776512       |
| train/                  |               |
|    approx_kl            | 0.00024097369 |
|    clip_fraction        | 0.00083       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00276      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.87          |
|    n_updates            | 4600          |
|    policy_gradient_loss | -3.28e-05     |
|    value_loss           | 7.55          |
-------------------------------------------
Eval num_timesteps=3780000, episode_reward=-29.70 +/- 15.16
Episode length: 1724.60 +/- 245.12
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1.72e+03 |
|    mean_reward          | -29.7    |
| time/                   |          |
|    total_timesteps      | 3780000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00222 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 4.14     |
|    n_updates            | 4610     |
|    policy_gradient_loss | 1.12e-10 |
|    value_loss           | 8.63     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -7.47    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 462      |
|    time_elapsed    | 7955     |
|    total_timesteps | 3784704  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.62e+03      |
|    ep_rew_mean          | -7.91         |
| time/                   |               |
|    fps                  | 475           |
|    iterations           | 463           |
|    time_elapsed         | 7970          |
|    total_timesteps      | 3792896       |
| train/                  |               |
|    approx_kl            | 1.6879138e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00164      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.82          |
|    n_updates            | 4620          |
|    policy_gradient_loss | -3.86e-05     |
|    value_loss           | 9.1           |
-------------------------------------------
Eval num_timesteps=3800000, episode_reward=-5.90 +/- 40.33
Episode length: 1725.20 +/- 254.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.73e+03      |
|    mean_reward          | -5.9          |
| time/                   |               |
|    total_timesteps      | 3800000       |
| train/                  |               |
|    approx_kl            | 2.3748726e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00218      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.54          |
|    n_updates            | 4630          |
|    policy_gradient_loss | 4e-10         |
|    value_loss           | 8.59          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -8.75    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 464      |
|    time_elapsed    | 7993     |
|    total_timesteps | 3801088  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.64e+03      |
|    ep_rew_mean          | -10.7         |
| time/                   |               |
|    fps                  | 475           |
|    iterations           | 465           |
|    time_elapsed         | 8006          |
|    total_timesteps      | 3809280       |
| train/                  |               |
|    approx_kl            | 3.1199306e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00341      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.6           |
|    n_updates            | 4640          |
|    policy_gradient_loss | -2.93e-10     |
|    value_loss           | 8.24          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.65e+03      |
|    ep_rew_mean          | -12.2         |
| time/                   |               |
|    fps                  | 475           |
|    iterations           | 466           |
|    time_elapsed         | 8020          |
|    total_timesteps      | 3817472       |
| train/                  |               |
|    approx_kl            | 1.1120443e-05 |
|    clip_fraction        | 0.000122      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00394      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.28          |
|    n_updates            | 4650          |
|    policy_gradient_loss | -4.07e-06     |
|    value_loss           | 7.26          |
-------------------------------------------
Eval num_timesteps=3820000, episode_reward=-49.20 +/- 22.30
Episode length: 1610.40 +/- 343.13
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 1.61e+03       |
|    mean_reward          | -49.2          |
| time/                   |                |
|    total_timesteps      | 3820000        |
| train/                  |                |
|    approx_kl            | 1.19277975e-05 |
|    clip_fraction        | 0.000208       |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00508       |
|    explained_variance   | 0              |
|    learning_rate        | 0.0003         |
|    loss                 | 6.66           |
|    n_updates            | 4660           |
|    policy_gradient_loss | 6.21e-06       |
|    value_loss           | 8.4            |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -11.4    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 467      |
|    time_elapsed    | 8042     |
|    total_timesteps | 3825664  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.67e+03     |
|    ep_rew_mean          | -11.8        |
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 468          |
|    time_elapsed         | 8055         |
|    total_timesteps      | 3833856      |
| train/                  |              |
|    approx_kl            | 9.237781e-06 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00531     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.09         |
|    n_updates            | 4670         |
|    policy_gradient_loss | -3.38e-06    |
|    value_loss           | 8.09         |
------------------------------------------
Eval num_timesteps=3840000, episode_reward=-32.10 +/- 16.53
Episode length: 1564.60 +/- 134.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.56e+03     |
|    mean_reward          | -32.1        |
| time/                   |              |
|    total_timesteps      | 3840000      |
| train/                  |              |
|    approx_kl            | 9.832998e-06 |
|    clip_fraction        | 0.00033      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00476     |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 4.97         |
|    n_updates            | 4680         |
|    policy_gradient_loss | 2.53e-05     |
|    value_loss           | 9.41         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -11.5    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 469      |
|    time_elapsed    | 8076     |
|    total_timesteps | 3842048  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.69e+03      |
|    ep_rew_mean          | -11.2         |
| time/                   |               |
|    fps                  | 475           |
|    iterations           | 470           |
|    time_elapsed         | 8090          |
|    total_timesteps      | 3850240       |
| train/                  |               |
|    approx_kl            | 1.9216066e-05 |
|    clip_fraction        | 0.000403      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00559      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.52          |
|    n_updates            | 4690          |
|    policy_gradient_loss | 1.46e-05      |
|    value_loss           | 7.85          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.68e+03      |
|    ep_rew_mean          | -11.6         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 471           |
|    time_elapsed         | 8104          |
|    total_timesteps      | 3858432       |
| train/                  |               |
|    approx_kl            | 8.3250416e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00223      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.19          |
|    n_updates            | 4700          |
|    policy_gradient_loss | 6.56e-07      |
|    value_loss           | 8.13          |
-------------------------------------------
Eval num_timesteps=3860000, episode_reward=4.50 +/- 37.31
Episode length: 1751.00 +/- 317.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.75e+03      |
|    mean_reward          | 4.5           |
| time/                   |               |
|    total_timesteps      | 3860000       |
| train/                  |               |
|    approx_kl            | 1.5624973e-05 |
|    clip_fraction        | 0.00011       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00178      |
|    explained_variance   | 2.38e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.83          |
|    n_updates            | 4710          |
|    policy_gradient_loss | -2.35e-05     |
|    value_loss           | 8.65          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -11.8    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 472      |
|    time_elapsed    | 8126     |
|    total_timesteps | 3866624  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.71e+03      |
|    ep_rew_mean          | -11           |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 473           |
|    time_elapsed         | 8139          |
|    total_timesteps      | 3874816       |
| train/                  |               |
|    approx_kl            | 3.0267984e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0028       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.17          |
|    n_updates            | 4720          |
|    policy_gradient_loss | 9.6e-11       |
|    value_loss           | 7.51          |
-------------------------------------------
Eval num_timesteps=3880000, episode_reward=-28.20 +/- 32.92
Episode length: 1570.00 +/- 306.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.57e+03      |
|    mean_reward          | -28.2         |
| time/                   |               |
|    total_timesteps      | 3880000       |
| train/                  |               |
|    approx_kl            | 4.3589025e-06 |
|    clip_fraction        | 7.32e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00306      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 6.13          |
|    n_updates            | 4730          |
|    policy_gradient_loss | -1.22e-05     |
|    value_loss           | 8.16          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -9.94    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 474      |
|    time_elapsed    | 8159     |
|    total_timesteps | 3883008  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.7e+03       |
|    ep_rew_mean          | -6.55         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 475           |
|    time_elapsed         | 8173          |
|    total_timesteps      | 3891200       |
| train/                  |               |
|    approx_kl            | 1.2940487e-05 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00288      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.23          |
|    n_updates            | 4740          |
|    policy_gradient_loss | 2.09e-05      |
|    value_loss           | 8.57          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.69e+03      |
|    ep_rew_mean          | -6.61         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 476           |
|    time_elapsed         | 8187          |
|    total_timesteps      | 3899392       |
| train/                  |               |
|    approx_kl            | 3.2479875e-08 |
|    clip_fraction        | 8.54e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00223      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.34          |
|    n_updates            | 4750          |
|    policy_gradient_loss | 5.19e-05      |
|    value_loss           | 8.46          |
-------------------------------------------
Eval num_timesteps=3900000, episode_reward=-14.80 +/- 41.83
Episode length: 1833.00 +/- 340.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.83e+03      |
|    mean_reward          | -14.8         |
| time/                   |               |
|    total_timesteps      | 3900000       |
| train/                  |               |
|    approx_kl            | 2.8502604e-05 |
|    clip_fraction        | 0.00072       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0039       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.47          |
|    n_updates            | 4760          |
|    policy_gradient_loss | -5.66e-05     |
|    value_loss           | 8.46          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -10.2    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 477      |
|    time_elapsed    | 8209     |
|    total_timesteps | 3907584  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.69e+03     |
|    ep_rew_mean          | -9.26        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 478          |
|    time_elapsed         | 8224         |
|    total_timesteps      | 3915776      |
| train/                  |              |
|    approx_kl            | 4.494172e-05 |
|    clip_fraction        | 0.000232     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00686     |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 3.16         |
|    n_updates            | 4770         |
|    policy_gradient_loss | 1.35e-05     |
|    value_loss           | 8.2          |
------------------------------------------
Eval num_timesteps=3920000, episode_reward=8.70 +/- 41.63
Episode length: 1711.80 +/- 329.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.71e+03      |
|    mean_reward          | 8.7           |
| time/                   |               |
|    total_timesteps      | 3920000       |
| train/                  |               |
|    approx_kl            | 1.5427904e-05 |
|    clip_fraction        | 0.000256      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00762      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.8           |
|    n_updates            | 4780          |
|    policy_gradient_loss | -1.07e-05     |
|    value_loss           | 7.64          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -9.15    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 479      |
|    time_elapsed    | 8244     |
|    total_timesteps | 3923968  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.69e+03      |
|    ep_rew_mean          | -8.2          |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 480           |
|    time_elapsed         | 8258          |
|    total_timesteps      | 3932160       |
| train/                  |               |
|    approx_kl            | 2.1329535e-05 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00636      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.08          |
|    n_updates            | 4790          |
|    policy_gradient_loss | -4.72e-05     |
|    value_loss           | 7.72          |
-------------------------------------------
Eval num_timesteps=3940000, episode_reward=2.10 +/- 31.56
Episode length: 1637.20 +/- 165.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.64e+03      |
|    mean_reward          | 2.1           |
| time/                   |               |
|    total_timesteps      | 3940000       |
| train/                  |               |
|    approx_kl            | 1.3022145e-06 |
|    clip_fraction        | 0.000513      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0052       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 1.36          |
|    n_updates            | 4800          |
|    policy_gradient_loss | 3.78e-05      |
|    value_loss           | 7.97          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -8.49    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 481      |
|    time_elapsed    | 8280     |
|    total_timesteps | 3940352  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.7e+03      |
|    ep_rew_mean          | -9.2         |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 482          |
|    time_elapsed         | 8294         |
|    total_timesteps      | 3948544      |
| train/                  |              |
|    approx_kl            | 6.867446e-05 |
|    clip_fraction        | 0.00072      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0064      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 8.12         |
|    n_updates            | 4810         |
|    policy_gradient_loss | -9.31e-05    |
|    value_loss           | 8.74         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.7e+03       |
|    ep_rew_mean          | -10.3         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 483           |
|    time_elapsed         | 8308          |
|    total_timesteps      | 3956736       |
| train/                  |               |
|    approx_kl            | 2.9613133e-05 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00594      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.96          |
|    n_updates            | 4820          |
|    policy_gradient_loss | -4.47e-05     |
|    value_loss           | 7.96          |
-------------------------------------------
Eval num_timesteps=3960000, episode_reward=-28.30 +/- 35.86
Episode length: 1535.40 +/- 218.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.54e+03      |
|    mean_reward          | -28.3         |
| time/                   |               |
|    total_timesteps      | 3960000       |
| train/                  |               |
|    approx_kl            | 3.9350583e-05 |
|    clip_fraction        | 0.00061       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00472      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.66          |
|    n_updates            | 4830          |
|    policy_gradient_loss | -2.57e-05     |
|    value_loss           | 8.11          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -12.1    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 484      |
|    time_elapsed    | 8328     |
|    total_timesteps | 3964928  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.7e+03       |
|    ep_rew_mean          | -13.8         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 485           |
|    time_elapsed         | 8342          |
|    total_timesteps      | 3973120       |
| train/                  |               |
|    approx_kl            | 0.00010107118 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00893      |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 4.57          |
|    n_updates            | 4840          |
|    policy_gradient_loss | -9.43e-06     |
|    value_loss           | 7.63          |
-------------------------------------------
Eval num_timesteps=3980000, episode_reward=-11.70 +/- 34.82
Episode length: 1719.80 +/- 297.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.72e+03      |
|    mean_reward          | -11.7         |
| time/                   |               |
|    total_timesteps      | 3980000       |
| train/                  |               |
|    approx_kl            | 1.2139004e-05 |
|    clip_fraction        | 0.000549      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0109       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.02          |
|    n_updates            | 4850          |
|    policy_gradient_loss | -7.1e-06      |
|    value_loss           | 8.12          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -14      |
| time/              |          |
|    fps             | 475      |
|    iterations      | 486      |
|    time_elapsed    | 8365     |
|    total_timesteps | 3981312  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.69e+03     |
|    ep_rew_mean          | -14.5        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 487          |
|    time_elapsed         | 8379         |
|    total_timesteps      | 3989504      |
| train/                  |              |
|    approx_kl            | 2.648867e-05 |
|    clip_fraction        | 0.00125      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0129      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.66         |
|    n_updates            | 4860         |
|    policy_gradient_loss | 1.88e-05     |
|    value_loss           | 8.46         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.69e+03     |
|    ep_rew_mean          | -12.1        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 488          |
|    time_elapsed         | 8393         |
|    total_timesteps      | 3997696      |
| train/                  |              |
|    approx_kl            | 4.401295e-05 |
|    clip_fraction        | 0.00061      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.011       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.94         |
|    n_updates            | 4870         |
|    policy_gradient_loss | -3.61e-05    |
|    value_loss           | 7.1          |
------------------------------------------
Eval num_timesteps=4000000, episode_reward=-21.30 +/- 18.93
Episode length: 1736.00 +/- 195.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.74e+03      |
|    mean_reward          | -21.3         |
| time/                   |               |
|    total_timesteps      | 4000000       |
| train/                  |               |
|    approx_kl            | 1.6609905e-05 |
|    clip_fraction        | 0.000891      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00771      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.78          |
|    n_updates            | 4880          |
|    policy_gradient_loss | 0.000215      |
|    value_loss           | 9.17          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -9.28    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 489      |
|    time_elapsed    | 8416     |
|    total_timesteps | 4005888  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.7e+03       |
|    ep_rew_mean          | -9.34         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 490           |
|    time_elapsed         | 8430          |
|    total_timesteps      | 4014080       |
| train/                  |               |
|    approx_kl            | 2.1937638e-05 |
|    clip_fraction        | 0.000537      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0142       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.05          |
|    n_updates            | 4890          |
|    policy_gradient_loss | -4.81e-05     |
|    value_loss           | 8.74          |
-------------------------------------------
Eval num_timesteps=4020000, episode_reward=1.10 +/- 36.59
Episode length: 1883.80 +/- 275.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.88e+03      |
|    mean_reward          | 1.1           |
| time/                   |               |
|    total_timesteps      | 4020000       |
| train/                  |               |
|    approx_kl            | 5.1405827e-05 |
|    clip_fraction        | 0.00116       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0126       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.53          |
|    n_updates            | 4900          |
|    policy_gradient_loss | -0.000158     |
|    value_loss           | 8.83          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -9.93    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 491      |
|    time_elapsed    | 8454     |
|    total_timesteps | 4022272  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.68e+03      |
|    ep_rew_mean          | -10.6         |
| time/                   |               |
|    fps                  | 475           |
|    iterations           | 492           |
|    time_elapsed         | 8468          |
|    total_timesteps      | 4030464       |
| train/                  |               |
|    approx_kl            | 2.5526475e-05 |
|    clip_fraction        | 0.000122      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0102       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.42          |
|    n_updates            | 4910          |
|    policy_gradient_loss | -2.24e-06     |
|    value_loss           | 8.27          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.7e+03       |
|    ep_rew_mean          | -10.6         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 493           |
|    time_elapsed         | 8482          |
|    total_timesteps      | 4038656       |
| train/                  |               |
|    approx_kl            | 2.9749921e-05 |
|    clip_fraction        | 0.000745      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00857      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.59          |
|    n_updates            | 4920          |
|    policy_gradient_loss | -5.75e-05     |
|    value_loss           | 8.99          |
-------------------------------------------
Eval num_timesteps=4040000, episode_reward=-20.00 +/- 19.62
Episode length: 1692.80 +/- 317.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.69e+03      |
|    mean_reward          | -20           |
| time/                   |               |
|    total_timesteps      | 4040000       |
| train/                  |               |
|    approx_kl            | 1.2051038e-05 |
|    clip_fraction        | 0.000183      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00755      |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 4.37          |
|    n_updates            | 4930          |
|    policy_gradient_loss | -1.03e-05     |
|    value_loss           | 7.41          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -9.55    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 494      |
|    time_elapsed    | 8503     |
|    total_timesteps | 4046848  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.7e+03      |
|    ep_rew_mean          | -11.1        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 495          |
|    time_elapsed         | 8517         |
|    total_timesteps      | 4055040      |
| train/                  |              |
|    approx_kl            | 7.893039e-06 |
|    clip_fraction        | 0.000378     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00575     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.14         |
|    n_updates            | 4940         |
|    policy_gradient_loss | -2.73e-05    |
|    value_loss           | 8.02         |
------------------------------------------
Eval num_timesteps=4060000, episode_reward=18.30 +/- 27.32
Episode length: 1726.60 +/- 209.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.73e+03      |
|    mean_reward          | 18.3          |
| time/                   |               |
|    total_timesteps      | 4060000       |
| train/                  |               |
|    approx_kl            | 2.0905682e-05 |
|    clip_fraction        | 0.000903      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00743      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.29          |
|    n_updates            | 4950          |
|    policy_gradient_loss | -1.76e-05     |
|    value_loss           | 7.9           |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -11.3    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 496      |
|    time_elapsed    | 8539     |
|    total_timesteps | 4063232  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.68e+03     |
|    ep_rew_mean          | -14.5        |
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 497          |
|    time_elapsed         | 8553         |
|    total_timesteps      | 4071424      |
| train/                  |              |
|    approx_kl            | 9.123934e-06 |
|    clip_fraction        | 1.22e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00665     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.35         |
|    n_updates            | 4960         |
|    policy_gradient_loss | 8.48e-06     |
|    value_loss           | 8.49         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.7e+03       |
|    ep_rew_mean          | -16.1         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 498           |
|    time_elapsed         | 8567          |
|    total_timesteps      | 4079616       |
| train/                  |               |
|    approx_kl            | 3.0189178e-05 |
|    clip_fraction        | 0.000366      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00688      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.19          |
|    n_updates            | 4970          |
|    policy_gradient_loss | 1.36e-05      |
|    value_loss           | 8.08          |
-------------------------------------------
Eval num_timesteps=4080000, episode_reward=2.30 +/- 16.57
Episode length: 1957.20 +/- 216.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.96e+03     |
|    mean_reward          | 2.3          |
| time/                   |              |
|    total_timesteps      | 4080000      |
| train/                  |              |
|    approx_kl            | 2.159753e-05 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00625     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.92         |
|    n_updates            | 4980         |
|    policy_gradient_loss | -1.71e-05    |
|    value_loss           | 8.07         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.71e+03 |
|    ep_rew_mean     | -13.5    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 499      |
|    time_elapsed    | 8589     |
|    total_timesteps | 4087808  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.72e+03      |
|    ep_rew_mean          | -14.5         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 500           |
|    time_elapsed         | 8603          |
|    total_timesteps      | 4096000       |
| train/                  |               |
|    approx_kl            | 2.9206494e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00717      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 6.25          |
|    n_updates            | 4990          |
|    policy_gradient_loss | 1.37e-05      |
|    value_loss           | 8.4           |
-------------------------------------------
Eval num_timesteps=4100000, episode_reward=-9.20 +/- 45.39
Episode length: 1418.80 +/- 304.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.42e+03      |
|    mean_reward          | -9.2          |
| time/                   |               |
|    total_timesteps      | 4100000       |
| train/                  |               |
|    approx_kl            | 1.3190547e-05 |
|    clip_fraction        | 0.000122      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00807      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.88          |
|    n_updates            | 5000          |
|    policy_gradient_loss | -6.69e-06     |
|    value_loss           | 8.69          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.71e+03 |
|    ep_rew_mean     | -15.5    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 501      |
|    time_elapsed    | 8623     |
|    total_timesteps | 4104192  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.69e+03      |
|    ep_rew_mean          | -15.3         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 502           |
|    time_elapsed         | 8637          |
|    total_timesteps      | 4112384       |
| train/                  |               |
|    approx_kl            | 6.2762483e-06 |
|    clip_fraction        | 3.66e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00818      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.35          |
|    n_updates            | 5010          |
|    policy_gradient_loss | -3.32e-07     |
|    value_loss           | 8.25          |
-------------------------------------------
Eval num_timesteps=4120000, episode_reward=-25.50 +/- 27.12
Episode length: 1783.20 +/- 447.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.78e+03    |
|    mean_reward          | -25.5       |
| time/                   |             |
|    total_timesteps      | 4120000     |
| train/                  |             |
|    approx_kl            | 3.06925e-05 |
|    clip_fraction        | 0.000781    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00641    |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 3.81        |
|    n_updates            | 5020        |
|    policy_gradient_loss | -3.65e-05   |
|    value_loss           | 7.52        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.71e+03 |
|    ep_rew_mean     | -17      |
| time/              |          |
|    fps             | 475      |
|    iterations      | 503      |
|    time_elapsed    | 8662     |
|    total_timesteps | 4120576  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.71e+03      |
|    ep_rew_mean          | -17.6         |
| time/                   |               |
|    fps                  | 475           |
|    iterations           | 504           |
|    time_elapsed         | 8676          |
|    total_timesteps      | 4128768       |
| train/                  |               |
|    approx_kl            | 1.7523453e-05 |
|    clip_fraction        | 0.000549      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00564      |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 5.73          |
|    n_updates            | 5030          |
|    policy_gradient_loss | -5.5e-07      |
|    value_loss           | 8.84          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.7e+03      |
|    ep_rew_mean          | -18.1        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 505          |
|    time_elapsed         | 8690         |
|    total_timesteps      | 4136960      |
| train/                  |              |
|    approx_kl            | 2.765379e-05 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00399     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.86         |
|    n_updates            | 5040         |
|    policy_gradient_loss | -2.56e-05    |
|    value_loss           | 7.99         |
------------------------------------------
Eval num_timesteps=4140000, episode_reward=-23.00 +/- 22.30
Episode length: 1877.40 +/- 366.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.88e+03     |
|    mean_reward          | -23          |
| time/                   |              |
|    total_timesteps      | 4140000      |
| train/                  |              |
|    approx_kl            | 9.293668e-06 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00481     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.09         |
|    n_updates            | 5050         |
|    policy_gradient_loss | 6.53e-07     |
|    value_loss           | 8.83         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -18.1    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 506      |
|    time_elapsed    | 8712     |
|    total_timesteps | 4145152  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.7e+03       |
|    ep_rew_mean          | -18.2         |
| time/                   |               |
|    fps                  | 475           |
|    iterations           | 507           |
|    time_elapsed         | 8727          |
|    total_timesteps      | 4153344       |
| train/                  |               |
|    approx_kl            | 5.0848394e-05 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00377      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.74          |
|    n_updates            | 5060          |
|    policy_gradient_loss | -0.000156     |
|    value_loss           | 7.83          |
-------------------------------------------
Eval num_timesteps=4160000, episode_reward=-3.50 +/- 60.80
Episode length: 1212.20 +/- 310.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.21e+03      |
|    mean_reward          | -3.5          |
| time/                   |               |
|    total_timesteps      | 4160000       |
| train/                  |               |
|    approx_kl            | 3.7673184e-05 |
|    clip_fraction        | 0.000122      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00643      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.65          |
|    n_updates            | 5070          |
|    policy_gradient_loss | -5.07e-06     |
|    value_loss           | 8.48          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -18.9    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 508      |
|    time_elapsed    | 8747     |
|    total_timesteps | 4161536  |
---------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 1.68e+03       |
|    ep_rew_mean          | -18            |
| time/                   |                |
|    fps                  | 475            |
|    iterations           | 509            |
|    time_elapsed         | 8761           |
|    total_timesteps      | 4169728        |
| train/                  |                |
|    approx_kl            | 1.24563885e-05 |
|    clip_fraction        | 0.000244       |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00787       |
|    explained_variance   | 0              |
|    learning_rate        | 0.0003         |
|    loss                 | 6.18           |
|    n_updates            | 5080           |
|    policy_gradient_loss | -9.66e-05      |
|    value_loss           | 8.92           |
--------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.69e+03     |
|    ep_rew_mean          | -19          |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 510          |
|    time_elapsed         | 8776         |
|    total_timesteps      | 4177920      |
| train/                  |              |
|    approx_kl            | 2.909916e-05 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00825     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.3          |
|    n_updates            | 5090         |
|    policy_gradient_loss | -7.98e-05    |
|    value_loss           | 8.66         |
------------------------------------------
Eval num_timesteps=4180000, episode_reward=-3.20 +/- 25.58
Episode length: 1595.20 +/- 143.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.6e+03      |
|    mean_reward          | -3.2         |
| time/                   |              |
|    total_timesteps      | 4180000      |
| train/                  |              |
|    approx_kl            | 3.937119e-05 |
|    clip_fraction        | 0.000769     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00618     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.12         |
|    n_updates            | 5100         |
|    policy_gradient_loss | -5.24e-06    |
|    value_loss           | 8.13         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -19.9    |
| time/              |          |
|    fps             | 475      |
|    iterations      | 511      |
|    time_elapsed    | 8797     |
|    total_timesteps | 4186112  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.7e+03      |
|    ep_rew_mean          | -21.2        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 512          |
|    time_elapsed         | 8811         |
|    total_timesteps      | 4194304      |
| train/                  |              |
|    approx_kl            | 1.932236e-05 |
|    clip_fraction        | 0.000464     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0052      |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 3.09         |
|    n_updates            | 5110         |
|    policy_gradient_loss | -2.58e-05    |
|    value_loss           | 6.91         |
------------------------------------------
Eval num_timesteps=4200000, episode_reward=-14.70 +/- 52.64
Episode length: 1504.20 +/- 226.08
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 1.5e+03        |
|    mean_reward          | -14.7          |
| time/                   |                |
|    total_timesteps      | 4200000        |
| train/                  |                |
|    approx_kl            | 1.05996005e-05 |
|    clip_fraction        | 8.54e-05       |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00468       |
|    explained_variance   | 0              |
|    learning_rate        | 0.0003         |
|    loss                 | 4.88           |
|    n_updates            | 5120           |
|    policy_gradient_loss | -7.02e-05      |
|    value_loss           | 8.01           |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -21      |
| time/              |          |
|    fps             | 475      |
|    iterations      | 513      |
|    time_elapsed    | 8832     |
|    total_timesteps | 4202496  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.72e+03      |
|    ep_rew_mean          | -18.1         |
| time/                   |               |
|    fps                  | 475           |
|    iterations           | 514           |
|    time_elapsed         | 8846          |
|    total_timesteps      | 4210688       |
| train/                  |               |
|    approx_kl            | 2.1416745e-05 |
|    clip_fraction        | 0.000574      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00342      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.4           |
|    n_updates            | 5130          |
|    policy_gradient_loss | -5.59e-05     |
|    value_loss           | 8.56          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.7e+03       |
|    ep_rew_mean          | -18.1         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 515           |
|    time_elapsed         | 8859          |
|    total_timesteps      | 4218880       |
| train/                  |               |
|    approx_kl            | 1.2029945e-05 |
|    clip_fraction        | 3.66e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00343      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.32          |
|    n_updates            | 5140          |
|    policy_gradient_loss | -4.21e-06     |
|    value_loss           | 7.7           |
-------------------------------------------
Eval num_timesteps=4220000, episode_reward=-10.00 +/- 58.97
Episode length: 1514.40 +/- 302.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.51e+03     |
|    mean_reward          | -10          |
| time/                   |              |
|    total_timesteps      | 4220000      |
| train/                  |              |
|    approx_kl            | 3.140574e-05 |
|    clip_fraction        | 0.000598     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0022      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.03         |
|    n_updates            | 5150         |
|    policy_gradient_loss | -4.27e-05    |
|    value_loss           | 8.54         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -18.2    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 516      |
|    time_elapsed    | 8880     |
|    total_timesteps | 4227072  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.68e+03      |
|    ep_rew_mean          | -18.5         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 517           |
|    time_elapsed         | 8893          |
|    total_timesteps      | 4235264       |
| train/                  |               |
|    approx_kl            | 6.1609753e-06 |
|    clip_fraction        | 0.000134      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0022       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.58          |
|    n_updates            | 5160          |
|    policy_gradient_loss | -7.82e-06     |
|    value_loss           | 8.41          |
-------------------------------------------
Eval num_timesteps=4240000, episode_reward=-24.00 +/- 29.56
Episode length: 1670.80 +/- 458.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.67e+03     |
|    mean_reward          | -24          |
| time/                   |              |
|    total_timesteps      | 4240000      |
| train/                  |              |
|    approx_kl            | 4.176938e-06 |
|    clip_fraction        | 6.1e-05      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00254     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.25         |
|    n_updates            | 5170         |
|    policy_gradient_loss | -4.51e-06    |
|    value_loss           | 8.14         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -18      |
| time/              |          |
|    fps             | 476      |
|    iterations      | 518      |
|    time_elapsed    | 8914     |
|    total_timesteps | 4243456  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.69e+03     |
|    ep_rew_mean          | -16.5        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 519          |
|    time_elapsed         | 8928         |
|    total_timesteps      | 4251648      |
| train/                  |              |
|    approx_kl            | 3.932568e-06 |
|    clip_fraction        | 0.00011      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00333     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.86         |
|    n_updates            | 5180         |
|    policy_gradient_loss | -7.4e-06     |
|    value_loss           | 8.5          |
------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 1.69e+03       |
|    ep_rew_mean          | -16.1          |
| time/                   |                |
|    fps                  | 476            |
|    iterations           | 520            |
|    time_elapsed         | 8941           |
|    total_timesteps      | 4259840        |
| train/                  |                |
|    approx_kl            | 1.19750985e-05 |
|    clip_fraction        | 0.000208       |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00269       |
|    explained_variance   | 0              |
|    learning_rate        | 0.0003         |
|    loss                 | 2.85           |
|    n_updates            | 5190           |
|    policy_gradient_loss | -5.08e-06      |
|    value_loss           | 9.06           |
--------------------------------------------
Eval num_timesteps=4260000, episode_reward=-15.00 +/- 35.24
Episode length: 1437.60 +/- 138.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.44e+03      |
|    mean_reward          | -15           |
| time/                   |               |
|    total_timesteps      | 4260000       |
| train/                  |               |
|    approx_kl            | 4.2730535e-06 |
|    clip_fraction        | 0.000256      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00329      |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 4.98          |
|    n_updates            | 5200          |
|    policy_gradient_loss | -3.39e-06     |
|    value_loss           | 7.95          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -15.7    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 521      |
|    time_elapsed    | 8960     |
|    total_timesteps | 4268032  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.68e+03      |
|    ep_rew_mean          | -14.8         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 522           |
|    time_elapsed         | 8974          |
|    total_timesteps      | 4276224       |
| train/                  |               |
|    approx_kl            | 5.8368823e-06 |
|    clip_fraction        | 3.66e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00296      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 6.9           |
|    n_updates            | 5210          |
|    policy_gradient_loss | 2.21e-06      |
|    value_loss           | 9.46          |
-------------------------------------------
Eval num_timesteps=4280000, episode_reward=-40.10 +/- 30.51
Episode length: 1538.80 +/- 224.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.54e+03      |
|    mean_reward          | -40.1         |
| time/                   |               |
|    total_timesteps      | 4280000       |
| train/                  |               |
|    approx_kl            | 1.9740248e-05 |
|    clip_fraction        | 0.000183      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00314      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 1.97          |
|    n_updates            | 5220          |
|    policy_gradient_loss | 1.06e-05      |
|    value_loss           | 8.11          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -14.7    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 523      |
|    time_elapsed    | 8994     |
|    total_timesteps | 4284416  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.69e+03      |
|    ep_rew_mean          | -15.2         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 524           |
|    time_elapsed         | 9007          |
|    total_timesteps      | 4292608       |
| train/                  |               |
|    approx_kl            | 1.4774996e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00302      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.06          |
|    n_updates            | 5230          |
|    policy_gradient_loss | -1.79e-08     |
|    value_loss           | 8.04          |
-------------------------------------------
Eval num_timesteps=4300000, episode_reward=-20.60 +/- 47.02
Episode length: 1516.80 +/- 423.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.52e+03      |
|    mean_reward          | -20.6         |
| time/                   |               |
|    total_timesteps      | 4300000       |
| train/                  |               |
|    approx_kl            | 1.6461396e-05 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00341      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.51          |
|    n_updates            | 5240          |
|    policy_gradient_loss | -4.49e-06     |
|    value_loss           | 7.4           |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -15.3    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 525      |
|    time_elapsed    | 9029     |
|    total_timesteps | 4300800  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.72e+03      |
|    ep_rew_mean          | -14.8         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 526           |
|    time_elapsed         | 9042          |
|    total_timesteps      | 4308992       |
| train/                  |               |
|    approx_kl            | 2.0238724e-05 |
|    clip_fraction        | 0.00033       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00294      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.69          |
|    n_updates            | 5250          |
|    policy_gradient_loss | -5.04e-05     |
|    value_loss           | 7.31          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.71e+03      |
|    ep_rew_mean          | -15.4         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 527           |
|    time_elapsed         | 9056          |
|    total_timesteps      | 4317184       |
| train/                  |               |
|    approx_kl            | 1.6151622e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00362      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 3.48          |
|    n_updates            | 5260          |
|    policy_gradient_loss | -8.52e-06     |
|    value_loss           | 7.87          |
-------------------------------------------
Eval num_timesteps=4320000, episode_reward=-22.10 +/- 44.09
Episode length: 1571.60 +/- 216.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.57e+03      |
|    mean_reward          | -22.1         |
| time/                   |               |
|    total_timesteps      | 4320000       |
| train/                  |               |
|    approx_kl            | 2.6515132e-05 |
|    clip_fraction        | 0.000232      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00524      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.04          |
|    n_updates            | 5270          |
|    policy_gradient_loss | -6.21e-05     |
|    value_loss           | 8.69          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.71e+03 |
|    ep_rew_mean     | -15.4    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 528      |
|    time_elapsed    | 9075     |
|    total_timesteps | 4325376  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.71e+03      |
|    ep_rew_mean          | -12.5         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 529           |
|    time_elapsed         | 9089          |
|    total_timesteps      | 4333568       |
| train/                  |               |
|    approx_kl            | 1.8398117e-05 |
|    clip_fraction        | 0.000647      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0067       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.56          |
|    n_updates            | 5280          |
|    policy_gradient_loss | -4.52e-05     |
|    value_loss           | 8.47          |
-------------------------------------------
Eval num_timesteps=4340000, episode_reward=-8.60 +/- 28.45
Episode length: 1552.40 +/- 383.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.55e+03      |
|    mean_reward          | -8.6          |
| time/                   |               |
|    total_timesteps      | 4340000       |
| train/                  |               |
|    approx_kl            | 1.6211947e-05 |
|    clip_fraction        | 0.00033       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00686      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.92          |
|    n_updates            | 5290          |
|    policy_gradient_loss | -6.19e-06     |
|    value_loss           | 9.06          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.71e+03 |
|    ep_rew_mean     | -11.7    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 530      |
|    time_elapsed    | 9109     |
|    total_timesteps | 4341760  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.73e+03      |
|    ep_rew_mean          | -10.8         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 531           |
|    time_elapsed         | 9122          |
|    total_timesteps      | 4349952       |
| train/                  |               |
|    approx_kl            | 2.5383568e-05 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00794      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.47          |
|    n_updates            | 5300          |
|    policy_gradient_loss | -6.97e-05     |
|    value_loss           | 9.1           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.73e+03      |
|    ep_rew_mean          | -10.7         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 532           |
|    time_elapsed         | 9137          |
|    total_timesteps      | 4358144       |
| train/                  |               |
|    approx_kl            | 1.8682636e-05 |
|    clip_fraction        | 0.000525      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00592      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.99          |
|    n_updates            | 5310          |
|    policy_gradient_loss | -2.93e-06     |
|    value_loss           | 7.58          |
-------------------------------------------
Eval num_timesteps=4360000, episode_reward=-18.40 +/- 48.83
Episode length: 1443.20 +/- 131.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.44e+03     |
|    mean_reward          | -18.4        |
| time/                   |              |
|    total_timesteps      | 4360000      |
| train/                  |              |
|    approx_kl            | 8.579102e-05 |
|    clip_fraction        | 0.00129      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00749     |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 3.94         |
|    n_updates            | 5320         |
|    policy_gradient_loss | -0.000103    |
|    value_loss           | 9.28         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.73e+03 |
|    ep_rew_mean     | -11.8    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 533      |
|    time_elapsed    | 9158     |
|    total_timesteps | 4366336  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.72e+03      |
|    ep_rew_mean          | -11.6         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 534           |
|    time_elapsed         | 9172          |
|    total_timesteps      | 4374528       |
| train/                  |               |
|    approx_kl            | 3.2176256e-05 |
|    clip_fraction        | 0.00115       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00952      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.53          |
|    n_updates            | 5330          |
|    policy_gradient_loss | -9.08e-05     |
|    value_loss           | 7.54          |
-------------------------------------------
Eval num_timesteps=4380000, episode_reward=-16.20 +/- 22.17
Episode length: 1734.20 +/- 203.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.73e+03      |
|    mean_reward          | -16.2         |
| time/                   |               |
|    total_timesteps      | 4380000       |
| train/                  |               |
|    approx_kl            | 1.0970078e-05 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0113       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.44          |
|    n_updates            | 5340          |
|    policy_gradient_loss | -9.56e-06     |
|    value_loss           | 8.36          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.72e+03 |
|    ep_rew_mean     | -11.7    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 535      |
|    time_elapsed    | 9194     |
|    total_timesteps | 4382720  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.71e+03      |
|    ep_rew_mean          | -13.3         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 536           |
|    time_elapsed         | 9207          |
|    total_timesteps      | 4390912       |
| train/                  |               |
|    approx_kl            | 2.3215216e-05 |
|    clip_fraction        | 0.00011       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0114       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 4.22          |
|    n_updates            | 5350          |
|    policy_gradient_loss | 2.3e-05       |
|    value_loss           | 8.23          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.73e+03     |
|    ep_rew_mean          | -13.5        |
| time/                   |              |
|    fps                  | 477          |
|    iterations           | 537          |
|    time_elapsed         | 9221         |
|    total_timesteps      | 4399104      |
| train/                  |              |
|    approx_kl            | 1.588698e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00892     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.77         |
|    n_updates            | 5360         |
|    policy_gradient_loss | 3.33e-08     |
|    value_loss           | 7.9          |
------------------------------------------
Eval num_timesteps=4400000, episode_reward=-0.20 +/- 49.64
Episode length: 1570.00 +/- 296.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.57e+03     |
|    mean_reward          | -0.2         |
| time/                   |              |
|    total_timesteps      | 4400000      |
| train/                  |              |
|    approx_kl            | 2.364078e-05 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00759     |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 4.36         |
|    n_updates            | 5370         |
|    policy_gradient_loss | -9.78e-06    |
|    value_loss           | 7.44         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.73e+03 |
|    ep_rew_mean     | -13.1    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 538      |
|    time_elapsed    | 9242     |
|    total_timesteps | 4407296  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.72e+03      |
|    ep_rew_mean          | -13.3         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 539           |
|    time_elapsed         | 9256          |
|    total_timesteps      | 4415488       |
| train/                  |               |
|    approx_kl            | 1.7354192e-05 |
|    clip_fraction        | 0.000354      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00603      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.75          |
|    n_updates            | 5380          |
|    policy_gradient_loss | -4.34e-05     |
|    value_loss           | 8.66          |
-------------------------------------------
Eval num_timesteps=4420000, episode_reward=-21.90 +/- 31.87
Episode length: 1691.00 +/- 336.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.69e+03     |
|    mean_reward          | -21.9        |
| time/                   |              |
|    total_timesteps      | 4420000      |
| train/                  |              |
|    approx_kl            | 2.896296e-05 |
|    clip_fraction        | 0.00061      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00384     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.2          |
|    n_updates            | 5390         |
|    policy_gradient_loss | -7.12e-05    |
|    value_loss           | 9.22         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.73e+03 |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 540      |
|    time_elapsed    | 9276     |
|    total_timesteps | 4423680  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.72e+03      |
|    ep_rew_mean          | -11.9         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 541           |
|    time_elapsed         | 9291          |
|    total_timesteps      | 4431872       |
| train/                  |               |
|    approx_kl            | 1.1536307e-05 |
|    clip_fraction        | 0.000354      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0049       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.18          |
|    n_updates            | 5400          |
|    policy_gradient_loss | -3.32e-05     |
|    value_loss           | 8.22          |
-------------------------------------------
Eval num_timesteps=4440000, episode_reward=34.90 +/- 36.76
Episode length: 1481.60 +/- 427.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.48e+03      |
|    mean_reward          | 34.9          |
| time/                   |               |
|    total_timesteps      | 4440000       |
| train/                  |               |
|    approx_kl            | 1.8160572e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00534      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 5.85          |
|    n_updates            | 5410          |
|    policy_gradient_loss | 1.8e-05       |
|    value_loss           | 8.36          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.72e+03 |
|    ep_rew_mean     | -13.2    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 542      |
|    time_elapsed    | 9313     |
|    total_timesteps | 4440064  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.73e+03     |
|    ep_rew_mean          | -13.4        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 543          |
|    time_elapsed         | 9328         |
|    total_timesteps      | 4448256      |
| train/                  |              |
|    approx_kl            | 1.423635e-05 |
|    clip_fraction        | 0.000598     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00604     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.08         |
|    n_updates            | 5420         |
|    policy_gradient_loss | -6.43e-05    |
|    value_loss           | 8.16         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.71e+03     |
|    ep_rew_mean          | -13.2        |
| time/                   |              |
|    fps                  | 477          |
|    iterations           | 544          |
|    time_elapsed         | 9341         |
|    total_timesteps      | 4456448      |
| train/                  |              |
|    approx_kl            | 1.612799e-05 |
|    clip_fraction        | 0.000317     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00586     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.18         |
|    n_updates            | 5430         |
|    policy_gradient_loss | -3.47e-05    |
|    value_loss           | 8.02         |
------------------------------------------
Eval num_timesteps=4460000, episode_reward=-20.30 +/- 44.40
Episode length: 1606.40 +/- 287.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.61e+03      |
|    mean_reward          | -20.3         |
| time/                   |               |
|    total_timesteps      | 4460000       |
| train/                  |               |
|    approx_kl            | 4.2695618e-05 |
|    clip_fraction        | 0.000452      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00364      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.36          |
|    n_updates            | 5440          |
|    policy_gradient_loss | -2.45e-05     |
|    value_loss           | 9.1           |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.72e+03 |
|    ep_rew_mean     | -12.5    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 545      |
|    time_elapsed    | 9363     |
|    total_timesteps | 4464640  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.69e+03      |
|    ep_rew_mean          | -11.7         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 546           |
|    time_elapsed         | 9376          |
|    total_timesteps      | 4472832       |
| train/                  |               |
|    approx_kl            | 6.7523724e-06 |
|    clip_fraction        | 6.1e-05       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0037       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 5.36          |
|    n_updates            | 5450          |
|    policy_gradient_loss | -5.93e-06     |
|    value_loss           | 9.19          |
-------------------------------------------
Eval num_timesteps=4480000, episode_reward=-17.40 +/- 25.36
Episode length: 1886.60 +/- 260.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.89e+03      |
|    mean_reward          | -17.4         |
| time/                   |               |
|    total_timesteps      | 4480000       |
| train/                  |               |
|    approx_kl            | 2.0002873e-05 |
|    clip_fraction        | 0.000232      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00311      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.31          |
|    n_updates            | 5460          |
|    policy_gradient_loss | 1.31e-05      |
|    value_loss           | 8.49          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -10.7    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 547      |
|    time_elapsed    | 9400     |
|    total_timesteps | 4481024  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.69e+03     |
|    ep_rew_mean          | -10.5        |
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 548          |
|    time_elapsed         | 9414         |
|    total_timesteps      | 4489216      |
| train/                  |              |
|    approx_kl            | 2.332832e-05 |
|    clip_fraction        | 0.000171     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00325     |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 4.12         |
|    n_updates            | 5470         |
|    policy_gradient_loss | -6.04e-06    |
|    value_loss           | 8.03         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.67e+03      |
|    ep_rew_mean          | -10           |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 549           |
|    time_elapsed         | 9428          |
|    total_timesteps      | 4497408       |
| train/                  |               |
|    approx_kl            | 1.2913464e-05 |
|    clip_fraction        | 0.000269      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00422      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.19          |
|    n_updates            | 5480          |
|    policy_gradient_loss | -4.71e-05     |
|    value_loss           | 8.2           |
-------------------------------------------
Eval num_timesteps=4500000, episode_reward=-28.60 +/- 39.17
Episode length: 1457.20 +/- 274.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.46e+03     |
|    mean_reward          | -28.6        |
| time/                   |              |
|    total_timesteps      | 4500000      |
| train/                  |              |
|    approx_kl            | 9.613403e-06 |
|    clip_fraction        | 0.000122     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00356     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.42         |
|    n_updates            | 5490         |
|    policy_gradient_loss | 4.42e-05     |
|    value_loss           | 8.9          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -9.65    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 550      |
|    time_elapsed    | 9448     |
|    total_timesteps | 4505600  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.68e+03      |
|    ep_rew_mean          | -9.12         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 551           |
|    time_elapsed         | 9462          |
|    total_timesteps      | 4513792       |
| train/                  |               |
|    approx_kl            | 3.0974697e-05 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00564      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.08          |
|    n_updates            | 5500          |
|    policy_gradient_loss | -6.52e-05     |
|    value_loss           | 7.83          |
-------------------------------------------
Eval num_timesteps=4520000, episode_reward=-39.80 +/- 25.61
Episode length: 1631.20 +/- 475.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.63e+03    |
|    mean_reward          | -39.8       |
| time/                   |             |
|    total_timesteps      | 4520000     |
| train/                  |             |
|    approx_kl            | 1.80461e-05 |
|    clip_fraction        | 0.000598    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00442    |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 2.95        |
|    n_updates            | 5510        |
|    policy_gradient_loss | -2.72e-05   |
|    value_loss           | 9.02        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -9.28    |
| time/              |          |
|    fps             | 476      |
|    iterations      | 552      |
|    time_elapsed    | 9483     |
|    total_timesteps | 4521984  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.65e+03      |
|    ep_rew_mean          | -8.61         |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 553           |
|    time_elapsed         | 9497          |
|    total_timesteps      | 4530176       |
| train/                  |               |
|    approx_kl            | 1.0598815e-06 |
|    clip_fraction        | 0.00011       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00531      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.76          |
|    n_updates            | 5520          |
|    policy_gradient_loss | 1.18e-05      |
|    value_loss           | 7.92          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.65e+03      |
|    ep_rew_mean          | -7.97         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 554           |
|    time_elapsed         | 9511          |
|    total_timesteps      | 4538368       |
| train/                  |               |
|    approx_kl            | 1.7842074e-05 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00512      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 4.67          |
|    n_updates            | 5530          |
|    policy_gradient_loss | -0.000121     |
|    value_loss           | 8.72          |
-------------------------------------------
Eval num_timesteps=4540000, episode_reward=-8.30 +/- 30.09
Episode length: 1678.80 +/- 358.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.68e+03      |
|    mean_reward          | -8.3          |
| time/                   |               |
|    total_timesteps      | 4540000       |
| train/                  |               |
|    approx_kl            | 1.1188684e-05 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00453      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.38          |
|    n_updates            | 5540          |
|    policy_gradient_loss | -4.53e-05     |
|    value_loss           | 7.82          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -7.64    |
| time/              |          |
|    fps             | 477      |
|    iterations      | 555      |
|    time_elapsed    | 9531     |
|    total_timesteps | 4546560  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.66e+03      |
|    ep_rew_mean          | -8.45         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 556           |
|    time_elapsed         | 9544          |
|    total_timesteps      | 4554752       |
| train/                  |               |
|    approx_kl            | 2.2091299e-05 |
|    clip_fraction        | 0.000598      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00475      |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 4.22          |
|    n_updates            | 5550          |
|    policy_gradient_loss | -8.53e-05     |
|    value_loss           | 8.85          |
-------------------------------------------
Eval num_timesteps=4560000, episode_reward=-11.60 +/- 35.70
Episode length: 1745.20 +/- 323.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.75e+03      |
|    mean_reward          | -11.6         |
| time/                   |               |
|    total_timesteps      | 4560000       |
| train/                  |               |
|    approx_kl            | 0.00014343196 |
|    clip_fraction        | 0.000452      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00758      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 3.51          |
|    n_updates            | 5560          |
|    policy_gradient_loss | -5.37e-05     |
|    value_loss           | 7.45          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -9.95    |
| time/              |          |
|    fps             | 477      |
|    iterations      | 557      |
|    time_elapsed    | 9565     |
|    total_timesteps | 4562944  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.65e+03      |
|    ep_rew_mean          | -10.3         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 558           |
|    time_elapsed         | 9579          |
|    total_timesteps      | 4571136       |
| train/                  |               |
|    approx_kl            | 1.7894032e-05 |
|    clip_fraction        | 0.000281      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00712      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.87          |
|    n_updates            | 5570          |
|    policy_gradient_loss | -1.81e-05     |
|    value_loss           | 8.12          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.66e+03     |
|    ep_rew_mean          | -10.8        |
| time/                   |              |
|    fps                  | 477          |
|    iterations           | 559          |
|    time_elapsed         | 9592         |
|    total_timesteps      | 4579328      |
| train/                  |              |
|    approx_kl            | 8.775212e-06 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00552     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.67         |
|    n_updates            | 5580         |
|    policy_gradient_loss | -2.26e-05    |
|    value_loss           | 6.68         |
------------------------------------------
Eval num_timesteps=4580000, episode_reward=-6.50 +/- 54.60
Episode length: 1833.20 +/- 385.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.83e+03      |
|    mean_reward          | -6.5          |
| time/                   |               |
|    total_timesteps      | 4580000       |
| train/                  |               |
|    approx_kl            | 2.1733606e-05 |
|    clip_fraction        | 0.000464      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00705      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.18          |
|    n_updates            | 5590          |
|    policy_gradient_loss | -2.91e-05     |
|    value_loss           | 7.75          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -11.5    |
| time/              |          |
|    fps             | 477      |
|    iterations      | 560      |
|    time_elapsed    | 9613     |
|    total_timesteps | 4587520  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.67e+03      |
|    ep_rew_mean          | -10.5         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 561           |
|    time_elapsed         | 9627          |
|    total_timesteps      | 4595712       |
| train/                  |               |
|    approx_kl            | 3.0016257e-05 |
|    clip_fraction        | 0.000403      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00529      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.23          |
|    n_updates            | 5600          |
|    policy_gradient_loss | -7.29e-05     |
|    value_loss           | 8.59          |
-------------------------------------------
Eval num_timesteps=4600000, episode_reward=-15.30 +/- 58.13
Episode length: 1291.80 +/- 120.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.29e+03      |
|    mean_reward          | -15.3         |
| time/                   |               |
|    total_timesteps      | 4600000       |
| train/                  |               |
|    approx_kl            | 7.8166195e-06 |
|    clip_fraction        | 0.00011       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00399      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.37          |
|    n_updates            | 5610          |
|    policy_gradient_loss | -5.68e-05     |
|    value_loss           | 8.85          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -9.1     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 562      |
|    time_elapsed    | 9647     |
|    total_timesteps | 4603904  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.65e+03     |
|    ep_rew_mean          | -8.76        |
| time/                   |              |
|    fps                  | 477          |
|    iterations           | 563          |
|    time_elapsed         | 9660         |
|    total_timesteps      | 4612096      |
| train/                  |              |
|    approx_kl            | 7.237021e-05 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00188     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.76         |
|    n_updates            | 5620         |
|    policy_gradient_loss | -4.95e-06    |
|    value_loss           | 8.5          |
------------------------------------------
Eval num_timesteps=4620000, episode_reward=-37.80 +/- 28.25
Episode length: 1569.60 +/- 273.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.57e+03      |
|    mean_reward          | -37.8         |
| time/                   |               |
|    total_timesteps      | 4620000       |
| train/                  |               |
|    approx_kl            | 2.6934613e-05 |
|    clip_fraction        | 0.000122      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00288      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.39          |
|    n_updates            | 5630          |
|    policy_gradient_loss | -2.19e-05     |
|    value_loss           | 8.09          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -8.7     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 564      |
|    time_elapsed    | 9682     |
|    total_timesteps | 4620288  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.68e+03     |
|    ep_rew_mean          | -11          |
| time/                   |              |
|    fps                  | 477          |
|    iterations           | 565          |
|    time_elapsed         | 9695         |
|    total_timesteps      | 4628480      |
| train/                  |              |
|    approx_kl            | 5.834656e-06 |
|    clip_fraction        | 0.00011      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0037      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.21         |
|    n_updates            | 5640         |
|    policy_gradient_loss | -3.01e-06    |
|    value_loss           | 8.21         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.69e+03      |
|    ep_rew_mean          | -11.2         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 566           |
|    time_elapsed         | 9709          |
|    total_timesteps      | 4636672       |
| train/                  |               |
|    approx_kl            | 1.1535667e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00328      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.15          |
|    n_updates            | 5650          |
|    policy_gradient_loss | -3.44e-05     |
|    value_loss           | 8.01          |
-------------------------------------------
Eval num_timesteps=4640000, episode_reward=-13.60 +/- 34.89
Episode length: 1861.80 +/- 281.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.86e+03      |
|    mean_reward          | -13.6         |
| time/                   |               |
|    total_timesteps      | 4640000       |
| train/                  |               |
|    approx_kl            | 2.7086418e-05 |
|    clip_fraction        | 0.000354      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00438      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.4           |
|    n_updates            | 5660          |
|    policy_gradient_loss | -4.64e-05     |
|    value_loss           | 7.67          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -11.6    |
| time/              |          |
|    fps             | 477      |
|    iterations      | 567      |
|    time_elapsed    | 9731     |
|    total_timesteps | 4644864  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.67e+03     |
|    ep_rew_mean          | -12.4        |
| time/                   |              |
|    fps                  | 477          |
|    iterations           | 568          |
|    time_elapsed         | 9745         |
|    total_timesteps      | 4653056      |
| train/                  |              |
|    approx_kl            | 1.375952e-05 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00447     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.25         |
|    n_updates            | 5670         |
|    policy_gradient_loss | -4.93e-06    |
|    value_loss           | 9.34         |
------------------------------------------
Eval num_timesteps=4660000, episode_reward=17.40 +/- 47.81
Episode length: 1491.20 +/- 283.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.49e+03     |
|    mean_reward          | 17.4         |
| time/                   |              |
|    total_timesteps      | 4660000      |
| train/                  |              |
|    approx_kl            | 4.317033e-05 |
|    clip_fraction        | 0.000354     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00303     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.83         |
|    n_updates            | 5680         |
|    policy_gradient_loss | -1.15e-06    |
|    value_loss           | 8.74         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -11.9    |
| time/              |          |
|    fps             | 477      |
|    iterations      | 569      |
|    time_elapsed    | 9765     |
|    total_timesteps | 4661248  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.67e+03     |
|    ep_rew_mean          | -10.9        |
| time/                   |              |
|    fps                  | 477          |
|    iterations           | 570          |
|    time_elapsed         | 9779         |
|    total_timesteps      | 4669440      |
| train/                  |              |
|    approx_kl            | 1.596236e-06 |
|    clip_fraction        | 1.22e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00318     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.33         |
|    n_updates            | 5690         |
|    policy_gradient_loss | -9.83e-07    |
|    value_loss           | 8.05         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.69e+03      |
|    ep_rew_mean          | -8.95         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 571           |
|    time_elapsed         | 9793          |
|    total_timesteps      | 4677632       |
| train/                  |               |
|    approx_kl            | 1.5770092e-05 |
|    clip_fraction        | 0.000232      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00198      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.42          |
|    n_updates            | 5700          |
|    policy_gradient_loss | -2.17e-07     |
|    value_loss           | 8.19          |
-------------------------------------------
Eval num_timesteps=4680000, episode_reward=0.80 +/- 47.61
Episode length: 1723.80 +/- 418.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.72e+03      |
|    mean_reward          | 0.8           |
| time/                   |               |
|    total_timesteps      | 4680000       |
| train/                  |               |
|    approx_kl            | 6.9582893e-06 |
|    clip_fraction        | 0.00011       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0023       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.28          |
|    n_updates            | 5710          |
|    policy_gradient_loss | -4.75e-05     |
|    value_loss           | 8.39          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -9.49    |
| time/              |          |
|    fps             | 477      |
|    iterations      | 572      |
|    time_elapsed    | 9814     |
|    total_timesteps | 4685824  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.7e+03       |
|    ep_rew_mean          | -11.8         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 573           |
|    time_elapsed         | 9828          |
|    total_timesteps      | 4694016       |
| train/                  |               |
|    approx_kl            | 1.5067199e-06 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00214      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.94          |
|    n_updates            | 5720          |
|    policy_gradient_loss | 1.26e-05      |
|    value_loss           | 9.06          |
-------------------------------------------
Eval num_timesteps=4700000, episode_reward=-39.50 +/- 39.79
Episode length: 1630.60 +/- 417.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.63e+03     |
|    mean_reward          | -39.5        |
| time/                   |              |
|    total_timesteps      | 4700000      |
| train/                  |              |
|    approx_kl            | 9.538926e-07 |
|    clip_fraction        | 7.32e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00294     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.24         |
|    n_updates            | 5730         |
|    policy_gradient_loss | 8.12e-06     |
|    value_loss           | 8.75         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -12.9    |
| time/              |          |
|    fps             | 477      |
|    iterations      | 574      |
|    time_elapsed    | 9851     |
|    total_timesteps | 4702208  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.69e+03     |
|    ep_rew_mean          | -14.9        |
| time/                   |              |
|    fps                  | 477          |
|    iterations           | 575          |
|    time_elapsed         | 9865         |
|    total_timesteps      | 4710400      |
| train/                  |              |
|    approx_kl            | 4.656613e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00272     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.24         |
|    n_updates            | 5740         |
|    policy_gradient_loss | 7.39e-10     |
|    value_loss           | 8.35         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.69e+03     |
|    ep_rew_mean          | -13.5        |
| time/                   |              |
|    fps                  | 477          |
|    iterations           | 576          |
|    time_elapsed         | 9879         |
|    total_timesteps      | 4718592      |
| train/                  |              |
|    approx_kl            | 5.175971e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00276     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.01         |
|    n_updates            | 5750         |
|    policy_gradient_loss | -6.04e-08    |
|    value_loss           | 8.02         |
------------------------------------------
Eval num_timesteps=4720000, episode_reward=-6.40 +/- 38.02
Episode length: 1516.40 +/- 212.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.52e+03      |
|    mean_reward          | -6.4          |
| time/                   |               |
|    total_timesteps      | 4720000       |
| train/                  |               |
|    approx_kl            | 6.0455495e-06 |
|    clip_fraction        | 0.00011       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00318      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.73          |
|    n_updates            | 5760          |
|    policy_gradient_loss | -5.56e-05     |
|    value_loss           | 7.64          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -14.3    |
| time/              |          |
|    fps             | 477      |
|    iterations      | 577      |
|    time_elapsed    | 9902     |
|    total_timesteps | 4726784  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.69e+03     |
|    ep_rew_mean          | -13.4        |
| time/                   |              |
|    fps                  | 477          |
|    iterations           | 578          |
|    time_elapsed         | 9915         |
|    total_timesteps      | 4734976      |
| train/                  |              |
|    approx_kl            | 1.303514e-05 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00268     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.21         |
|    n_updates            | 5770         |
|    policy_gradient_loss | -5.13e-05    |
|    value_loss           | 9.23         |
------------------------------------------
Eval num_timesteps=4740000, episode_reward=8.90 +/- 36.85
Episode length: 1533.40 +/- 107.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.53e+03      |
|    mean_reward          | 8.9           |
| time/                   |               |
|    total_timesteps      | 4740000       |
| train/                  |               |
|    approx_kl            | 4.7339636e-06 |
|    clip_fraction        | 2.44e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0029       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.57          |
|    n_updates            | 5780          |
|    policy_gradient_loss | -3.97e-07     |
|    value_loss           | 8.29          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -12.3    |
| time/              |          |
|    fps             | 477      |
|    iterations      | 579      |
|    time_elapsed    | 9935     |
|    total_timesteps | 4743168  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.68e+03      |
|    ep_rew_mean          | -10.2         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 580           |
|    time_elapsed         | 9951          |
|    total_timesteps      | 4751360       |
| train/                  |               |
|    approx_kl            | 2.3690955e-06 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00299      |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 6.18          |
|    n_updates            | 5790          |
|    policy_gradient_loss | -1.42e-06     |
|    value_loss           | 7.95          |
-------------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1.66e+03 |
|    ep_rew_mean          | -11.5    |
| time/                   |          |
|    fps                  | 477      |
|    iterations           | 581      |
|    time_elapsed         | 9965     |
|    total_timesteps      | 4759552  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00185 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 3.14     |
|    n_updates            | 5800     |
|    policy_gradient_loss | -9.1e-10 |
|    value_loss           | 8.44     |
--------------------------------------
Eval num_timesteps=4760000, episode_reward=-3.50 +/- 39.10
Episode length: 1747.80 +/- 358.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.75e+03     |
|    mean_reward          | -3.5         |
| time/                   |              |
|    total_timesteps      | 4760000      |
| train/                  |              |
|    approx_kl            | 7.131093e-05 |
|    clip_fraction        | 0.000122     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00373     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.57         |
|    n_updates            | 5810         |
|    policy_gradient_loss | -6.43e-05    |
|    value_loss           | 8.43         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -13      |
| time/              |          |
|    fps             | 477      |
|    iterations      | 582      |
|    time_elapsed    | 9987     |
|    total_timesteps | 4767744  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.66e+03      |
|    ep_rew_mean          | -15.7         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 583           |
|    time_elapsed         | 10001         |
|    total_timesteps      | 4775936       |
| train/                  |               |
|    approx_kl            | 4.9576993e-06 |
|    clip_fraction        | 0.000122      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00455      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.25          |
|    n_updates            | 5820          |
|    policy_gradient_loss | -2.95e-07     |
|    value_loss           | 7.43          |
-------------------------------------------
Eval num_timesteps=4780000, episode_reward=7.90 +/- 52.03
Episode length: 1628.00 +/- 337.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.63e+03      |
|    mean_reward          | 7.9           |
| time/                   |               |
|    total_timesteps      | 4780000       |
| train/                  |               |
|    approx_kl            | 1.0984397e-05 |
|    clip_fraction        | 0.000183      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00399      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 6.3           |
|    n_updates            | 5830          |
|    policy_gradient_loss | -2.42e-05     |
|    value_loss           | 9.25          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -15.8    |
| time/              |          |
|    fps             | 477      |
|    iterations      | 584      |
|    time_elapsed    | 10023    |
|    total_timesteps | 4784128  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.65e+03      |
|    ep_rew_mean          | -15.7         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 585           |
|    time_elapsed         | 10037         |
|    total_timesteps      | 4792320       |
| train/                  |               |
|    approx_kl            | 3.5559642e-06 |
|    clip_fraction        | 0.00011       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00433      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.92          |
|    n_updates            | 5840          |
|    policy_gradient_loss | -4.52e-05     |
|    value_loss           | 9.02          |
-------------------------------------------
Eval num_timesteps=4800000, episode_reward=-16.30 +/- 23.58
Episode length: 1649.40 +/- 332.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.65e+03      |
|    mean_reward          | -16.3         |
| time/                   |               |
|    total_timesteps      | 4800000       |
| train/                  |               |
|    approx_kl            | 5.9875136e-05 |
|    clip_fraction        | 0.000549      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00587      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.53          |
|    n_updates            | 5850          |
|    policy_gradient_loss | -0.000129     |
|    value_loss           | 8.43          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -16      |
| time/              |          |
|    fps             | 477      |
|    iterations      | 586      |
|    time_elapsed    | 10058    |
|    total_timesteps | 4800512  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.63e+03      |
|    ep_rew_mean          | -16.2         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 587           |
|    time_elapsed         | 10074         |
|    total_timesteps      | 4808704       |
| train/                  |               |
|    approx_kl            | 3.4124023e-06 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0071       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.98          |
|    n_updates            | 5860          |
|    policy_gradient_loss | -2.54e-05     |
|    value_loss           | 9.07          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.63e+03     |
|    ep_rew_mean          | -17.7        |
| time/                   |              |
|    fps                  | 477          |
|    iterations           | 588          |
|    time_elapsed         | 10087        |
|    total_timesteps      | 4816896      |
| train/                  |              |
|    approx_kl            | 4.738936e-05 |
|    clip_fraction        | 0.000598     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00695     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.88         |
|    n_updates            | 5870         |
|    policy_gradient_loss | -2.49e-05    |
|    value_loss           | 8.93         |
------------------------------------------
Eval num_timesteps=4820000, episode_reward=3.40 +/- 46.16
Episode length: 1544.00 +/- 190.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.54e+03     |
|    mean_reward          | 3.4          |
| time/                   |              |
|    total_timesteps      | 4820000      |
| train/                  |              |
|    approx_kl            | 8.350689e-06 |
|    clip_fraction        | 3.66e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00768     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.34         |
|    n_updates            | 5880         |
|    policy_gradient_loss | -1.03e-05    |
|    value_loss           | 8.92         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -15      |
| time/              |          |
|    fps             | 477      |
|    iterations      | 589      |
|    time_elapsed    | 10107    |
|    total_timesteps | 4825088  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.61e+03      |
|    ep_rew_mean          | -16.9         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 590           |
|    time_elapsed         | 10121         |
|    total_timesteps      | 4833280       |
| train/                  |               |
|    approx_kl            | 2.7252048e-05 |
|    clip_fraction        | 0.000598      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00511      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.83          |
|    n_updates            | 5890          |
|    policy_gradient_loss | 1.54e-05      |
|    value_loss           | 8.06          |
-------------------------------------------
Eval num_timesteps=4840000, episode_reward=-7.70 +/- 51.56
Episode length: 1536.20 +/- 478.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.54e+03      |
|    mean_reward          | -7.7          |
| time/                   |               |
|    total_timesteps      | 4840000       |
| train/                  |               |
|    approx_kl            | 2.5018766e-05 |
|    clip_fraction        | 0.000366      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00708      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 3.44          |
|    n_updates            | 5900          |
|    policy_gradient_loss | -1.93e-05     |
|    value_loss           | 7.89          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -18.2    |
| time/              |          |
|    fps             | 477      |
|    iterations      | 591      |
|    time_elapsed    | 10141    |
|    total_timesteps | 4841472  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.63e+03      |
|    ep_rew_mean          | -18.6         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 592           |
|    time_elapsed         | 10155         |
|    total_timesteps      | 4849664       |
| train/                  |               |
|    approx_kl            | 2.4591354e-05 |
|    clip_fraction        | 0.000562      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00596      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.63          |
|    n_updates            | 5910          |
|    policy_gradient_loss | -6.97e-05     |
|    value_loss           | 7.32          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.65e+03      |
|    ep_rew_mean          | -18.3         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 593           |
|    time_elapsed         | 10168         |
|    total_timesteps      | 4857856       |
| train/                  |               |
|    approx_kl            | 7.4839045e-06 |
|    clip_fraction        | 0.000159      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00702      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.43          |
|    n_updates            | 5920          |
|    policy_gradient_loss | -2.32e-05     |
|    value_loss           | 7.26          |
-------------------------------------------
Eval num_timesteps=4860000, episode_reward=13.80 +/- 40.47
Episode length: 1614.60 +/- 233.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.61e+03      |
|    mean_reward          | 13.8          |
| time/                   |               |
|    total_timesteps      | 4860000       |
| train/                  |               |
|    approx_kl            | 2.2120992e-05 |
|    clip_fraction        | 0.00033       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00625      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.5           |
|    n_updates            | 5930          |
|    policy_gradient_loss | -9.35e-06     |
|    value_loss           | 7.91          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -16.1    |
| time/              |          |
|    fps             | 477      |
|    iterations      | 594      |
|    time_elapsed    | 10189    |
|    total_timesteps | 4866048  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.65e+03     |
|    ep_rew_mean          | -15.5        |
| time/                   |              |
|    fps                  | 477          |
|    iterations           | 595          |
|    time_elapsed         | 10203        |
|    total_timesteps      | 4874240      |
| train/                  |              |
|    approx_kl            | 4.783272e-05 |
|    clip_fraction        | 0.0005       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00669     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.25         |
|    n_updates            | 5940         |
|    policy_gradient_loss | -5.71e-05    |
|    value_loss           | 8.55         |
------------------------------------------
Eval num_timesteps=4880000, episode_reward=-33.80 +/- 13.19
Episode length: 1669.40 +/- 299.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.67e+03      |
|    mean_reward          | -33.8         |
| time/                   |               |
|    total_timesteps      | 4880000       |
| train/                  |               |
|    approx_kl            | 4.9959373e-05 |
|    clip_fraction        | 0.000903      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00488      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.13          |
|    n_updates            | 5950          |
|    policy_gradient_loss | 1e-05         |
|    value_loss           | 8.3           |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -16.3    |
| time/              |          |
|    fps             | 477      |
|    iterations      | 596      |
|    time_elapsed    | 10226    |
|    total_timesteps | 4882432  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.65e+03      |
|    ep_rew_mean          | -17.6         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 597           |
|    time_elapsed         | 10240         |
|    total_timesteps      | 4890624       |
| train/                  |               |
|    approx_kl            | 2.4462206e-05 |
|    clip_fraction        | 0.000452      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00669      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.47          |
|    n_updates            | 5960          |
|    policy_gradient_loss | -3.65e-05     |
|    value_loss           | 6.88          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.66e+03      |
|    ep_rew_mean          | -16.8         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 598           |
|    time_elapsed         | 10255         |
|    total_timesteps      | 4898816       |
| train/                  |               |
|    approx_kl            | 1.8033868e-05 |
|    clip_fraction        | 0.000366      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00709      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.89          |
|    n_updates            | 5970          |
|    policy_gradient_loss | -4.67e-05     |
|    value_loss           | 8.98          |
-------------------------------------------
Eval num_timesteps=4900000, episode_reward=-38.20 +/- 26.08
Episode length: 1542.40 +/- 155.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.54e+03      |
|    mean_reward          | -38.2         |
| time/                   |               |
|    total_timesteps      | 4900000       |
| train/                  |               |
|    approx_kl            | 2.0506013e-05 |
|    clip_fraction        | 0.000513      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00683      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.25          |
|    n_updates            | 5980          |
|    policy_gradient_loss | -3.61e-05     |
|    value_loss           | 8.01          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -17.6    |
| time/              |          |
|    fps             | 477      |
|    iterations      | 599      |
|    time_elapsed    | 10275    |
|    total_timesteps | 4907008  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.67e+03      |
|    ep_rew_mean          | -17.6         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 600           |
|    time_elapsed         | 10290         |
|    total_timesteps      | 4915200       |
| train/                  |               |
|    approx_kl            | 7.0465976e-06 |
|    clip_fraction        | 6.1e-05       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00675      |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 5.57          |
|    n_updates            | 5990          |
|    policy_gradient_loss | -2.02e-06     |
|    value_loss           | 7.19          |
-------------------------------------------
Eval num_timesteps=4920000, episode_reward=-6.50 +/- 44.06
Episode length: 1453.60 +/- 312.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.45e+03    |
|    mean_reward          | -6.5        |
| time/                   |             |
|    total_timesteps      | 4920000     |
| train/                  |             |
|    approx_kl            | 1.36952e-05 |
|    clip_fraction        | 0.000354    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00534    |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.67        |
|    n_updates            | 6000        |
|    policy_gradient_loss | 6.46e-06    |
|    value_loss           | 8.18        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -16.7    |
| time/              |          |
|    fps             | 477      |
|    iterations      | 601      |
|    time_elapsed    | 10310    |
|    total_timesteps | 4923392  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.67e+03      |
|    ep_rew_mean          | -16.2         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 602           |
|    time_elapsed         | 10325         |
|    total_timesteps      | 4931584       |
| train/                  |               |
|    approx_kl            | 2.9915165e-05 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00593      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 5.34          |
|    n_updates            | 6010          |
|    policy_gradient_loss | -0.000106     |
|    value_loss           | 10            |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.67e+03     |
|    ep_rew_mean          | -15.1        |
| time/                   |              |
|    fps                  | 477          |
|    iterations           | 603          |
|    time_elapsed         | 10339        |
|    total_timesteps      | 4939776      |
| train/                  |              |
|    approx_kl            | 7.996707e-06 |
|    clip_fraction        | 0.000208     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00547     |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 3.76         |
|    n_updates            | 6020         |
|    policy_gradient_loss | -2.34e-05    |
|    value_loss           | 8.7          |
------------------------------------------
Eval num_timesteps=4940000, episode_reward=-11.80 +/- 16.57
Episode length: 1899.60 +/- 272.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.9e+03      |
|    mean_reward          | -11.8        |
| time/                   |              |
|    total_timesteps      | 4940000      |
| train/                  |              |
|    approx_kl            | 4.717194e-05 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00538     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 7.2          |
|    n_updates            | 6030         |
|    policy_gradient_loss | -0.000156    |
|    value_loss           | 8.63         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -15.1    |
| time/              |          |
|    fps             | 477      |
|    iterations      | 604      |
|    time_elapsed    | 10362    |
|    total_timesteps | 4947968  |
---------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 1.68e+03       |
|    ep_rew_mean          | -15.5          |
| time/                   |                |
|    fps                  | 477            |
|    iterations           | 605            |
|    time_elapsed         | 10377          |
|    total_timesteps      | 4956160        |
| train/                  |                |
|    approx_kl            | 1.06817315e-05 |
|    clip_fraction        | 0.000342       |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00662       |
|    explained_variance   | 0              |
|    learning_rate        | 0.0003         |
|    loss                 | 4.02           |
|    n_updates            | 6040           |
|    policy_gradient_loss | -5.29e-06      |
|    value_loss           | 8.36           |
--------------------------------------------
Eval num_timesteps=4960000, episode_reward=-13.80 +/- 34.76
Episode length: 1763.00 +/- 300.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.76e+03     |
|    mean_reward          | -13.8        |
| time/                   |              |
|    total_timesteps      | 4960000      |
| train/                  |              |
|    approx_kl            | 9.897034e-06 |
|    clip_fraction        | 6.1e-05      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00665     |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 4.48         |
|    n_updates            | 6050         |
|    policy_gradient_loss | -1.51e-05    |
|    value_loss           | 6.9          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -16.5    |
| time/              |          |
|    fps             | 477      |
|    iterations      | 606      |
|    time_elapsed    | 10399    |
|    total_timesteps | 4964352  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.68e+03      |
|    ep_rew_mean          | -16.3         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 607           |
|    time_elapsed         | 10413         |
|    total_timesteps      | 4972544       |
| train/                  |               |
|    approx_kl            | 1.3949328e-05 |
|    clip_fraction        | 0.000513      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00802      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.01          |
|    n_updates            | 6060          |
|    policy_gradient_loss | -0.000116     |
|    value_loss           | 8.88          |
-------------------------------------------
Eval num_timesteps=4980000, episode_reward=-1.60 +/- 46.65
Episode length: 1608.20 +/- 239.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.61e+03      |
|    mean_reward          | -1.6          |
| time/                   |               |
|    total_timesteps      | 4980000       |
| train/                  |               |
|    approx_kl            | 3.1074786e-05 |
|    clip_fraction        | 0.000452      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0105       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.47          |
|    n_updates            | 6070          |
|    policy_gradient_loss | -0.000144     |
|    value_loss           | 7.8           |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -17.8    |
| time/              |          |
|    fps             | 477      |
|    iterations      | 608      |
|    time_elapsed    | 10436    |
|    total_timesteps | 4980736  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.68e+03      |
|    ep_rew_mean          | -18.2         |
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 609           |
|    time_elapsed         | 10450         |
|    total_timesteps      | 4988928       |
| train/                  |               |
|    approx_kl            | 0.00012559339 |
|    clip_fraction        | 0.00117       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0153       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.07          |
|    n_updates            | 6080          |
|    policy_gradient_loss | -0.000124     |
|    value_loss           | 7.74          |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.69e+03    |
|    ep_rew_mean          | -20.3       |
| time/                   |             |
|    fps                  | 477         |
|    iterations           | 610         |
|    time_elapsed         | 10465       |
|    total_timesteps      | 4997120     |
| train/                  |             |
|    approx_kl            | 2.06072e-05 |
|    clip_fraction        | 0.000391    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0169     |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 6.07        |
|    n_updates            | 6090        |
|    policy_gradient_loss | -5.12e-06   |
|    value_loss           | 9.14        |
-----------------------------------------
Eval num_timesteps=5000000, episode_reward=-17.80 +/- 22.25
Episode length: 1716.40 +/- 298.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.72e+03      |
|    mean_reward          | -17.8         |
| time/                   |               |
|    total_timesteps      | 5000000       |
| train/                  |               |
|    approx_kl            | 1.3951874e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0136       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.31          |
|    n_updates            | 6100          |
|    policy_gradient_loss | 3.04e-06      |
|    value_loss           | 8.27          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -19.3    |
| time/              |          |
|    fps             | 477      |
|    iterations      | 611      |
|    time_elapsed    | 10488    |
|    total_timesteps | 5005312  |
---------------------------------