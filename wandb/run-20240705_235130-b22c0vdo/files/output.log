
Using cuda device
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:155: UserWarning: You have specified a mini-batch size of 424, but because the `RolloutBuffer` is of size `n_steps * n_envs = 32272`, after every 76 untruncated mini-batches, there will be a truncated mini-batch of size 48
We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.
Info: (n_steps=8068 and n_envs=4)
  warnings.warn(
Logging to ./logs/pong/PPO_1
Eval num_timesteps=20000, episode_reward=20.00 +/- 26.90
Episode length: 2070.00 +/- 261.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.07e+03 |
|    mean_reward     | 20       |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -8.59    |
| time/              |          |
|    fps             | 843      |
|    iterations      | 1        |
|    time_elapsed    | 38       |
|    total_timesteps | 32272    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-18.30 +/- 37.15
Episode length: 1616.80 +/- 286.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.62e+03     |
|    mean_reward          | -18.3        |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0016545156 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 2.58         |
|    n_updates            | 7            |
|    policy_gradient_loss | -0.000245    |
|    value_loss           | 2.51         |
------------------------------------------
Eval num_timesteps=60000, episode_reward=-20.90 +/- 38.80
Episode length: 1440.60 +/- 136.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.44e+03 |
|    mean_reward     | -20.9    |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -6.68    |
| time/              |          |
|    fps             | 723      |
|    iterations      | 2        |
|    time_elapsed    | 89       |
|    total_timesteps | 64544    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-23.10 +/- 32.74
Episode length: 1452.40 +/- 122.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.45e+03     |
|    mean_reward          | -23.1        |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0010405547 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 4.37         |
|    n_updates            | 14           |
|    policy_gradient_loss | -5.95e-05    |
|    value_loss           | 2.37         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -9.53    |
| time/              |          |
|    fps             | 737      |
|    iterations      | 3        |
|    time_elapsed    | 131      |
|    total_timesteps | 96816    |
---------------------------------
Eval num_timesteps=100000, episode_reward=29.40 +/- 23.87
Episode length: 1953.60 +/- 342.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.95e+03     |
|    mean_reward          | 29.4         |
| time/                   |              |
|    total_timesteps      | 100000       |
| train/                  |              |
|    approx_kl            | 0.0032113458 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 2.47         |
|    n_updates            | 21           |
|    policy_gradient_loss | -0.000356    |
|    value_loss           | 2.45         |
------------------------------------------
New best mean reward!
Eval num_timesteps=120000, episode_reward=20.50 +/- 34.05
Episode length: 2084.20 +/- 347.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.08e+03 |
|    mean_reward     | 20.5     |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -8.63    |
| time/              |          |
|    fps             | 700      |
|    iterations      | 4        |
|    time_elapsed    | 184      |
|    total_timesteps | 129088   |
---------------------------------
Eval num_timesteps=140000, episode_reward=25.60 +/- 39.85
Episode length: 1942.00 +/- 479.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.94e+03    |
|    mean_reward          | 25.6        |
| time/                   |             |
|    total_timesteps      | 140000      |
| train/                  |             |
|    approx_kl            | 0.000602236 |
|    clip_fraction        | 0           |
|    clip_range           | 0.305       |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0           |
|    learning_rate        | 0.000888    |
|    loss                 | 0.198       |
|    n_updates            | 28          |
|    policy_gradient_loss | -3.39e-05   |
|    value_loss           | 2.31        |
-----------------------------------------
Eval num_timesteps=160000, episode_reward=24.50 +/- 28.23
Episode length: 2059.00 +/- 531.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.06e+03 |
|    mean_reward     | 24.5     |
| time/              |          |
|    total_timesteps | 160000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -11.9    |
| time/              |          |
|    fps             | 676      |
|    iterations      | 5        |
|    time_elapsed    | 238      |
|    total_timesteps | 161360   |
---------------------------------
Eval num_timesteps=180000, episode_reward=23.40 +/- 15.65
Episode length: 2325.40 +/- 304.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 2.33e+03      |
|    mean_reward          | 23.4          |
| time/                   |               |
|    total_timesteps      | 180000        |
| train/                  |               |
|    approx_kl            | 0.00077357295 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.09         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 0.398         |
|    n_updates            | 35            |
|    policy_gradient_loss | -4.08e-05     |
|    value_loss           | 2.4           |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -9.49    |
| time/              |          |
|    fps             | 680      |
|    iterations      | 6        |
|    time_elapsed    | 284      |
|    total_timesteps | 193632   |
---------------------------------
Eval num_timesteps=200000, episode_reward=43.10 +/- 27.25
Episode length: 1785.60 +/- 512.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.79e+03      |
|    mean_reward          | 43.1          |
| time/                   |               |
|    total_timesteps      | 200000        |
| train/                  |               |
|    approx_kl            | 8.2524675e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.08         |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.000888      |
|    loss                 | 1.61          |
|    n_updates            | 42            |
|    policy_gradient_loss | 3.3e-05       |
|    value_loss           | 2.32          |
-------------------------------------------
New best mean reward!
Eval num_timesteps=220000, episode_reward=13.00 +/- 36.64
Episode length: 1815.00 +/- 243.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.82e+03 |
|    mean_reward     | 13       |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -6.75    |
| time/              |          |
|    fps             | 668      |
|    iterations      | 7        |
|    time_elapsed    | 337      |
|    total_timesteps | 225904   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-11.70 +/- 37.86
Episode length: 1767.00 +/- 453.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.77e+03     |
|    mean_reward          | -11.7        |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0030820356 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.512        |
|    n_updates            | 49           |
|    policy_gradient_loss | -0.000305    |
|    value_loss           | 2.35         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.71e+03 |
|    ep_rew_mean     | -2.84    |
| time/              |          |
|    fps             | 679      |
|    iterations      | 8        |
|    time_elapsed    | 379      |
|    total_timesteps | 258176   |
---------------------------------
Eval num_timesteps=260000, episode_reward=24.10 +/- 46.27
Episode length: 1939.40 +/- 455.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.94e+03     |
|    mean_reward          | 24.1         |
| time/                   |              |
|    total_timesteps      | 260000       |
| train/                  |              |
|    approx_kl            | 0.0009775095 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 3.36         |
|    n_updates            | 56           |
|    policy_gradient_loss | -0.000138    |
|    value_loss           | 2.26         |
------------------------------------------
Eval num_timesteps=280000, episode_reward=27.90 +/- 30.12
Episode length: 1978.00 +/- 221.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.98e+03 |
|    mean_reward     | 27.9     |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -2.84    |
| time/              |          |
|    fps             | 671      |
|    iterations      | 9        |
|    time_elapsed    | 432      |
|    total_timesteps | 290448   |
---------------------------------
Eval num_timesteps=300000, episode_reward=-19.50 +/- 30.60
Episode length: 1787.00 +/- 305.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.79e+03     |
|    mean_reward          | -19.5        |
| time/                   |              |
|    total_timesteps      | 300000       |
| train/                  |              |
|    approx_kl            | 0.0036501056 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.35         |
|    n_updates            | 63           |
|    policy_gradient_loss | -0.000605    |
|    value_loss           | 2.41         |
------------------------------------------
Eval num_timesteps=320000, episode_reward=-2.10 +/- 37.16
Episode length: 1654.40 +/- 416.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.65e+03 |
|    mean_reward     | -2.1     |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -3.08    |
| time/              |          |
|    fps             | 667      |
|    iterations      | 10       |
|    time_elapsed    | 483      |
|    total_timesteps | 322720   |
---------------------------------
Eval num_timesteps=340000, episode_reward=19.60 +/- 38.74
Episode length: 1878.00 +/- 320.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.88e+03     |
|    mean_reward          | 19.6         |
| time/                   |              |
|    total_timesteps      | 340000       |
| train/                  |              |
|    approx_kl            | 0.0051946705 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.1         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.000888     |
|    loss                 | 1.42         |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.000855    |
|    value_loss           | 2.52         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -2.69    |
| time/              |          |
|    fps             | 673      |
|    iterations      | 11       |
|    time_elapsed    | 526      |
|    total_timesteps | 354992   |
---------------------------------
Eval num_timesteps=360000, episode_reward=11.60 +/- 44.72
Episode length: 1853.00 +/- 471.99
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 1.85e+03       |
|    mean_reward          | 11.6           |
| time/                   |                |
|    total_timesteps      | 360000         |
| train/                  |                |
|    approx_kl            | 0.000103713566 |
|    clip_fraction        | 0              |
|    clip_range           | 0.305          |
|    entropy_loss         | -1.1           |
|    explained_variance   | 0              |
|    learning_rate        | 0.000888       |
|    loss                 | 2.22           |
|    n_updates            | 77             |
|    policy_gradient_loss | 8.41e-06       |
|    value_loss           | 2.4            |
--------------------------------------------
Eval num_timesteps=380000, episode_reward=26.60 +/- 32.47
Episode length: 1693.80 +/- 198.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.69e+03 |
|    mean_reward     | 26.6     |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -6.42    |
| time/              |          |
|    fps             | 669      |
|    iterations      | 12       |
|    time_elapsed    | 578      |
|    total_timesteps | 387264   |
---------------------------------
Eval num_timesteps=400000, episode_reward=23.60 +/- 37.74
Episode length: 1858.00 +/- 329.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.86e+03      |
|    mean_reward          | 23.6          |
| time/                   |               |
|    total_timesteps      | 400000        |
| train/                  |               |
|    approx_kl            | 0.00066985504 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 3             |
|    n_updates            | 84            |
|    policy_gradient_loss | -9.63e-05     |
|    value_loss           | 2.46          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -8.22    |
| time/              |          |
|    fps             | 673      |
|    iterations      | 13       |
|    time_elapsed    | 622      |
|    total_timesteps | 419536   |
---------------------------------
Eval num_timesteps=420000, episode_reward=-1.30 +/- 28.51
Episode length: 1892.20 +/- 388.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.89e+03     |
|    mean_reward          | -1.3         |
| time/                   |              |
|    total_timesteps      | 420000       |
| train/                  |              |
|    approx_kl            | 0.0011920528 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 2.35         |
|    n_updates            | 91           |
|    policy_gradient_loss | -0.000203    |
|    value_loss           | 2.27         |
------------------------------------------
Eval num_timesteps=440000, episode_reward=-3.20 +/- 34.74
Episode length: 2095.00 +/- 321.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.1e+03  |
|    mean_reward     | -3.2     |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -6.87    |
| time/              |          |
|    fps             | 665      |
|    iterations      | 14       |
|    time_elapsed    | 679      |
|    total_timesteps | 451808   |
---------------------------------
Eval num_timesteps=460000, episode_reward=23.00 +/- 36.91
Episode length: 2152.40 +/- 421.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.15e+03     |
|    mean_reward          | 23           |
| time/                   |              |
|    total_timesteps      | 460000       |
| train/                  |              |
|    approx_kl            | 0.0006443853 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.43         |
|    n_updates            | 98           |
|    policy_gradient_loss | -6.77e-05    |
|    value_loss           | 2.28         |
------------------------------------------
Eval num_timesteps=480000, episode_reward=51.50 +/- 7.81
Episode length: 1678.60 +/- 235.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.68e+03 |
|    mean_reward     | 51.5     |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -4.88    |
| time/              |          |
|    fps             | 661      |
|    iterations      | 15       |
|    time_elapsed    | 732      |
|    total_timesteps | 484080   |
---------------------------------
Eval num_timesteps=500000, episode_reward=4.50 +/- 41.54
Episode length: 1697.80 +/- 242.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.7e+03     |
|    mean_reward          | 4.5         |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.000605484 |
|    clip_fraction        | 0           |
|    clip_range           | 0.305       |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0           |
|    learning_rate        | 0.000888    |
|    loss                 | 1.24        |
|    n_updates            | 105         |
|    policy_gradient_loss | -8.7e-05    |
|    value_loss           | 2.47        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.71e+03 |
|    ep_rew_mean     | -0.74    |
| time/              |          |
|    fps             | 665      |
|    iterations      | 16       |
|    time_elapsed    | 775      |
|    total_timesteps | 516352   |
---------------------------------
Eval num_timesteps=520000, episode_reward=-4.20 +/- 32.96
Episode length: 2011.00 +/- 493.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.01e+03     |
|    mean_reward          | -4.2         |
| time/                   |              |
|    total_timesteps      | 520000       |
| train/                  |              |
|    approx_kl            | 9.101259e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.65         |
|    n_updates            | 112          |
|    policy_gradient_loss | 1.55e-06     |
|    value_loss           | 2.25         |
------------------------------------------
Eval num_timesteps=540000, episode_reward=42.80 +/- 10.91
Episode length: 2047.00 +/- 336.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.05e+03 |
|    mean_reward     | 42.8     |
| time/              |          |
|    total_timesteps | 540000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.71e+03 |
|    ep_rew_mean     | 3.19     |
| time/              |          |
|    fps             | 661      |
|    iterations      | 17       |
|    time_elapsed    | 829      |
|    total_timesteps | 548624   |
---------------------------------
Eval num_timesteps=560000, episode_reward=18.50 +/- 34.66
Episode length: 2049.60 +/- 551.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.05e+03     |
|    mean_reward          | 18.5         |
| time/                   |              |
|    total_timesteps      | 560000       |
| train/                  |              |
|    approx_kl            | 0.0011138552 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.87         |
|    n_updates            | 119          |
|    policy_gradient_loss | -0.00018     |
|    value_loss           | 2.45         |
------------------------------------------
Eval num_timesteps=580000, episode_reward=43.10 +/- 30.27
Episode length: 1649.60 +/- 361.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.65e+03 |
|    mean_reward     | 43.1     |
| time/              |          |
|    total_timesteps | 580000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.73e+03 |
|    ep_rew_mean     | 5.42     |
| time/              |          |
|    fps             | 659      |
|    iterations      | 18       |
|    time_elapsed    | 880      |
|    total_timesteps | 580896   |
---------------------------------
Eval num_timesteps=600000, episode_reward=15.20 +/- 14.49
Episode length: 2274.60 +/- 384.01
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2.27e+03   |
|    mean_reward          | 15.2       |
| time/                   |            |
|    total_timesteps      | 600000     |
| train/                  |            |
|    approx_kl            | 0.00302821 |
|    clip_fraction        | 0          |
|    clip_range           | 0.305      |
|    entropy_loss         | -1.09      |
|    explained_variance   | 0          |
|    learning_rate        | 0.000888   |
|    loss                 | 0.406      |
|    n_updates            | 126        |
|    policy_gradient_loss | -0.00041   |
|    value_loss           | 2.41       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.71e+03 |
|    ep_rew_mean     | 6.61     |
| time/              |          |
|    fps             | 662      |
|    iterations      | 19       |
|    time_elapsed    | 926      |
|    total_timesteps | 613168   |
---------------------------------
Eval num_timesteps=620000, episode_reward=37.00 +/- 20.45
Episode length: 2166.60 +/- 185.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 2.17e+03      |
|    mean_reward          | 37            |
| time/                   |               |
|    total_timesteps      | 620000        |
| train/                  |               |
|    approx_kl            | 0.00042835044 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.09         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 0.393         |
|    n_updates            | 133           |
|    policy_gradient_loss | -3.93e-05     |
|    value_loss           | 2.36          |
-------------------------------------------
Eval num_timesteps=640000, episode_reward=27.00 +/- 14.65
Episode length: 2230.80 +/- 730.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.23e+03 |
|    mean_reward     | 27       |
| time/              |          |
|    total_timesteps | 640000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.72e+03 |
|    ep_rew_mean     | 1.11     |
| time/              |          |
|    fps             | 658      |
|    iterations      | 20       |
|    time_elapsed    | 980      |
|    total_timesteps | 645440   |
---------------------------------
Eval num_timesteps=660000, episode_reward=42.80 +/- 25.34
Episode length: 1782.40 +/- 208.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.78e+03     |
|    mean_reward          | 42.8         |
| time/                   |              |
|    total_timesteps      | 660000       |
| train/                  |              |
|    approx_kl            | 0.0028122463 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 2.39         |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.000474    |
|    value_loss           | 2.35         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -2.94    |
| time/              |          |
|    fps             | 661      |
|    iterations      | 21       |
|    time_elapsed    | 1025     |
|    total_timesteps | 677712   |
---------------------------------
Eval num_timesteps=680000, episode_reward=47.80 +/- 19.09
Episode length: 1941.80 +/- 544.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.94e+03     |
|    mean_reward          | 47.8         |
| time/                   |              |
|    total_timesteps      | 680000       |
| train/                  |              |
|    approx_kl            | 0.0017050524 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.664        |
|    n_updates            | 147          |
|    policy_gradient_loss | -0.000256    |
|    value_loss           | 2.49         |
------------------------------------------
Eval num_timesteps=700000, episode_reward=15.80 +/- 39.36
Episode length: 1935.40 +/- 413.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.94e+03 |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 700000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -3.78    |
| time/              |          |
|    fps             | 656      |
|    iterations      | 22       |
|    time_elapsed    | 1081     |
|    total_timesteps | 709984   |
---------------------------------
Eval num_timesteps=720000, episode_reward=13.60 +/- 24.39
Episode length: 2025.40 +/- 446.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.03e+03     |
|    mean_reward          | 13.6         |
| time/                   |              |
|    total_timesteps      | 720000       |
| train/                  |              |
|    approx_kl            | 0.0010532542 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.898        |
|    n_updates            | 154          |
|    policy_gradient_loss | -0.000161    |
|    value_loss           | 2.49         |
------------------------------------------
Eval num_timesteps=740000, episode_reward=25.30 +/- 41.22
Episode length: 1786.00 +/- 438.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.79e+03 |
|    mean_reward     | 25.3     |
| time/              |          |
|    total_timesteps | 740000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -3.1     |
| time/              |          |
|    fps             | 655      |
|    iterations      | 23       |
|    time_elapsed    | 1132     |
|    total_timesteps | 742256   |
---------------------------------
Eval num_timesteps=760000, episode_reward=54.30 +/- 41.12
Episode length: 1448.80 +/- 291.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.45e+03      |
|    mean_reward          | 54.3          |
| time/                   |               |
|    total_timesteps      | 760000        |
| train/                  |               |
|    approx_kl            | 0.00022016582 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.09         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 1.14          |
|    n_updates            | 161           |
|    policy_gradient_loss | -1.63e-05     |
|    value_loss           | 2.53          |
-------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -2.8     |
| time/              |          |
|    fps             | 657      |
|    iterations      | 24       |
|    time_elapsed    | 1177     |
|    total_timesteps | 774528   |
---------------------------------
Eval num_timesteps=780000, episode_reward=30.40 +/- 34.85
Episode length: 2066.40 +/- 301.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.07e+03     |
|    mean_reward          | 30.4         |
| time/                   |              |
|    total_timesteps      | 780000       |
| train/                  |              |
|    approx_kl            | 0.0032073597 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.605        |
|    n_updates            | 168          |
|    policy_gradient_loss | -0.000591    |
|    value_loss           | 2.27         |
------------------------------------------
Eval num_timesteps=800000, episode_reward=24.80 +/- 34.70
Episode length: 1700.40 +/- 616.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.7e+03  |
|    mean_reward     | 24.8     |
| time/              |          |
|    total_timesteps | 800000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -1.19    |
| time/              |          |
|    fps             | 654      |
|    iterations      | 25       |
|    time_elapsed    | 1232     |
|    total_timesteps | 806800   |
---------------------------------
Eval num_timesteps=820000, episode_reward=-28.20 +/- 37.94
Episode length: 1565.60 +/- 406.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.57e+03      |
|    mean_reward          | -28.2         |
| time/                   |               |
|    total_timesteps      | 820000        |
| train/                  |               |
|    approx_kl            | 0.00012576576 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.08         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 1.51          |
|    n_updates            | 175           |
|    policy_gradient_loss | 4.34e-06      |
|    value_loss           | 2.3           |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -3.8     |
| time/              |          |
|    fps             | 657      |
|    iterations      | 26       |
|    time_elapsed    | 1275     |
|    total_timesteps | 839072   |
---------------------------------
Eval num_timesteps=840000, episode_reward=25.60 +/- 33.27
Episode length: 1935.00 +/- 316.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.94e+03      |
|    mean_reward          | 25.6          |
| time/                   |               |
|    total_timesteps      | 840000        |
| train/                  |               |
|    approx_kl            | 6.6795656e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.08         |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.000888      |
|    loss                 | 0.592         |
|    n_updates            | 182           |
|    policy_gradient_loss | 2.68e-05      |
|    value_loss           | 2.51          |
-------------------------------------------
Eval num_timesteps=860000, episode_reward=21.70 +/- 42.14
Episode length: 1898.20 +/- 411.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.9e+03  |
|    mean_reward     | 21.7     |
| time/              |          |
|    total_timesteps | 860000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -4.01    |
| time/              |          |
|    fps             | 656      |
|    iterations      | 27       |
|    time_elapsed    | 1327     |
|    total_timesteps | 871344   |
---------------------------------
Eval num_timesteps=880000, episode_reward=1.80 +/- 37.65
Episode length: 2055.20 +/- 767.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 2.06e+03      |
|    mean_reward          | 1.8           |
| time/                   |               |
|    total_timesteps      | 880000        |
| train/                  |               |
|    approx_kl            | 2.0285544e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.08         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 1.86          |
|    n_updates            | 189           |
|    policy_gradient_loss | 2.46e-05      |
|    value_loss           | 2.39          |
-------------------------------------------
Eval num_timesteps=900000, episode_reward=30.20 +/- 42.98
Episode length: 1564.00 +/- 391.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.56e+03 |
|    mean_reward     | 30.2     |
| time/              |          |
|    total_timesteps | 900000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -4.37    |
| time/              |          |
|    fps             | 654      |
|    iterations      | 28       |
|    time_elapsed    | 1379     |
|    total_timesteps | 903616   |
---------------------------------
Eval num_timesteps=920000, episode_reward=-21.50 +/- 36.67
Episode length: 1809.20 +/- 300.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.81e+03      |
|    mean_reward          | -21.5         |
| time/                   |               |
|    total_timesteps      | 920000        |
| train/                  |               |
|    approx_kl            | 0.00026629382 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.08         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 0.623         |
|    n_updates            | 196           |
|    policy_gradient_loss | -3.05e-05     |
|    value_loss           | 2.41          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -6.34    |
| time/              |          |
|    fps             | 656      |
|    iterations      | 29       |
|    time_elapsed    | 1426     |
|    total_timesteps | 935888   |
---------------------------------
Eval num_timesteps=940000, episode_reward=0.20 +/- 31.52
Episode length: 1737.20 +/- 235.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.74e+03     |
|    mean_reward          | 0.2          |
| time/                   |              |
|    total_timesteps      | 940000       |
| train/                  |              |
|    approx_kl            | 0.0005714368 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.27         |
|    n_updates            | 203          |
|    policy_gradient_loss | -0.000121    |
|    value_loss           | 2.36         |
------------------------------------------
Eval num_timesteps=960000, episode_reward=-18.70 +/- 40.08
Episode length: 1462.00 +/- 247.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.46e+03 |
|    mean_reward     | -18.7    |
| time/              |          |
|    total_timesteps | 960000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -7.4     |
| time/              |          |
|    fps             | 656      |
|    iterations      | 30       |
|    time_elapsed    | 1475     |
|    total_timesteps | 968160   |
---------------------------------
Eval num_timesteps=980000, episode_reward=29.60 +/- 44.60
Episode length: 1913.40 +/- 522.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.91e+03      |
|    mean_reward          | 29.6          |
| time/                   |               |
|    total_timesteps      | 980000        |
| train/                  |               |
|    approx_kl            | 0.00024264588 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.07         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 0.437         |
|    n_updates            | 210           |
|    policy_gradient_loss | -1.3e-05      |
|    value_loss           | 2.37          |
-------------------------------------------
Eval num_timesteps=1000000, episode_reward=42.50 +/- 24.74
Episode length: 2012.00 +/- 363.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.01e+03 |
|    mean_reward     | 42.5     |
| time/              |          |
|    total_timesteps | 1000000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -1.02    |
| time/              |          |
|    fps             | 654      |
|    iterations      | 31       |
|    time_elapsed    | 1528     |
|    total_timesteps | 1000432  |
---------------------------------
Eval num_timesteps=1020000, episode_reward=43.80 +/- 9.50
Episode length: 1954.00 +/- 510.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.95e+03     |
|    mean_reward          | 43.8         |
| time/                   |              |
|    total_timesteps      | 1020000      |
| train/                  |              |
|    approx_kl            | 0.0027221877 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.08        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.000888     |
|    loss                 | 2.02         |
|    n_updates            | 217          |
|    policy_gradient_loss | -0.000448    |
|    value_loss           | 2.47         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -2.11    |
| time/              |          |
|    fps             | 655      |
|    iterations      | 32       |
|    time_elapsed    | 1575     |
|    total_timesteps | 1032704  |
---------------------------------
Eval num_timesteps=1040000, episode_reward=15.30 +/- 40.46
Episode length: 1862.00 +/- 279.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.86e+03     |
|    mean_reward          | 15.3         |
| time/                   |              |
|    total_timesteps      | 1040000      |
| train/                  |              |
|    approx_kl            | 0.0005549114 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 2.6          |
|    n_updates            | 224          |
|    policy_gradient_loss | -8.4e-05     |
|    value_loss           | 2.39         |
------------------------------------------
Eval num_timesteps=1060000, episode_reward=36.80 +/- 18.99
Episode length: 1743.60 +/- 242.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.74e+03 |
|    mean_reward     | 36.8     |
| time/              |          |
|    total_timesteps | 1060000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -3.17    |
| time/              |          |
|    fps             | 654      |
|    iterations      | 33       |
|    time_elapsed    | 1627     |
|    total_timesteps | 1064976  |
---------------------------------
Eval num_timesteps=1080000, episode_reward=34.40 +/- 30.16
Episode length: 1611.60 +/- 220.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.61e+03     |
|    mean_reward          | 34.4         |
| time/                   |              |
|    total_timesteps      | 1080000      |
| train/                  |              |
|    approx_kl            | 0.0005814507 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.83         |
|    n_updates            | 231          |
|    policy_gradient_loss | -9.59e-05    |
|    value_loss           | 2.35         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | 0.455    |
| time/              |          |
|    fps             | 656      |
|    iterations      | 34       |
|    time_elapsed    | 1670     |
|    total_timesteps | 1097248  |
---------------------------------
Eval num_timesteps=1100000, episode_reward=35.60 +/- 30.31
Episode length: 1972.80 +/- 517.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.97e+03     |
|    mean_reward          | 35.6         |
| time/                   |              |
|    total_timesteps      | 1100000      |
| train/                  |              |
|    approx_kl            | 6.432795e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.862        |
|    n_updates            | 238          |
|    policy_gradient_loss | 1.53e-05     |
|    value_loss           | 2.34         |
------------------------------------------
Eval num_timesteps=1120000, episode_reward=10.50 +/- 28.75
Episode length: 2204.60 +/- 433.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.2e+03  |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 1120000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -4.21    |
| time/              |          |
|    fps             | 653      |
|    iterations      | 35       |
|    time_elapsed    | 1727     |
|    total_timesteps | 1129520  |
---------------------------------
Eval num_timesteps=1140000, episode_reward=14.50 +/- 52.13
Episode length: 1561.60 +/- 387.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.56e+03     |
|    mean_reward          | 14.5         |
| time/                   |              |
|    total_timesteps      | 1140000      |
| train/                  |              |
|    approx_kl            | 0.0025714969 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 2.81         |
|    n_updates            | 245          |
|    policy_gradient_loss | -0.00037     |
|    value_loss           | 2.32         |
------------------------------------------
Eval num_timesteps=1160000, episode_reward=30.00 +/- 46.06
Episode length: 1817.00 +/- 304.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.82e+03 |
|    mean_reward     | 30       |
| time/              |          |
|    total_timesteps | 1160000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.71e+03 |
|    ep_rew_mean     | -3.15    |
| time/              |          |
|    fps             | 652      |
|    iterations      | 36       |
|    time_elapsed    | 1779     |
|    total_timesteps | 1161792  |
---------------------------------
Eval num_timesteps=1180000, episode_reward=46.90 +/- 21.87
Episode length: 1716.20 +/- 249.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.72e+03     |
|    mean_reward          | 46.9         |
| time/                   |              |
|    total_timesteps      | 1180000      |
| train/                  |              |
|    approx_kl            | 0.0014229768 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.07        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.0508       |
|    n_updates            | 252          |
|    policy_gradient_loss | -0.00018     |
|    value_loss           | 2.31         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.73e+03 |
|    ep_rew_mean     | -4.43    |
| time/              |          |
|    fps             | 654      |
|    iterations      | 37       |
|    time_elapsed    | 1823     |
|    total_timesteps | 1194064  |
---------------------------------
Eval num_timesteps=1200000, episode_reward=20.00 +/- 30.78
Episode length: 2389.00 +/- 834.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.39e+03     |
|    mean_reward          | 20           |
| time/                   |              |
|    total_timesteps      | 1200000      |
| train/                  |              |
|    approx_kl            | 0.0004845189 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.91         |
|    n_updates            | 259          |
|    policy_gradient_loss | -7.6e-05     |
|    value_loss           | 2.24         |
------------------------------------------
Eval num_timesteps=1220000, episode_reward=29.90 +/- 21.44
Episode length: 2052.20 +/- 801.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.05e+03 |
|    mean_reward     | 29.9     |
| time/              |          |
|    total_timesteps | 1220000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.73e+03 |
|    ep_rew_mean     | -3.28    |
| time/              |          |
|    fps             | 652      |
|    iterations      | 38       |
|    time_elapsed    | 1880     |
|    total_timesteps | 1226336  |
---------------------------------
Eval num_timesteps=1240000, episode_reward=42.80 +/- 29.48
Episode length: 1690.60 +/- 275.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.69e+03     |
|    mean_reward          | 42.8         |
| time/                   |              |
|    total_timesteps      | 1240000      |
| train/                  |              |
|    approx_kl            | 0.0004691031 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.343        |
|    n_updates            | 266          |
|    policy_gradient_loss | -7.49e-05    |
|    value_loss           | 2.41         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.73e+03 |
|    ep_rew_mean     | -7.25    |
| time/              |          |
|    fps             | 654      |
|    iterations      | 39       |
|    time_elapsed    | 1923     |
|    total_timesteps | 1258608  |
---------------------------------
Eval num_timesteps=1260000, episode_reward=3.20 +/- 39.46
Episode length: 2209.20 +/- 378.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.21e+03     |
|    mean_reward          | 3.2          |
| time/                   |              |
|    total_timesteps      | 1260000      |
| train/                  |              |
|    approx_kl            | 0.0039569316 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.12         |
|    n_updates            | 273          |
|    policy_gradient_loss | -0.000617    |
|    value_loss           | 2.49         |
------------------------------------------
Eval num_timesteps=1280000, episode_reward=12.50 +/- 39.95
Episode length: 2052.60 +/- 347.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.05e+03 |
|    mean_reward     | 12.5     |
| time/              |          |
|    total_timesteps | 1280000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.72e+03 |
|    ep_rew_mean     | -7.43    |
| time/              |          |
|    fps             | 652      |
|    iterations      | 40       |
|    time_elapsed    | 1979     |
|    total_timesteps | 1290880  |
---------------------------------
Eval num_timesteps=1300000, episode_reward=27.30 +/- 22.92
Episode length: 1838.80 +/- 265.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.84e+03      |
|    mean_reward          | 27.3          |
| time/                   |               |
|    total_timesteps      | 1300000       |
| train/                  |               |
|    approx_kl            | 8.6876325e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.09         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 2.5           |
|    n_updates            | 280           |
|    policy_gradient_loss | 2.02e-05      |
|    value_loss           | 2.4           |
-------------------------------------------
Eval num_timesteps=1320000, episode_reward=61.90 +/- 22.99
Episode length: 1818.80 +/- 295.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.82e+03 |
|    mean_reward     | 61.9     |
| time/              |          |
|    total_timesteps | 1320000  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -8.28    |
| time/              |          |
|    fps             | 650      |
|    iterations      | 41       |
|    time_elapsed    | 2033     |
|    total_timesteps | 1323152  |
---------------------------------
Eval num_timesteps=1340000, episode_reward=45.30 +/- 17.28
Episode length: 2222.20 +/- 696.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.22e+03     |
|    mean_reward          | 45.3         |
| time/                   |              |
|    total_timesteps      | 1340000      |
| train/                  |              |
|    approx_kl            | 0.0029849394 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.8          |
|    n_updates            | 287          |
|    policy_gradient_loss | -0.000477    |
|    value_loss           | 2.43         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -9.63    |
| time/              |          |
|    fps             | 651      |
|    iterations      | 42       |
|    time_elapsed    | 2080     |
|    total_timesteps | 1355424  |
---------------------------------
Eval num_timesteps=1360000, episode_reward=25.50 +/- 15.40
Episode length: 1948.80 +/- 288.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.95e+03     |
|    mean_reward          | 25.5         |
| time/                   |              |
|    total_timesteps      | 1360000      |
| train/                  |              |
|    approx_kl            | 0.0006275281 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.38         |
|    n_updates            | 294          |
|    policy_gradient_loss | -8.91e-05    |
|    value_loss           | 2.37         |
------------------------------------------
Eval num_timesteps=1380000, episode_reward=16.30 +/- 29.25
Episode length: 2098.40 +/- 386.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.1e+03  |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 1380000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -5.64    |
| time/              |          |
|    fps             | 650      |
|    iterations      | 43       |
|    time_elapsed    | 2134     |
|    total_timesteps | 1387696  |
---------------------------------
Eval num_timesteps=1400000, episode_reward=-4.60 +/- 23.41
Episode length: 1935.80 +/- 312.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.94e+03      |
|    mean_reward          | -4.6          |
| time/                   |               |
|    total_timesteps      | 1400000       |
| train/                  |               |
|    approx_kl            | 0.00015548097 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.09         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 1.88          |
|    n_updates            | 301           |
|    policy_gradient_loss | 4.34e-06      |
|    value_loss           | 2.45          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -4.28    |
| time/              |          |
|    fps             | 651      |
|    iterations      | 44       |
|    time_elapsed    | 2179     |
|    total_timesteps | 1419968  |
---------------------------------
Eval num_timesteps=1420000, episode_reward=0.10 +/- 26.69
Episode length: 1843.40 +/- 220.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.84e+03    |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 1420000     |
| train/                  |             |
|    approx_kl            | 0.003473172 |
|    clip_fraction        | 0           |
|    clip_range           | 0.305       |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0           |
|    learning_rate        | 0.000888    |
|    loss                 | 1.05        |
|    n_updates            | 308         |
|    policy_gradient_loss | -0.000531   |
|    value_loss           | 2.32        |
-----------------------------------------
Eval num_timesteps=1440000, episode_reward=-45.20 +/- 35.79
Episode length: 1779.20 +/- 417.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.78e+03 |
|    mean_reward     | -45.2    |
| time/              |          |
|    total_timesteps | 1440000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -3.92    |
| time/              |          |
|    fps             | 650      |
|    iterations      | 45       |
|    time_elapsed    | 2232     |
|    total_timesteps | 1452240  |
---------------------------------
Eval num_timesteps=1460000, episode_reward=2.10 +/- 34.48
Episode length: 1643.60 +/- 297.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.64e+03     |
|    mean_reward          | 2.1          |
| time/                   |              |
|    total_timesteps      | 1460000      |
| train/                  |              |
|    approx_kl            | 0.0023390334 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.493        |
|    n_updates            | 315          |
|    policy_gradient_loss | -0.000377    |
|    value_loss           | 2.37         |
------------------------------------------
Eval num_timesteps=1480000, episode_reward=-24.20 +/- 28.32
Episode length: 1669.80 +/- 63.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.67e+03 |
|    mean_reward     | -24.2    |
| time/              |          |
|    total_timesteps | 1480000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -3.32    |
| time/              |          |
|    fps             | 650      |
|    iterations      | 46       |
|    time_elapsed    | 2282     |
|    total_timesteps | 1484512  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=3.70 +/- 42.35
Episode length: 1490.80 +/- 332.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.49e+03     |
|    mean_reward          | 3.7          |
| time/                   |              |
|    total_timesteps      | 1500000      |
| train/                  |              |
|    approx_kl            | 0.0009887876 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.612        |
|    n_updates            | 322          |
|    policy_gradient_loss | -0.000116    |
|    value_loss           | 2.41         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -2.54    |
| time/              |          |
|    fps             | 652      |
|    iterations      | 47       |
|    time_elapsed    | 2324     |
|    total_timesteps | 1516784  |
---------------------------------
Eval num_timesteps=1520000, episode_reward=-21.80 +/- 40.15
Episode length: 1647.00 +/- 444.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.65e+03      |
|    mean_reward          | -21.8         |
| time/                   |               |
|    total_timesteps      | 1520000       |
| train/                  |               |
|    approx_kl            | 0.00066500716 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 0.149         |
|    n_updates            | 329           |
|    policy_gradient_loss | -9.09e-05     |
|    value_loss           | 2.4           |
-------------------------------------------
Eval num_timesteps=1540000, episode_reward=-29.70 +/- 32.25
Episode length: 1681.80 +/- 249.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.68e+03 |
|    mean_reward     | -29.7    |
| time/              |          |
|    total_timesteps | 1540000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -6.82    |
| time/              |          |
|    fps             | 652      |
|    iterations      | 48       |
|    time_elapsed    | 2373     |
|    total_timesteps | 1549056  |
---------------------------------
Eval num_timesteps=1560000, episode_reward=-7.90 +/- 28.65
Episode length: 1768.20 +/- 337.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.77e+03     |
|    mean_reward          | -7.9         |
| time/                   |              |
|    total_timesteps      | 1560000      |
| train/                  |              |
|    approx_kl            | 0.0007493662 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 2.14         |
|    n_updates            | 336          |
|    policy_gradient_loss | -9.8e-05     |
|    value_loss           | 2.3          |
------------------------------------------
Eval num_timesteps=1580000, episode_reward=-33.50 +/- 20.73
Episode length: 1768.60 +/- 219.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.77e+03 |
|    mean_reward     | -33.5    |
| time/              |          |
|    total_timesteps | 1580000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -6.25    |
| time/              |          |
|    fps             | 651      |
|    iterations      | 49       |
|    time_elapsed    | 2425     |
|    total_timesteps | 1581328  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=-30.10 +/- 21.42
Episode length: 1670.40 +/- 130.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.67e+03    |
|    mean_reward          | -30.1       |
| time/                   |             |
|    total_timesteps      | 1600000     |
| train/                  |             |
|    approx_kl            | 0.000919928 |
|    clip_fraction        | 0           |
|    clip_range           | 0.305       |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0           |
|    learning_rate        | 0.000888    |
|    loss                 | 0.17        |
|    n_updates            | 343         |
|    policy_gradient_loss | -0.00014    |
|    value_loss           | 2.47        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -5.08    |
| time/              |          |
|    fps             | 653      |
|    iterations      | 50       |
|    time_elapsed    | 2469     |
|    total_timesteps | 1613600  |
---------------------------------
Eval num_timesteps=1620000, episode_reward=-0.10 +/- 35.03
Episode length: 1774.00 +/- 263.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.77e+03     |
|    mean_reward          | -0.1         |
| time/                   |              |
|    total_timesteps      | 1620000      |
| train/                  |              |
|    approx_kl            | 0.0031150114 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 2.58         |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.000489    |
|    value_loss           | 2.41         |
------------------------------------------
Eval num_timesteps=1640000, episode_reward=-53.90 +/- 17.84
Episode length: 1507.40 +/- 334.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.51e+03 |
|    mean_reward     | -53.9    |
| time/              |          |
|    total_timesteps | 1640000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.6e+03  |
|    ep_rew_mean     | -5.62    |
| time/              |          |
|    fps             | 652      |
|    iterations      | 51       |
|    time_elapsed    | 2520     |
|    total_timesteps | 1645872  |
---------------------------------
Eval num_timesteps=1660000, episode_reward=-22.60 +/- 37.55
Episode length: 1585.80 +/- 343.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.59e+03      |
|    mean_reward          | -22.6         |
| time/                   |               |
|    total_timesteps      | 1660000       |
| train/                  |               |
|    approx_kl            | 0.00071709673 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.08         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 1.68          |
|    n_updates            | 357           |
|    policy_gradient_loss | -0.000115     |
|    value_loss           | 2.51          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.57e+03 |
|    ep_rew_mean     | -6.75    |
| time/              |          |
|    fps             | 654      |
|    iterations      | 52       |
|    time_elapsed    | 2564     |
|    total_timesteps | 1678144  |
---------------------------------
Eval num_timesteps=1680000, episode_reward=-8.50 +/- 34.27
Episode length: 1569.00 +/- 242.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.57e+03     |
|    mean_reward          | -8.5         |
| time/                   |              |
|    total_timesteps      | 1680000      |
| train/                  |              |
|    approx_kl            | 0.0006805043 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.211        |
|    n_updates            | 364          |
|    policy_gradient_loss | -0.000122    |
|    value_loss           | 2.53         |
------------------------------------------
Eval num_timesteps=1700000, episode_reward=-9.10 +/- 32.01
Episode length: 1884.40 +/- 375.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.88e+03 |
|    mean_reward     | -9.1     |
| time/              |          |
|    total_timesteps | 1700000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.59e+03 |
|    ep_rew_mean     | -5.93    |
| time/              |          |
|    fps             | 653      |
|    iterations      | 53       |
|    time_elapsed    | 2616     |
|    total_timesteps | 1710416  |
---------------------------------
Eval num_timesteps=1720000, episode_reward=-3.90 +/- 37.33
Episode length: 1660.20 +/- 275.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.66e+03      |
|    mean_reward          | -3.9          |
| time/                   |               |
|    total_timesteps      | 1720000       |
| train/                  |               |
|    approx_kl            | 9.0963425e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.09         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 4.17          |
|    n_updates            | 371           |
|    policy_gradient_loss | 1.4e-05       |
|    value_loss           | 2.43          |
-------------------------------------------
Eval num_timesteps=1740000, episode_reward=-22.20 +/- 39.25
Episode length: 1652.20 +/- 465.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.65e+03 |
|    mean_reward     | -22.2    |
| time/              |          |
|    total_timesteps | 1740000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.58e+03 |
|    ep_rew_mean     | -8.43    |
| time/              |          |
|    fps             | 652      |
|    iterations      | 54       |
|    time_elapsed    | 2669     |
|    total_timesteps | 1742688  |
---------------------------------
Eval num_timesteps=1760000, episode_reward=9.30 +/- 53.22
Episode length: 1425.20 +/- 189.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.43e+03      |
|    mean_reward          | 9.3           |
| time/                   |               |
|    total_timesteps      | 1760000       |
| train/                  |               |
|    approx_kl            | 0.00064245914 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.09         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 1.53          |
|    n_updates            | 378           |
|    policy_gradient_loss | -0.000119     |
|    value_loss           | 2.5           |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.59e+03 |
|    ep_rew_mean     | -10.2    |
| time/              |          |
|    fps             | 654      |
|    iterations      | 55       |
|    time_elapsed    | 2712     |
|    total_timesteps | 1774960  |
---------------------------------
Eval num_timesteps=1780000, episode_reward=-37.80 +/- 23.59
Episode length: 1707.20 +/- 255.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.71e+03    |
|    mean_reward          | -37.8       |
| time/                   |             |
|    total_timesteps      | 1780000     |
| train/                  |             |
|    approx_kl            | 0.001252698 |
|    clip_fraction        | 0           |
|    clip_range           | 0.305       |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0           |
|    learning_rate        | 0.000888    |
|    loss                 | 1.14        |
|    n_updates            | 385         |
|    policy_gradient_loss | -0.000239   |
|    value_loss           | 2.36        |
-----------------------------------------
Eval num_timesteps=1800000, episode_reward=-31.50 +/- 29.57
Episode length: 1718.00 +/- 392.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.72e+03 |
|    mean_reward     | -31.5    |
| time/              |          |
|    total_timesteps | 1800000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | -8.46    |
| time/              |          |
|    fps             | 653      |
|    iterations      | 56       |
|    time_elapsed    | 2765     |
|    total_timesteps | 1807232  |
---------------------------------
Eval num_timesteps=1820000, episode_reward=-34.30 +/- 19.07
Episode length: 1633.60 +/- 514.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.63e+03     |
|    mean_reward          | -34.3        |
| time/                   |              |
|    total_timesteps      | 1820000      |
| train/                  |              |
|    approx_kl            | 0.0020520957 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.28         |
|    n_updates            | 392          |
|    policy_gradient_loss | -0.00035     |
|    value_loss           | 2.42         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -9.47    |
| time/              |          |
|    fps             | 655      |
|    iterations      | 57       |
|    time_elapsed    | 2807     |
|    total_timesteps | 1839504  |
---------------------------------
Eval num_timesteps=1840000, episode_reward=-29.90 +/- 40.65
Episode length: 1564.80 +/- 537.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.56e+03      |
|    mean_reward          | -29.9         |
| time/                   |               |
|    total_timesteps      | 1840000       |
| train/                  |               |
|    approx_kl            | 0.00084849604 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.08         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.000888      |
|    loss                 | 0.0522        |
|    n_updates            | 399           |
|    policy_gradient_loss | -0.000146     |
|    value_loss           | 2.31          |
-------------------------------------------
Eval num_timesteps=1860000, episode_reward=26.00 +/- 26.16
Episode length: 1636.80 +/- 110.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.64e+03 |
|    mean_reward     | 26       |
| time/              |          |
|    total_timesteps | 1860000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -8.79    |
| time/              |          |
|    fps             | 654      |
|    iterations      | 58       |
|    time_elapsed    | 2859     |
|    total_timesteps | 1871776  |
---------------------------------
Eval num_timesteps=1880000, episode_reward=-15.20 +/- 28.68
Episode length: 1815.40 +/- 378.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.82e+03      |
|    mean_reward          | -15.2         |
| time/                   |               |
|    total_timesteps      | 1880000       |
| train/                  |               |
|    approx_kl            | 0.00059727154 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.08         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 0.968         |
|    n_updates            | 406           |
|    policy_gradient_loss | -4.46e-05     |
|    value_loss           | 2.33          |
-------------------------------------------
Eval num_timesteps=1900000, episode_reward=10.50 +/- 43.92
Episode length: 1673.60 +/- 350.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.67e+03 |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 1900000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -10.2    |
| time/              |          |
|    fps             | 653      |
|    iterations      | 59       |
|    time_elapsed    | 2912     |
|    total_timesteps | 1904048  |
---------------------------------
Eval num_timesteps=1920000, episode_reward=-20.80 +/- 32.83
Episode length: 1890.40 +/- 355.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.89e+03     |
|    mean_reward          | -20.8        |
| time/                   |              |
|    total_timesteps      | 1920000      |
| train/                  |              |
|    approx_kl            | 0.0024695424 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 2.68         |
|    n_updates            | 413          |
|    policy_gradient_loss | -0.000418    |
|    value_loss           | 2.36         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -11.3    |
| time/              |          |
|    fps             | 654      |
|    iterations      | 60       |
|    time_elapsed    | 2958     |
|    total_timesteps | 1936320  |
---------------------------------
Eval num_timesteps=1940000, episode_reward=-28.40 +/- 45.06
Episode length: 1493.40 +/- 234.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.49e+03     |
|    mean_reward          | -28.4        |
| time/                   |              |
|    total_timesteps      | 1940000      |
| train/                  |              |
|    approx_kl            | 0.0011208798 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.0338       |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.000186    |
|    value_loss           | 2.32         |
------------------------------------------
Eval num_timesteps=1960000, episode_reward=-25.80 +/- 43.31
Episode length: 1714.20 +/- 299.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.71e+03 |
|    mean_reward     | -25.8    |
| time/              |          |
|    total_timesteps | 1960000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -11.9    |
| time/              |          |
|    fps             | 654      |
|    iterations      | 61       |
|    time_elapsed    | 3009     |
|    total_timesteps | 1968592  |
---------------------------------
Eval num_timesteps=1980000, episode_reward=46.30 +/- 34.87
Episode length: 2011.60 +/- 656.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 2.01e+03      |
|    mean_reward          | 46.3          |
| time/                   |               |
|    total_timesteps      | 1980000       |
| train/                  |               |
|    approx_kl            | 1.0097118e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.09         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 0.584         |
|    n_updates            | 427           |
|    policy_gradient_loss | 2.37e-05      |
|    value_loss           | 2.57          |
-------------------------------------------
Eval num_timesteps=2000000, episode_reward=53.60 +/- 32.09
Episode length: 1865.40 +/- 622.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.87e+03 |
|    mean_reward     | 53.6     |
| time/              |          |
|    total_timesteps | 2000000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -11.6    |
| time/              |          |
|    fps             | 653      |
|    iterations      | 62       |
|    time_elapsed    | 3061     |
|    total_timesteps | 2000864  |
---------------------------------
Eval num_timesteps=2020000, episode_reward=19.30 +/- 36.48
Episode length: 1913.60 +/- 235.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.91e+03     |
|    mean_reward          | 19.3         |
| time/                   |              |
|    total_timesteps      | 2020000      |
| train/                  |              |
|    approx_kl            | 0.0016756189 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.378        |
|    n_updates            | 434          |
|    policy_gradient_loss | -0.000296    |
|    value_loss           | 2.26         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -11.3    |
| time/              |          |
|    fps             | 654      |
|    iterations      | 63       |
|    time_elapsed    | 3106     |
|    total_timesteps | 2033136  |
---------------------------------
Eval num_timesteps=2040000, episode_reward=25.80 +/- 28.77
Episode length: 1977.40 +/- 280.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.98e+03     |
|    mean_reward          | 25.8         |
| time/                   |              |
|    total_timesteps      | 2040000      |
| train/                  |              |
|    approx_kl            | 7.253471e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.23         |
|    n_updates            | 441          |
|    policy_gradient_loss | 3.26e-06     |
|    value_loss           | 2.18         |
------------------------------------------
Eval num_timesteps=2060000, episode_reward=31.40 +/- 37.91
Episode length: 1852.20 +/- 319.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.85e+03 |
|    mean_reward     | 31.4     |
| time/              |          |
|    total_timesteps | 2060000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.71e+03 |
|    ep_rew_mean     | -7.03    |
| time/              |          |
|    fps             | 653      |
|    iterations      | 64       |
|    time_elapsed    | 3159     |
|    total_timesteps | 2065408  |
---------------------------------
Eval num_timesteps=2080000, episode_reward=-3.90 +/- 49.53
Episode length: 1581.00 +/- 279.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.58e+03     |
|    mean_reward          | -3.9         |
| time/                   |              |
|    total_timesteps      | 2080000      |
| train/                  |              |
|    approx_kl            | 0.0019703228 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.09        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.000888     |
|    loss                 | 1.12         |
|    n_updates            | 448          |
|    policy_gradient_loss | -0.000338    |
|    value_loss           | 2.31         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.72e+03 |
|    ep_rew_mean     | -7.61    |
| time/              |          |
|    fps             | 654      |
|    iterations      | 65       |
|    time_elapsed    | 3203     |
|    total_timesteps | 2097680  |
---------------------------------
Eval num_timesteps=2100000, episode_reward=-7.90 +/- 24.83
Episode length: 1688.60 +/- 193.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.69e+03      |
|    mean_reward          | -7.9          |
| time/                   |               |
|    total_timesteps      | 2100000       |
| train/                  |               |
|    approx_kl            | 6.0537546e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.09         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 1.06          |
|    n_updates            | 455           |
|    policy_gradient_loss | 1.19e-05      |
|    value_loss           | 2.26          |
-------------------------------------------
Eval num_timesteps=2120000, episode_reward=-21.50 +/- 38.95
Episode length: 1771.80 +/- 285.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.77e+03 |
|    mean_reward     | -21.5    |
| time/              |          |
|    total_timesteps | 2120000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -8.13    |
| time/              |          |
|    fps             | 654      |
|    iterations      | 66       |
|    time_elapsed    | 3254     |
|    total_timesteps | 2129952  |
---------------------------------
Eval num_timesteps=2140000, episode_reward=-32.30 +/- 17.96
Episode length: 1632.40 +/- 160.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.63e+03     |
|    mean_reward          | -32.3        |
| time/                   |              |
|    total_timesteps      | 2140000      |
| train/                  |              |
|    approx_kl            | 0.0008639701 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.09        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.000888     |
|    loss                 | 0.711        |
|    n_updates            | 462          |
|    policy_gradient_loss | -0.00012     |
|    value_loss           | 2.37         |
------------------------------------------
Eval num_timesteps=2160000, episode_reward=-2.30 +/- 34.46
Episode length: 1940.80 +/- 263.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.94e+03 |
|    mean_reward     | -2.3     |
| time/              |          |
|    total_timesteps | 2160000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.71e+03 |
|    ep_rew_mean     | -5.99    |
| time/              |          |
|    fps             | 653      |
|    iterations      | 67       |
|    time_elapsed    | 3306     |
|    total_timesteps | 2162224  |
---------------------------------
Eval num_timesteps=2180000, episode_reward=10.10 +/- 44.09
Episode length: 1632.80 +/- 478.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.63e+03     |
|    mean_reward          | 10.1         |
| time/                   |              |
|    total_timesteps      | 2180000      |
| train/                  |              |
|    approx_kl            | 0.0008343338 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.229        |
|    n_updates            | 469          |
|    policy_gradient_loss | -0.00011     |
|    value_loss           | 2.34         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -8.83    |
| time/              |          |
|    fps             | 655      |
|    iterations      | 68       |
|    time_elapsed    | 3348     |
|    total_timesteps | 2194496  |
---------------------------------
Eval num_timesteps=2200000, episode_reward=-36.10 +/- 43.47
Episode length: 1426.80 +/- 333.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.43e+03      |
|    mean_reward          | -36.1         |
| time/                   |               |
|    total_timesteps      | 2200000       |
| train/                  |               |
|    approx_kl            | 0.00081462134 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.08         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 2.25          |
|    n_updates            | 476           |
|    policy_gradient_loss | -0.000142     |
|    value_loss           | 2.44          |
-------------------------------------------
Eval num_timesteps=2220000, episode_reward=-32.10 +/- 13.50
Episode length: 1588.40 +/- 288.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.59e+03 |
|    mean_reward     | -32.1    |
| time/              |          |
|    total_timesteps | 2220000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -13.7    |
| time/              |          |
|    fps             | 655      |
|    iterations      | 69       |
|    time_elapsed    | 3397     |
|    total_timesteps | 2226768  |
---------------------------------
Eval num_timesteps=2240000, episode_reward=-10.30 +/- 45.37
Episode length: 1538.00 +/- 370.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.54e+03     |
|    mean_reward          | -10.3        |
| time/                   |              |
|    total_timesteps      | 2240000      |
| train/                  |              |
|    approx_kl            | 0.0004357028 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.08        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.000888     |
|    loss                 | 0.944        |
|    n_updates            | 483          |
|    policy_gradient_loss | -5.93e-05    |
|    value_loss           | 2.43         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -8.97    |
| time/              |          |
|    fps             | 656      |
|    iterations      | 70       |
|    time_elapsed    | 3441     |
|    total_timesteps | 2259040  |
---------------------------------
Eval num_timesteps=2260000, episode_reward=-11.40 +/- 31.36
Episode length: 1785.40 +/- 348.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.79e+03    |
|    mean_reward          | -11.4       |
| time/                   |             |
|    total_timesteps      | 2260000     |
| train/                  |             |
|    approx_kl            | 0.001951973 |
|    clip_fraction        | 0           |
|    clip_range           | 0.305       |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0           |
|    learning_rate        | 0.000888    |
|    loss                 | 0.579       |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00039    |
|    value_loss           | 2.51        |
-----------------------------------------
Eval num_timesteps=2280000, episode_reward=9.00 +/- 31.53
Episode length: 1909.40 +/- 481.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.91e+03 |
|    mean_reward     | 9        |
| time/              |          |
|    total_timesteps | 2280000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -8.04    |
| time/              |          |
|    fps             | 656      |
|    iterations      | 71       |
|    time_elapsed    | 3491     |
|    total_timesteps | 2291312  |
---------------------------------
Eval num_timesteps=2300000, episode_reward=-30.10 +/- 50.20
Episode length: 1516.00 +/- 326.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.52e+03    |
|    mean_reward          | -30.1       |
| time/                   |             |
|    total_timesteps      | 2300000     |
| train/                  |             |
|    approx_kl            | 0.001000615 |
|    clip_fraction        | 0           |
|    clip_range           | 0.305       |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0           |
|    learning_rate        | 0.000888    |
|    loss                 | 1.13        |
|    n_updates            | 497         |
|    policy_gradient_loss | -0.000175   |
|    value_loss           | 2.4         |
-----------------------------------------
Eval num_timesteps=2320000, episode_reward=-18.50 +/- 52.76
Episode length: 1447.60 +/- 425.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.45e+03 |
|    mean_reward     | -18.5    |
| time/              |          |
|    total_timesteps | 2320000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | -9.83    |
| time/              |          |
|    fps             | 656      |
|    iterations      | 72       |
|    time_elapsed    | 3540     |
|    total_timesteps | 2323584  |
---------------------------------
Eval num_timesteps=2340000, episode_reward=-6.20 +/- 40.77
Episode length: 1638.20 +/- 223.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.64e+03      |
|    mean_reward          | -6.2          |
| time/                   |               |
|    total_timesteps      | 2340000       |
| train/                  |               |
|    approx_kl            | 0.00085057656 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.05         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 0.498         |
|    n_updates            | 504           |
|    policy_gradient_loss | -0.000158     |
|    value_loss           | 2.3           |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -11.9    |
| time/              |          |
|    fps             | 657      |
|    iterations      | 73       |
|    time_elapsed    | 3582     |
|    total_timesteps | 2355856  |
---------------------------------
Eval num_timesteps=2360000, episode_reward=-17.30 +/- 24.61
Episode length: 1770.80 +/- 246.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.77e+03     |
|    mean_reward          | -17.3        |
| time/                   |              |
|    total_timesteps      | 2360000      |
| train/                  |              |
|    approx_kl            | 0.0036874549 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.04        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.000888     |
|    loss                 | 0.717        |
|    n_updates            | 511          |
|    policy_gradient_loss | -0.000648    |
|    value_loss           | 2.32         |
------------------------------------------
Eval num_timesteps=2380000, episode_reward=-14.30 +/- 29.85
Episode length: 1679.60 +/- 273.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.68e+03 |
|    mean_reward     | -14.3    |
| time/              |          |
|    total_timesteps | 2380000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -11.5    |
| time/              |          |
|    fps             | 657      |
|    iterations      | 74       |
|    time_elapsed    | 3633     |
|    total_timesteps | 2388128  |
---------------------------------
Eval num_timesteps=2400000, episode_reward=-6.90 +/- 40.65
Episode length: 1387.60 +/- 300.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.39e+03      |
|    mean_reward          | -6.9          |
| time/                   |               |
|    total_timesteps      | 2400000       |
| train/                  |               |
|    approx_kl            | 0.00084971957 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.04         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 0.597         |
|    n_updates            | 518           |
|    policy_gradient_loss | -0.00012      |
|    value_loss           | 2.29          |
-------------------------------------------
Eval num_timesteps=2420000, episode_reward=-21.60 +/- 28.42
Episode length: 1769.40 +/- 174.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.77e+03 |
|    mean_reward     | -21.6    |
| time/              |          |
|    total_timesteps | 2420000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -9.95    |
| time/              |          |
|    fps             | 657      |
|    iterations      | 75       |
|    time_elapsed    | 3683     |
|    total_timesteps | 2420400  |
---------------------------------
Eval num_timesteps=2440000, episode_reward=-44.70 +/- 40.50
Episode length: 1334.20 +/- 188.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.33e+03     |
|    mean_reward          | -44.7        |
| time/                   |              |
|    total_timesteps      | 2440000      |
| train/                  |              |
|    approx_kl            | 0.0001531395 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.501        |
|    n_updates            | 525          |
|    policy_gradient_loss | 9.83e-06     |
|    value_loss           | 2.45         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -13.4    |
| time/              |          |
|    fps             | 658      |
|    iterations      | 76       |
|    time_elapsed    | 3725     |
|    total_timesteps | 2452672  |
---------------------------------
Eval num_timesteps=2460000, episode_reward=10.10 +/- 26.63
Episode length: 1791.20 +/- 396.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.79e+03    |
|    mean_reward          | 10.1        |
| time/                   |             |
|    total_timesteps      | 2460000     |
| train/                  |             |
|    approx_kl            | 0.000922884 |
|    clip_fraction        | 0           |
|    clip_range           | 0.305       |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0           |
|    learning_rate        | 0.000888    |
|    loss                 | 1.04        |
|    n_updates            | 532         |
|    policy_gradient_loss | -0.000148   |
|    value_loss           | 2.52        |
-----------------------------------------
Eval num_timesteps=2480000, episode_reward=-32.30 +/- 19.05
Episode length: 1589.20 +/- 332.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.59e+03 |
|    mean_reward     | -32.3    |
| time/              |          |
|    total_timesteps | 2480000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -15.5    |
| time/              |          |
|    fps             | 658      |
|    iterations      | 77       |
|    time_elapsed    | 3775     |
|    total_timesteps | 2484944  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=-5.60 +/- 34.35
Episode length: 1648.20 +/- 144.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.65e+03     |
|    mean_reward          | -5.6         |
| time/                   |              |
|    total_timesteps      | 2500000      |
| train/                  |              |
|    approx_kl            | 0.0003203397 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.02        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.148        |
|    n_updates            | 539          |
|    policy_gradient_loss | -2.65e-05    |
|    value_loss           | 2.49         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.6e+03  |
|    ep_rew_mean     | -14.5    |
| time/              |          |
|    fps             | 659      |
|    iterations      | 78       |
|    time_elapsed    | 3818     |
|    total_timesteps | 2517216  |
---------------------------------
Eval num_timesteps=2520000, episode_reward=12.50 +/- 33.16
Episode length: 1699.00 +/- 384.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.7e+03       |
|    mean_reward          | 12.5          |
| time/                   |               |
|    total_timesteps      | 2520000       |
| train/                  |               |
|    approx_kl            | 5.7705987e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.02         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 0.973         |
|    n_updates            | 546           |
|    policy_gradient_loss | 1.04e-05      |
|    value_loss           | 2.55          |
-------------------------------------------
Eval num_timesteps=2540000, episode_reward=-26.60 +/- 29.39
Episode length: 1522.00 +/- 142.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.52e+03 |
|    mean_reward     | -26.6    |
| time/              |          |
|    total_timesteps | 2540000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.57e+03 |
|    ep_rew_mean     | -12      |
| time/              |          |
|    fps             | 659      |
|    iterations      | 79       |
|    time_elapsed    | 3867     |
|    total_timesteps | 2549488  |
---------------------------------
Eval num_timesteps=2560000, episode_reward=-37.30 +/- 20.55
Episode length: 1755.60 +/- 142.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.76e+03     |
|    mean_reward          | -37.3        |
| time/                   |              |
|    total_timesteps      | 2560000      |
| train/                  |              |
|    approx_kl            | 0.0001268628 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.02        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.758        |
|    n_updates            | 553          |
|    policy_gradient_loss | 2.57e-06     |
|    value_loss           | 2.5          |
------------------------------------------
Eval num_timesteps=2580000, episode_reward=-18.10 +/- 28.27
Episode length: 1673.20 +/- 354.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.67e+03 |
|    mean_reward     | -18.1    |
| time/              |          |
|    total_timesteps | 2580000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.59e+03 |
|    ep_rew_mean     | -12.2    |
| time/              |          |
|    fps             | 659      |
|    iterations      | 80       |
|    time_elapsed    | 3916     |
|    total_timesteps | 2581760  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=-25.10 +/- 49.49
Episode length: 1316.20 +/- 198.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.32e+03      |
|    mean_reward          | -25.1         |
| time/                   |               |
|    total_timesteps      | 2600000       |
| train/                  |               |
|    approx_kl            | 0.00023725895 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.01         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 0.628         |
|    n_updates            | 560           |
|    policy_gradient_loss | -9.26e-06     |
|    value_loss           | 2.39          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | -12.1    |
| time/              |          |
|    fps             | 660      |
|    iterations      | 81       |
|    time_elapsed    | 3957     |
|    total_timesteps | 2614032  |
---------------------------------
Eval num_timesteps=2620000, episode_reward=-16.50 +/- 21.60
Episode length: 1699.20 +/- 375.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.7e+03       |
|    mean_reward          | -16.5         |
| time/                   |               |
|    total_timesteps      | 2620000       |
| train/                  |               |
|    approx_kl            | 3.4309203e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.01         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 0.0786        |
|    n_updates            | 567           |
|    policy_gradient_loss | 1.98e-05      |
|    value_loss           | 2.28          |
-------------------------------------------
Eval num_timesteps=2640000, episode_reward=-18.40 +/- 21.04
Episode length: 1506.60 +/- 120.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.51e+03 |
|    mean_reward     | -18.4    |
| time/              |          |
|    total_timesteps | 2640000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -11.3    |
| time/              |          |
|    fps             | 660      |
|    iterations      | 82       |
|    time_elapsed    | 4008     |
|    total_timesteps | 2646304  |
---------------------------------
Eval num_timesteps=2660000, episode_reward=-31.40 +/- 25.00
Episode length: 1767.80 +/- 259.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.77e+03     |
|    mean_reward          | -31.4        |
| time/                   |              |
|    total_timesteps      | 2660000      |
| train/                  |              |
|    approx_kl            | 0.0018984564 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1           |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.579        |
|    n_updates            | 574          |
|    policy_gradient_loss | -0.000362    |
|    value_loss           | 2.23         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -8.97    |
| time/              |          |
|    fps             | 660      |
|    iterations      | 83       |
|    time_elapsed    | 4052     |
|    total_timesteps | 2678576  |
---------------------------------
Eval num_timesteps=2680000, episode_reward=-13.70 +/- 38.50
Episode length: 1801.40 +/- 256.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.8e+03      |
|    mean_reward          | -13.7        |
| time/                   |              |
|    total_timesteps      | 2680000      |
| train/                  |              |
|    approx_kl            | 0.0023235579 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -0.995       |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.55         |
|    n_updates            | 581          |
|    policy_gradient_loss | -0.00031     |
|    value_loss           | 2.34         |
------------------------------------------
Eval num_timesteps=2700000, episode_reward=9.70 +/- 22.05
Episode length: 1627.60 +/- 256.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.63e+03 |
|    mean_reward     | 9.7      |
| time/              |          |
|    total_timesteps | 2700000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -12.8    |
| time/              |          |
|    fps             | 660      |
|    iterations      | 84       |
|    time_elapsed    | 4105     |
|    total_timesteps | 2710848  |
---------------------------------
Eval num_timesteps=2720000, episode_reward=8.30 +/- 36.19
Episode length: 1760.00 +/- 339.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.76e+03      |
|    mean_reward          | 8.3           |
| time/                   |               |
|    total_timesteps      | 2720000       |
| train/                  |               |
|    approx_kl            | 0.00085804565 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.01         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 2.31          |
|    n_updates            | 588           |
|    policy_gradient_loss | -0.000157     |
|    value_loss           | 2.5           |
-------------------------------------------
Eval num_timesteps=2740000, episode_reward=-4.50 +/- 62.02
Episode length: 1402.20 +/- 326.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.4e+03  |
|    mean_reward     | -4.5     |
| time/              |          |
|    total_timesteps | 2740000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -17.3    |
| time/              |          |
|    fps             | 659      |
|    iterations      | 85       |
|    time_elapsed    | 4157     |
|    total_timesteps | 2743120  |
---------------------------------
Eval num_timesteps=2760000, episode_reward=-1.80 +/- 38.89
Episode length: 1697.20 +/- 499.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.7e+03      |
|    mean_reward          | -1.8         |
| time/                   |              |
|    total_timesteps      | 2760000      |
| train/                  |              |
|    approx_kl            | 0.0009956962 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.02        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.862        |
|    n_updates            | 595          |
|    policy_gradient_loss | -0.000208    |
|    value_loss           | 2.4          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -16.1    |
| time/              |          |
|    fps             | 660      |
|    iterations      | 86       |
|    time_elapsed    | 4199     |
|    total_timesteps | 2775392  |
---------------------------------
Eval num_timesteps=2780000, episode_reward=-44.20 +/- 18.82
Episode length: 1552.00 +/- 291.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.55e+03      |
|    mean_reward          | -44.2         |
| time/                   |               |
|    total_timesteps      | 2780000       |
| train/                  |               |
|    approx_kl            | 0.00017581004 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.03         |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.000888      |
|    loss                 | 2.8           |
|    n_updates            | 602           |
|    policy_gradient_loss | -7.89e-06     |
|    value_loss           | 2.46          |
-------------------------------------------
Eval num_timesteps=2800000, episode_reward=-32.90 +/- 37.91
Episode length: 1478.60 +/- 364.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.48e+03 |
|    mean_reward     | -32.9    |
| time/              |          |
|    total_timesteps | 2800000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -16.1    |
| time/              |          |
|    fps             | 660      |
|    iterations      | 87       |
|    time_elapsed    | 4249     |
|    total_timesteps | 2807664  |
---------------------------------
Eval num_timesteps=2820000, episode_reward=0.20 +/- 39.50
Episode length: 1722.20 +/- 529.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.72e+03     |
|    mean_reward          | 0.2          |
| time/                   |              |
|    total_timesteps      | 2820000      |
| train/                  |              |
|    approx_kl            | 6.119795e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.02        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.923        |
|    n_updates            | 609          |
|    policy_gradient_loss | 1.29e-05     |
|    value_loss           | 2.42         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -19.9    |
| time/              |          |
|    fps             | 661      |
|    iterations      | 88       |
|    time_elapsed    | 4291     |
|    total_timesteps | 2839936  |
---------------------------------
Eval num_timesteps=2840000, episode_reward=-5.80 +/- 42.11
Episode length: 1639.40 +/- 162.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.64e+03      |
|    mean_reward          | -5.8          |
| time/                   |               |
|    total_timesteps      | 2840000       |
| train/                  |               |
|    approx_kl            | 0.00037215007 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.03         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 2.23          |
|    n_updates            | 616           |
|    policy_gradient_loss | -2.02e-05     |
|    value_loss           | 2.36          |
-------------------------------------------
Eval num_timesteps=2860000, episode_reward=-13.50 +/- 24.78
Episode length: 1912.00 +/- 170.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.91e+03 |
|    mean_reward     | -13.5    |
| time/              |          |
|    total_timesteps | 2860000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -17.9    |
| time/              |          |
|    fps             | 661      |
|    iterations      | 89       |
|    time_elapsed    | 4342     |
|    total_timesteps | 2872208  |
---------------------------------
Eval num_timesteps=2880000, episode_reward=-24.40 +/- 18.24
Episode length: 1747.40 +/- 229.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.75e+03     |
|    mean_reward          | -24.4        |
| time/                   |              |
|    total_timesteps      | 2880000      |
| train/                  |              |
|    approx_kl            | 0.0006310386 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.35         |
|    n_updates            | 623          |
|    policy_gradient_loss | -6.52e-05    |
|    value_loss           | 2.43         |
------------------------------------------
Eval num_timesteps=2900000, episode_reward=-28.10 +/- 40.34
Episode length: 1456.60 +/- 228.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.46e+03 |
|    mean_reward     | -28.1    |
| time/              |          |
|    total_timesteps | 2900000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -16.5    |
| time/              |          |
|    fps             | 661      |
|    iterations      | 90       |
|    time_elapsed    | 4392     |
|    total_timesteps | 2904480  |
---------------------------------
Eval num_timesteps=2920000, episode_reward=15.80 +/- 42.62
Episode length: 1555.00 +/- 231.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.56e+03     |
|    mean_reward          | 15.8         |
| time/                   |              |
|    total_timesteps      | 2920000      |
| train/                  |              |
|    approx_kl            | 0.0015378763 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.0884       |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.000233    |
|    value_loss           | 2.3          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -17.2    |
| time/              |          |
|    fps             | 662      |
|    iterations      | 91       |
|    time_elapsed    | 4435     |
|    total_timesteps | 2936752  |
---------------------------------
Eval num_timesteps=2940000, episode_reward=-19.70 +/- 43.11
Episode length: 1566.20 +/- 267.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.57e+03     |
|    mean_reward          | -19.7        |
| time/                   |              |
|    total_timesteps      | 2940000      |
| train/                  |              |
|    approx_kl            | 0.0039904304 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.02        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.09         |
|    n_updates            | 637          |
|    policy_gradient_loss | -0.000565    |
|    value_loss           | 2.33         |
------------------------------------------
Eval num_timesteps=2960000, episode_reward=-21.70 +/- 38.55
Episode length: 1647.60 +/- 347.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.65e+03 |
|    mean_reward     | -21.7    |
| time/              |          |
|    total_timesteps | 2960000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -17.4    |
| time/              |          |
|    fps             | 662      |
|    iterations      | 92       |
|    time_elapsed    | 4484     |
|    total_timesteps | 2969024  |
---------------------------------
Eval num_timesteps=2980000, episode_reward=-7.70 +/- 31.24
Episode length: 1927.80 +/- 433.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.93e+03     |
|    mean_reward          | -7.7         |
| time/                   |              |
|    total_timesteps      | 2980000      |
| train/                  |              |
|    approx_kl            | 0.0007250567 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 3.18         |
|    n_updates            | 644          |
|    policy_gradient_loss | -0.000109    |
|    value_loss           | 2.47         |
------------------------------------------
Eval num_timesteps=3000000, episode_reward=-16.60 +/- 36.07
Episode length: 1516.00 +/- 148.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.52e+03 |
|    mean_reward     | -16.6    |
| time/              |          |
|    total_timesteps | 3000000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -10.4    |
| time/              |          |
|    fps             | 661      |
|    iterations      | 93       |
|    time_elapsed    | 4538     |
|    total_timesteps | 3001296  |
---------------------------------
Eval num_timesteps=3020000, episode_reward=7.00 +/- 40.31
Episode length: 1446.80 +/- 108.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.45e+03      |
|    mean_reward          | 7             |
| time/                   |               |
|    total_timesteps      | 3020000       |
| train/                  |               |
|    approx_kl            | 0.00070358475 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.03         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 1.52          |
|    n_updates            | 651           |
|    policy_gradient_loss | -0.000118     |
|    value_loss           | 2.44          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | -10.2    |
| time/              |          |
|    fps             | 662      |
|    iterations      | 94       |
|    time_elapsed    | 4582     |
|    total_timesteps | 3033568  |
---------------------------------
Eval num_timesteps=3040000, episode_reward=34.10 +/- 35.54
Episode length: 1643.00 +/- 211.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.64e+03      |
|    mean_reward          | 34.1          |
| time/                   |               |
|    total_timesteps      | 3040000       |
| train/                  |               |
|    approx_kl            | 0.00012919243 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.03         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 1.36          |
|    n_updates            | 658           |
|    policy_gradient_loss | -1.54e-06     |
|    value_loss           | 2.43          |
-------------------------------------------
Eval num_timesteps=3060000, episode_reward=18.20 +/- 34.76
Episode length: 1609.20 +/- 100.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.61e+03 |
|    mean_reward     | 18.2     |
| time/              |          |
|    total_timesteps | 3060000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    fps             | 661      |
|    iterations      | 95       |
|    time_elapsed    | 4632     |
|    total_timesteps | 3065840  |
---------------------------------
Eval num_timesteps=3080000, episode_reward=9.40 +/- 35.05
Episode length: 2271.00 +/- 308.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.27e+03    |
|    mean_reward          | 9.4         |
| time/                   |             |
|    total_timesteps      | 3080000     |
| train/                  |             |
|    approx_kl            | 0.000370319 |
|    clip_fraction        | 0           |
|    clip_range           | 0.305       |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0           |
|    learning_rate        | 0.000888    |
|    loss                 | 0.289       |
|    n_updates            | 665         |
|    policy_gradient_loss | -2.75e-05   |
|    value_loss           | 2.3         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -8.92    |
| time/              |          |
|    fps             | 662      |
|    iterations      | 96       |
|    time_elapsed    | 4678     |
|    total_timesteps | 3098112  |
---------------------------------
Eval num_timesteps=3100000, episode_reward=20.30 +/- 37.36
Episode length: 2046.40 +/- 375.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.05e+03     |
|    mean_reward          | 20.3         |
| time/                   |              |
|    total_timesteps      | 3100000      |
| train/                  |              |
|    approx_kl            | 0.0020598623 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 2.71         |
|    n_updates            | 672          |
|    policy_gradient_loss | -0.000271    |
|    value_loss           | 2.31         |
------------------------------------------
Eval num_timesteps=3120000, episode_reward=33.80 +/- 33.02
Episode length: 1974.60 +/- 384.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.97e+03 |
|    mean_reward     | 33.8     |
| time/              |          |
|    total_timesteps | 3120000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -5.99    |
| time/              |          |
|    fps             | 661      |
|    iterations      | 97       |
|    time_elapsed    | 4735     |
|    total_timesteps | 3130384  |
---------------------------------
Eval num_timesteps=3140000, episode_reward=30.80 +/- 31.29
Episode length: 2145.00 +/- 779.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.14e+03     |
|    mean_reward          | 30.8         |
| time/                   |              |
|    total_timesteps      | 3140000      |
| train/                  |              |
|    approx_kl            | 0.0010079239 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.36         |
|    n_updates            | 679          |
|    policy_gradient_loss | -0.000144    |
|    value_loss           | 2.38         |
------------------------------------------
Eval num_timesteps=3160000, episode_reward=40.00 +/- 17.39
Episode length: 1959.40 +/- 289.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.96e+03 |
|    mean_reward     | 40       |
| time/              |          |
|    total_timesteps | 3160000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    fps             | 660      |
|    iterations      | 98       |
|    time_elapsed    | 4787     |
|    total_timesteps | 3162656  |
---------------------------------
Eval num_timesteps=3180000, episode_reward=-33.60 +/- 36.60
Episode length: 1679.00 +/- 363.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.68e+03     |
|    mean_reward          | -33.6        |
| time/                   |              |
|    total_timesteps      | 3180000      |
| train/                  |              |
|    approx_kl            | 0.0038117007 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.373        |
|    n_updates            | 686          |
|    policy_gradient_loss | -0.00058     |
|    value_loss           | 2.46         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -12.7    |
| time/              |          |
|    fps             | 661      |
|    iterations      | 99       |
|    time_elapsed    | 4830     |
|    total_timesteps | 3194928  |
---------------------------------
Eval num_timesteps=3200000, episode_reward=-30.30 +/- 30.62
Episode length: 1619.00 +/- 421.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.62e+03     |
|    mean_reward          | -30.3        |
| time/                   |              |
|    total_timesteps      | 3200000      |
| train/                  |              |
|    approx_kl            | 0.0018522029 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.689        |
|    n_updates            | 693          |
|    policy_gradient_loss | -0.00031     |
|    value_loss           | 2.29         |
------------------------------------------
Eval num_timesteps=3220000, episode_reward=-12.30 +/- 38.22
Episode length: 1490.40 +/- 324.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.49e+03 |
|    mean_reward     | -12.3    |
| time/              |          |
|    total_timesteps | 3220000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -10.8    |
| time/              |          |
|    fps             | 661      |
|    iterations      | 100      |
|    time_elapsed    | 4882     |
|    total_timesteps | 3227200  |
---------------------------------
Eval num_timesteps=3240000, episode_reward=-22.30 +/- 35.19
Episode length: 1578.00 +/- 247.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.58e+03    |
|    mean_reward          | -22.3       |
| time/                   |             |
|    total_timesteps      | 3240000     |
| train/                  |             |
|    approx_kl            | 0.004871897 |
|    clip_fraction        | 0           |
|    clip_range           | 0.305       |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0           |
|    learning_rate        | 0.000888    |
|    loss                 | 3.51        |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.00075    |
|    value_loss           | 2.48        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -12.1    |
| time/              |          |
|    fps             | 661      |
|    iterations      | 101      |
|    time_elapsed    | 4925     |
|    total_timesteps | 3259472  |
---------------------------------
Eval num_timesteps=3260000, episode_reward=-28.00 +/- 10.61
Episode length: 1617.60 +/- 142.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.62e+03      |
|    mean_reward          | -28           |
| time/                   |               |
|    total_timesteps      | 3260000       |
| train/                  |               |
|    approx_kl            | 0.00077428814 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.04         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 1.99          |
|    n_updates            | 707           |
|    policy_gradient_loss | -0.000119     |
|    value_loss           | 2.5           |
-------------------------------------------
Eval num_timesteps=3280000, episode_reward=-48.20 +/- 24.08
Episode length: 1578.00 +/- 396.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.58e+03 |
|    mean_reward     | -48.2    |
| time/              |          |
|    total_timesteps | 3280000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -13.7    |
| time/              |          |
|    fps             | 661      |
|    iterations      | 102      |
|    time_elapsed    | 4978     |
|    total_timesteps | 3291744  |
---------------------------------
Eval num_timesteps=3300000, episode_reward=-20.80 +/- 25.85
Episode length: 1876.40 +/- 352.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.88e+03     |
|    mean_reward          | -20.8        |
| time/                   |              |
|    total_timesteps      | 3300000      |
| train/                  |              |
|    approx_kl            | 0.0076397643 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.01         |
|    n_updates            | 714          |
|    policy_gradient_loss | -0.00112     |
|    value_loss           | 2.38         |
------------------------------------------
Eval num_timesteps=3320000, episode_reward=9.60 +/- 58.92
Episode length: 1341.00 +/- 334.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.34e+03 |
|    mean_reward     | 9.6      |
| time/              |          |
|    total_timesteps | 3320000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -9.02    |
| time/              |          |
|    fps             | 660      |
|    iterations      | 103      |
|    time_elapsed    | 5031     |
|    total_timesteps | 3324016  |
---------------------------------
Eval num_timesteps=3340000, episode_reward=-11.20 +/- 34.13
Episode length: 1777.80 +/- 315.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.78e+03     |
|    mean_reward          | -11.2        |
| time/                   |              |
|    total_timesteps      | 3340000      |
| train/                  |              |
|    approx_kl            | 0.0009722401 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 2.13         |
|    n_updates            | 721          |
|    policy_gradient_loss | -0.00014     |
|    value_loss           | 2.44         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -9.23    |
| time/              |          |
|    fps             | 661      |
|    iterations      | 104      |
|    time_elapsed    | 5074     |
|    total_timesteps | 3356288  |
---------------------------------
Eval num_timesteps=3360000, episode_reward=23.70 +/- 31.14
Episode length: 1981.40 +/- 174.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.98e+03      |
|    mean_reward          | 23.7          |
| time/                   |               |
|    total_timesteps      | 3360000       |
| train/                  |               |
|    approx_kl            | 0.00013285804 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.05         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 2.96          |
|    n_updates            | 728           |
|    policy_gradient_loss | -6.85e-06     |
|    value_loss           | 2.39          |
-------------------------------------------
Eval num_timesteps=3380000, episode_reward=-1.00 +/- 38.07
Episode length: 1784.00 +/- 257.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.78e+03 |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 3380000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -6.46    |
| time/              |          |
|    fps             | 660      |
|    iterations      | 105      |
|    time_elapsed    | 5126     |
|    total_timesteps | 3388560  |
---------------------------------
Eval num_timesteps=3400000, episode_reward=26.30 +/- 21.78
Episode length: 1689.20 +/- 253.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.69e+03     |
|    mean_reward          | 26.3         |
| time/                   |              |
|    total_timesteps      | 3400000      |
| train/                  |              |
|    approx_kl            | 0.0006646246 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.648        |
|    n_updates            | 735          |
|    policy_gradient_loss | -7.86e-05    |
|    value_loss           | 2.47         |
------------------------------------------
Eval num_timesteps=3420000, episode_reward=15.10 +/- 44.75
Episode length: 1756.20 +/- 382.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.76e+03 |
|    mean_reward     | 15.1     |
| time/              |          |
|    total_timesteps | 3420000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -6.01    |
| time/              |          |
|    fps             | 660      |
|    iterations      | 106      |
|    time_elapsed    | 5178     |
|    total_timesteps | 3420832  |
---------------------------------
Eval num_timesteps=3440000, episode_reward=31.70 +/- 44.55
Episode length: 2225.60 +/- 526.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.23e+03     |
|    mean_reward          | 31.7         |
| time/                   |              |
|    total_timesteps      | 3440000      |
| train/                  |              |
|    approx_kl            | 0.0004238249 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.83         |
|    n_updates            | 742          |
|    policy_gradient_loss | -6.59e-05    |
|    value_loss           | 2.4          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -8.19    |
| time/              |          |
|    fps             | 660      |
|    iterations      | 107      |
|    time_elapsed    | 5224     |
|    total_timesteps | 3453104  |
---------------------------------
Eval num_timesteps=3460000, episode_reward=21.00 +/- 39.98
Episode length: 2079.00 +/- 446.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.08e+03    |
|    mean_reward          | 21          |
| time/                   |             |
|    total_timesteps      | 3460000     |
| train/                  |             |
|    approx_kl            | 9.35294e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.305       |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0           |
|    learning_rate        | 0.000888    |
|    loss                 | 1.9         |
|    n_updates            | 749         |
|    policy_gradient_loss | 1.11e-05    |
|    value_loss           | 2.37        |
-----------------------------------------
Eval num_timesteps=3480000, episode_reward=39.20 +/- 31.38
Episode length: 1802.80 +/- 258.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.8e+03  |
|    mean_reward     | 39.2     |
| time/              |          |
|    total_timesteps | 3480000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -11      |
| time/              |          |
|    fps             | 660      |
|    iterations      | 108      |
|    time_elapsed    | 5278     |
|    total_timesteps | 3485376  |
---------------------------------
Eval num_timesteps=3500000, episode_reward=14.80 +/- 35.89
Episode length: 1745.00 +/- 221.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.74e+03     |
|    mean_reward          | 14.8         |
| time/                   |              |
|    total_timesteps      | 3500000      |
| train/                  |              |
|    approx_kl            | 0.0018905329 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.23         |
|    n_updates            | 756          |
|    policy_gradient_loss | -0.000273    |
|    value_loss           | 2.41         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -9.77    |
| time/              |          |
|    fps             | 660      |
|    iterations      | 109      |
|    time_elapsed    | 5323     |
|    total_timesteps | 3517648  |
---------------------------------
Eval num_timesteps=3520000, episode_reward=40.90 +/- 24.62
Episode length: 1791.00 +/- 317.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.79e+03      |
|    mean_reward          | 40.9          |
| time/                   |               |
|    total_timesteps      | 3520000       |
| train/                  |               |
|    approx_kl            | 0.00056730025 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.04         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 1.17          |
|    n_updates            | 763           |
|    policy_gradient_loss | -9.47e-05     |
|    value_loss           | 2.32          |
-------------------------------------------
Eval num_timesteps=3540000, episode_reward=32.10 +/- 19.90
Episode length: 2043.20 +/- 458.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.04e+03 |
|    mean_reward     | 32.1     |
| time/              |          |
|    total_timesteps | 3540000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -7.92    |
| time/              |          |
|    fps             | 660      |
|    iterations      | 110      |
|    time_elapsed    | 5374     |
|    total_timesteps | 3549920  |
---------------------------------
Eval num_timesteps=3560000, episode_reward=46.50 +/- 23.75
Episode length: 1606.80 +/- 347.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.61e+03      |
|    mean_reward          | 46.5          |
| time/                   |               |
|    total_timesteps      | 3560000       |
| train/                  |               |
|    approx_kl            | 0.00014129818 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.04         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 0.82          |
|    n_updates            | 770           |
|    policy_gradient_loss | -6.73e-07     |
|    value_loss           | 2.42          |
-------------------------------------------
Eval num_timesteps=3580000, episode_reward=4.40 +/- 33.19
Episode length: 2194.40 +/- 297.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.19e+03 |
|    mean_reward     | 4.4      |
| time/              |          |
|    total_timesteps | 3580000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -11.3    |
| time/              |          |
|    fps             | 659      |
|    iterations      | 111      |
|    time_elapsed    | 5429     |
|    total_timesteps | 3582192  |
---------------------------------
Eval num_timesteps=3600000, episode_reward=46.20 +/- 39.64
Episode length: 2251.80 +/- 506.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.25e+03     |
|    mean_reward          | 46.2         |
| time/                   |              |
|    total_timesteps      | 3600000      |
| train/                  |              |
|    approx_kl            | 0.0007089358 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.717        |
|    n_updates            | 777          |
|    policy_gradient_loss | -5.79e-05    |
|    value_loss           | 2.44         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -11      |
| time/              |          |
|    fps             | 660      |
|    iterations      | 112      |
|    time_elapsed    | 5472     |
|    total_timesteps | 3614464  |
---------------------------------
Eval num_timesteps=3620000, episode_reward=40.80 +/- 32.97
Episode length: 2044.80 +/- 322.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.04e+03     |
|    mean_reward          | 40.8         |
| time/                   |              |
|    total_timesteps      | 3620000      |
| train/                  |              |
|    approx_kl            | 0.0004091805 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.362        |
|    n_updates            | 784          |
|    policy_gradient_loss | -4.03e-05    |
|    value_loss           | 2.46         |
------------------------------------------
Eval num_timesteps=3640000, episode_reward=15.70 +/- 36.07
Episode length: 1973.80 +/- 406.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.97e+03 |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 3640000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -8.24    |
| time/              |          |
|    fps             | 659      |
|    iterations      | 113      |
|    time_elapsed    | 5526     |
|    total_timesteps | 3646736  |
---------------------------------
Eval num_timesteps=3660000, episode_reward=26.00 +/- 29.21
Episode length: 1970.00 +/- 523.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.97e+03      |
|    mean_reward          | 26            |
| time/                   |               |
|    total_timesteps      | 3660000       |
| train/                  |               |
|    approx_kl            | 0.00019877937 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.03         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 1.72          |
|    n_updates            | 791           |
|    policy_gradient_loss | -2.7e-06      |
|    value_loss           | 2.41          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -8.71    |
| time/              |          |
|    fps             | 660      |
|    iterations      | 114      |
|    time_elapsed    | 5571     |
|    total_timesteps | 3679008  |
---------------------------------
Eval num_timesteps=3680000, episode_reward=26.10 +/- 47.31
Episode length: 1944.40 +/- 218.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.94e+03      |
|    mean_reward          | 26.1          |
| time/                   |               |
|    total_timesteps      | 3680000       |
| train/                  |               |
|    approx_kl            | 0.00035467546 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.04         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 1.06          |
|    n_updates            | 798           |
|    policy_gradient_loss | -4.99e-05     |
|    value_loss           | 2.43          |
-------------------------------------------
Eval num_timesteps=3700000, episode_reward=45.50 +/- 24.89
Episode length: 1829.80 +/- 282.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.83e+03 |
|    mean_reward     | 45.5     |
| time/              |          |
|    total_timesteps | 3700000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -7.89    |
| time/              |          |
|    fps             | 659      |
|    iterations      | 115      |
|    time_elapsed    | 5625     |
|    total_timesteps | 3711280  |
---------------------------------
Eval num_timesteps=3720000, episode_reward=13.30 +/- 26.68
Episode length: 2241.40 +/- 284.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.24e+03     |
|    mean_reward          | 13.3         |
| time/                   |              |
|    total_timesteps      | 3720000      |
| train/                  |              |
|    approx_kl            | 0.0001645618 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.102        |
|    n_updates            | 805          |
|    policy_gradient_loss | -3.32e-05    |
|    value_loss           | 2.37         |
------------------------------------------
Eval num_timesteps=3740000, episode_reward=47.90 +/- 46.62
Episode length: 1357.40 +/- 403.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.36e+03 |
|    mean_reward     | 47.9     |
| time/              |          |
|    total_timesteps | 3740000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -9.9     |
| time/              |          |
|    fps             | 659      |
|    iterations      | 116      |
|    time_elapsed    | 5676     |
|    total_timesteps | 3743552  |
---------------------------------
Eval num_timesteps=3760000, episode_reward=30.20 +/- 31.99
Episode length: 2015.00 +/- 522.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 2.02e+03      |
|    mean_reward          | 30.2          |
| time/                   |               |
|    total_timesteps      | 3760000       |
| train/                  |               |
|    approx_kl            | 0.00048526426 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.05         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 1.11          |
|    n_updates            | 812           |
|    policy_gradient_loss | -1.42e-05     |
|    value_loss           | 2.5           |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -10.3    |
| time/              |          |
|    fps             | 659      |
|    iterations      | 117      |
|    time_elapsed    | 5723     |
|    total_timesteps | 3775824  |
---------------------------------
Eval num_timesteps=3780000, episode_reward=18.40 +/- 26.90
Episode length: 1670.40 +/- 243.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.67e+03      |
|    mean_reward          | 18.4          |
| time/                   |               |
|    total_timesteps      | 3780000       |
| train/                  |               |
|    approx_kl            | 0.00060858624 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.04         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 0.199         |
|    n_updates            | 819           |
|    policy_gradient_loss | -0.000109     |
|    value_loss           | 2.4           |
-------------------------------------------
Eval num_timesteps=3800000, episode_reward=-11.20 +/- 28.36
Episode length: 2178.00 +/- 501.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.18e+03 |
|    mean_reward     | -11.2    |
| time/              |          |
|    total_timesteps | 3800000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    fps             | 659      |
|    iterations      | 118      |
|    time_elapsed    | 5777     |
|    total_timesteps | 3808096  |
---------------------------------
Eval num_timesteps=3820000, episode_reward=35.00 +/- 25.59
Episode length: 1830.80 +/- 205.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.83e+03     |
|    mean_reward          | 35           |
| time/                   |              |
|    total_timesteps      | 3820000      |
| train/                  |              |
|    approx_kl            | 0.0018285706 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.0135       |
|    n_updates            | 826          |
|    policy_gradient_loss | -0.000297    |
|    value_loss           | 2.35         |
------------------------------------------
Eval num_timesteps=3840000, episode_reward=25.00 +/- 19.83
Episode length: 2261.60 +/- 210.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.26e+03 |
|    mean_reward     | 25       |
| time/              |          |
|    total_timesteps | 3840000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -11.1    |
| time/              |          |
|    fps             | 658      |
|    iterations      | 119      |
|    time_elapsed    | 5832     |
|    total_timesteps | 3840368  |
---------------------------------
Eval num_timesteps=3860000, episode_reward=-16.30 +/- 34.42
Episode length: 1623.80 +/- 258.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.62e+03     |
|    mean_reward          | -16.3        |
| time/                   |              |
|    total_timesteps      | 3860000      |
| train/                  |              |
|    approx_kl            | 0.0011010044 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.907        |
|    n_updates            | 833          |
|    policy_gradient_loss | -0.000166    |
|    value_loss           | 2.46         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | -9.2     |
| time/              |          |
|    fps             | 659      |
|    iterations      | 120      |
|    time_elapsed    | 5875     |
|    total_timesteps | 3872640  |
---------------------------------
Eval num_timesteps=3880000, episode_reward=49.30 +/- 27.71
Episode length: 1868.80 +/- 307.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.87e+03     |
|    mean_reward          | 49.3         |
| time/                   |              |
|    total_timesteps      | 3880000      |
| train/                  |              |
|    approx_kl            | 0.0044703986 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.05        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.000888     |
|    loss                 | 1.34         |
|    n_updates            | 840          |
|    policy_gradient_loss | -0.000648    |
|    value_loss           | 2.37         |
------------------------------------------
Eval num_timesteps=3900000, episode_reward=56.30 +/- 22.64
Episode length: 1583.00 +/- 400.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.58e+03 |
|    mean_reward     | 56.3     |
| time/              |          |
|    total_timesteps | 3900000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -7.04    |
| time/              |          |
|    fps             | 659      |
|    iterations      | 121      |
|    time_elapsed    | 5924     |
|    total_timesteps | 3904912  |
---------------------------------
Eval num_timesteps=3920000, episode_reward=44.20 +/- 21.37
Episode length: 2030.60 +/- 393.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.03e+03     |
|    mean_reward          | 44.2         |
| time/                   |              |
|    total_timesteps      | 3920000      |
| train/                  |              |
|    approx_kl            | 0.0003210545 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.655        |
|    n_updates            | 847          |
|    policy_gradient_loss | -1.42e-05    |
|    value_loss           | 2.27         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -7.12    |
| time/              |          |
|    fps             | 659      |
|    iterations      | 122      |
|    time_elapsed    | 5968     |
|    total_timesteps | 3937184  |
---------------------------------
Eval num_timesteps=3940000, episode_reward=-0.80 +/- 30.77
Episode length: 2217.40 +/- 406.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 2.22e+03      |
|    mean_reward          | -0.8          |
| time/                   |               |
|    total_timesteps      | 3940000       |
| train/                  |               |
|    approx_kl            | 0.00020935561 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.04         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 0.496         |
|    n_updates            | 854           |
|    policy_gradient_loss | -1.87e-05     |
|    value_loss           | 2.48          |
-------------------------------------------
Eval num_timesteps=3960000, episode_reward=9.60 +/- 38.98
Episode length: 2382.80 +/- 620.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.38e+03 |
|    mean_reward     | 9.6      |
| time/              |          |
|    total_timesteps | 3960000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | -4.95    |
| time/              |          |
|    fps             | 658      |
|    iterations      | 123      |
|    time_elapsed    | 6025     |
|    total_timesteps | 3969456  |
---------------------------------
Eval num_timesteps=3980000, episode_reward=58.10 +/- 9.53
Episode length: 1895.40 +/- 291.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.9e+03      |
|    mean_reward          | 58.1         |
| time/                   |              |
|    total_timesteps      | 3980000      |
| train/                  |              |
|    approx_kl            | 0.0027244745 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 2            |
|    n_updates            | 861          |
|    policy_gradient_loss | -0.000434    |
|    value_loss           | 2.3          |
------------------------------------------
Eval num_timesteps=4000000, episode_reward=22.30 +/- 28.70
Episode length: 2035.20 +/- 348.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.04e+03 |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 4000000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -5.04    |
| time/              |          |
|    fps             | 657      |
|    iterations      | 124      |
|    time_elapsed    | 6083     |
|    total_timesteps | 4001728  |
---------------------------------
Eval num_timesteps=4020000, episode_reward=41.90 +/- 26.60
Episode length: 1900.60 +/- 671.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.9e+03      |
|    mean_reward          | 41.9         |
| time/                   |              |
|    total_timesteps      | 4020000      |
| train/                  |              |
|    approx_kl            | 0.0014851915 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.02        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.631        |
|    n_updates            | 868          |
|    policy_gradient_loss | -0.000309    |
|    value_loss           | 2.45         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -9.67    |
| time/              |          |
|    fps             | 658      |
|    iterations      | 125      |
|    time_elapsed    | 6126     |
|    total_timesteps | 4034000  |
---------------------------------
Eval num_timesteps=4040000, episode_reward=1.80 +/- 51.11
Episode length: 1526.80 +/- 264.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.53e+03    |
|    mean_reward          | 1.8         |
| time/                   |             |
|    total_timesteps      | 4040000     |
| train/                  |             |
|    approx_kl            | 0.005033209 |
|    clip_fraction        | 0           |
|    clip_range           | 0.305       |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0           |
|    learning_rate        | 0.000888    |
|    loss                 | 0.604       |
|    n_updates            | 875         |
|    policy_gradient_loss | -0.000763   |
|    value_loss           | 2.37        |
-----------------------------------------
Eval num_timesteps=4060000, episode_reward=18.50 +/- 33.45
Episode length: 1706.20 +/- 227.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.71e+03 |
|    mean_reward     | 18.5     |
| time/              |          |
|    total_timesteps | 4060000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.72e+03 |
|    ep_rew_mean     | -7.76    |
| time/              |          |
|    fps             | 658      |
|    iterations      | 126      |
|    time_elapsed    | 6179     |
|    total_timesteps | 4066272  |
---------------------------------
Eval num_timesteps=4080000, episode_reward=1.50 +/- 27.46
Episode length: 1998.20 +/- 313.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2e+03        |
|    mean_reward          | 1.5          |
| time/                   |              |
|    total_timesteps      | 4080000      |
| train/                  |              |
|    approx_kl            | 0.0003810491 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 2.08         |
|    n_updates            | 882          |
|    policy_gradient_loss | -4e-05       |
|    value_loss           | 2.29         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.71e+03 |
|    ep_rew_mean     | -10.5    |
| time/              |          |
|    fps             | 658      |
|    iterations      | 127      |
|    time_elapsed    | 6223     |
|    total_timesteps | 4098544  |
---------------------------------
Eval num_timesteps=4100000, episode_reward=17.10 +/- 43.43
Episode length: 1763.80 +/- 246.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.76e+03     |
|    mean_reward          | 17.1         |
| time/                   |              |
|    total_timesteps      | 4100000      |
| train/                  |              |
|    approx_kl            | 0.0017106723 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.18         |
|    n_updates            | 889          |
|    policy_gradient_loss | -0.000203    |
|    value_loss           | 2.34         |
------------------------------------------
Eval num_timesteps=4120000, episode_reward=47.30 +/- 28.45
Episode length: 1802.00 +/- 379.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.8e+03  |
|    mean_reward     | 47.3     |
| time/              |          |
|    total_timesteps | 4120000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -13.9    |
| time/              |          |
|    fps             | 658      |
|    iterations      | 128      |
|    time_elapsed    | 6275     |
|    total_timesteps | 4130816  |
---------------------------------
Eval num_timesteps=4140000, episode_reward=-10.50 +/- 28.56
Episode length: 1915.00 +/- 222.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.92e+03     |
|    mean_reward          | -10.5        |
| time/                   |              |
|    total_timesteps      | 4140000      |
| train/                  |              |
|    approx_kl            | 0.0007433615 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.61         |
|    n_updates            | 896          |
|    policy_gradient_loss | -9.17e-05    |
|    value_loss           | 2.33         |
------------------------------------------
Eval num_timesteps=4160000, episode_reward=37.50 +/- 29.67
Episode length: 2226.40 +/- 496.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.23e+03 |
|    mean_reward     | 37.5     |
| time/              |          |
|    total_timesteps | 4160000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.72e+03 |
|    ep_rew_mean     | -10.1    |
| time/              |          |
|    fps             | 657      |
|    iterations      | 129      |
|    time_elapsed    | 6329     |
|    total_timesteps | 4163088  |
---------------------------------
Eval num_timesteps=4180000, episode_reward=1.80 +/- 47.58
Episode length: 1831.20 +/- 345.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.83e+03      |
|    mean_reward          | 1.8           |
| time/                   |               |
|    total_timesteps      | 4180000       |
| train/                  |               |
|    approx_kl            | 5.1209587e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 2.26          |
|    n_updates            | 903           |
|    policy_gradient_loss | 1.93e-05      |
|    value_loss           | 2.35          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.69e+03 |
|    ep_rew_mean     | -7.6     |
| time/              |          |
|    fps             | 658      |
|    iterations      | 130      |
|    time_elapsed    | 6375     |
|    total_timesteps | 4195360  |
---------------------------------
Eval num_timesteps=4200000, episode_reward=42.80 +/- 32.32
Episode length: 1634.60 +/- 374.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.63e+03      |
|    mean_reward          | 42.8          |
| time/                   |               |
|    total_timesteps      | 4200000       |
| train/                  |               |
|    approx_kl            | 0.00037970743 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 1.34          |
|    n_updates            | 910           |
|    policy_gradient_loss | -4.06e-05     |
|    value_loss           | 2.51          |
-------------------------------------------
Eval num_timesteps=4220000, episode_reward=16.60 +/- 38.32
Episode length: 1707.80 +/- 270.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.71e+03 |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 4220000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -6.8     |
| time/              |          |
|    fps             | 657      |
|    iterations      | 131      |
|    time_elapsed    | 6425     |
|    total_timesteps | 4227632  |
---------------------------------
Eval num_timesteps=4240000, episode_reward=19.40 +/- 41.81
Episode length: 1870.00 +/- 555.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.87e+03    |
|    mean_reward          | 19.4        |
| time/                   |             |
|    total_timesteps      | 4240000     |
| train/                  |             |
|    approx_kl            | 0.000311803 |
|    clip_fraction        | 0           |
|    clip_range           | 0.305       |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0           |
|    learning_rate        | 0.000888    |
|    loss                 | 1.27        |
|    n_updates            | 917         |
|    policy_gradient_loss | -2.63e-05   |
|    value_loss           | 2.37        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -5.88    |
| time/              |          |
|    fps             | 658      |
|    iterations      | 132      |
|    time_elapsed    | 6468     |
|    total_timesteps | 4259904  |
---------------------------------
Eval num_timesteps=4260000, episode_reward=11.10 +/- 40.67
Episode length: 1903.40 +/- 237.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.9e+03      |
|    mean_reward          | 11.1         |
| time/                   |              |
|    total_timesteps      | 4260000      |
| train/                  |              |
|    approx_kl            | 0.0008271653 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.18         |
|    n_updates            | 924          |
|    policy_gradient_loss | -9.22e-05    |
|    value_loss           | 2.39         |
------------------------------------------
Eval num_timesteps=4280000, episode_reward=-46.90 +/- 40.23
Episode length: 1361.40 +/- 387.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.36e+03 |
|    mean_reward     | -46.9    |
| time/              |          |
|    total_timesteps | 4280000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -0.015   |
| time/              |          |
|    fps             | 658      |
|    iterations      | 133      |
|    time_elapsed    | 6518     |
|    total_timesteps | 4292176  |
---------------------------------
Eval num_timesteps=4300000, episode_reward=-11.50 +/- 57.07
Episode length: 1548.60 +/- 215.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.55e+03     |
|    mean_reward          | -11.5        |
| time/                   |              |
|    total_timesteps      | 4300000      |
| train/                  |              |
|    approx_kl            | 2.400784e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.28         |
|    n_updates            | 931          |
|    policy_gradient_loss | 2.73e-05     |
|    value_loss           | 2.47         |
------------------------------------------
Eval num_timesteps=4320000, episode_reward=6.80 +/- 26.54
Episode length: 1849.80 +/- 469.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.85e+03 |
|    mean_reward     | 6.8      |
| time/              |          |
|    total_timesteps | 4320000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -1.75    |
| time/              |          |
|    fps             | 658      |
|    iterations      | 134      |
|    time_elapsed    | 6568     |
|    total_timesteps | 4324448  |
---------------------------------
Eval num_timesteps=4340000, episode_reward=0.30 +/- 27.00
Episode length: 1832.20 +/- 505.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.83e+03      |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 4340000       |
| train/                  |               |
|    approx_kl            | 0.00054941245 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.06         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 0.309         |
|    n_updates            | 938           |
|    policy_gradient_loss | -8.96e-05     |
|    value_loss           | 2.36          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -8.96    |
| time/              |          |
|    fps             | 658      |
|    iterations      | 135      |
|    time_elapsed    | 6614     |
|    total_timesteps | 4356720  |
---------------------------------
Eval num_timesteps=4360000, episode_reward=-43.30 +/- 18.20
Episode length: 1565.00 +/- 357.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.56e+03      |
|    mean_reward          | -43.3         |
| time/                   |               |
|    total_timesteps      | 4360000       |
| train/                  |               |
|    approx_kl            | 0.00016294538 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.07         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 0.243         |
|    n_updates            | 945           |
|    policy_gradient_loss | -7.58e-06     |
|    value_loss           | 2.45          |
-------------------------------------------
Eval num_timesteps=4380000, episode_reward=-22.30 +/- 26.65
Episode length: 1769.80 +/- 278.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.77e+03 |
|    mean_reward     | -22.3    |
| time/              |          |
|    total_timesteps | 4380000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -9.25    |
| time/              |          |
|    fps             | 658      |
|    iterations      | 136      |
|    time_elapsed    | 6667     |
|    total_timesteps | 4388992  |
---------------------------------
Eval num_timesteps=4400000, episode_reward=-22.60 +/- 39.47
Episode length: 1482.40 +/- 325.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.48e+03   |
|    mean_reward          | -22.6      |
| time/                   |            |
|    total_timesteps      | 4400000    |
| train/                  |            |
|    approx_kl            | 0.00661411 |
|    clip_fraction        | 0          |
|    clip_range           | 0.305      |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0          |
|    learning_rate        | 0.000888   |
|    loss                 | 2.77       |
|    n_updates            | 952        |
|    policy_gradient_loss | -0.00122   |
|    value_loss           | 2.42       |
----------------------------------------
Eval num_timesteps=4420000, episode_reward=-7.60 +/- 49.38
Episode length: 1639.60 +/- 331.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.64e+03 |
|    mean_reward     | -7.6     |
| time/              |          |
|    total_timesteps | 4420000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -8.78    |
| time/              |          |
|    fps             | 658      |
|    iterations      | 137      |
|    time_elapsed    | 6717     |
|    total_timesteps | 4421264  |
---------------------------------
Eval num_timesteps=4440000, episode_reward=17.60 +/- 27.12
Episode length: 1962.40 +/- 226.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.96e+03      |
|    mean_reward          | 17.6          |
| time/                   |               |
|    total_timesteps      | 4440000       |
| train/                  |               |
|    approx_kl            | 0.00012104677 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.09         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 1.39          |
|    n_updates            | 959           |
|    policy_gradient_loss | -3.8e-06      |
|    value_loss           | 2.37          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -9.23    |
| time/              |          |
|    fps             | 658      |
|    iterations      | 138      |
|    time_elapsed    | 6761     |
|    total_timesteps | 4453536  |
---------------------------------
Eval num_timesteps=4460000, episode_reward=36.00 +/- 14.56
Episode length: 1998.00 +/- 302.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2e+03        |
|    mean_reward          | 36           |
| time/                   |              |
|    total_timesteps      | 4460000      |
| train/                  |              |
|    approx_kl            | 9.646506e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.34         |
|    n_updates            | 966          |
|    policy_gradient_loss | 1.48e-05     |
|    value_loss           | 2.54         |
------------------------------------------
Eval num_timesteps=4480000, episode_reward=40.30 +/- 40.48
Episode length: 1795.40 +/- 323.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.8e+03  |
|    mean_reward     | 40.3     |
| time/              |          |
|    total_timesteps | 4480000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -9.74    |
| time/              |          |
|    fps             | 658      |
|    iterations      | 139      |
|    time_elapsed    | 6816     |
|    total_timesteps | 4485808  |
---------------------------------
Eval num_timesteps=4500000, episode_reward=-29.90 +/- 36.32
Episode length: 1603.40 +/- 220.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.6e+03      |
|    mean_reward          | -29.9        |
| time/                   |              |
|    total_timesteps      | 4500000      |
| train/                  |              |
|    approx_kl            | 0.0023792826 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.495        |
|    n_updates            | 973          |
|    policy_gradient_loss | -0.000324    |
|    value_loss           | 2.35         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | -6.29    |
| time/              |          |
|    fps             | 658      |
|    iterations      | 140      |
|    time_elapsed    | 6858     |
|    total_timesteps | 4518080  |
---------------------------------
Eval num_timesteps=4520000, episode_reward=-33.00 +/- 31.65
Episode length: 1796.80 +/- 388.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.8e+03     |
|    mean_reward          | -33         |
| time/                   |             |
|    total_timesteps      | 4520000     |
| train/                  |             |
|    approx_kl            | 0.001753339 |
|    clip_fraction        | 0           |
|    clip_range           | 0.305       |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0           |
|    learning_rate        | 0.000888    |
|    loss                 | 3.36        |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.000311   |
|    value_loss           | 2.43        |
-----------------------------------------
Eval num_timesteps=4540000, episode_reward=-30.00 +/- 31.59
Episode length: 1616.40 +/- 465.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.62e+03 |
|    mean_reward     | -30      |
| time/              |          |
|    total_timesteps | 4540000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -7.3     |
| time/              |          |
|    fps             | 658      |
|    iterations      | 141      |
|    time_elapsed    | 6908     |
|    total_timesteps | 4550352  |
---------------------------------
Eval num_timesteps=4560000, episode_reward=15.70 +/- 35.69
Episode length: 1641.00 +/- 276.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.64e+03     |
|    mean_reward          | 15.7         |
| time/                   |              |
|    total_timesteps      | 4560000      |
| train/                  |              |
|    approx_kl            | 0.0002018229 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.05         |
|    n_updates            | 987          |
|    policy_gradient_loss | -1.05e-05    |
|    value_loss           | 2.36         |
------------------------------------------
Eval num_timesteps=4580000, episode_reward=-16.30 +/- 34.89
Episode length: 1996.00 +/- 52.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -16.3    |
| time/              |          |
|    total_timesteps | 4580000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -7.44    |
| time/              |          |
|    fps             | 658      |
|    iterations      | 142      |
|    time_elapsed    | 6960     |
|    total_timesteps | 4582624  |
---------------------------------
Eval num_timesteps=4600000, episode_reward=-25.90 +/- 27.56
Episode length: 1620.00 +/- 321.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.62e+03      |
|    mean_reward          | -25.9         |
| time/                   |               |
|    total_timesteps      | 4600000       |
| train/                  |               |
|    approx_kl            | 0.00014983966 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 1.32          |
|    n_updates            | 994           |
|    policy_gradient_loss | -3e-06        |
|    value_loss           | 2.28          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -7.38    |
| time/              |          |
|    fps             | 659      |
|    iterations      | 143      |
|    time_elapsed    | 7001     |
|    total_timesteps | 4614896  |
---------------------------------
Eval num_timesteps=4620000, episode_reward=-8.30 +/- 31.96
Episode length: 1640.00 +/- 348.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.64e+03    |
|    mean_reward          | -8.3        |
| time/                   |             |
|    total_timesteps      | 4620000     |
| train/                  |             |
|    approx_kl            | 0.005699623 |
|    clip_fraction        | 0           |
|    clip_range           | 0.305       |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0           |
|    learning_rate        | 0.000888    |
|    loss                 | 1.1         |
|    n_updates            | 1001        |
|    policy_gradient_loss | -0.000987   |
|    value_loss           | 2.41        |
-----------------------------------------
Eval num_timesteps=4640000, episode_reward=-25.20 +/- 33.56
Episode length: 1753.60 +/- 264.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.75e+03 |
|    mean_reward     | -25.2    |
| time/              |          |
|    total_timesteps | 4640000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -6.41    |
| time/              |          |
|    fps             | 658      |
|    iterations      | 144      |
|    time_elapsed    | 7054     |
|    total_timesteps | 4647168  |
---------------------------------
Eval num_timesteps=4660000, episode_reward=-3.60 +/- 23.61
Episode length: 1798.40 +/- 329.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.8e+03      |
|    mean_reward          | -3.6         |
| time/                   |              |
|    total_timesteps      | 4660000      |
| train/                  |              |
|    approx_kl            | 0.0021163076 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.844        |
|    n_updates            | 1008         |
|    policy_gradient_loss | -0.000352    |
|    value_loss           | 2.45         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -11      |
| time/              |          |
|    fps             | 659      |
|    iterations      | 145      |
|    time_elapsed    | 7096     |
|    total_timesteps | 4679440  |
---------------------------------
Eval num_timesteps=4680000, episode_reward=-31.80 +/- 22.89
Episode length: 1695.00 +/- 281.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.7e+03      |
|    mean_reward          | -31.8        |
| time/                   |              |
|    total_timesteps      | 4680000      |
| train/                  |              |
|    approx_kl            | 0.0010665627 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.0799       |
|    n_updates            | 1015         |
|    policy_gradient_loss | -0.000168    |
|    value_loss           | 2.32         |
------------------------------------------
Eval num_timesteps=4700000, episode_reward=0.10 +/- 43.79
Episode length: 1625.80 +/- 142.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.63e+03 |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 4700000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | -9.31    |
| time/              |          |
|    fps             | 659      |
|    iterations      | 146      |
|    time_elapsed    | 7146     |
|    total_timesteps | 4711712  |
---------------------------------
Eval num_timesteps=4720000, episode_reward=-18.20 +/- 27.17
Episode length: 1646.60 +/- 166.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.65e+03     |
|    mean_reward          | -18.2        |
| time/                   |              |
|    total_timesteps      | 4720000      |
| train/                  |              |
|    approx_kl            | 0.0018403768 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.69         |
|    n_updates            | 1022         |
|    policy_gradient_loss | -0.00031     |
|    value_loss           | 2.44         |
------------------------------------------
Eval num_timesteps=4740000, episode_reward=-40.00 +/- 10.43
Episode length: 1617.40 +/- 301.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.62e+03 |
|    mean_reward     | -40      |
| time/              |          |
|    total_timesteps | 4740000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | -9.65    |
| time/              |          |
|    fps             | 659      |
|    iterations      | 147      |
|    time_elapsed    | 7197     |
|    total_timesteps | 4743984  |
---------------------------------
Eval num_timesteps=4760000, episode_reward=-10.10 +/- 33.67
Episode length: 1660.40 +/- 239.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.66e+03      |
|    mean_reward          | -10.1         |
| time/                   |               |
|    total_timesteps      | 4760000       |
| train/                  |               |
|    approx_kl            | 0.00011359669 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 0.513         |
|    n_updates            | 1029          |
|    policy_gradient_loss | 7.95e-06      |
|    value_loss           | 2.5           |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -9.21    |
| time/              |          |
|    fps             | 659      |
|    iterations      | 148      |
|    time_elapsed    | 7239     |
|    total_timesteps | 4776256  |
---------------------------------
Eval num_timesteps=4780000, episode_reward=-17.80 +/- 30.21
Episode length: 1832.40 +/- 428.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.83e+03      |
|    mean_reward          | -17.8         |
| time/                   |               |
|    total_timesteps      | 4780000       |
| train/                  |               |
|    approx_kl            | 0.00074011186 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 1.86          |
|    n_updates            | 1036          |
|    policy_gradient_loss | -8.21e-05     |
|    value_loss           | 2.53          |
-------------------------------------------
Eval num_timesteps=4800000, episode_reward=-35.90 +/- 17.67
Episode length: 1734.00 +/- 253.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.73e+03 |
|    mean_reward     | -35.9    |
| time/              |          |
|    total_timesteps | 4800000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | -11.7    |
| time/              |          |
|    fps             | 659      |
|    iterations      | 149      |
|    time_elapsed    | 7291     |
|    total_timesteps | 4808528  |
---------------------------------
Eval num_timesteps=4820000, episode_reward=3.80 +/- 47.15
Episode length: 1568.40 +/- 416.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.57e+03     |
|    mean_reward          | 3.8          |
| time/                   |              |
|    total_timesteps      | 4820000      |
| train/                  |              |
|    approx_kl            | 0.0019208791 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.12         |
|    n_updates            | 1043         |
|    policy_gradient_loss | -0.000319    |
|    value_loss           | 2.32         |
------------------------------------------
Eval num_timesteps=4840000, episode_reward=-52.20 +/- 18.75
Episode length: 1509.40 +/- 220.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.51e+03 |
|    mean_reward     | -52.2    |
| time/              |          |
|    total_timesteps | 4840000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | -8.92    |
| time/              |          |
|    fps             | 659      |
|    iterations      | 150      |
|    time_elapsed    | 7341     |
|    total_timesteps | 4840800  |
---------------------------------
Eval num_timesteps=4860000, episode_reward=-3.80 +/- 35.55
Episode length: 1444.20 +/- 260.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.44e+03    |
|    mean_reward          | -3.8        |
| time/                   |             |
|    total_timesteps      | 4860000     |
| train/                  |             |
|    approx_kl            | 0.002867221 |
|    clip_fraction        | 0           |
|    clip_range           | 0.305       |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0           |
|    learning_rate        | 0.000888    |
|    loss                 | 2.8         |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.000405   |
|    value_loss           | 2.37        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | -14.2    |
| time/              |          |
|    fps             | 660      |
|    iterations      | 151      |
|    time_elapsed    | 7383     |
|    total_timesteps | 4873072  |
---------------------------------
Eval num_timesteps=4880000, episode_reward=-16.50 +/- 62.17
Episode length: 1409.20 +/- 564.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.41e+03     |
|    mean_reward          | -16.5        |
| time/                   |              |
|    total_timesteps      | 4880000      |
| train/                  |              |
|    approx_kl            | 0.0008731394 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.07        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.000888     |
|    loss                 | 1.64         |
|    n_updates            | 1057         |
|    policy_gradient_loss | -0.00015     |
|    value_loss           | 2.47         |
------------------------------------------
Eval num_timesteps=4900000, episode_reward=-29.10 +/- 17.36
Episode length: 1465.40 +/- 366.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.47e+03 |
|    mean_reward     | -29.1    |
| time/              |          |
|    total_timesteps | 4900000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | -13.8    |
| time/              |          |
|    fps             | 659      |
|    iterations      | 152      |
|    time_elapsed    | 7433     |
|    total_timesteps | 4905344  |
---------------------------------
Eval num_timesteps=4920000, episode_reward=-19.90 +/- 31.38
Episode length: 1846.60 +/- 422.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.85e+03      |
|    mean_reward          | -19.9         |
| time/                   |               |
|    total_timesteps      | 4920000       |
| train/                  |               |
|    approx_kl            | 0.00092485896 |
|    clip_fraction        | 0             |
|    clip_range           | 0.305         |
|    entropy_loss         | -1.08         |
|    explained_variance   | 0             |
|    learning_rate        | 0.000888      |
|    loss                 | 1.93          |
|    n_updates            | 1064          |
|    policy_gradient_loss | -8.91e-05     |
|    value_loss           | 2.38          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | -11.7    |
| time/              |          |
|    fps             | 660      |
|    iterations      | 153      |
|    time_elapsed    | 7478     |
|    total_timesteps | 4937616  |
---------------------------------
Eval num_timesteps=4940000, episode_reward=-15.90 +/- 41.45
Episode length: 1717.20 +/- 410.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.72e+03     |
|    mean_reward          | -15.9        |
| time/                   |              |
|    total_timesteps      | 4940000      |
| train/                  |              |
|    approx_kl            | 0.0012657288 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 0.155        |
|    n_updates            | 1071         |
|    policy_gradient_loss | -0.000202    |
|    value_loss           | 2.4          |
------------------------------------------
Eval num_timesteps=4960000, episode_reward=-36.20 +/- 27.36
Episode length: 1494.40 +/- 295.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.49e+03 |
|    mean_reward     | -36.2    |
| time/              |          |
|    total_timesteps | 4960000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.6e+03  |
|    ep_rew_mean     | -13.5    |
| time/              |          |
|    fps             | 660      |
|    iterations      | 154      |
|    time_elapsed    | 7527     |
|    total_timesteps | 4969888  |
---------------------------------
Eval num_timesteps=4980000, episode_reward=-6.60 +/- 39.58
Episode length: 1595.60 +/- 449.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.6e+03      |
|    mean_reward          | -6.6         |
| time/                   |              |
|    total_timesteps      | 4980000      |
| train/                  |              |
|    approx_kl            | 0.0016434316 |
|    clip_fraction        | 0            |
|    clip_range           | 0.305        |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0            |
|    learning_rate        | 0.000888     |
|    loss                 | 1.04         |
|    n_updates            | 1078         |
|    policy_gradient_loss | -0.000221    |
|    value_loss           | 2.5          |
------------------------------------------
Eval num_timesteps=5000000, episode_reward=17.20 +/- 34.41
Episode length: 1532.40 +/- 242.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.53e+03 |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 5000000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.6e+03  |
|    ep_rew_mean     | -11.7    |
| time/              |          |
|    fps             | 660      |
|    iterations      | 155      |
|    time_elapsed    | 7577     |
|    total_timesteps | 5002160  |
---------------------------------