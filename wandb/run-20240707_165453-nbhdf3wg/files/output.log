Using cpu device
Logging to ./logs/PPO_13
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 743      |
|    ep_rew_mean     | 19.9     |
| time/              |          |
|    fps             | 202      |
|    iterations      | 1        |
|    time_elapsed    | 40       |
|    total_timesteps | 8192     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 679         |
|    ep_rew_mean          | 19.9        |
| time/                   |             |
|    fps                  | 61          |
|    iterations           | 2           |
|    time_elapsed         | 268         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.016216923 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.0509      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.18        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00368    |
|    value_loss           | 3.3         |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=1.60 +/- 49.48
Episode length: 579.40 +/- 43.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 579          |
|    mean_reward          | 1.6          |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0038200347 |
|    clip_fraction        | 0.0237       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.755        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.11         |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00125     |
|    value_loss           | 5.49         |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 674      |
|    ep_rew_mean     | 22.5     |
| time/              |          |
|    fps             | 47       |
|    iterations      | 3        |
|    time_elapsed    | 513      |
|    total_timesteps | 24576    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 675         |
|    ep_rew_mean          | 22.9        |
| time/                   |             |
|    fps                  | 44          |
|    iterations           | 4           |
|    time_elapsed         | 738         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.007387816 |
|    clip_fraction        | 0.0851      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.799       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.85        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00415    |
|    value_loss           | 7.18        |
