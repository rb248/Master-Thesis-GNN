Using cuda device
Logging to ./logs/pong/PPO_2
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 796      |
|    ep_rew_mean     | -63.2    |
| time/              |          |
|    fps             | 1111     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 739          |
|    ep_rew_mean          | -67.9        |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 2            |
|    time_elapsed         | 21           |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 5.007728e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 7.51         |
|    n_updates            | 10           |
|    policy_gradient_loss | 9.13e-07     |
|    value_loss           | 14.3         |
------------------------------------------
Eval num_timesteps=20000, episode_reward=-84.20 +/- 15.27
Episode length: 686.40 +/- 185.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 686           |
|    mean_reward          | -84.2         |
| time/                   |               |
|    total_timesteps      | 20000         |
| train/                  |               |
|    approx_kl            | 0.00042503752 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 10            |
|    n_updates            | 20            |
|    policy_gradient_loss | -0.000206     |
|    value_loss           | 16.2          |
-------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 764      |
|    ep_rew_mean     | -65.4    |
| time/              |          |
|    fps             | 662      |
|    iterations      | 3        |
|    time_elapsed    | 37       |
|    total_timesteps | 24576    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 768         |
|    ep_rew_mean          | -64.7       |
| time/                   |             |
|    fps                  | 664         |
|    iterations           | 4           |
|    time_elapsed         | 49          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.000588513 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 7.23        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00022    |
|    value_loss           | 14.2        |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=-71.40 +/- 16.85
Episode length: 590.40 +/- 132.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 590          |
|    mean_reward          | -71.4        |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 2.287372e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.99         |
|    n_updates            | 40           |
|    policy_gradient_loss | 1.54e-05     |
|    value_loss           | 13.5         |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 766      |
|    ep_rew_mean     | -64.6    |
| time/              |          |
|    fps             | 636      |
|    iterations      | 5        |
|    time_elapsed    | 64       |
|    total_timesteps | 40960    |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 760           |
|    ep_rew_mean          | -65.7         |
| time/                   |               |
|    fps                  | 632           |
|    iterations           | 6             |
|    time_elapsed         | 77            |
|    total_timesteps      | 49152         |
| train/                  |               |
|    approx_kl            | 0.00024379371 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.1          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 7.62          |
|    n_updates            | 50            |
|    policy_gradient_loss | -7.69e-05     |
|    value_loss           | 14.5          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 769           |
|    ep_rew_mean          | -65.6         |
| time/                   |               |
|    fps                  | 627           |
|    iterations           | 7             |
|    time_elapsed         | 91            |
|    total_timesteps      | 57344         |
| train/                  |               |
|    approx_kl            | 5.5400902e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.1          |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 3.91          |
|    n_updates            | 60            |
|    policy_gradient_loss | -1.43e-05     |
|    value_loss           | 12.4          |
