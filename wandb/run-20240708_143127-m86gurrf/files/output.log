
Using cuda device
Logging to ./logs/pong/PPO_5
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 846      |
|    ep_rew_mean     | -56.2    |
| time/              |          |
|    fps             | 1303     |
|    iterations      | 1        |
|    time_elapsed    | 6        |
|    total_timesteps | 8192     |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 806           |
|    ep_rew_mean          | -63.7         |
| time/                   |               |
|    fps                  | 844           |
|    iterations           | 2             |
|    time_elapsed         | 19            |
|    total_timesteps      | 16384         |
| train/                  |               |
|    approx_kl            | 3.7804639e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.5           |
|    n_updates            | 10            |
|    policy_gradient_loss | 2.18e-05      |
|    value_loss           | 14.1          |
-------------------------------------------
Eval num_timesteps=20000, episode_reward=-69.00 +/- 18.53
Episode length: 631.80 +/- 131.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 632          |
|    mean_reward          | -69          |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0002834844 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 6.21         |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.000123    |
|    value_loss           | 13.3         |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 822      |
|    ep_rew_mean     | -65      |
| time/              |          |
|    fps             | 695      |
|    iterations      | 3        |
|    time_elapsed    | 35       |
|    total_timesteps | 24576    |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 816           |
|    ep_rew_mean          | -65.1         |
| time/                   |               |
|    fps                  | 685           |
|    iterations           | 4             |
|    time_elapsed         | 47            |
|    total_timesteps      | 32768         |
| train/                  |               |
|    approx_kl            | 2.9250004e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 6.99          |
|    n_updates            | 30            |
|    policy_gradient_loss | 2.46e-05      |
|    value_loss           | 11.8          |
