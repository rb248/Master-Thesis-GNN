Using cuda device
Logging to ./logs/Pong-GNN/PPO_7
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 756      |
|    ep_rew_mean     | -63.4    |
| time/              |          |
|    fps             | 960      |
|    iterations      | 1        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 765         |
|    ep_rew_mean          | -62.5       |
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 2           |
|    time_elapsed         | 143         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.009110045 |
|    clip_fraction        | 0.0196      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | -0.0107     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.015       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00146    |
|    value_loss           | 0.087       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 784         |
|    ep_rew_mean          | -63.6       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 3           |
|    time_elapsed         | 273         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.008408057 |
|    clip_fraction        | 0.0259      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.747       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00717     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00229    |
|    value_loss           | 0.053       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 874         |
|    ep_rew_mean          | -60.3       |
| time/                   |             |
|    fps                  | 161         |
|    iterations           | 4           |
|    time_elapsed         | 405         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.010247756 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.055       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 0.0684      |
-----------------------------------------
Traceback (most recent call last):
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/work/rleap1/rishabh.bhatia/Master-Thesis-GNN/games/pong/run_supervised_gnn.py", line 114, in <module>
    model.learn(total_timesteps=1000000, callback=[callback, wandb_callback])
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 300, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 201, in collect_rollouts
    if not callback.on_step():
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py", line 114, in on_step
    return self._on_step()
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py", line 219, in _on_step
    continue_training = callback.on_step() and continue_training
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py", line 114, in on_step
    return self._on_step()
  File "/work/rleap1/rishabh.bhatia/Master-Thesis-GNN/games/pong/run_supervised_gnn.py", line 37, in _on_step
    x, y = ts2xy(load_results(self.log_dir), "timesteps")
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/monitor.py", line 239, in load_results
    raise LoadMonitorResultsError(f"No monitor files of the form *{Monitor.EXT} found in {path}")
stable_baselines3.common.monitor.LoadMonitorResultsError: No monitor files of the form *monitor.csv found in ./logs/Pong-GNN/