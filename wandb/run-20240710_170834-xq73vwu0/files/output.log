
Using cuda device
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f32bc310130> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f3269e35510>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 669      |
|    ep_rew_mean     | -68.9    |
| time/              |          |
|    fps             | 1292     |
|    iterations      | 1        |
|    time_elapsed    | 12       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 693         |
|    ep_rew_mean          | -65.4       |
| time/                   |             |
|    fps                  | 775         |
|    iterations           | 2           |
|    time_elapsed         | 42          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.009592387 |
|    clip_fraction        | 0.0802      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 4.8         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.000662   |
|    value_loss           | 11.4        |
-----------------------------------------
Num timesteps: 40000
Best mean reward: -inf - Last mean reward per episode: -66.38
Saving new best model at 36369 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:250: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  data_frame = pandas.concat(data_frames)
Eval num_timesteps=40000, episode_reward=-22.90 +/- 31.37
Episode length: 2507.60 +/- 984.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2.51e+03   |
|    mean_reward          | -22.9      |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.00699314 |
|    clip_fraction        | 0.071      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0.519      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.31       |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.000876  |
|    value_loss           | 8.26       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 698      |
|    ep_rew_mean     | -68.9    |
| time/              |          |
|    fps             | 494      |
|    iterations      | 3        |
|    time_elapsed    | 99       |
|    total_timesteps | 49152    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 701          |
|    ep_rew_mean          | -66.6        |
| time/                   |              |
|    fps                  | 509          |
|    iterations           | 4            |
|    time_elapsed         | 128          |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0060532666 |
|    clip_fraction        | 0.0507       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.675        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.76         |
|    n_updates            | 30           |
|    policy_gradient_loss | 0.000176     |
|    value_loss           | 7.09         |
------------------------------------------
Num timesteps: 80000
Best mean reward: -66.38 - Last mean reward per episode: -63.13
Saving new best model at 88283 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=80000, episode_reward=-6.20 +/- 40.43
Episode length: 2530.20 +/- 939.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.53e+03     |
|    mean_reward          | -6.2         |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0069295727 |
|    clip_fraction        | 0.0763       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.685        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.92         |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.000692    |
|    value_loss           | 7.5          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 719      |
|    ep_rew_mean     | -66      |
| time/              |          |
|    fps             | 434      |
|    iterations      | 5        |
|    time_elapsed    | 188      |
|    total_timesteps | 81920    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 734        |
|    ep_rew_mean          | -66.4      |
| time/                   |            |
|    fps                  | 449        |
|    iterations           | 6          |
|    time_elapsed         | 218        |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.00728489 |
|    clip_fraction        | 0.0782     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0.686      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.64       |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.00263   |
|    value_loss           | 7.17       |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 757          |
|    ep_rew_mean          | -65.5        |
| time/                   |              |
|    fps                  | 462          |
|    iterations           | 7            |
|    time_elapsed         | 248          |
|    total_timesteps      | 114688       |
| train/                  |              |
|    approx_kl            | 0.0073710056 |
|    clip_fraction        | 0.088        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.07        |
|    explained_variance   | 0.722        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.81         |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00289     |
|    value_loss           | 6.69         |
------------------------------------------
Num timesteps: 120000
Best mean reward: -63.13 - Last mean reward per episode: -58.88
Saving new best model at 142240 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=120000, episode_reward=-49.50 +/- 42.04
Episode length: 1154.60 +/- 938.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.15e+03     |
|    mean_reward          | -49.5        |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 0.0070312973 |
|    clip_fraction        | 0.0736       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.07        |
|    explained_variance   | 0.708        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.88         |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00178     |
|    value_loss           | 6.41         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 796      |
|    ep_rew_mean     | -60      |
| time/              |          |
|    fps             | 450      |
|    iterations      | 8        |
|    time_elapsed    | 290      |
|    total_timesteps | 131072   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 843         |
|    ep_rew_mean          | -56.4       |
| time/                   |             |
|    fps                  | 460         |
|    iterations           | 9           |
|    time_elapsed         | 320         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.008330785 |
|    clip_fraction        | 0.0859      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.723       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.63        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00303    |
|    value_loss           | 5.88        |
-----------------------------------------
Num timesteps: 160000
Best mean reward: -58.88 - Last mean reward per episode: -50.10
Saving new best model at 185573 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=160000, episode_reward=-57.40 +/- 32.28
Episode length: 1180.40 +/- 923.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.18e+03     |
|    mean_reward          | -57.4        |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0074363225 |
|    clip_fraction        | 0.0764       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0.756        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.9          |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00348     |
|    value_loss           | 6.17         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 886      |
|    ep_rew_mean     | -53.2    |
| time/              |          |
|    fps             | 451      |
|    iterations      | 10       |
|    time_elapsed    | 363      |
|    total_timesteps | 163840   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 939         |
|    ep_rew_mean          | -46.2       |
| time/                   |             |
|    fps                  | 458         |
|    iterations           | 11          |
|    time_elapsed         | 393         |
|    total_timesteps      | 180224      |
| train/                  |             |
|    approx_kl            | 0.007756562 |
|    clip_fraction        | 0.0717      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.726       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.98        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00223    |
|    value_loss           | 6.57        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 985          |
|    ep_rew_mean          | -40.5        |
| time/                   |              |
|    fps                  | 464          |
|    iterations           | 12           |
|    time_elapsed         | 422          |
|    total_timesteps      | 196608       |
| train/                  |              |
|    approx_kl            | 0.0077961767 |
|    clip_fraction        | 0.0745       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0.76         |
|    learning_rate        | 0.0003       |
|    loss                 | 3.61         |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00446     |
|    value_loss           | 6.15         |
------------------------------------------
Num timesteps: 200000
Best mean reward: -50.10 - Last mean reward per episode: -37.80
Saving new best model at 229902 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=200000, episode_reward=-76.90 +/- 12.74
Episode length: 624.60 +/- 87.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 625         |
|    mean_reward          | -76.9       |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.008994542 |
|    clip_fraction        | 0.0936      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.65        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00395    |
|    value_loss           | 5.96        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.08e+03 |
|    ep_rew_mean     | -35.2    |
| time/              |          |
|    fps             | 463      |
|    iterations      | 13       |
|    time_elapsed    | 459      |
|    total_timesteps | 212992   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.13e+03    |
|    ep_rew_mean          | -33         |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 14          |
|    time_elapsed         | 489         |
|    total_timesteps      | 229376      |
| train/                  |             |
|    approx_kl            | 0.008466862 |
|    clip_fraction        | 0.0859      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.04        |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00372    |
|    value_loss           | 5.31        |
-----------------------------------------
Num timesteps: 240000
Best mean reward: -37.80 - Last mean reward per episode: -32.77
Saving new best model at 272302 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=240000, episode_reward=-62.60 +/- 20.69
Episode length: 1244.60 +/- 880.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.24e+03    |
|    mean_reward          | -62.6       |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.009004803 |
|    clip_fraction        | 0.0863      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.2         |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00473    |
|    value_loss           | 4.73        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.19e+03 |
|    ep_rew_mean     | -27.2    |
| time/              |          |
|    fps             | 461      |
|    iterations      | 15       |
|    time_elapsed    | 532      |
|    total_timesteps | 245760   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.24e+03    |
|    ep_rew_mean          | -23.9       |
| time/                   |             |
|    fps                  | 466         |
|    iterations           | 16          |
|    time_elapsed         | 562         |
|    total_timesteps      | 262144      |
| train/                  |             |
|    approx_kl            | 0.009798749 |
|    clip_fraction        | 0.097       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.788       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.04        |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00393    |
|    value_loss           | 4.02        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.34e+03    |
|    ep_rew_mean          | -16         |
| time/                   |             |
|    fps                  | 471         |
|    iterations           | 17          |
|    time_elapsed         | 591         |
|    total_timesteps      | 278528      |
| train/                  |             |
|    approx_kl            | 0.007920507 |
|    clip_fraction        | 0.0877      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.992      |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.51        |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00374    |
|    value_loss           | 3.78        |
-----------------------------------------
Num timesteps: 280000
Best mean reward: -32.77 - Last mean reward per episode: -18.25
Saving new best model at 322220 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=280000, episode_reward=-78.40 +/- 11.79
Episode length: 677.00 +/- 105.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 677         |
|    mean_reward          | -78.4       |
| time/                   |             |
|    total_timesteps      | 280000      |
| train/                  |             |
|    approx_kl            | 0.009520474 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.998      |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.94        |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00391    |
|    value_loss           | 4.01        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.4e+03  |
|    ep_rew_mean     | -9.81    |
| time/              |          |
|    fps             | 469      |
|    iterations      | 18       |
|    time_elapsed    | 627      |
|    total_timesteps | 294912   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.47e+03    |
|    ep_rew_mean          | -5.26       |
| time/                   |             |
|    fps                  | 473         |
|    iterations           | 19          |
|    time_elapsed         | 657         |
|    total_timesteps      | 311296      |
| train/                  |             |
|    approx_kl            | 0.009316739 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.946      |
|    explained_variance   | 0.717       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.3         |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00549    |
|    value_loss           | 3.8         |
-----------------------------------------
Num timesteps: 320000
Best mean reward: -18.25 - Last mean reward per episode: -9.26
Saving new best model at 360384 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=320000, episode_reward=19.00 +/- 19.86
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 19          |
| time/                   |             |
|    total_timesteps      | 320000      |
| train/                  |             |
|    approx_kl            | 0.009395888 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.949      |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.817       |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0054     |
|    value_loss           | 2.53        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.55e+03 |
|    ep_rew_mean     | 0.29     |
| time/              |          |
|    fps             | 455      |
|    iterations      | 20       |
|    time_elapsed    | 718      |
|    total_timesteps | 327680   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.59e+03   |
|    ep_rew_mean          | 7.22       |
| time/                   |            |
|    fps                  | 459        |
|    iterations           | 21         |
|    time_elapsed         | 748        |
|    total_timesteps      | 344064     |
| train/                  |            |
|    approx_kl            | 0.01061288 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.946     |
|    explained_variance   | 0.784      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.03       |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.00613   |
|    value_loss           | 2.93       |
----------------------------------------
Num timesteps: 360000
Best mean reward: -9.26 - Last mean reward per episode: 6.21
Saving new best model at 413133 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=360000, episode_reward=26.80 +/- 10.53
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 26.8        |
| time/                   |             |
|    total_timesteps      | 360000      |
| train/                  |             |
|    approx_kl            | 0.010492167 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.929      |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.07        |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00584    |
|    value_loss           | 2.74        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | 9.65     |
| time/              |          |
|    fps             | 444      |
|    iterations      | 22       |
|    time_elapsed    | 810      |
|    total_timesteps | 360448   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.72e+03     |
|    ep_rew_mean          | 12.2         |
| time/                   |              |
|    fps                  | 448          |
|    iterations           | 23           |
|    time_elapsed         | 840          |
|    total_timesteps      | 376832       |
| train/                  |              |
|    approx_kl            | 0.0111196395 |
|    clip_fraction        | 0.112        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.891       |
|    explained_variance   | 0.755        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.02         |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00642     |
|    value_loss           | 2.88         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.77e+03    |
|    ep_rew_mean          | 15          |
| time/                   |             |
|    fps                  | 452         |
|    iterations           | 24          |
|    time_elapsed         | 869         |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.011567067 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.86       |
|    explained_variance   | 0.754       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.735       |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00645    |
|    value_loss           | 2.7         |
-----------------------------------------
Num timesteps: 400000
Best mean reward: 6.21 - Last mean reward per episode: 17.41
Saving new best model at 469990 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=400000, episode_reward=-5.90 +/- 19.33
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | -5.9        |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.010806158 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.823      |
|    explained_variance   | 0.701       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.649       |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00783    |
|    value_loss           | 2.18        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.83e+03 |
|    ep_rew_mean     | 18.4     |
| time/              |          |
|    fps             | 439      |
|    iterations      | 25       |
|    time_elapsed    | 932      |
|    total_timesteps | 409600   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.9e+03     |
|    ep_rew_mean          | 22.4        |
| time/                   |             |
|    fps                  | 443         |
|    iterations           | 26          |
|    time_elapsed         | 961         |
|    total_timesteps      | 425984      |
| train/                  |             |
|    approx_kl            | 0.012836676 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.843      |
|    explained_variance   | 0.761       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.422       |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00746    |
|    value_loss           | 1.86        |
-----------------------------------------
Num timesteps: 440000
Best mean reward: 17.41 - Last mean reward per episode: 21.68
Saving new best model at 525905 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=440000, episode_reward=22.90 +/- 16.39
Episode length: 3000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 3e+03      |
|    mean_reward          | 22.9       |
| time/                   |            |
|    total_timesteps      | 440000     |
| train/                  |            |
|    approx_kl            | 0.01319203 |
|    clip_fraction        | 0.129      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.812     |
|    explained_variance   | 0.749      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.317      |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.00884   |
|    value_loss           | 1.69       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | 25.7     |
| time/              |          |
|    fps             | 432      |
|    iterations      | 27       |
|    time_elapsed    | 1022     |
|    total_timesteps | 442368   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2e+03       |
|    ep_rew_mean          | 26.7        |
| time/                   |             |
|    fps                  | 436         |
|    iterations           | 28          |
|    time_elapsed         | 1052        |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.013152904 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.841      |
|    explained_variance   | 0.739       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.723       |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00852    |
|    value_loss           | 2.31        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.04e+03    |
|    ep_rew_mean          | 29          |
| time/                   |             |
|    fps                  | 439         |
|    iterations           | 29          |
|    time_elapsed         | 1080        |
|    total_timesteps      | 475136      |
| train/                  |             |
|    approx_kl            | 0.014365446 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.843      |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.278       |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00889    |
|    value_loss           | 1.99        |
-----------------------------------------
Num timesteps: 480000
Best mean reward: 21.68 - Last mean reward per episode: 27.12
Saving new best model at 581526 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=480000, episode_reward=12.70 +/- 35.95
Episode length: 2522.60 +/- 954.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2.52e+03   |
|    mean_reward          | 12.7       |
| time/                   |            |
|    total_timesteps      | 480000     |
| train/                  |            |
|    approx_kl            | 0.01380094 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.797     |
|    explained_variance   | 0.716      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.435      |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.00946   |
|    value_loss           | 1.9        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.08e+03 |
|    ep_rew_mean     | 30.5     |
| time/              |          |
|    fps             | 432      |
|    iterations      | 30       |
|    time_elapsed    | 1136     |
|    total_timesteps | 491520   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.12e+03     |
|    ep_rew_mean          | 33.2         |
| time/                   |              |
|    fps                  | 435          |
|    iterations           | 31           |
|    time_elapsed         | 1166         |
|    total_timesteps      | 507904       |
| train/                  |              |
|    approx_kl            | 0.0138411205 |
|    clip_fraction        | 0.138        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.807       |
|    explained_variance   | 0.768        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.398        |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.0116      |
|    value_loss           | 1.49         |
------------------------------------------
Num timesteps: 520000
Best mean reward: 27.12 - Last mean reward per episode: 30.11
Saving new best model at 630741 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=520000, episode_reward=-17.10 +/- 36.68
Episode length: 2045.80 +/- 1168.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.05e+03    |
|    mean_reward          | -17.1       |
| time/                   |             |
|    total_timesteps      | 520000      |
| train/                  |             |
|    approx_kl            | 0.015324262 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.811      |
|    explained_variance   | 0.78        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.254       |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 1.52        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.14e+03 |
|    ep_rew_mean     | 36.4     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 32       |
|    time_elapsed    | 1218     |
|    total_timesteps | 524288   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.17e+03    |
|    ep_rew_mean          | 36.2        |
| time/                   |             |
|    fps                  | 433         |
|    iterations           | 33          |
|    time_elapsed         | 1248        |
|    total_timesteps      | 540672      |
| train/                  |             |
|    approx_kl            | 0.015088205 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.778      |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.235       |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.913       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.2e+03     |
|    ep_rew_mean          | 37.5        |
| time/                   |             |
|    fps                  | 436         |
|    iterations           | 34          |
|    time_elapsed         | 1277        |
|    total_timesteps      | 557056      |
| train/                  |             |
|    approx_kl            | 0.015274625 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.792      |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.28        |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 1.62        |
-----------------------------------------
Num timesteps: 560000
Best mean reward: 30.11 - Last mean reward per episode: 33.02
Saving new best model at 683205 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=560000, episode_reward=6.50 +/- 19.84
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 6.5         |
| time/                   |             |
|    total_timesteps      | 560000      |
| train/                  |             |
|    approx_kl            | 0.016193252 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.849      |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.688       |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 1.78        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.21e+03 |
|    ep_rew_mean     | 38.9     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 35       |
|    time_elapsed    | 1338     |
|    total_timesteps | 573440   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.18e+03    |
|    ep_rew_mean          | 44.5        |
| time/                   |             |
|    fps                  | 431         |
|    iterations           | 36          |
|    time_elapsed         | 1367        |
|    total_timesteps      | 589824      |
| train/                  |             |
|    approx_kl            | 0.017231269 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.843      |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.284       |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 1.57        |
-----------------------------------------
Num timesteps: 600000
Best mean reward: 33.02 - Last mean reward per episode: 41.96
Saving new best model at 735527 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=600000, episode_reward=-23.20 +/- 53.40
Episode length: 2001.40 +/- 1224.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | -23.2       |
| time/                   |             |
|    total_timesteps      | 600000      |
| train/                  |             |
|    approx_kl            | 0.016330637 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.865      |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.3         |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 1.6         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.21e+03 |
|    ep_rew_mean     | 48.4     |
| time/              |          |
|    fps             | 427      |
|    iterations      | 37       |
|    time_elapsed    | 1417     |
|    total_timesteps | 606208   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.18e+03    |
|    ep_rew_mean          | 52.8        |
| time/                   |             |
|    fps                  | 430         |
|    iterations           | 38          |
|    time_elapsed         | 1446        |
|    total_timesteps      | 622592      |
| train/                  |             |
|    approx_kl            | 0.017319128 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.843      |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.191       |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 1.22        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.19e+03    |
|    ep_rew_mean          | 54.4        |
| time/                   |             |
|    fps                  | 433         |
|    iterations           | 39          |
|    time_elapsed         | 1475        |
|    total_timesteps      | 638976      |
| train/                  |             |
|    approx_kl            | 0.016959352 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.836      |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.683       |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 1.39        |
-----------------------------------------
Num timesteps: 640000
Best mean reward: 41.96 - Last mean reward per episode: 48.16
Saving new best model at 786411 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=640000, episode_reward=10.30 +/- 40.51
Episode length: 2492.00 +/- 1016.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2.49e+03   |
|    mean_reward          | 10.3       |
| time/                   |            |
|    total_timesteps      | 640000     |
| train/                  |            |
|    approx_kl            | 0.01726909 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.828     |
|    explained_variance   | 0.761      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.473      |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.0156    |
|    value_loss           | 1.11       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.21e+03 |
|    ep_rew_mean     | 56.1     |
| time/              |          |
|    fps             | 427      |
|    iterations      | 40       |
|    time_elapsed    | 1531     |
|    total_timesteps | 655360   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22e+03    |
|    ep_rew_mean          | 60.4        |
| time/                   |             |
|    fps                  | 430         |
|    iterations           | 41          |
|    time_elapsed         | 1561        |
|    total_timesteps      | 671744      |
| train/                  |             |
|    approx_kl            | 0.018732881 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.829      |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.16        |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.93        |
-----------------------------------------
Num timesteps: 680000
Best mean reward: 48.16 - Last mean reward per episode: 50.34
Saving new best model at 840096 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=680000, episode_reward=-5.30 +/- 39.31
Episode length: 2543.40 +/- 913.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.54e+03    |
|    mean_reward          | -5.3        |
| time/                   |             |
|    total_timesteps      | 680000      |
| train/                  |             |
|    approx_kl            | 0.020708153 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.85       |
|    explained_variance   | 0.818       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.173       |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 0.983       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.19e+03 |
|    ep_rew_mean     | 63       |
| time/              |          |
|    fps             | 425      |
|    iterations      | 42       |
|    time_elapsed    | 1617     |
|    total_timesteps | 688128   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.19e+03    |
|    ep_rew_mean          | 64          |
| time/                   |             |
|    fps                  | 427         |
|    iterations           | 43          |
|    time_elapsed         | 1647        |
|    total_timesteps      | 704512      |
| train/                  |             |
|    approx_kl            | 0.018943343 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.83       |
|    explained_variance   | 0.78        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.493       |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 1.28        |
-----------------------------------------
Num timesteps: 720000
Best mean reward: 50.34 - Last mean reward per episode: 53.03
Saving new best model at 895513 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=720000, episode_reward=7.30 +/- 29.31
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 7.3         |
| time/                   |             |
|    total_timesteps      | 720000      |
| train/                  |             |
|    approx_kl            | 0.022382181 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.844      |
|    explained_variance   | 0.803       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.368       |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0186     |
|    value_loss           | 1.32        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.24e+03 |
|    ep_rew_mean     | 65.1     |
| time/              |          |
|    fps             | 422      |
|    iterations      | 44       |
|    time_elapsed    | 1708     |
|    total_timesteps | 720896   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.23e+03   |
|    ep_rew_mean          | 65.6       |
| time/                   |            |
|    fps                  | 424        |
|    iterations           | 45         |
|    time_elapsed         | 1737       |
|    total_timesteps      | 737280     |
| train/                  |            |
|    approx_kl            | 0.02163595 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.838     |
|    explained_variance   | 0.824      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0529     |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.021     |
|    value_loss           | 0.721      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22e+03    |
|    ep_rew_mean          | 66.6        |
| time/                   |             |
|    fps                  | 426         |
|    iterations           | 46          |
|    time_elapsed         | 1767        |
|    total_timesteps      | 753664      |
| train/                  |             |
|    approx_kl            | 0.021170778 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.833      |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.159       |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 1.03        |
-----------------------------------------
Num timesteps: 760000
Best mean reward: 53.03 - Last mean reward per episode: 54.44
Saving new best model at 948030 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=760000, episode_reward=-20.90 +/- 61.56
Episode length: 1995.20 +/- 1230.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | -20.9       |
| time/                   |             |
|    total_timesteps      | 760000      |
| train/                  |             |
|    approx_kl            | 0.021151688 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.826      |
|    explained_variance   | 0.823       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.185       |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 1.1         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.24e+03 |
|    ep_rew_mean     | 69.1     |
| time/              |          |
|    fps             | 423      |
|    iterations      | 47       |
|    time_elapsed    | 1817     |
|    total_timesteps | 770048   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.25e+03    |
|    ep_rew_mean          | 70          |
| time/                   |             |
|    fps                  | 425         |
|    iterations           | 48          |
|    time_elapsed         | 1846        |
|    total_timesteps      | 786432      |
| train/                  |             |
|    approx_kl            | 0.020953719 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.82       |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.288       |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 1.34        |
-----------------------------------------
Num timesteps: 800000
Best mean reward: 54.44 - Last mean reward per episode: 55.05
Saving new best model at 998237 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=800000, episode_reward=9.20 +/- 16.48
Episode length: 3000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 3e+03      |
|    mean_reward          | 9.2        |
| time/                   |            |
|    total_timesteps      | 800000     |
| train/                  |            |
|    approx_kl            | 0.02265251 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.815     |
|    explained_variance   | 0.782      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.206      |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.0183    |
|    value_loss           | 1.19       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.21e+03 |
|    ep_rew_mean     | 69.9     |
| time/              |          |
|    fps             | 420      |
|    iterations      | 49       |
|    time_elapsed    | 1907     |
|    total_timesteps | 802816   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22e+03    |
|    ep_rew_mean          | 70.6        |
| time/                   |             |
|    fps                  | 422         |
|    iterations           | 50          |
|    time_elapsed         | 1936        |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.025160646 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.789      |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.25        |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 1.34        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.2e+03     |
|    ep_rew_mean          | 71          |
| time/                   |             |
|    fps                  | 424         |
|    iterations           | 51          |
|    time_elapsed         | 1966        |
|    total_timesteps      | 835584      |
| train/                  |             |
|    approx_kl            | 0.024590272 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.781      |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.104       |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0197     |
|    value_loss           | 0.978       |
-----------------------------------------
Num timesteps: 840000
Best mean reward: 55.05 - Last mean reward per episode: 56.29
Saving new best model at 1053731 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=840000, episode_reward=-10.20 +/- 43.06
Episode length: 2485.40 +/- 1029.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.49e+03    |
|    mean_reward          | -10.2       |
| time/                   |             |
|    total_timesteps      | 840000      |
| train/                  |             |
|    approx_kl            | 0.024001058 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.798      |
|    explained_variance   | 0.816       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.178       |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 1.05        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.2e+03  |
|    ep_rew_mean     | 71.9     |
| time/              |          |
|    fps             | 421      |
|    iterations      | 52       |
|    time_elapsed    | 2022     |
|    total_timesteps | 851968   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.15e+03    |
|    ep_rew_mean          | 73.1        |
| time/                   |             |
|    fps                  | 423         |
|    iterations           | 53          |
|    time_elapsed         | 2051        |
|    total_timesteps      | 868352      |
| train/                  |             |
|    approx_kl            | 0.025001876 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.788      |
|    explained_variance   | 0.812       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.167       |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.0225     |
|    value_loss           | 1.03        |
-----------------------------------------
Num timesteps: 880000
Best mean reward: 56.29 - Last mean reward per episode: 60.18
Saving new best model at 1105824 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=880000, episode_reward=-67.60 +/- 35.55
Episode length: 1130.20 +/- 935.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.13e+03    |
|    mean_reward          | -67.6       |
| time/                   |             |
|    total_timesteps      | 880000      |
| train/                  |             |
|    approx_kl            | 0.024573084 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.784      |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.374       |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 1.27        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.13e+03 |
|    ep_rew_mean     | 73.2     |
| time/              |          |
|    fps             | 422      |
|    iterations      | 54       |
|    time_elapsed    | 2094     |
|    total_timesteps | 884736   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.13e+03    |
|    ep_rew_mean          | 75.5        |
| time/                   |             |
|    fps                  | 424         |
|    iterations           | 55          |
|    time_elapsed         | 2123        |
|    total_timesteps      | 901120      |
| train/                  |             |
|    approx_kl            | 0.024433924 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.786      |
|    explained_variance   | 0.817       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.226       |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0215     |
|    value_loss           | 1.17        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.09e+03    |
|    ep_rew_mean          | 77.5        |
| time/                   |             |
|    fps                  | 425         |
|    iterations           | 56          |
|    time_elapsed         | 2157        |
|    total_timesteps      | 917504      |
| train/                  |             |
|    approx_kl            | 0.027281456 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.776      |
|    explained_variance   | 0.814       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.195       |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.987       |
-----------------------------------------
Num timesteps: 920000
Best mean reward: 60.18 - Last mean reward per episode: 58.94
Eval num_timesteps=920000, episode_reward=-51.30 +/- 30.18
Episode length: 1633.60 +/- 1123.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.63e+03    |
|    mean_reward          | -51.3       |
| time/                   |             |
|    total_timesteps      | 920000      |
| train/                  |             |
|    approx_kl            | 0.024419922 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.767      |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.158       |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.021      |
|    value_loss           | 1.14        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.04e+03 |
|    ep_rew_mean     | 78.3     |
| time/              |          |
|    fps             | 423      |
|    iterations      | 57       |
|    time_elapsed    | 2204     |
|    total_timesteps | 933888   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2e+03       |
|    ep_rew_mean          | 80          |
| time/                   |             |
|    fps                  | 425         |
|    iterations           | 58          |
|    time_elapsed         | 2234        |
|    total_timesteps      | 950272      |
| train/                  |             |
|    approx_kl            | 0.026411107 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.763      |
|    explained_variance   | 0.794       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.176       |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 1.31        |
-----------------------------------------
Num timesteps: 960000
Best mean reward: 60.18 - Last mean reward per episode: 61.62
Saving new best model at 1201055 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=960000, episode_reward=27.70 +/- 9.74
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 27.7        |
| time/                   |             |
|    total_timesteps      | 960000      |
| train/                  |             |
|    approx_kl            | 0.025342578 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.743      |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.351       |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 0.897       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | 81.7     |
| time/              |          |
|    fps             | 420      |
|    iterations      | 59       |
|    time_elapsed    | 2297     |
|    total_timesteps | 966656   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.96e+03   |
|    ep_rew_mean          | 83         |
| time/                   |            |
|    fps                  | 422        |
|    iterations           | 60         |
|    time_elapsed         | 2327       |
|    total_timesteps      | 983040     |
| train/                  |            |
|    approx_kl            | 0.02800306 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.748     |
|    explained_variance   | 0.831      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0898     |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.0247    |
|    value_loss           | 0.856      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.97e+03   |
|    ep_rew_mean          | 84.7       |
| time/                   |            |
|    fps                  | 423        |
|    iterations           | 61         |
|    time_elapsed         | 2357       |
|    total_timesteps      | 999424     |
| train/                  |            |
|    approx_kl            | 0.02714514 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.74      |
|    explained_variance   | 0.816      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.372      |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.0203    |
|    value_loss           | 1.22       |
----------------------------------------
Num timesteps: 1000000
Best mean reward: 61.62 - Last mean reward per episode: 66.70
Saving new best model at 1259264 timesteps
Saving new best model to ./logs/Pong-CNN-training/best_model.zip
Eval num_timesteps=1000000, episode_reward=25.90 +/- 12.33
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | 25.9        |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.026263319 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.705      |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.135       |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.0229     |
|    value_loss           | 0.731       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | 84.2     |
| time/              |          |
|    fps             | 419      |
|    iterations      | 62       |
|    time_elapsed    | 2418     |
|    total_timesteps | 1015808  |
---------------------------------