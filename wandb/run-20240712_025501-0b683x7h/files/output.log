
Using cpu device
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f8aebcb7ee0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f8b482435b0>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
-----------------------------
| time/              |      |
|    fps             | 1093 |
|    iterations      | 1    |
|    time_elapsed    | 7    |
|    total_timesteps | 8192 |
-----------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3e+03      |
|    ep_rew_mean          | -1.5e+03   |
| time/                   |            |
|    fps                  | 371        |
|    iterations           | 2          |
|    time_elapsed         | 44         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.01091888 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.09      |
|    explained_variance   | 2.38e-07   |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0512     |
|    n_updates            | 10         |
|    policy_gradient_loss | 0.00127    |
|    value_loss           | 1.33       |
----------------------------------------
Num timesteps: 20000
Best mean reward: -inf - Last mean reward per episode: -1500.00
Saving new best model at 12000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:250: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  data_frame = pandas.concat(data_frames)
Eval num_timesteps=20000, episode_reward=-1500.00 +/- 0.00
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | -1.5e+03    |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.022097029 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.000558    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.033       |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.00257     |
|    value_loss           | 1.5         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -1.5e+03 |
| time/              |          |
|    fps             | 256      |
|    iterations      | 3        |
|    time_elapsed    | 95       |
|    total_timesteps | 24576    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 248         |
|    iterations           | 4           |
|    time_elapsed         | 131         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.010538621 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.00192     |
|    learning_rate        | 0.0003      |
|    loss                 | 2.04        |
|    n_updates            | 30          |
|    policy_gradient_loss | 0.00206     |
|    value_loss           | 2.2         |
-----------------------------------------
Num timesteps: 40000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
Eval num_timesteps=40000, episode_reward=-1500.00 +/- 0.00
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | -1.5e+03    |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.010182714 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.0181      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0253      |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.00226     |
|    value_loss           | 0.291       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -1.5e+03 |
| time/              |          |
|    fps             | 224      |
|    iterations      | 5        |
|    time_elapsed    | 182      |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3e+03      |
|    ep_rew_mean          | -1.5e+03   |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 6          |
|    time_elapsed         | 217        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.04861313 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.967     |
|    explained_variance   | 0.0012     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.331      |
|    n_updates            | 50         |
|    policy_gradient_loss | 0.00873    |
|    value_loss           | 4.05       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 7           |
|    time_elapsed         | 252         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.056174275 |
|    clip_fraction        | 0.342       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.977      |
|    explained_variance   | 0.00132     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.341       |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.00973     |
|    value_loss           | 5.85        |
-----------------------------------------
Num timesteps: 60000
Best mean reward: -1500.00 - Last mean reward per episode: -1469.00
Saving new best model at 90000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=60000, episode_reward=-1500.00 +/- 0.00
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | -1.5e+03    |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.029070046 |
|    clip_fraction        | 0.371       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.93       |
|    explained_variance   | 0.00324     |
|    learning_rate        | 0.0003      |
|    loss                 | 8.42        |
|    n_updates            | 70          |
|    policy_gradient_loss | 0.012       |
|    value_loss           | 14.9        |
-----------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 3e+03     |
|    ep_rew_mean     | -1.45e+03 |
| time/              |           |
|    fps             | 213       |
|    iterations      | 8         |
|    time_elapsed    | 306       |
|    total_timesteps | 65536     |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | -1.38e+03   |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 9           |
|    time_elapsed         | 343         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.008698134 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.835      |
|    explained_variance   | 0.00169     |
|    learning_rate        | 0.0003      |
|    loss                 | 23.5        |
|    n_updates            | 80          |
|    policy_gradient_loss | 0.00777     |
|    value_loss           | 39.9        |
-----------------------------------------
Num timesteps: 80000
Best mean reward: -1469.00 - Last mean reward per episode: -1425.38
Saving new best model at 117000 timesteps
Saving new best model to ./logs/Freeway-CNN-training/best_model.zip
Eval num_timesteps=80000, episode_reward=-192.00 +/- 30.59
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | -192        |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.012517497 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.806      |
|    explained_variance   | 0.0249      |
|    learning_rate        | 0.0003      |
|    loss                 | 16.1        |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.00761     |
|    value_loss           | 43.8        |
-----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 3e+03     |
|    ep_rew_mean     | -1.38e+03 |
| time/              |           |
|    fps             | 205       |
|    iterations      | 10        |
|    time_elapsed    | 397       |
|    total_timesteps | 81920     |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | -1.27e+03   |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 11          |
|    time_elapsed         | 435         |
|    total_timesteps      | 90112       |
| train/                  |             |
|    approx_kl            | 0.022759391 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.776      |
|    explained_variance   | 0.0205      |
|    learning_rate        | 0.0003      |
|    loss                 | 19.6        |
|    n_updates            | 100         |
|    policy_gradient_loss | 0.0013      |
|    value_loss           | 53.8        |
-----------------------------------------
Traceback (most recent call last):
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/work/rleap1/rishabh.bhatia/Master-Thesis-GNN/games/freeway/run_supervised_cnn.py", line 104, in <module>
    model.learn(total_timesteps=1000000, callback=[callback, eval_callback])
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 313, in learn
    self.train()
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 217, in train
    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 740, in evaluate_actions
    entropy = distribution.entropy()
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/distributions.py", line 295, in entropy
    return self.distribution.entropy()
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/torch/distributions/categorical.py", line 146, in entropy
    p_log_p = logits * self.probs
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/torch/distributions/utils.py", line 126, in __get__
    value = self.wrapped(instance)
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/torch/distributions/categorical.py", line 100, in probs
    return logits_to_probs(self.logits)
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/torch/distributions/utils.py", line 89, in logits_to_probs
    return F.softmax(logits, dim=-1)
  File "/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/torch/nn/functional.py", line 1885, in softmax
    ret = input.softmax(dim)
KeyboardInterrupt