
Using cpu device
/work/rleap1/rishabh.bhatia/miniconda3/envs/train/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f9336b6ae60> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f91c771ad10>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
No data available for logging.
No data available for logging.
-----------------------------
| time/              |      |
|    fps             | 201  |
|    iterations      | 1    |
|    time_elapsed    | 40   |
|    total_timesteps | 8192 |
-----------------------------
Num timesteps: 12000
Best mean reward: -inf - Last mean reward per episode: -1500.00
Saving new best model at 12000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 16000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -1.5e+03     |
| time/                   |              |
|    fps                  | 51           |
|    iterations           | 2            |
|    time_elapsed         | 316          |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 0.0073929923 |
|    clip_fraction        | 0.0239       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | -0.0275      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.02         |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.000135    |
|    value_loss           | 1.76         |
------------------------------------------
Num timesteps: 20000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
Eval num_timesteps=20000, episode_reward=-1500.00 +/- 0.00
Episode length: 3000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3e+03        |
|    mean_reward          | -1.5e+03     |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0059059593 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 2.98e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.603        |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.000624    |
|    value_loss           | 3.66         |
------------------------------------------
New best mean reward!
Num timesteps: 24000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -1.5e+03 |
| time/              |          |
|    fps             | 31       |
|    iterations      | 3        |
|    time_elapsed    | 786      |
|    total_timesteps | 24576    |
---------------------------------
Num timesteps: 28000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
Num timesteps: 32000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 30          |
|    iterations           | 4           |
|    time_elapsed         | 1064        |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.011274125 |
|    clip_fraction        | 0.0642      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 5.72        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00174    |
|    value_loss           | 3.97        |
-----------------------------------------
Num timesteps: 36000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
Num timesteps: 40000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
Eval num_timesteps=40000, episode_reward=-1500.00 +/- 0.00
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | -1.5e+03    |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.015000172 |
|    clip_fraction        | 0.063       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.999      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0171     |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.00077     |
|    value_loss           | 1.45        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -1.5e+03 |
| time/              |          |
|    fps             | 26       |
|    iterations      | 5        |
|    time_elapsed    | 1531     |
|    total_timesteps | 40960    |
---------------------------------
Num timesteps: 44000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
Num timesteps: 48000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -1.5e+03     |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 6            |
|    time_elapsed         | 1806         |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0020650052 |
|    clip_fraction        | 0.0035       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.998       |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.41         |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.000155    |
|    value_loss           | 4.26         |
------------------------------------------
Num timesteps: 52000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
Num timesteps: 56000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -1.5e+03     |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 7            |
|    time_elapsed         | 2084         |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0076437388 |
|    clip_fraction        | 0.0348       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.99        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0778       |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.000743    |
|    value_loss           | 4.89         |
------------------------------------------
Num timesteps: 60000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
Eval num_timesteps=60000, episode_reward=-1500.00 +/- 0.00
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | -1.5e+03    |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.008235863 |
|    clip_fraction        | 0.0182      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.968      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.027       |
|    n_updates            | 70          |
|    policy_gradient_loss | 0.000659    |
|    value_loss           | 0.344       |
-----------------------------------------
Num timesteps: 64000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -1.5e+03 |
| time/              |          |
|    fps             | 25       |
|    iterations      | 8        |
|    time_elapsed    | 2554     |
|    total_timesteps | 65536    |
---------------------------------
Num timesteps: 68000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
Num timesteps: 72000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 26          |
|    iterations           | 9           |
|    time_elapsed         | 2831        |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.014198493 |
|    clip_fraction        | 0.0641      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.928      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 8.46        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.000471   |
|    value_loss           | 6.04        |
-----------------------------------------
Num timesteps: 76000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
Num timesteps: 80000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
Eval num_timesteps=80000, episode_reward=-1500.00 +/- 0.00
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | -1.5e+03    |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.012486905 |
|    clip_fraction        | 0.0478      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.89       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 2.43        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00067    |
|    value_loss           | 6.58        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -1.5e+03 |
| time/              |          |
|    fps             | 24       |
|    iterations      | 10       |
|    time_elapsed    | 3298     |
|    total_timesteps | 81920    |
---------------------------------
Num timesteps: 84000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
Num timesteps: 88000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -1.5e+03     |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 11           |
|    time_elapsed         | 3574         |
|    total_timesteps      | 90112        |
| train/                  |              |
|    approx_kl            | 0.0010876383 |
|    clip_fraction        | 0.0178       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.867       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00158     |
|    n_updates            | 100          |
|    policy_gradient_loss | 0.00122      |
|    value_loss           | 0.0864       |
------------------------------------------
Num timesteps: 92000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
Num timesteps: 96000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -1.5e+03     |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 12           |
|    time_elapsed         | 3848         |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0015230365 |
|    clip_fraction        | 0.0201       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.839       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 2.96         |
|    n_updates            | 110          |
|    policy_gradient_loss | 0.000825     |
|    value_loss           | 7.5          |
------------------------------------------
Num timesteps: 100000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
Eval num_timesteps=100000, episode_reward=-1500.00 +/- 0.00
Episode length: 3000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3e+03       |
|    mean_reward          | -1.5e+03    |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.006035558 |
|    clip_fraction        | 0.0323      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.827      |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.307       |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.000302   |
|    value_loss           | 7.85        |
-----------------------------------------
Num timesteps: 104000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -1.5e+03 |
| time/              |          |
|    fps             | 24       |
|    iterations      | 13       |
|    time_elapsed    | 4314     |
|    total_timesteps | 106496   |
---------------------------------
Num timesteps: 108000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
Num timesteps: 112000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 24          |
|    iterations           | 14          |
|    time_elapsed         | 4589        |
|    total_timesteps      | 114688      |
| train/                  |             |
|    approx_kl            | 0.009747071 |
|    clip_fraction        | 0.0754      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.762      |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0123     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.000394   |
|    value_loss           | 0.0247      |
-----------------------------------------
Num timesteps: 116000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
Num timesteps: 120000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
Eval num_timesteps=120000, episode_reward=-1500.00 +/- 0.00
Episode length: 3000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3e+03        |
|    mean_reward          | -1.5e+03     |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 0.0061841784 |
|    clip_fraction        | 0.0483       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.678       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.43         |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.000228    |
|    value_loss           | 8.45         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -1.5e+03 |
| time/              |          |
|    fps             | 24       |
|    iterations      | 15       |
|    time_elapsed    | 5056     |
|    total_timesteps | 122880   |
---------------------------------
Num timesteps: 124000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
Num timesteps: 128000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 24          |
|    iterations           | 16          |
|    time_elapsed         | 5332        |
|    total_timesteps      | 131072      |
| train/                  |             |
|    approx_kl            | 0.005371399 |
|    clip_fraction        | 0.0295      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.619      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 7.96        |
|    n_updates            | 150         |
|    policy_gradient_loss | 0.000247    |
|    value_loss           | 8.63        |
-----------------------------------------
Num timesteps: 132000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
Num timesteps: 136000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 24          |
|    iterations           | 17          |
|    time_elapsed         | 5608        |
|    total_timesteps      | 139264      |
| train/                  |             |
|    approx_kl            | 0.004665536 |
|    clip_fraction        | 0.0489      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.53       |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00534    |
|    n_updates            | 160         |
|    policy_gradient_loss | -7.71e-05   |
|    value_loss           | 0.00843     |
-----------------------------------------
Num timesteps: 140000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
Eval num_timesteps=140000, episode_reward=-1500.00 +/- 0.00
Episode length: 3000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3e+03        |
|    mean_reward          | -1.5e+03     |
| time/                   |              |
|    total_timesteps      | 140000       |
| train/                  |              |
|    approx_kl            | 0.0033769219 |
|    clip_fraction        | 0.0496       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.46        |
|    explained_variance   | 2.38e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0312       |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.000552    |
|    value_loss           | 9.03         |
------------------------------------------
Num timesteps: 144000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3e+03    |
|    ep_rew_mean     | -1.5e+03 |
| time/              |          |
|    fps             | 24       |
|    iterations      | 18       |
|    time_elapsed    | 6076     |
|    total_timesteps | 147456   |
