
Using cpu device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -500     |
| time/              |          |
|    fps             | 277      |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 96          |
|    iterations           | 2           |
|    time_elapsed         | 42          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.010296643 |
|    clip_fraction        | 0.0647      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.0566      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.671       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00341    |
|    value_loss           | 3.81        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 81           |
|    iterations           | 3            |
|    time_elapsed         | 75           |
|    total_timesteps      | 6144         |
| train/                  |              |
|    approx_kl            | 0.0031024362 |
|    clip_fraction        | 0.016        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.12         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.157        |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00117     |
|    value_loss           | 13.2         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 76          |
|    iterations           | 4           |
|    time_elapsed         | 107         |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.008721184 |
|    clip_fraction        | 0.015       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | -6.65e-05   |
|    learning_rate        | 0.0003      |
|    loss                 | 3.95        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0007     |
|    value_loss           | 11.5        |
-----------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -500.00
Saving new best model at 10000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 74           |
|    iterations           | 5            |
|    time_elapsed         | 138          |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.0100895595 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.07        |
|    explained_variance   | 1.01e-06     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0867       |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00311     |
|    value_loss           | 10.4         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 73          |
|    iterations           | 6           |
|    time_elapsed         | 167         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.008232256 |
|    clip_fraction        | 0.0221      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 5.99        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00126    |
|    value_loss           | 9.66        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 73          |
|    iterations           | 7           |
|    time_elapsed         | 195         |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.013438352 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.143       |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00322    |
|    value_loss           | 9.96        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 8           |
|    time_elapsed         | 224         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.019127164 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.122       |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00331    |
|    value_loss           | 10.5        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 9           |
|    time_elapsed         | 253         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.008770918 |
|    clip_fraction        | 0.0647      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.873       |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00154    |
|    value_loss           | 11.4        |
-----------------------------------------
Num timesteps: 20000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 10          |
|    time_elapsed         | 284         |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.012284323 |
|    clip_fraction        | 0.0582      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 8.13        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00108    |
|    value_loss           | 12.2        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 71          |
|    iterations           | 11          |
|    time_elapsed         | 315         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.007964151 |
|    clip_fraction        | 0.0171      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.998      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 7.95        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.000195   |
|    value_loss           | 13          |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 71           |
|    iterations           | 12           |
|    time_elapsed         | 345          |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0071262843 |
|    clip_fraction        | 0.0638       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.935       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 12.4         |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.000813    |
|    value_loss           | 13.7         |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | -500       |
| time/                   |            |
|    fps                  | 70         |
|    iterations           | 13         |
|    time_elapsed         | 375        |
|    total_timesteps      | 26624      |
| train/                  |            |
|    approx_kl            | 0.01021958 |
|    clip_fraction        | 0.0369     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.99      |
|    explained_variance   | -2.38e-07  |
|    learning_rate        | 0.0003     |
|    loss                 | 2.76       |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.00209   |
|    value_loss           | 14.3       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | -500       |
| time/                   |            |
|    fps                  | 70         |
|    iterations           | 14         |
|    time_elapsed         | 405        |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.01189184 |
|    clip_fraction        | 0.0307     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.971     |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0003     |
|    loss                 | 3.8        |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.000525  |
|    value_loss           | 14.8       |
----------------------------------------
Num timesteps: 30000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 69          |
|    iterations           | 15          |
|    time_elapsed         | 439         |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.013100185 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.876      |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.71        |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00406    |
|    value_loss           | 15.3        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 69           |
|    iterations           | 16           |
|    time_elapsed         | 471          |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0065678526 |
|    clip_fraction        | 0.083        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.819       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 12.8         |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00218     |
|    value_loss           | 15.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 69           |
|    iterations           | 17           |
|    time_elapsed         | 503          |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0040238095 |
|    clip_fraction        | 0.00977      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.806       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.245        |
|    n_updates            | 160          |
|    policy_gradient_loss | 0.000135     |
|    value_loss           | 16           |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 68          |
|    iterations           | 18          |
|    time_elapsed         | 534         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.009026393 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 7.31        |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00131    |
|    value_loss           | 16.2        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 19           |
|    time_elapsed         | 566          |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0049124276 |
|    clip_fraction        | 0.0221       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.711       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.313        |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00156     |
|    value_loss           | 16.4         |
------------------------------------------
Num timesteps: 40000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 20           |
|    time_elapsed         | 598          |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0003264031 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.702       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.55         |
|    n_updates            | 190          |
|    policy_gradient_loss | 0.000822     |
|    value_loss           | 16.5         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 67           |
|    iterations           | 21           |
|    time_elapsed         | 632          |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0056646597 |
|    clip_fraction        | 0.0811       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.668       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 16.2         |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00151     |
|    value_loss           | 16.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -499         |
| time/                   |              |
|    fps                  | 67           |
|    iterations           | 22           |
|    time_elapsed         | 665          |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0018797416 |
|    clip_fraction        | 0.0291       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.692       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.64         |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00102     |
|    value_loss           | 24.8         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -499        |
| time/                   |             |
|    fps                  | 67          |
|    iterations           | 23          |
|    time_elapsed         | 699         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.004094925 |
|    clip_fraction        | 0.00605     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.723      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 16.5        |
|    n_updates            | 220         |
|    policy_gradient_loss | -1.42e-05   |
|    value_loss           | 19.4        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -499         |
| time/                   |              |
|    fps                  | 67           |
|    iterations           | 24           |
|    time_elapsed         | 732          |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0027088234 |
|    clip_fraction        | 0.0269       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.769       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.29         |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.000604    |
|    value_loss           | 18           |
------------------------------------------
Num timesteps: 50000
Best mean reward: -500.00 - Last mean reward per episode: -498.40
Saving new best model at 50000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -498        |
| time/                   |             |
|    fps                  | 66          |
|    iterations           | 25          |
|    time_elapsed         | 769         |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.013185196 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.652      |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 9.57        |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00281    |
|    value_loss           | 16.7        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -498         |
| time/                   |              |
|    fps                  | 66           |
|    iterations           | 26           |
|    time_elapsed         | 803          |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0040974505 |
|    clip_fraction        | 0.0614       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.568       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.38         |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00106     |
|    value_loss           | 18.2         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -499         |
| time/                   |              |
|    fps                  | 65           |
|    iterations           | 27           |
|    time_elapsed         | 838          |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0040683164 |
|    clip_fraction        | 0.0412       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.508       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.84         |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.000729    |
|    value_loss           | 16.8         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -498        |
| time/                   |             |
|    fps                  | 65          |
|    iterations           | 28          |
|    time_elapsed         | 872         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.003245194 |
|    clip_fraction        | 0.0623      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.48       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 1.04        |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00271    |
|    value_loss           | 16.9        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -498         |
| time/                   |              |
|    fps                  | 65           |
|    iterations           | 29           |
|    time_elapsed         | 905          |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0018554394 |
|    clip_fraction        | 0.00723      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.479       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.387        |
|    n_updates            | 280          |
|    policy_gradient_loss | 0.000216     |
|    value_loss           | 18.4         |
------------------------------------------
Num timesteps: 60000
Best mean reward: -498.40 - Last mean reward per episode: -498.00
Saving new best model at 60000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -498         |
| time/                   |              |
|    fps                  | 65           |
|    iterations           | 30           |
|    time_elapsed         | 940          |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0022186227 |
|    clip_fraction        | 0.0346       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.46        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 7.2          |
|    n_updates            | 290          |
|    policy_gradient_loss | 0.000313     |
|    value_loss           | 18.3         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -497         |
| time/                   |              |
|    fps                  | 65           |
|    iterations           | 31           |
|    time_elapsed         | 972          |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.0018509657 |
|    clip_fraction        | 0.0392       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.369       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 7.64         |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00059     |
|    value_loss           | 18.3         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -497         |
| time/                   |              |
|    fps                  | 65           |
|    iterations           | 32           |
|    time_elapsed         | 1006         |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0015327326 |
|    clip_fraction        | 0.0274       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.321       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 4.99         |
|    n_updates            | 310          |
|    policy_gradient_loss | 1.31e-05     |
|    value_loss           | 18.3         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -497         |
| time/                   |              |
|    fps                  | 64           |
|    iterations           | 33           |
|    time_elapsed         | 1042         |
|    total_timesteps      | 67584        |
| train/                  |              |
|    approx_kl            | 0.0018740357 |
|    clip_fraction        | 0.0293       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.329       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 10.6         |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.000155    |
|    value_loss           | 19.8         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -496         |
| time/                   |              |
|    fps                  | 64           |
|    iterations           | 34           |
|    time_elapsed         | 1077         |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.0018502562 |
|    clip_fraction        | 0.0295       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.391       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 4.87         |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.000979    |
|    value_loss           | 19.6         |
------------------------------------------
Num timesteps: 70000
Best mean reward: -498.00 - Last mean reward per episode: -496.00
Saving new best model at 70000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -496        |
| time/                   |             |
|    fps                  | 64          |
|    iterations           | 35          |
|    time_elapsed         | 1113        |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.002899703 |
|    clip_fraction        | 0.0389      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.32       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 14          |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.000642   |
|    value_loss           | 19.5        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -496        |
| time/                   |             |
|    fps                  | 64          |
|    iterations           | 36          |
|    time_elapsed         | 1148        |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.001667727 |
|    clip_fraction        | 0.021       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.282      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 2.6         |
|    n_updates            | 350         |
|    policy_gradient_loss | 9.66e-06    |
|    value_loss           | 19.9        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -496         |
| time/                   |              |
|    fps                  | 64           |
|    iterations           | 37           |
|    time_elapsed         | 1181         |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0006252582 |
|    clip_fraction        | 0.00591      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.282       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.8          |
|    n_updates            | 360          |
|    policy_gradient_loss | 9.8e-06      |
|    value_loss           | 16.6         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -496         |
| time/                   |              |
|    fps                  | 63           |
|    iterations           | 38           |
|    time_elapsed         | 1216         |
|    total_timesteps      | 77824        |
| train/                  |              |
|    approx_kl            | 0.0006088635 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.299       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 16.3         |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.000299    |
|    value_loss           | 16.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -495         |
| time/                   |              |
|    fps                  | 63           |
|    iterations           | 39           |
|    time_elapsed         | 1249         |
|    total_timesteps      | 79872        |
| train/                  |              |
|    approx_kl            | 6.302097e-05 |
|    clip_fraction        | 0.00654      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.281       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 4.16         |
|    n_updates            | 380          |
|    policy_gradient_loss | -2.54e-05    |
|    value_loss           | 16.9         |
------------------------------------------
Num timesteps: 80000
Best mean reward: -496.00 - Last mean reward per episode: -495.25
Saving new best model at 80000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -495         |
| time/                   |              |
|    fps                  | 63           |
|    iterations           | 40           |
|    time_elapsed         | 1285         |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.0012602574 |
|    clip_fraction        | 0.0187       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.281       |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 4.17         |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00014     |
|    value_loss           | 19.8         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -495         |
| time/                   |              |
|    fps                  | 63           |
|    iterations           | 41           |
|    time_elapsed         | 1321         |
|    total_timesteps      | 83968        |
| train/                  |              |
|    approx_kl            | 0.0021188399 |
|    clip_fraction        | 0.0236       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.289       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 19.6         |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.000192    |
|    value_loss           | 19.6         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -494         |
| time/                   |              |
|    fps                  | 63           |
|    iterations           | 42           |
|    time_elapsed         | 1355         |
|    total_timesteps      | 86016        |
| train/                  |              |
|    approx_kl            | 0.0013219866 |
|    clip_fraction        | 0.0237       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.307       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 26.1         |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.00115     |
|    value_loss           | 20.8         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -493         |
| time/                   |              |
|    fps                  | 63           |
|    iterations           | 43           |
|    time_elapsed         | 1389         |
|    total_timesteps      | 88064        |
| train/                  |              |
|    approx_kl            | 0.0008044664 |
|    clip_fraction        | 0.0309       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.371       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.49         |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.00119     |
|    value_loss           | 29.3         |
------------------------------------------
Num timesteps: 90000
Best mean reward: -495.25 - Last mean reward per episode: -492.44
Saving new best model at 90000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -492         |
| time/                   |              |
|    fps                  | 62           |
|    iterations           | 44           |
|    time_elapsed         | 1436         |
|    total_timesteps      | 90112        |
| train/                  |              |
|    approx_kl            | 0.0024287621 |
|    clip_fraction        | 0.0218       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.329       |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 17.7         |
|    n_updates            | 430          |
|    policy_gradient_loss | 6.6e-05      |
|    value_loss           | 21.9         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -492          |
| time/                   |               |
|    fps                  | 61            |
|    iterations           | 45            |
|    time_elapsed         | 1489          |
|    total_timesteps      | 92160         |
| train/                  |               |
|    approx_kl            | 0.00017227413 |
|    clip_fraction        | 0.0133        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.323        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.37          |
|    n_updates            | 440           |
|    policy_gradient_loss | -0.000239     |
|    value_loss           | 20            |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -491          |
| time/                   |               |
|    fps                  | 61            |
|    iterations           | 46            |
|    time_elapsed         | 1540          |
|    total_timesteps      | 94208         |
| train/                  |               |
|    approx_kl            | 0.00051470986 |
|    clip_fraction        | 0.0135        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.347        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 10.5          |
|    n_updates            | 450           |
|    policy_gradient_loss | -0.000648     |
|    value_loss           | 20            |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -491         |
| time/                   |              |
|    fps                  | 60           |
|    iterations           | 47           |
|    time_elapsed         | 1594         |
|    total_timesteps      | 96256        |
| train/                  |              |
|    approx_kl            | 0.0015730035 |
|    clip_fraction        | 0.0175       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.365       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 7.7          |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.00127     |
|    value_loss           | 19.9         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -491         |
| time/                   |              |
|    fps                  | 59           |
|    iterations           | 48           |
|    time_elapsed         | 1650         |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0022948133 |
|    clip_fraction        | 0.0284       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.307       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 12.4         |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.000557    |
|    value_loss           | 18.6         |
------------------------------------------
Num timesteps: 100000
Best mean reward: -492.44 - Last mean reward per episode: -490.20
Saving new best model at 100000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -490         |
| time/                   |              |
|    fps                  | 59           |
|    iterations           | 49           |
|    time_elapsed         | 1699         |
|    total_timesteps      | 100352       |
| train/                  |              |
|    approx_kl            | 0.0013548043 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.296       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 10.6         |
|    n_updates            | 480          |
|    policy_gradient_loss | 0.000546     |
|    value_loss           | 21.2         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -490         |
| time/                   |              |
|    fps                  | 58           |
|    iterations           | 50           |
|    time_elapsed         | 1752         |
|    total_timesteps      | 102400       |
| train/                  |              |
|    approx_kl            | 0.0014074962 |
|    clip_fraction        | 0.0327       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.303       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.06         |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00382     |
|    value_loss           | 19.8         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -489         |
| time/                   |              |
|    fps                  | 57           |
|    iterations           | 51           |
|    time_elapsed         | 1803         |
|    total_timesteps      | 104448       |
| train/                  |              |
|    approx_kl            | 0.0014933529 |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.32        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.42         |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.000155    |
|    value_loss           | 17.1         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -489         |
| time/                   |              |
|    fps                  | 57           |
|    iterations           | 52           |
|    time_elapsed         | 1856         |
|    total_timesteps      | 106496       |
| train/                  |              |
|    approx_kl            | 9.504438e-05 |
|    clip_fraction        | 0.00479      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.282       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 13.4         |
|    n_updates            | 510          |
|    policy_gradient_loss | -5.31e-05    |
|    value_loss           | 26.5         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -488         |
| time/                   |              |
|    fps                  | 56           |
|    iterations           | 53           |
|    time_elapsed         | 1905         |
|    total_timesteps      | 108544       |
| train/                  |              |
|    approx_kl            | 0.0005714112 |
|    clip_fraction        | 0.0161       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.29        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.3          |
|    n_updates            | 520          |
|    policy_gradient_loss | 0.000207     |
|    value_loss           | 21.4         |
------------------------------------------
Num timesteps: 110000
Best mean reward: -490.20 - Last mean reward per episode: -487.00
Saving new best model at 110000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -487          |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 54            |
|    time_elapsed         | 1954          |
|    total_timesteps      | 110592        |
| train/                  |               |
|    approx_kl            | 0.00038367603 |
|    clip_fraction        | 0.00366       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.314        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 7.07          |
|    n_updates            | 530           |
|    policy_gradient_loss | 0.000199      |
|    value_loss           | 21.2          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -486         |
| time/                   |              |
|    fps                  | 56           |
|    iterations           | 55           |
|    time_elapsed         | 2006         |
|    total_timesteps      | 112640       |
| train/                  |              |
|    approx_kl            | 0.0014397736 |
|    clip_fraction        | 0.0383       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.341       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 33.6         |
|    n_updates            | 540          |
|    policy_gradient_loss | -0.00267     |
|    value_loss           | 23.5         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -485          |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 56            |
|    time_elapsed         | 2044          |
|    total_timesteps      | 114688        |
| train/                  |               |
|    approx_kl            | 0.00058981415 |
|    clip_fraction        | 0.0163        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.356        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 9.54          |
|    n_updates            | 550           |
|    policy_gradient_loss | -0.000104     |
|    value_loss           | 22.1          |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 1e+03          |
|    ep_rew_mean          | -484           |
| time/                   |                |
|    fps                  | 56             |
|    iterations           | 57             |
|    time_elapsed         | 2080           |
|    total_timesteps      | 116736         |
| train/                  |                |
|    approx_kl            | 0.000107593194 |
|    clip_fraction        | 0.015          |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.329         |
|    explained_variance   | -1.19e-07      |
|    learning_rate        | 0.0003         |
|    loss                 | 6.25           |
|    n_updates            | 560            |
|    policy_gradient_loss | -0.000503      |
|    value_loss           | 23             |
--------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -484         |
| time/                   |              |
|    fps                  | 56           |
|    iterations           | 58           |
|    time_elapsed         | 2115         |
|    total_timesteps      | 118784       |
| train/                  |              |
|    approx_kl            | 0.0004867739 |
|    clip_fraction        | 0.00898      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.319       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 2.53         |
|    n_updates            | 570          |
|    policy_gradient_loss | -0.000224    |
|    value_loss           | 17.8         |
------------------------------------------
Num timesteps: 120000
Best mean reward: -487.00 - Last mean reward per episode: -482.80
Saving new best model at 120000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -483         |
| time/                   |              |
|    fps                  | 56           |
|    iterations           | 59           |
|    time_elapsed         | 2149         |
|    total_timesteps      | 120832       |
| train/                  |              |
|    approx_kl            | 0.0022191158 |
|    clip_fraction        | 0.0327       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.372       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 25.9         |
|    n_updates            | 580          |
|    policy_gradient_loss | -0.00204     |
|    value_loss           | 19.3         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -482         |
| time/                   |              |
|    fps                  | 56           |
|    iterations           | 60           |
|    time_elapsed         | 2186         |
|    total_timesteps      | 122880       |
| train/                  |              |
|    approx_kl            | 0.0018882977 |
|    clip_fraction        | 0.0168       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.398       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 24.5         |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.000709    |
|    value_loss           | 24.6         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -481         |
| time/                   |              |
|    fps                  | 56           |
|    iterations           | 61           |
|    time_elapsed         | 2219         |
|    total_timesteps      | 124928       |
| train/                  |              |
|    approx_kl            | 0.0024108444 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.411       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 7.8          |
|    n_updates            | 600          |
|    policy_gradient_loss | -7.51e-05    |
|    value_loss           | 22.9         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -479         |
| time/                   |              |
|    fps                  | 56           |
|    iterations           | 62           |
|    time_elapsed         | 2252         |
|    total_timesteps      | 126976       |
| train/                  |              |
|    approx_kl            | 0.0020094723 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.423       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 11.9         |
|    n_updates            | 610          |
|    policy_gradient_loss | 0.000549     |
|    value_loss           | 22.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -477         |
| time/                   |              |
|    fps                  | 56           |
|    iterations           | 63           |
|    time_elapsed         | 2285         |
|    total_timesteps      | 129024       |
| train/                  |              |
|    approx_kl            | 0.0017730965 |
|    clip_fraction        | 0.0291       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.425       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 18.7         |
|    n_updates            | 620          |
|    policy_gradient_loss | -0.00216     |
|    value_loss           | 21.5         |
------------------------------------------
Num timesteps: 130000
Best mean reward: -482.80 - Last mean reward per episode: -476.80
Saving new best model at 130000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -476         |
| time/                   |              |
|    fps                  | 56           |
|    iterations           | 64           |
|    time_elapsed         | 2319         |
|    total_timesteps      | 131072       |
| train/                  |              |
|    approx_kl            | 0.0029797454 |
|    clip_fraction        | 0.0211       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.467       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 26.1         |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.00125     |
|    value_loss           | 32.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -475         |
| time/                   |              |
|    fps                  | 56           |
|    iterations           | 65           |
|    time_elapsed         | 2358         |
|    total_timesteps      | 133120       |
| train/                  |              |
|    approx_kl            | 0.0030708655 |
|    clip_fraction        | 0.0516       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.526       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 31.8         |
|    n_updates            | 640          |
|    policy_gradient_loss | -0.00394     |
|    value_loss           | 27.5         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -472        |
| time/                   |             |
|    fps                  | 56          |
|    iterations           | 66          |
|    time_elapsed         | 2393        |
|    total_timesteps      | 135168      |
| train/                  |             |
|    approx_kl            | 0.004237342 |
|    clip_fraction        | 0.0376      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.554      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 5.91        |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.00329    |
|    value_loss           | 23.1        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -469         |
| time/                   |              |
|    fps                  | 56           |
|    iterations           | 67           |
|    time_elapsed         | 2433         |
|    total_timesteps      | 137216       |
| train/                  |              |
|    approx_kl            | 0.0049380725 |
|    clip_fraction        | 0.0555       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.6         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 17           |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.00584     |
|    value_loss           | 29.6         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -465         |
| time/                   |              |
|    fps                  | 56           |
|    iterations           | 68           |
|    time_elapsed         | 2468         |
|    total_timesteps      | 139264       |
| train/                  |              |
|    approx_kl            | 0.0071337745 |
|    clip_fraction        | 0.0615       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.641       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 18.2         |
|    n_updates            | 670          |
|    policy_gradient_loss | -0.00569     |
|    value_loss           | 30.4         |
------------------------------------------
Num timesteps: 140000
Best mean reward: -476.80 - Last mean reward per episode: -463.20
Saving new best model at 140000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -461        |
| time/                   |             |
|    fps                  | 56          |
|    iterations           | 69          |
|    time_elapsed         | 2510        |
|    total_timesteps      | 141312      |
| train/                  |             |
|    approx_kl            | 0.008308847 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.676      |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 34          |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.00801    |
|    value_loss           | 32.6        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -456         |
| time/                   |              |
|    fps                  | 56           |
|    iterations           | 70           |
|    time_elapsed         | 2553         |
|    total_timesteps      | 143360       |
| train/                  |              |
|    approx_kl            | 0.0031668982 |
|    clip_fraction        | 0.0634       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.692       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 10.5         |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.00276     |
|    value_loss           | 34.9         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -451        |
| time/                   |             |
|    fps                  | 56          |
|    iterations           | 71          |
|    time_elapsed         | 2592        |
|    total_timesteps      | 145408      |
| train/                  |             |
|    approx_kl            | 0.015864149 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.691      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 31.1        |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 32.3        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -445        |
| time/                   |             |
|    fps                  | 55          |
|    iterations           | 72          |
|    time_elapsed         | 2634        |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.008242505 |
|    clip_fraction        | 0.0154      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.674      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 24.4        |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.00123    |
|    value_loss           | 38.2        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -437        |
| time/                   |             |
|    fps                  | 55          |
|    iterations           | 73          |
|    time_elapsed         | 2671        |
|    total_timesteps      | 149504      |
| train/                  |             |
|    approx_kl            | 0.010062026 |
|    clip_fraction        | 0.0822      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.614      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 19.9        |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.00637    |
|    value_loss           | 39.4        |
-----------------------------------------
Num timesteps: 150000
Best mean reward: -463.20 - Last mean reward per episode: -433.80
Saving new best model at 150000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -429        |
| time/                   |             |
|    fps                  | 56          |
|    iterations           | 74          |
|    time_elapsed         | 2706        |
|    total_timesteps      | 151552      |
| train/                  |             |
|    approx_kl            | 0.005698459 |
|    clip_fraction        | 0.0131      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.595      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 20.5        |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.00116    |
|    value_loss           | 35.3        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -421        |
| time/                   |             |
|    fps                  | 56          |
|    iterations           | 75          |
|    time_elapsed         | 2741        |
|    total_timesteps      | 153600      |
| train/                  |             |
|    approx_kl            | 0.007011358 |
|    clip_fraction        | 0.0302      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.523      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 21.1        |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.00105    |
|    value_loss           | 37.3        |
-----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -412          |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 76            |
|    time_elapsed         | 2776          |
|    total_timesteps      | 155648        |
| train/                  |               |
|    approx_kl            | 0.00081342546 |
|    clip_fraction        | 0.00371       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.498        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 17.4          |
|    n_updates            | 750           |
|    policy_gradient_loss | -0.000146     |
|    value_loss           | 39.4          |
-------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | -402       |
| time/                   |            |
|    fps                  | 56         |
|    iterations           | 77         |
|    time_elapsed         | 2814       |
|    total_timesteps      | 157696     |
| train/                  |            |
|    approx_kl            | 0.00118135 |
|    clip_fraction        | 0.0637     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.438     |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 17.7       |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.00489   |
|    value_loss           | 40.1       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -394        |
| time/                   |             |
|    fps                  | 55          |
|    iterations           | 78          |
|    time_elapsed         | 2856        |
|    total_timesteps      | 159744      |
| train/                  |             |
|    approx_kl            | 0.002905732 |
|    clip_fraction        | 0.000732    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.434      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 16.5        |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.000626   |
|    value_loss           | 35.6        |
-----------------------------------------
Num timesteps: 160000
Best mean reward: -433.80 - Last mean reward per episode: -388.80
Saving new best model at 160000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -384        |
| time/                   |             |
|    fps                  | 55          |
|    iterations           | 79          |
|    time_elapsed         | 2899        |
|    total_timesteps      | 161792      |
| train/                  |             |
|    approx_kl            | 0.004006046 |
|    clip_fraction        | 0.0458      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.376      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 20.6        |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.00146    |
|    value_loss           | 35.7        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -374        |
| time/                   |             |
|    fps                  | 55          |
|    iterations           | 80          |
|    time_elapsed         | 2934        |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.002418454 |
|    clip_fraction        | 0.0205      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.317      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 14.4        |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.00152    |
|    value_loss           | 37          |
-----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -363          |
| time/                   |               |
|    fps                  | 55            |
|    iterations           | 81            |
|    time_elapsed         | 2971          |
|    total_timesteps      | 165888        |
| train/                  |               |
|    approx_kl            | 0.00030740577 |
|    clip_fraction        | 0.0187        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.267        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 17.7          |
|    n_updates            | 800           |
|    policy_gradient_loss | -0.00023      |
|    value_loss           | 36.1          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -353         |
| time/                   |              |
|    fps                  | 55           |
|    iterations           | 82           |
|    time_elapsed         | 3006         |
|    total_timesteps      | 167936       |
| train/                  |              |
|    approx_kl            | 0.0014973643 |
|    clip_fraction        | 0.0207       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.243       |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 21.1         |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.00153     |
|    value_loss           | 38.6         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -343         |
| time/                   |              |
|    fps                  | 55           |
|    iterations           | 83           |
|    time_elapsed         | 3042         |
|    total_timesteps      | 169984       |
| train/                  |              |
|    approx_kl            | 0.0015660938 |
|    clip_fraction        | 0.0274       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.211       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 21.6         |
|    n_updates            | 820          |
|    policy_gradient_loss | -0.00165     |
|    value_loss           | 36.9         |
------------------------------------------
Num timesteps: 170000
Best mean reward: -388.80 - Last mean reward per episode: -338.20
Saving new best model at 170000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -328         |
| time/                   |              |
|    fps                  | 55           |
|    iterations           | 84           |
|    time_elapsed         | 3078         |
|    total_timesteps      | 172032       |
| train/                  |              |
|    approx_kl            | 0.0010396421 |
|    clip_fraction        | 0.00645      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.212       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 17.2         |
|    n_updates            | 830          |
|    policy_gradient_loss | 0.000139     |
|    value_loss           | 38.2         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -318         |
| time/                   |              |
|    fps                  | 55           |
|    iterations           | 85           |
|    time_elapsed         | 3114         |
|    total_timesteps      | 174080       |
| train/                  |              |
|    approx_kl            | 0.0015217956 |
|    clip_fraction        | 0.00596      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.166       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 21.5         |
|    n_updates            | 840          |
|    policy_gradient_loss | -0.00016     |
|    value_loss           | 36.9         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -307         |
| time/                   |              |
|    fps                  | 55           |
|    iterations           | 86           |
|    time_elapsed         | 3151         |
|    total_timesteps      | 176128       |
| train/                  |              |
|    approx_kl            | 0.0005570088 |
|    clip_fraction        | 0.0117       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.115       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 18.6         |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.000209    |
|    value_loss           | 37.6         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -296         |
| time/                   |              |
|    fps                  | 55           |
|    iterations           | 87           |
|    time_elapsed         | 3185         |
|    total_timesteps      | 178176       |
| train/                  |              |
|    approx_kl            | 0.0006036664 |
|    clip_fraction        | 0.00962      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0958      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 21.1         |
|    n_updates            | 860          |
|    policy_gradient_loss | -0.000662    |
|    value_loss           | 38.8         |
------------------------------------------
Num timesteps: 180000
Best mean reward: -338.20 - Last mean reward per episode: -285.20
Saving new best model at 180000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -285        |
| time/                   |             |
|    fps                  | 55          |
|    iterations           | 88          |
|    time_elapsed         | 3221        |
|    total_timesteps      | 180224      |
| train/                  |             |
|    approx_kl            | 0.000373759 |
|    clip_fraction        | 0.00444     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0812     |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 20.7        |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.000339   |
|    value_loss           | 38.7        |
-----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 55            |
|    iterations           | 89            |
|    time_elapsed         | 3257          |
|    total_timesteps      | 182272        |
| train/                  |               |
|    approx_kl            | 0.00014932381 |
|    clip_fraction        | 0.00488       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0822       |
|    explained_variance   | 1.79e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 18.5          |
|    n_updates            | 880           |
|    policy_gradient_loss | 0.000208      |
|    value_loss           | 37.8          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -264          |
| time/                   |               |
|    fps                  | 55            |
|    iterations           | 90            |
|    time_elapsed         | 3292          |
|    total_timesteps      | 184320        |
| train/                  |               |
|    approx_kl            | 0.00041915826 |
|    clip_fraction        | 0.0108        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0649       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 15.7          |
|    n_updates            | 890           |
|    policy_gradient_loss | -0.00079      |
|    value_loss           | 36.7          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -254          |
| time/                   |               |
|    fps                  | 55            |
|    iterations           | 91            |
|    time_elapsed         | 3330          |
|    total_timesteps      | 186368        |
| train/                  |               |
|    approx_kl            | 0.00017304323 |
|    clip_fraction        | 0.00283       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0609       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 19.7          |
|    n_updates            | 900           |
|    policy_gradient_loss | -0.000171     |
|    value_loss           | 37.8          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -244         |
| time/                   |              |
|    fps                  | 55           |
|    iterations           | 92           |
|    time_elapsed         | 3365         |
|    total_timesteps      | 188416       |
| train/                  |              |
|    approx_kl            | 0.0001367811 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0627      |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 20.7         |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.000106    |
|    value_loss           | 37.3         |
------------------------------------------
Num timesteps: 190000
Best mean reward: -285.20 - Last mean reward per episode: -233.80
Saving new best model at 190000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -234          |
| time/                   |               |
|    fps                  | 55            |
|    iterations           | 93            |
|    time_elapsed         | 3402          |
|    total_timesteps      | 190464        |
| train/                  |               |
|    approx_kl            | 0.00010114236 |
|    clip_fraction        | 0.00342       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0424       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 18.5          |
|    n_updates            | 920           |
|    policy_gradient_loss | -0.00019      |
|    value_loss           | 38.8          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -223         |
| time/                   |              |
|    fps                  | 56           |
|    iterations           | 94           |
|    time_elapsed         | 3435         |
|    total_timesteps      | 192512       |
| train/                  |              |
|    approx_kl            | 7.855019e-05 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0411      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 18.8         |
|    n_updates            | 930          |
|    policy_gradient_loss | -4.51e-05    |
|    value_loss           | 39.5         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -212          |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 95            |
|    time_elapsed         | 3470          |
|    total_timesteps      | 194560        |
| train/                  |               |
|    approx_kl            | 0.00016428914 |
|    clip_fraction        | 0.00439       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0318       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 21.9          |
|    n_updates            | 940           |
|    policy_gradient_loss | -0.000307     |
|    value_loss           | 39.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -202          |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 96            |
|    time_elapsed         | 3505          |
|    total_timesteps      | 196608        |
| train/                  |               |
|    approx_kl            | 0.00019321949 |
|    clip_fraction        | 0.00547       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0262       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 22.6          |
|    n_updates            | 950           |
|    policy_gradient_loss | -0.0006       |
|    value_loss           | 37.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -192          |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 97            |
|    time_elapsed         | 3540          |
|    total_timesteps      | 198656        |
| train/                  |               |
|    approx_kl            | 2.8128969e-05 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0289       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 20.4          |
|    n_updates            | 960           |
|    policy_gradient_loss | -4.95e-05     |
|    value_loss           | 41.3          |
-------------------------------------------
Num timesteps: 200000
Best mean reward: -233.80 - Last mean reward per episode: -181.20
Saving new best model at 200000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -181          |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 98            |
|    time_elapsed         | 3577          |
|    total_timesteps      | 200704        |
| train/                  |               |
|    approx_kl            | 0.00012503905 |
|    clip_fraction        | 0.00278       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0264       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 15.8          |
|    n_updates            | 970           |
|    policy_gradient_loss | 0.000153      |
|    value_loss           | 37.5          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -170          |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 99            |
|    time_elapsed         | 3612          |
|    total_timesteps      | 202752        |
| train/                  |               |
|    approx_kl            | 1.7102691e-05 |
|    clip_fraction        | 0.00137       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0424       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 23.8          |
|    n_updates            | 980           |
|    policy_gradient_loss | -1.88e-05     |
|    value_loss           | 37.7          |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -160        |
| time/                   |             |
|    fps                  | 56          |
|    iterations           | 100         |
|    time_elapsed         | 3647        |
|    total_timesteps      | 204800      |
| train/                  |             |
|    approx_kl            | 0.000195676 |
|    clip_fraction        | 0.00103     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0354     |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 20          |
|    n_updates            | 990         |
|    policy_gradient_loss | 5e-05       |
|    value_loss           | 39.2        |
-----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -150          |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 101           |
|    time_elapsed         | 3684          |
|    total_timesteps      | 206848        |
| train/                  |               |
|    approx_kl            | 0.00011181916 |
|    clip_fraction        | 0.00264       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0375       |
|    explained_variance   | 1.79e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 17.7          |
|    n_updates            | 1000          |
|    policy_gradient_loss | -0.000131     |
|    value_loss           | 38.9          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -139         |
| time/                   |              |
|    fps                  | 56           |
|    iterations           | 102          |
|    time_elapsed         | 3719         |
|    total_timesteps      | 208896       |
| train/                  |              |
|    approx_kl            | 7.673298e-05 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0314      |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 19.3         |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.000175    |
|    value_loss           | 38.4         |
------------------------------------------
Num timesteps: 210000
Best mean reward: -181.20 - Last mean reward per episode: -129.60
Saving new best model at 210000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -130         |
| time/                   |              |
|    fps                  | 56           |
|    iterations           | 103          |
|    time_elapsed         | 3755         |
|    total_timesteps      | 210944       |
| train/                  |              |
|    approx_kl            | 9.614884e-05 |
|    clip_fraction        | 0.00278      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0258      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 21.4         |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.000261    |
|    value_loss           | 38.3         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -120          |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 104           |
|    time_elapsed         | 3790          |
|    total_timesteps      | 212992        |
| train/                  |               |
|    approx_kl            | 0.00035745036 |
|    clip_fraction        | 0.00205       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0299       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 14.4          |
|    n_updates            | 1030          |
|    policy_gradient_loss | 4.08e-05      |
|    value_loss           | 38.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -104          |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 105           |
|    time_elapsed         | 3826          |
|    total_timesteps      | 215040        |
| train/                  |               |
|    approx_kl            | 0.00031990244 |
|    clip_fraction        | 0.00542       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0262       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 16.1          |
|    n_updates            | 1040          |
|    policy_gradient_loss | -0.000399     |
|    value_loss           | 39.1          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -93.8         |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 106           |
|    time_elapsed         | 3862          |
|    total_timesteps      | 217088        |
| train/                  |               |
|    approx_kl            | 0.00011550984 |
|    clip_fraction        | 0.00166       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0246       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 21.5          |
|    n_updates            | 1050          |
|    policy_gradient_loss | 1.94e-05      |
|    value_loss           | 38.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -83.4         |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 107           |
|    time_elapsed         | 3899          |
|    total_timesteps      | 219136        |
| train/                  |               |
|    approx_kl            | 1.0588265e-05 |
|    clip_fraction        | 0.000928      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.024        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 20.7          |
|    n_updates            | 1060          |
|    policy_gradient_loss | 1.88e-05      |
|    value_loss           | 38.7          |
-------------------------------------------
Num timesteps: 220000
Best mean reward: -129.60 - Last mean reward per episode: -79.20
Saving new best model at 220000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -74.8         |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 108           |
|    time_elapsed         | 3938          |
|    total_timesteps      | 221184        |
| train/                  |               |
|    approx_kl            | 0.00025187875 |
|    clip_fraction        | 0.00186       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0208       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 22            |
|    n_updates            | 1070          |
|    policy_gradient_loss | -0.000249     |
|    value_loss           | 40.3          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -65           |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 109           |
|    time_elapsed         | 3974          |
|    total_timesteps      | 223232        |
| train/                  |               |
|    approx_kl            | 1.7458078e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0157       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 19.8          |
|    n_updates            | 1080          |
|    policy_gradient_loss | -4.12e-05     |
|    value_loss           | 41            |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -55.4        |
| time/                   |              |
|    fps                  | 56           |
|    iterations           | 110          |
|    time_elapsed         | 4017         |
|    total_timesteps      | 225280       |
| train/                  |              |
|    approx_kl            | 8.955033e-05 |
|    clip_fraction        | 0.00215      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0122      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 18.9         |
|    n_updates            | 1090         |
|    policy_gradient_loss | -8.95e-05    |
|    value_loss           | 39           |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -45           |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 111           |
|    time_elapsed         | 4053          |
|    total_timesteps      | 227328        |
| train/                  |               |
|    approx_kl            | 0.00011582856 |
|    clip_fraction        | 0.00298       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00986      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 15.8          |
|    n_updates            | 1100          |
|    policy_gradient_loss | -0.000532     |
|    value_loss           | 39.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -35.8         |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 112           |
|    time_elapsed         | 4087          |
|    total_timesteps      | 229376        |
| train/                  |               |
|    approx_kl            | 5.5691955e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00887      |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 20            |
|    n_updates            | 1110          |
|    policy_gradient_loss | 3.87e-05      |
|    value_loss           | 39.3          |
-------------------------------------------
Num timesteps: 230000
Best mean reward: -79.20 - Last mean reward per episode: -31.00
Saving new best model at 230000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -26.6        |
| time/                   |              |
|    fps                  | 56           |
|    iterations           | 113          |
|    time_elapsed         | 4121         |
|    total_timesteps      | 231424       |
| train/                  |              |
|    approx_kl            | 3.453932e-05 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00657     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 23           |
|    n_updates            | 1120         |
|    policy_gradient_loss | 1.03e-05     |
|    value_loss           | 38.4         |
------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -17.2     |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 114       |
|    time_elapsed         | 4153      |
|    total_timesteps      | 233472    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00652  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 16.9      |
|    n_updates            | 1130      |
|    policy_gradient_loss | -5.76e-10 |
|    value_loss           | 38.3      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -9.2          |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 115           |
|    time_elapsed         | 4193          |
|    total_timesteps      | 235520        |
| train/                  |               |
|    approx_kl            | 2.3296743e-05 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0077       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 18.5          |
|    n_updates            | 1140          |
|    policy_gradient_loss | -0.000103     |
|    value_loss           | 39.1          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -1.2         |
| time/                   |              |
|    fps                  | 56           |
|    iterations           | 116          |
|    time_elapsed         | 4228         |
|    total_timesteps      | 237568       |
| train/                  |              |
|    approx_kl            | 7.161929e-06 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00831     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 21.6         |
|    n_updates            | 1150         |
|    policy_gradient_loss | 1.74e-05     |
|    value_loss           | 40           |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 6             |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 117           |
|    time_elapsed         | 4265          |
|    total_timesteps      | 239616        |
| train/                  |               |
|    approx_kl            | 0.00014170306 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00482      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 13.8          |
|    n_updates            | 1160          |
|    policy_gradient_loss | -0.000198     |
|    value_loss           | 38.4          |
-------------------------------------------
Num timesteps: 240000
Best mean reward: -31.00 - Last mean reward per episode: 9.60
Saving new best model at 240000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 12.6          |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 118           |
|    time_elapsed         | 4303          |
|    total_timesteps      | 241664        |
| train/                  |               |
|    approx_kl            | 2.3464207e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00567      |
|    explained_variance   | 1.79e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 15.7          |
|    n_updates            | 1170          |
|    policy_gradient_loss | -0.000177     |
|    value_loss           | 40.2          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 19.8         |
| time/                   |              |
|    fps                  | 56           |
|    iterations           | 119          |
|    time_elapsed         | 4339         |
|    total_timesteps      | 243712       |
| train/                  |              |
|    approx_kl            | 5.124777e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00609     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 19.2         |
|    n_updates            | 1180         |
|    policy_gradient_loss | -1.93e-06    |
|    value_loss           | 39.2         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 25.6          |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 120           |
|    time_elapsed         | 4382          |
|    total_timesteps      | 245760        |
| train/                  |               |
|    approx_kl            | 1.4284218e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00526      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 16            |
|    n_updates            | 1190          |
|    policy_gradient_loss | -5.84e-05     |
|    value_loss           | 36.9          |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 1e+03          |
|    ep_rew_mean          | 29.6           |
| time/                   |                |
|    fps                  | 56             |
|    iterations           | 121            |
|    time_elapsed         | 4418           |
|    total_timesteps      | 247808         |
| train/                  |                |
|    approx_kl            | 0.000115072384 |
|    clip_fraction        | 0.00127        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00678       |
|    explained_variance   | 0              |
|    learning_rate        | 0.0003         |
|    loss                 | 21.1           |
|    n_updates            | 1200           |
|    policy_gradient_loss | -0.000144      |
|    value_loss           | 39.2           |
--------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 33.6         |
| time/                   |              |
|    fps                  | 56           |
|    iterations           | 122          |
|    time_elapsed         | 4460         |
|    total_timesteps      | 249856       |
| train/                  |              |
|    approx_kl            | 1.316244e-05 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00617     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 16.5         |
|    n_updates            | 1210         |
|    policy_gradient_loss | -6.28e-06    |
|    value_loss           | 39           |
------------------------------------------
Num timesteps: 250000
Best mean reward: 9.60 - Last mean reward per episode: 35.60
Saving new best model at 250000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 37            |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 123           |
|    time_elapsed         | 4497          |
|    total_timesteps      | 251904        |
| train/                  |               |
|    approx_kl            | 1.7360755e-05 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00491      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 22.8          |
|    n_updates            | 1220          |
|    policy_gradient_loss | -1.16e-05     |
|    value_loss           | 37.9          |
-------------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | 39.6     |
| time/                   |          |
|    fps                  | 56       |
|    iterations           | 124      |
|    time_elapsed         | 4534     |
|    total_timesteps      | 253952   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00434 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 19.9     |
|    n_updates            | 1230     |
|    policy_gradient_loss | 1.19e-09 |
|    value_loss           | 38.1     |
--------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 41.4          |
| time/                   |               |
|    fps                  | 55            |
|    iterations           | 125           |
|    time_elapsed         | 4571          |
|    total_timesteps      | 256000        |
| train/                  |               |
|    approx_kl            | 2.7995498e-05 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00527      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 17.2          |
|    n_updates            | 1240          |
|    policy_gradient_loss | -0.000181     |
|    value_loss           | 39.6          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 43.2         |
| time/                   |              |
|    fps                  | 55           |
|    iterations           | 126          |
|    time_elapsed         | 4611         |
|    total_timesteps      | 258048       |
| train/                  |              |
|    approx_kl            | 1.882133e-05 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00624     |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 17           |
|    n_updates            | 1250         |
|    policy_gradient_loss | -1.51e-05    |
|    value_loss           | 39.3         |
------------------------------------------
Num timesteps: 260000
Best mean reward: 35.60 - Last mean reward per episode: 44.80
Saving new best model at 260000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 44.8          |
| time/                   |               |
|    fps                  | 55            |
|    iterations           | 127           |
|    time_elapsed         | 4649          |
|    total_timesteps      | 260096        |
| train/                  |               |
|    approx_kl            | 0.00052373565 |
|    clip_fraction        | 0.00234       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00366      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 20.5          |
|    n_updates            | 1260          |
|    policy_gradient_loss | -0.00025      |
|    value_loss           | 38.5          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 46           |
| time/                   |              |
|    fps                  | 55           |
|    iterations           | 128          |
|    time_elapsed         | 4688         |
|    total_timesteps      | 262144       |
| train/                  |              |
|    approx_kl            | 8.813391e-05 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00497     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 20.2         |
|    n_updates            | 1270         |
|    policy_gradient_loss | -0.000206    |
|    value_loss           | 38.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 47           |
| time/                   |              |
|    fps                  | 55           |
|    iterations           | 129          |
|    time_elapsed         | 4730         |
|    total_timesteps      | 264192       |
| train/                  |              |
|    approx_kl            | 0.0002844354 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00349     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 16.1         |
|    n_updates            | 1280         |
|    policy_gradient_loss | 0.000103     |
|    value_loss           | 39.4         |
------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 46.8      |
| time/                   |           |
|    fps                  | 55        |
|    iterations           | 130       |
|    time_elapsed         | 4767      |
|    total_timesteps      | 266240    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00287  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.1      |
|    n_updates            | 1290      |
|    policy_gradient_loss | -5.32e-10 |
|    value_loss           | 38.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 47.2      |
| time/                   |           |
|    fps                  | 55        |
|    iterations           | 131       |
|    time_elapsed         | 4806      |
|    total_timesteps      | 268288    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00287  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 20.3      |
|    n_updates            | 1300      |
|    policy_gradient_loss | -5.76e-10 |
|    value_loss           | 39.3      |
---------------------------------------
Num timesteps: 270000
Best mean reward: 44.80 - Last mean reward per episode: 48.00
Saving new best model at 270000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 48        |
| time/                   |           |
|    fps                  | 55        |
|    iterations           | 132       |
|    time_elapsed         | 4848      |
|    total_timesteps      | 270336    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00287  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 16.1      |
|    n_updates            | 1310      |
|    policy_gradient_loss | -4.13e-10 |
|    value_loss           | 37.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 48.6      |
| time/                   |           |
|    fps                  | 55        |
|    iterations           | 133       |
|    time_elapsed         | 4884      |
|    total_timesteps      | 272384    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00287  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.4      |
|    n_updates            | 1320      |
|    policy_gradient_loss | -9.32e-10 |
|    value_loss           | 38.5      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 49.4          |
| time/                   |               |
|    fps                  | 55            |
|    iterations           | 134           |
|    time_elapsed         | 4929          |
|    total_timesteps      | 274432        |
| train/                  |               |
|    approx_kl            | 0.00015138835 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00132      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 18.4          |
|    n_updates            | 1330          |
|    policy_gradient_loss | -0.000112     |
|    value_loss           | 38.4          |
-------------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | 49.6     |
| time/                   |          |
|    fps                  | 55       |
|    iterations           | 135      |
|    time_elapsed         | 4969     |
|    total_timesteps      | 276480   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00128 |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.0003   |
|    loss                 | 17.6     |
|    n_updates            | 1340     |
|    policy_gradient_loss | 7.1e-10  |
|    value_loss           | 38.1     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 48.8      |
| time/                   |           |
|    fps                  | 55        |
|    iterations           | 136       |
|    time_elapsed         | 5012      |
|    total_timesteps      | 278528    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00128  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 19.9      |
|    n_updates            | 1350      |
|    policy_gradient_loss | -1.28e-09 |
|    value_loss           | 40.2      |
---------------------------------------
Num timesteps: 280000
Best mean reward: 48.00 - Last mean reward per episode: 48.60
Saving new best model at 280000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 48.6      |
| time/                   |           |
|    fps                  | 55        |
|    iterations           | 137       |
|    time_elapsed         | 5050      |
|    total_timesteps      | 280576    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00128  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 20.2      |
|    n_updates            | 1360      |
|    policy_gradient_loss | 2.65e-10  |
|    value_loss           | 38.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 48.2      |
| time/                   |           |
|    fps                  | 55        |
|    iterations           | 138       |
|    time_elapsed         | 5085      |
|    total_timesteps      | 282624    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00128  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 21.1      |
|    n_updates            | 1370      |
|    policy_gradient_loss | -3.53e-10 |
|    value_loss           | 40.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 49        |
| time/                   |           |
|    fps                  | 55        |
|    iterations           | 139       |
|    time_elapsed         | 5124      |
|    total_timesteps      | 284672    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00128  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 20        |
|    n_updates            | 1380      |
|    policy_gradient_loss | -1.26e-09 |
|    value_loss           | 39.2      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 49.6          |
| time/                   |               |
|    fps                  | 55            |
|    iterations           | 140           |
|    time_elapsed         | 5160          |
|    total_timesteps      | 286720        |
| train/                  |               |
|    approx_kl            | 1.9913656e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00159      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 19.8          |
|    n_updates            | 1390          |
|    policy_gradient_loss | -4.56e-05     |
|    value_loss           | 37.5          |
-------------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | 49.6     |
| time/                   |          |
|    fps                  | 55       |
|    iterations           | 141      |
|    time_elapsed         | 5194     |
|    total_timesteps      | 288768   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00163 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 20       |
|    n_updates            | 1400     |
|    policy_gradient_loss | 1.38e-09 |
|    value_loss           | 38.8     |
--------------------------------------
Num timesteps: 290000
Best mean reward: 48.60 - Last mean reward per episode: 50.20
Saving new best model at 290000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | 50.2     |
| time/                   |          |
|    fps                  | 55       |
|    iterations           | 142      |
|    time_elapsed         | 5227     |
|    total_timesteps      | 290816   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00163 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 22.2     |
|    n_updates            | 1410     |
|    policy_gradient_loss | 4.74e-10 |
|    value_loss           | 39.3     |
--------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 50.2          |
| time/                   |               |
|    fps                  | 55            |
|    iterations           | 143           |
|    time_elapsed         | 5260          |
|    total_timesteps      | 292864        |
| train/                  |               |
|    approx_kl            | 8.9358655e-06 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00181      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 21.8          |
|    n_updates            | 1420          |
|    policy_gradient_loss | -4.55e-06     |
|    value_loss           | 39.4          |
-------------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | 49.4     |
| time/                   |          |
|    fps                  | 55       |
|    iterations           | 144      |
|    time_elapsed         | 5293     |
|    total_timesteps      | 294912   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00193 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 21.7     |
|    n_updates            | 1430     |
|    policy_gradient_loss | 1.48e-10 |
|    value_loss           | 38.3     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 49.6      |
| time/                   |           |
|    fps                  | 55        |
|    iterations           | 145       |
|    time_elapsed         | 5326      |
|    total_timesteps      | 296960    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00193  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 14.9      |
|    n_updates            | 1440      |
|    policy_gradient_loss | -6.91e-10 |
|    value_loss           | 39.1      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 49.8          |
| time/                   |               |
|    fps                  | 55            |
|    iterations           | 146           |
|    time_elapsed         | 5359          |
|    total_timesteps      | 299008        |
| train/                  |               |
|    approx_kl            | 8.3663705e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00119      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 17.2          |
|    n_updates            | 1450          |
|    policy_gradient_loss | -0.000115     |
|    value_loss           | 37.6          |
-------------------------------------------
Num timesteps: 300000
Best mean reward: 50.20 - Last mean reward per episode: 49.60
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | 49.6     |
| time/                   |          |
|    fps                  | 55       |
|    iterations           | 147      |
|    time_elapsed         | 5392     |
|    total_timesteps      | 301056   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00108 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 20       |
|    n_updates            | 1460     |
|    policy_gradient_loss | 7.55e-10 |
|    value_loss           | 38.3     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | 49.6     |
| time/                   |          |
|    fps                  | 55       |
|    iterations           | 148      |
|    time_elapsed         | 5425     |
|    total_timesteps      | 303104   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00108 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 15.4     |
|    n_updates            | 1470     |
|    policy_gradient_loss | 1.58e-09 |
|    value_loss           | 39.4     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | 50.6     |
| time/                   |          |
|    fps                  | 55       |
|    iterations           | 149      |
|    time_elapsed         | 5457     |
|    total_timesteps      | 305152   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00108 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 19.5     |
|    n_updates            | 1480     |
|    policy_gradient_loss | 6.61e-10 |
|    value_loss           | 39.4     |
--------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 50.2          |
| time/                   |               |
|    fps                  | 55            |
|    iterations           | 150           |
|    time_elapsed         | 5490          |
|    total_timesteps      | 307200        |
| train/                  |               |
|    approx_kl            | 0.00028192217 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0023       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 21            |
|    n_updates            | 1490          |
|    policy_gradient_loss | -1.58e-05     |
|    value_loss           | 38.8          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 50.2         |
| time/                   |              |
|    fps                  | 55           |
|    iterations           | 151          |
|    time_elapsed         | 5522         |
|    total_timesteps      | 309248       |
| train/                  |              |
|    approx_kl            | 0.0002066819 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00121     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 18.7         |
|    n_updates            | 1500         |
|    policy_gradient_loss | -3.64e-05    |
|    value_loss           | 39           |
------------------------------------------
Num timesteps: 310000
Best mean reward: 50.20 - Last mean reward per episode: 50.80
Saving new best model at 310000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 51.2          |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 152           |
|    time_elapsed         | 5555          |
|    total_timesteps      | 311296        |
| train/                  |               |
|    approx_kl            | 1.3117795e-05 |
|    clip_fraction        | 0.000537      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00136      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 17            |
|    n_updates            | 1510          |
|    policy_gradient_loss | 0.000133      |
|    value_loss           | 38.1          |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 50.6      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 153       |
|    time_elapsed         | 5587      |
|    total_timesteps      | 313344    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00102  |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 17.2      |
|    n_updates            | 1520      |
|    policy_gradient_loss | -4.13e-10 |
|    value_loss           | 38.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 50.2      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 154       |
|    time_elapsed         | 5620      |
|    total_timesteps      | 315392    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00102  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 19.5      |
|    n_updates            | 1530      |
|    policy_gradient_loss | -8.27e-10 |
|    value_loss           | 38.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 51.2      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 155       |
|    time_elapsed         | 5652      |
|    total_timesteps      | 317440    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00102  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 21.1      |
|    n_updates            | 1540      |
|    policy_gradient_loss | -1.28e-09 |
|    value_loss           | 39.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 51.4      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 156       |
|    time_elapsed         | 5685      |
|    total_timesteps      | 319488    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00102  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 20        |
|    n_updates            | 1550      |
|    policy_gradient_loss | -1.96e-09 |
|    value_loss           | 37.4      |
---------------------------------------
Num timesteps: 320000
Best mean reward: 50.80 - Last mean reward per episode: 51.80
Saving new best model at 320000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | 52.4     |
| time/                   |          |
|    fps                  | 56       |
|    iterations           | 157      |
|    time_elapsed         | 5718     |
|    total_timesteps      | 321536   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00102 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 20.7     |
|    n_updates            | 1560     |
|    policy_gradient_loss | 4.38e-10 |
|    value_loss           | 38.8     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | 52.6     |
| time/                   |          |
|    fps                  | 56       |
|    iterations           | 158      |
|    time_elapsed         | 5751     |
|    total_timesteps      | 323584   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00102 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 24.3     |
|    n_updates            | 1570     |
|    policy_gradient_loss | 1.66e-10 |
|    value_loss           | 39.1     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | 52.6     |
| time/                   |          |
|    fps                  | 56       |
|    iterations           | 159      |
|    time_elapsed         | 5784     |
|    total_timesteps      | 325632   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00102 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 22.1     |
|    n_updates            | 1580     |
|    policy_gradient_loss | 5.43e-10 |
|    value_loss           | 40.4     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | 52.4     |
| time/                   |          |
|    fps                  | 56       |
|    iterations           | 160      |
|    time_elapsed         | 5817     |
|    total_timesteps      | 327680   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00102 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 18.5     |
|    n_updates            | 1590     |
|    policy_gradient_loss | 1.33e-09 |
|    value_loss           | 37.6     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.2      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 161       |
|    time_elapsed         | 5850      |
|    total_timesteps      | 329728    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00102  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 15.7      |
|    n_updates            | 1600      |
|    policy_gradient_loss | -1.73e-09 |
|    value_loss           | 37.4      |
---------------------------------------
Num timesteps: 330000
Best mean reward: 51.80 - Last mean reward per episode: 53.40
Saving new best model at 330000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | 53.8     |
| time/                   |          |
|    fps                  | 56       |
|    iterations           | 162      |
|    time_elapsed         | 5882     |
|    total_timesteps      | 331776   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00102 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 22.6     |
|    n_updates            | 1610     |
|    policy_gradient_loss | 8.17e-10 |
|    value_loss           | 39       |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | 53.6     |
| time/                   |          |
|    fps                  | 56       |
|    iterations           | 163      |
|    time_elapsed         | 5915     |
|    total_timesteps      | 333824   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00102 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 22.2     |
|    n_updates            | 1620     |
|    policy_gradient_loss | -1.1e-09 |
|    value_loss           | 38.5     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | 53.6     |
| time/                   |          |
|    fps                  | 56       |
|    iterations           | 164      |
|    time_elapsed         | 5947     |
|    total_timesteps      | 335872   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00102 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 18       |
|    n_updates            | 1630     |
|    policy_gradient_loss | 4.67e-10 |
|    value_loss           | 39.4     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.8      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 165       |
|    time_elapsed         | 5980      |
|    total_timesteps      | 337920    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00102  |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 23.8      |
|    n_updates            | 1640      |
|    policy_gradient_loss | -1.33e-09 |
|    value_loss           | 38.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.8      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 166       |
|    time_elapsed         | 6012      |
|    total_timesteps      | 339968    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00102  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 17.6      |
|    n_updates            | 1650      |
|    policy_gradient_loss | 1.58e-10  |
|    value_loss           | 38        |
---------------------------------------
Num timesteps: 340000
Best mean reward: 53.40 - Last mean reward per episode: 53.20
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 53.2          |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 167           |
|    time_elapsed         | 6045          |
|    total_timesteps      | 342016        |
| train/                  |               |
|    approx_kl            | 0.00016747531 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00186      |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 20.5          |
|    n_updates            | 1660          |
|    policy_gradient_loss | -0.000189     |
|    value_loss           | 38.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 52.8          |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 168           |
|    time_elapsed         | 6078          |
|    total_timesteps      | 344064        |
| train/                  |               |
|    approx_kl            | 0.00015564007 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000984     |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 17.4          |
|    n_updates            | 1670          |
|    policy_gradient_loss | -4.03e-05     |
|    value_loss           | 40.1          |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.4      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 169       |
|    time_elapsed         | 6111      |
|    total_timesteps      | 346112    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000859 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 17.3      |
|    n_updates            | 1680      |
|    policy_gradient_loss | 1.28e-09  |
|    value_loss           | 37        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.4      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 170       |
|    time_elapsed         | 6143      |
|    total_timesteps      | 348160    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000859 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 17.5      |
|    n_updates            | 1690      |
|    policy_gradient_loss | 5.53e-10  |
|    value_loss           | 38.5      |
---------------------------------------
Num timesteps: 350000
Best mean reward: 53.40 - Last mean reward per episode: 52.20
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.2      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 171       |
|    time_elapsed         | 6177      |
|    total_timesteps      | 350208    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000859 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 19.4      |
|    n_updates            | 1700      |
|    policy_gradient_loss | -5.54e-10 |
|    value_loss           | 39.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.2      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 172       |
|    time_elapsed         | 6209      |
|    total_timesteps      | 352256    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000859 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 16.3      |
|    n_updates            | 1710      |
|    policy_gradient_loss | 1.22e-09  |
|    value_loss           | 39.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.2      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 173       |
|    time_elapsed         | 6241      |
|    total_timesteps      | 354304    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000859 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 17.5      |
|    n_updates            | 1720      |
|    policy_gradient_loss | -4.13e-10 |
|    value_loss           | 38.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 54        |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 174       |
|    time_elapsed         | 6275      |
|    total_timesteps      | 356352    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000859 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.7      |
|    n_updates            | 1730      |
|    policy_gradient_loss | 1.1e-09   |
|    value_loss           | 36.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.4      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 175       |
|    time_elapsed         | 6307      |
|    total_timesteps      | 358400    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000859 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 15.7      |
|    n_updates            | 1740      |
|    policy_gradient_loss | 1.69e-09  |
|    value_loss           | 38.7      |
---------------------------------------
Num timesteps: 360000
Best mean reward: 53.40 - Last mean reward per episode: 53.60
Saving new best model at 360000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.6      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 176       |
|    time_elapsed         | 6340      |
|    total_timesteps      | 360448    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000859 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 19.5      |
|    n_updates            | 1750      |
|    policy_gradient_loss | -1.11e-10 |
|    value_loss           | 39.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.4      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 177       |
|    time_elapsed         | 6373      |
|    total_timesteps      | 362496    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000859 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.2      |
|    n_updates            | 1760      |
|    policy_gradient_loss | 1.17e-09  |
|    value_loss           | 37.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.8      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 178       |
|    time_elapsed         | 6406      |
|    total_timesteps      | 364544    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000859 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.2      |
|    n_updates            | 1770      |
|    policy_gradient_loss | 4.76e-10  |
|    value_loss           | 38.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53        |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 179       |
|    time_elapsed         | 6439      |
|    total_timesteps      | 366592    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000859 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 15.8      |
|    n_updates            | 1780      |
|    policy_gradient_loss | -1.17e-09 |
|    value_loss           | 39.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.8      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 180       |
|    time_elapsed         | 6472      |
|    total_timesteps      | 368640    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000859 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 16.5      |
|    n_updates            | 1790      |
|    policy_gradient_loss | -1.34e-09 |
|    value_loss           | 38.6      |
---------------------------------------
Num timesteps: 370000
Best mean reward: 53.60 - Last mean reward per episode: 53.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53        |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 181       |
|    time_elapsed         | 6506      |
|    total_timesteps      | 370688    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000859 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 21.4      |
|    n_updates            | 1800      |
|    policy_gradient_loss | 1.2e-09   |
|    value_loss           | 40.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 182       |
|    time_elapsed         | 6538      |
|    total_timesteps      | 372736    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000859 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 16.8      |
|    n_updates            | 1810      |
|    policy_gradient_loss | 3.51e-10  |
|    value_loss           | 38.2      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 53.2          |
| time/                   |               |
|    fps                  | 57            |
|    iterations           | 183           |
|    time_elapsed         | 6571          |
|    total_timesteps      | 374784        |
| train/                  |               |
|    approx_kl            | 1.0430231e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00102      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 23            |
|    n_updates            | 1820          |
|    policy_gradient_loss | -5.93e-05     |
|    value_loss           | 38.7          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 52.6         |
| time/                   |              |
|    fps                  | 57           |
|    iterations           | 184          |
|    time_elapsed         | 6604         |
|    total_timesteps      | 376832       |
| train/                  |              |
|    approx_kl            | 0.0001897412 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000527    |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 26.7         |
|    n_updates            | 1830         |
|    policy_gradient_loss | -3.51e-05    |
|    value_loss           | 39.1         |
------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 185       |
|    time_elapsed         | 6637      |
|    total_timesteps      | 378880    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000406 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 21.4      |
|    n_updates            | 1840      |
|    policy_gradient_loss | 2.44e-10  |
|    value_loss           | 39.8      |
---------------------------------------
Num timesteps: 380000
Best mean reward: 53.60 - Last mean reward per episode: 53.40
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 186       |
|    time_elapsed         | 6670      |
|    total_timesteps      | 380928    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000406 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 20.7      |
|    n_updates            | 1850      |
|    policy_gradient_loss | 4.85e-10  |
|    value_loss           | 38.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.8      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 187       |
|    time_elapsed         | 6703      |
|    total_timesteps      | 382976    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000406 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 19        |
|    n_updates            | 1860      |
|    policy_gradient_loss | 5.8e-10   |
|    value_loss           | 38.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.2      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 188       |
|    time_elapsed         | 6736      |
|    total_timesteps      | 385024    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000406 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 21.5      |
|    n_updates            | 1870      |
|    policy_gradient_loss | 7.72e-10  |
|    value_loss           | 39        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 189       |
|    time_elapsed         | 6769      |
|    total_timesteps      | 387072    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000406 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 22.8      |
|    n_updates            | 1880      |
|    policy_gradient_loss | 4.23e-10  |
|    value_loss           | 40        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 51.8      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 190       |
|    time_elapsed         | 6801      |
|    total_timesteps      | 389120    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000406 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 21.1      |
|    n_updates            | 1890      |
|    policy_gradient_loss | -1.63e-09 |
|    value_loss           | 40.8      |
---------------------------------------
Num timesteps: 390000
Best mean reward: 53.60 - Last mean reward per episode: 51.80
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52        |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 191       |
|    time_elapsed         | 6833      |
|    total_timesteps      | 391168    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000406 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 16.3      |
|    n_updates            | 1900      |
|    policy_gradient_loss | 2.01e-09  |
|    value_loss           | 39.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 51.2      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 192       |
|    time_elapsed         | 6866      |
|    total_timesteps      | 393216    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000406 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 22.3      |
|    n_updates            | 1910      |
|    policy_gradient_loss | -1.23e-09 |
|    value_loss           | 40.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.2      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 193       |
|    time_elapsed         | 6898      |
|    total_timesteps      | 395264    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000406 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 19.9      |
|    n_updates            | 1920      |
|    policy_gradient_loss | 7.86e-10  |
|    value_loss           | 38.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.2      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 194       |
|    time_elapsed         | 6931      |
|    total_timesteps      | 397312    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000406 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.8      |
|    n_updates            | 1930      |
|    policy_gradient_loss | 9.97e-10  |
|    value_loss           | 37.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.2      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 195       |
|    time_elapsed         | 6963      |
|    total_timesteps      | 399360    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000406 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 13.7      |
|    n_updates            | 1940      |
|    policy_gradient_loss | 5.92e-10  |
|    value_loss           | 38.9      |
---------------------------------------
Num timesteps: 400000
Best mean reward: 53.60 - Last mean reward per episode: 52.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 196       |
|    time_elapsed         | 6996      |
|    total_timesteps      | 401408    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000406 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.9      |
|    n_updates            | 1950      |
|    policy_gradient_loss | 6.97e-10  |
|    value_loss           | 38.4      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 52            |
| time/                   |               |
|    fps                  | 57            |
|    iterations           | 197           |
|    time_elapsed         | 7029          |
|    total_timesteps      | 403456        |
| train/                  |               |
|    approx_kl            | 0.00011410029 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000225     |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 17.6          |
|    n_updates            | 1960          |
|    policy_gradient_loss | -7.29e-05     |
|    value_loss           | 37.8          |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 51.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 198       |
|    time_elapsed         | 7062      |
|    total_timesteps      | 405504    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000202 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18        |
|    n_updates            | 1970      |
|    policy_gradient_loss | 1.73e-10  |
|    value_loss           | 39.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 199       |
|    time_elapsed         | 7094      |
|    total_timesteps      | 407552    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000202 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 26.3      |
|    n_updates            | 1980      |
|    policy_gradient_loss | 5.43e-10  |
|    value_loss           | 38.7      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 52.6          |
| time/                   |               |
|    fps                  | 57            |
|    iterations           | 200           |
|    time_elapsed         | 7127          |
|    total_timesteps      | 409600        |
| train/                  |               |
|    approx_kl            | 2.6494003e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000142     |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 16            |
|    n_updates            | 1990          |
|    policy_gradient_loss | -7.26e-06     |
|    value_loss           | 37.2          |
-------------------------------------------
Num timesteps: 410000
Best mean reward: 53.60 - Last mean reward per episode: 52.40
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 201       |
|    time_elapsed         | 7160      |
|    total_timesteps      | 411648    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000148 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 15.5      |
|    n_updates            | 2000      |
|    policy_gradient_loss | 2.31e-10  |
|    value_loss           | 37.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 202       |
|    time_elapsed         | 7193      |
|    total_timesteps      | 413696    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000148 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 19.8      |
|    n_updates            | 2010      |
|    policy_gradient_loss | 3.64e-11  |
|    value_loss           | 37.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53        |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 203       |
|    time_elapsed         | 7225      |
|    total_timesteps      | 415744    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000148 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 24.1      |
|    n_updates            | 2020      |
|    policy_gradient_loss | 1.48e-09  |
|    value_loss           | 39.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53        |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 204       |
|    time_elapsed         | 7259      |
|    total_timesteps      | 417792    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000148 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 22.2      |
|    n_updates            | 2030      |
|    policy_gradient_loss | -2.27e-10 |
|    value_loss           | 38.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 205       |
|    time_elapsed         | 7291      |
|    total_timesteps      | 419840    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000148 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 20.5      |
|    n_updates            | 2040      |
|    policy_gradient_loss | 1.25e-09  |
|    value_loss           | 38.4      |
---------------------------------------
Num timesteps: 420000
Best mean reward: 53.60 - Last mean reward per episode: 53.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53        |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 206       |
|    time_elapsed         | 7324      |
|    total_timesteps      | 421888    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000148 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 21.1      |
|    n_updates            | 2050      |
|    policy_gradient_loss | 2.68e-10  |
|    value_loss           | 38.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.8      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 207       |
|    time_elapsed         | 7357      |
|    total_timesteps      | 423936    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000148 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 19.4      |
|    n_updates            | 2060      |
|    policy_gradient_loss | 3.13e-11  |
|    value_loss           | 37.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.2      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 208       |
|    time_elapsed         | 7390      |
|    total_timesteps      | 425984    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000148 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 26.3      |
|    n_updates            | 2070      |
|    policy_gradient_loss | -4.73e-10 |
|    value_loss           | 40.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 209       |
|    time_elapsed         | 7422      |
|    total_timesteps      | 428032    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000148 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 16.6      |
|    n_updates            | 2080      |
|    policy_gradient_loss | 4.59e-10  |
|    value_loss           | 37.9      |
---------------------------------------
Num timesteps: 430000
Best mean reward: 53.60 - Last mean reward per episode: 52.60
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 210       |
|    time_elapsed         | 7455      |
|    total_timesteps      | 430080    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000148 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 16.2      |
|    n_updates            | 2090      |
|    policy_gradient_loss | -8.09e-10 |
|    value_loss           | 38.3      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 52.8          |
| time/                   |               |
|    fps                  | 57            |
|    iterations           | 211           |
|    time_elapsed         | 7489          |
|    total_timesteps      | 432128        |
| train/                  |               |
|    approx_kl            | 0.00031849428 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000327     |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 19.5          |
|    n_updates            | 2100          |
|    policy_gradient_loss | -0.00015      |
|    value_loss           | 39.2          |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 212       |
|    time_elapsed         | 7522      |
|    total_timesteps      | 434176    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 14.9      |
|    n_updates            | 2110      |
|    policy_gradient_loss | -6.13e-10 |
|    value_loss           | 38.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 54.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 213       |
|    time_elapsed         | 7555      |
|    total_timesteps      | 436224    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 21.1      |
|    n_updates            | 2120      |
|    policy_gradient_loss | -2.91e-12 |
|    value_loss           | 38.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 54.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 214       |
|    time_elapsed         | 7587      |
|    total_timesteps      | 438272    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 16.7      |
|    n_updates            | 2130      |
|    policy_gradient_loss | -1.35e-10 |
|    value_loss           | 37.8      |
---------------------------------------
Num timesteps: 440000
Best mean reward: 53.60 - Last mean reward per episode: 55.20
Saving new best model at 440000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 55.2      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 215       |
|    time_elapsed         | 7620      |
|    total_timesteps      | 440320    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 17.7      |
|    n_updates            | 2140      |
|    policy_gradient_loss | -5.82e-10 |
|    value_loss           | 37.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 55.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 216       |
|    time_elapsed         | 7653      |
|    total_timesteps      | 442368    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 22.6      |
|    n_updates            | 2150      |
|    policy_gradient_loss | -6.2e-10  |
|    value_loss           | 38.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 55.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 217       |
|    time_elapsed         | 7685      |
|    total_timesteps      | 444416    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 16.1      |
|    n_updates            | 2160      |
|    policy_gradient_loss | -2.49e-10 |
|    value_loss           | 39        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 55.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 218       |
|    time_elapsed         | 7719      |
|    total_timesteps      | 446464    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 23        |
|    n_updates            | 2170      |
|    policy_gradient_loss | -9.88e-10 |
|    value_loss           | 38.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 55.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 219       |
|    time_elapsed         | 7752      |
|    total_timesteps      | 448512    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 17.2      |
|    n_updates            | 2180      |
|    policy_gradient_loss | 1.44e-09  |
|    value_loss           | 37.5      |
---------------------------------------
Num timesteps: 450000
Best mean reward: 55.20 - Last mean reward per episode: 55.60
Saving new best model at 450000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 55.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 220       |
|    time_elapsed         | 7785      |
|    total_timesteps      | 450560    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 19.4      |
|    n_updates            | 2190      |
|    policy_gradient_loss | 7.25e-10  |
|    value_loss           | 39.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 55.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 221       |
|    time_elapsed         | 7817      |
|    total_timesteps      | 452608    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 22.6      |
|    n_updates            | 2200      |
|    policy_gradient_loss | -5.54e-10 |
|    value_loss           | 39.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 54.8      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 222       |
|    time_elapsed         | 7850      |
|    total_timesteps      | 454656    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 17.9      |
|    n_updates            | 2210      |
|    policy_gradient_loss | 4.99e-10  |
|    value_loss           | 38.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 54        |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 223       |
|    time_elapsed         | 7882      |
|    total_timesteps      | 456704    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 19.8      |
|    n_updates            | 2220      |
|    policy_gradient_loss | -8.51e-10 |
|    value_loss           | 38.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 55        |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 224       |
|    time_elapsed         | 7915      |
|    total_timesteps      | 458752    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 24.6      |
|    n_updates            | 2230      |
|    policy_gradient_loss | -3.11e-10 |
|    value_loss           | 38.9      |
---------------------------------------
Num timesteps: 460000
Best mean reward: 55.60 - Last mean reward per episode: 55.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 55        |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 225       |
|    time_elapsed         | 7948      |
|    total_timesteps      | 460800    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 21        |
|    n_updates            | 2240      |
|    policy_gradient_loss | -1.66e-10 |
|    value_loss           | 37.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 55        |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 226       |
|    time_elapsed         | 7980      |
|    total_timesteps      | 462848    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 20.4      |
|    n_updates            | 2250      |
|    policy_gradient_loss | 1.83e-10  |
|    value_loss           | 38.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 55.4      |
| time/                   |           |
|    fps                  | 58        |
|    iterations           | 227       |
|    time_elapsed         | 8013      |
|    total_timesteps      | 464896    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 19.6      |
|    n_updates            | 2260      |
|    policy_gradient_loss | -7.32e-10 |
|    value_loss           | 37.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 56        |
| time/                   |           |
|    fps                  | 58        |
|    iterations           | 228       |
|    time_elapsed         | 8045      |
|    total_timesteps      | 466944    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.5      |
|    n_updates            | 2270      |
|    policy_gradient_loss | -4.74e-10 |
|    value_loss           | 37.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 56.8      |
| time/                   |           |
|    fps                  | 58        |
|    iterations           | 229       |
|    time_elapsed         | 8079      |
|    total_timesteps      | 468992    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 16.1      |
|    n_updates            | 2280      |
|    policy_gradient_loss | -6.22e-10 |
|    value_loss           | 38.7      |
---------------------------------------
Num timesteps: 470000
Best mean reward: 55.60 - Last mean reward per episode: 56.00
Saving new best model at 470000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 55.8      |
| time/                   |           |
|    fps                  | 58        |
|    iterations           | 230       |
|    time_elapsed         | 8112      |
|    total_timesteps      | 471040    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 25.1      |
|    n_updates            | 2290      |
|    policy_gradient_loss | 5.86e-10  |
|    value_loss           | 38.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 56.2      |
| time/                   |           |
|    fps                  | 58        |
|    iterations           | 231       |
|    time_elapsed         | 8147      |
|    total_timesteps      | 473088    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 24.9      |
|    n_updates            | 2300      |
|    policy_gradient_loss | 5.07e-10  |
|    value_loss           | 38.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 56.6      |
| time/                   |           |
|    fps                  | 58        |
|    iterations           | 232       |
|    time_elapsed         | 8183      |
|    total_timesteps      | 475136    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 21.5      |
|    n_updates            | 2310      |
|    policy_gradient_loss | 1.05e-09  |
|    value_loss           | 39.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 56.4      |
| time/                   |           |
|    fps                  | 58        |
|    iterations           | 233       |
|    time_elapsed         | 8219      |
|    total_timesteps      | 477184    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 15.8      |
|    n_updates            | 2320      |
|    policy_gradient_loss | -7.54e-10 |
|    value_loss           | 36.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 57.4      |
| time/                   |           |
|    fps                  | 58        |
|    iterations           | 234       |
|    time_elapsed         | 8252      |
|    total_timesteps      | 479232    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.6      |
|    n_updates            | 2330      |
|    policy_gradient_loss | 8.11e-10  |
|    value_loss           | 40.2      |
---------------------------------------
Num timesteps: 480000
Best mean reward: 56.00 - Last mean reward per episode: 56.80
Saving new best model at 480000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 56.6      |
| time/                   |           |
|    fps                  | 58        |
|    iterations           | 235       |
|    time_elapsed         | 8286      |
|    total_timesteps      | 481280    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000352 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 17.8      |
|    n_updates            | 2340      |
|    policy_gradient_loss | 7.83e-10  |
|    value_loss           | 39.2      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 56.2          |
| time/                   |               |
|    fps                  | 58            |
|    iterations           | 236           |
|    time_elapsed         | 8326          |
|    total_timesteps      | 483328        |
| train/                  |               |
|    approx_kl            | 0.00021987362 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00015      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 25.4          |
|    n_updates            | 2350          |
|    policy_gradient_loss | -9.21e-05     |
|    value_loss           | 41.2          |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 56        |
| time/                   |           |
|    fps                  | 58        |
|    iterations           | 237       |
|    time_elapsed         | 8367      |
|    total_timesteps      | 485376    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 15.3      |
|    n_updates            | 2360      |
|    policy_gradient_loss | -2.18e-10 |
|    value_loss           | 38        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 56.8      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 238       |
|    time_elapsed         | 8408      |
|    total_timesteps      | 487424    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 23        |
|    n_updates            | 2370      |
|    policy_gradient_loss | 3.54e-10  |
|    value_loss           | 39.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 57.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 239       |
|    time_elapsed         | 8445      |
|    total_timesteps      | 489472    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 16.8      |
|    n_updates            | 2380      |
|    policy_gradient_loss | -1.42e-09 |
|    value_loss           | 39        |
---------------------------------------
Num timesteps: 490000
Best mean reward: 56.80 - Last mean reward per episode: 57.60
Saving new best model at 490000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 57.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 240       |
|    time_elapsed         | 8482      |
|    total_timesteps      | 491520    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.2      |
|    n_updates            | 2390      |
|    policy_gradient_loss | 5.84e-10  |
|    value_loss           | 38.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 58.2      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 241       |
|    time_elapsed         | 8519      |
|    total_timesteps      | 493568    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 21.7      |
|    n_updates            | 2400      |
|    policy_gradient_loss | 1.16e-09  |
|    value_loss           | 38.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 58.2      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 242       |
|    time_elapsed         | 8554      |
|    total_timesteps      | 495616    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 19.8      |
|    n_updates            | 2410      |
|    policy_gradient_loss | -7.28e-11 |
|    value_loss           | 38.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 57.8      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 243       |
|    time_elapsed         | 8593      |
|    total_timesteps      | 497664    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 16.6      |
|    n_updates            | 2420      |
|    policy_gradient_loss | 5.98e-10  |
|    value_loss           | 37.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 57.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 244       |
|    time_elapsed         | 8633      |
|    total_timesteps      | 499712    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.3      |
|    n_updates            | 2430      |
|    policy_gradient_loss | 6.1e-10   |
|    value_loss           | 39.6      |
---------------------------------------
Num timesteps: 500000
Best mean reward: 57.60 - Last mean reward per episode: 57.40
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 57        |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 245       |
|    time_elapsed         | 8676      |
|    total_timesteps      | 501760    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 19.7      |
|    n_updates            | 2440      |
|    policy_gradient_loss | -4.66e-10 |
|    value_loss           | 39.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 57.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 246       |
|    time_elapsed         | 8717      |
|    total_timesteps      | 503808    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 19.6      |
|    n_updates            | 2450      |
|    policy_gradient_loss | -3.37e-10 |
|    value_loss           | 39.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 57        |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 247       |
|    time_elapsed         | 8755      |
|    total_timesteps      | 505856    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 23.4      |
|    n_updates            | 2460      |
|    policy_gradient_loss | -1.82e-09 |
|    value_loss           | 37.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 56.2      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 248       |
|    time_elapsed         | 8790      |
|    total_timesteps      | 507904    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 20        |
|    n_updates            | 2470      |
|    policy_gradient_loss | 4.48e-10  |
|    value_loss           | 38.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 55.8      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 249       |
|    time_elapsed         | 8826      |
|    total_timesteps      | 509952    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 19.9      |
|    n_updates            | 2480      |
|    policy_gradient_loss | 1.31e-09  |
|    value_loss           | 38.9      |
---------------------------------------
Num timesteps: 510000
Best mean reward: 57.60 - Last mean reward per episode: 56.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 54.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 250       |
|    time_elapsed         | 8863      |
|    total_timesteps      | 512000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 17.9      |
|    n_updates            | 2490      |
|    policy_gradient_loss | -1.05e-10 |
|    value_loss           | 38.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 54.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 251       |
|    time_elapsed         | 8897      |
|    total_timesteps      | 514048    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 17.8      |
|    n_updates            | 2500      |
|    policy_gradient_loss | 1.09e-09  |
|    value_loss           | 39.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 54.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 252       |
|    time_elapsed         | 8933      |
|    total_timesteps      | 516096    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.9      |
|    n_updates            | 2510      |
|    policy_gradient_loss | 4.09e-10  |
|    value_loss           | 40.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 253       |
|    time_elapsed         | 8967      |
|    total_timesteps      | 518144    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 16.4      |
|    n_updates            | 2520      |
|    policy_gradient_loss | 1.23e-09  |
|    value_loss           | 38.1      |
---------------------------------------
Num timesteps: 520000
Best mean reward: 57.60 - Last mean reward per episode: 53.60
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 254       |
|    time_elapsed         | 9006      |
|    total_timesteps      | 520192    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 19.6      |
|    n_updates            | 2530      |
|    policy_gradient_loss | -1.64e-09 |
|    value_loss           | 38.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 255       |
|    time_elapsed         | 9050      |
|    total_timesteps      | 522240    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 25.1      |
|    n_updates            | 2540      |
|    policy_gradient_loss | 3.69e-10  |
|    value_loss           | 37.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 54.2      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 256       |
|    time_elapsed         | 9086      |
|    total_timesteps      | 524288    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.2      |
|    n_updates            | 2550      |
|    policy_gradient_loss | 4.37e-10  |
|    value_loss           | 39.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 54        |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 257       |
|    time_elapsed         | 9123      |
|    total_timesteps      | 526336    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 16.4      |
|    n_updates            | 2560      |
|    policy_gradient_loss | 7.64e-10  |
|    value_loss           | 37.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 54.2      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 258       |
|    time_elapsed         | 9162      |
|    total_timesteps      | 528384    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 17.5      |
|    n_updates            | 2570      |
|    policy_gradient_loss | -1.72e-10 |
|    value_loss           | 38.2      |
---------------------------------------
Num timesteps: 530000
Best mean reward: 57.60 - Last mean reward per episode: 54.40
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 54.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 259       |
|    time_elapsed         | 9210      |
|    total_timesteps      | 530432    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 13.9      |
|    n_updates            | 2580      |
|    policy_gradient_loss | -2.21e-09 |
|    value_loss           | 37.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 260       |
|    time_elapsed         | 9248      |
|    total_timesteps      | 532480    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 22.2      |
|    n_updates            | 2590      |
|    policy_gradient_loss | -7.22e-10 |
|    value_loss           | 39.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.2      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 261       |
|    time_elapsed         | 9286      |
|    total_timesteps      | 534528    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 22.8      |
|    n_updates            | 2600      |
|    policy_gradient_loss | 2.4e-11   |
|    value_loss           | 40.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53        |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 262       |
|    time_elapsed         | 9322      |
|    total_timesteps      | 536576    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 17.8      |
|    n_updates            | 2610      |
|    policy_gradient_loss | -7.41e-10 |
|    value_loss           | 39.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.2      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 263       |
|    time_elapsed         | 9358      |
|    total_timesteps      | 538624    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 16.1      |
|    n_updates            | 2620      |
|    policy_gradient_loss | -1.39e-10 |
|    value_loss           | 39.2      |
---------------------------------------
Num timesteps: 540000
Best mean reward: 57.60 - Last mean reward per episode: 51.80
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 51.8      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 264       |
|    time_elapsed         | 9397      |
|    total_timesteps      | 540672    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 18.5      |
|    n_updates            | 2630      |
|    policy_gradient_loss | 4.34e-10  |
|    value_loss           | 38.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 51.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 265       |
|    time_elapsed         | 9434      |
|    total_timesteps      | 542720    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 17.9      |
|    n_updates            | 2640      |
|    policy_gradient_loss | 5.47e-10  |
|    value_loss           | 37.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 51.2      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 266       |
|    time_elapsed         | 9473      |
|    total_timesteps      | 544768    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 21.5      |
|    n_updates            | 2650      |
|    policy_gradient_loss | -6.75e-10 |
|    value_loss           | 40.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.2      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 267       |
|    time_elapsed         | 9508      |
|    total_timesteps      | 546816    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 17.1      |
|    n_updates            | 2660      |
|    policy_gradient_loss | 9.47e-10  |
|    value_loss           | 37.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53        |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 268       |
|    time_elapsed         | 9548      |
|    total_timesteps      | 548864    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 23.2      |
|    n_updates            | 2670      |
|    policy_gradient_loss | -1.35e-09 |
|    value_loss           | 37.1      |
---------------------------------------
Num timesteps: 550000
Best mean reward: 57.60 - Last mean reward per episode: 52.80
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.8      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 269       |
|    time_elapsed         | 9585      |
|    total_timesteps      | 550912    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 17.3      |
|    n_updates            | 2680      |
|    policy_gradient_loss | -8.03e-10 |
|    value_loss           | 38.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 270       |
|    time_elapsed         | 9618      |
|    total_timesteps      | 552960    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 20.6      |
|    n_updates            | 2690      |
|    policy_gradient_loss | 2.26e-10  |
|    value_loss           | 39.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 271       |
|    time_elapsed         | 9656      |
|    total_timesteps      | 555008    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 16.4      |
|    n_updates            | 2700      |
|    policy_gradient_loss | 8e-10     |
|    value_loss           | 38.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 272       |
|    time_elapsed         | 9692      |
|    total_timesteps      | 557056    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 20.5      |
|    n_updates            | 2710      |
|    policy_gradient_loss | -3.51e-10 |
|    value_loss           | 38.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 273       |
|    time_elapsed         | 9726      |
|    total_timesteps      | 559104    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 17.6      |
|    n_updates            | 2720      |
|    policy_gradient_loss | 2.25e-10  |
|    value_loss           | 38.1      |
---------------------------------------
Num timesteps: 560000
Best mean reward: 57.60 - Last mean reward per episode: 52.40
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52        |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 274       |
|    time_elapsed         | 9762      |
|    total_timesteps      | 561152    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 19.2      |
|    n_updates            | 2730      |
|    policy_gradient_loss | 5.03e-10  |
|    value_loss           | 37.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 51.8      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 275       |
|    time_elapsed         | 9796      |
|    total_timesteps      | 563200    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 17.3      |
|    n_updates            | 2740      |
|    policy_gradient_loss | 7.49e-10  |
|    value_loss           | 38.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 51.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 276       |
|    time_elapsed         | 9834      |
|    total_timesteps      | 565248    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 17.3      |
|    n_updates            | 2750      |
|    policy_gradient_loss | 1.24e-10  |
|    value_loss           | 39.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 51.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 277       |
|    time_elapsed         | 9872      |
|    total_timesteps      | 567296    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 14.8      |
|    n_updates            | 2760      |
|    policy_gradient_loss | -8.49e-10 |
|    value_loss           | 37.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 51        |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 278       |
|    time_elapsed         | 9914      |
|    total_timesteps      | 569344    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 19.6      |
|    n_updates            | 2770      |
|    policy_gradient_loss | 5.06e-10  |
|    value_loss           | 38.1      |
---------------------------------------
Num timesteps: 570000
Best mean reward: 57.60 - Last mean reward per episode: 51.40
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 51.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 279       |
|    time_elapsed         | 9952      |
|    total_timesteps      | 571392    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 21.2      |
|    n_updates            | 2780      |
|    policy_gradient_loss | -1.6e-10  |
|    value_loss           | 37.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 50.8      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 280       |
|    time_elapsed         | 9987      |
|    total_timesteps      | 573440    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 13.7      |
|    n_updates            | 2790      |
|    policy_gradient_loss | 6.8e-10   |
|    value_loss           | 38.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 50.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 281       |
|    time_elapsed         | 10034     |
|    total_timesteps      | 575488    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 22.4      |
|    n_updates            | 2800      |
|    policy_gradient_loss | 6.56e-10  |
|    value_loss           | 38.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 50.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 282       |
|    time_elapsed         | 10077     |
|    total_timesteps      | 577536    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 20.5      |
|    n_updates            | 2810      |
|    policy_gradient_loss | -2.69e-11 |
|    value_loss           | 37.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 49.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 283       |
|    time_elapsed         | 10115     |
|    total_timesteps      | 579584    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 21.1      |
|    n_updates            | 2820      |
|    policy_gradient_loss | 5.63e-10  |
|    value_loss           | 39.6      |
---------------------------------------
Num timesteps: 580000
Best mean reward: 57.60 - Last mean reward per episode: 50.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 49.8      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 284       |
|    time_elapsed         | 10151     |
|    total_timesteps      | 581632    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 25        |
|    n_updates            | 2830      |
|    policy_gradient_loss | 7.1e-10   |
|    value_loss           | 38.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 50.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 285       |
|    time_elapsed         | 10185     |
|    total_timesteps      | 583680    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 21.5      |
|    n_updates            | 2840      |
|    policy_gradient_loss | -1.78e-10 |
|    value_loss           | 39.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 50.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 286       |
|    time_elapsed         | 10222     |
|    total_timesteps      | 585728    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 18.7      |
|    n_updates            | 2850      |
|    policy_gradient_loss | 1.42e-09  |
|    value_loss           | 37.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 49.8      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 287       |
|    time_elapsed         | 10258     |
|    total_timesteps      | 587776    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.1      |
|    n_updates            | 2860      |
|    policy_gradient_loss | -6.48e-10 |
|    value_loss           | 39.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 50        |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 288       |
|    time_elapsed         | 10297     |
|    total_timesteps      | 589824    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.2      |
|    n_updates            | 2870      |
|    policy_gradient_loss | -9.93e-10 |
|    value_loss           | 38.8      |
---------------------------------------
Num timesteps: 590000
Best mean reward: 57.60 - Last mean reward per episode: 50.40
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 50.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 289       |
|    time_elapsed         | 10337     |
|    total_timesteps      | 591872    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 15.6      |
|    n_updates            | 2880      |
|    policy_gradient_loss | 5.37e-10  |
|    value_loss           | 37.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 50        |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 290       |
|    time_elapsed         | 10377     |
|    total_timesteps      | 593920    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 23.2      |
|    n_updates            | 2890      |
|    policy_gradient_loss | -4.05e-10 |
|    value_loss           | 38.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 49.8      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 291       |
|    time_elapsed         | 10415     |
|    total_timesteps      | 595968    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 19.8      |
|    n_updates            | 2900      |
|    policy_gradient_loss | -6.49e-10 |
|    value_loss           | 39.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 51        |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 292       |
|    time_elapsed         | 10451     |
|    total_timesteps      | 598016    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 17.7      |
|    n_updates            | 2910      |
|    policy_gradient_loss | 1.63e-09  |
|    value_loss           | 37.5      |
---------------------------------------
Num timesteps: 600000
Best mean reward: 57.60 - Last mean reward per episode: 51.60
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 51.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 293       |
|    time_elapsed         | 10492     |
|    total_timesteps      | 600064    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 20.4      |
|    n_updates            | 2920      |
|    policy_gradient_loss | 3.78e-11  |
|    value_loss           | 37.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 294       |
|    time_elapsed         | 10531     |
|    total_timesteps      | 602112    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 23.5      |
|    n_updates            | 2930      |
|    policy_gradient_loss | 1.19e-09  |
|    value_loss           | 37.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 295       |
|    time_elapsed         | 10567     |
|    total_timesteps      | 604160    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 17.6      |
|    n_updates            | 2940      |
|    policy_gradient_loss | -5.47e-10 |
|    value_loss           | 38.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 52.8      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 296       |
|    time_elapsed         | 10602     |
|    total_timesteps      | 606208    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 20.9      |
|    n_updates            | 2950      |
|    policy_gradient_loss | 1.21e-09  |
|    value_loss           | 39.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 297       |
|    time_elapsed         | 10639     |
|    total_timesteps      | 608256    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.2      |
|    n_updates            | 2960      |
|    policy_gradient_loss | 8.76e-10  |
|    value_loss           | 38.4      |
---------------------------------------
Num timesteps: 610000
Best mean reward: 57.60 - Last mean reward per episode: 53.20
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.2      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 298       |
|    time_elapsed         | 10677     |
|    total_timesteps      | 610304    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 24.7      |
|    n_updates            | 2970      |
|    policy_gradient_loss | -2.12e-09 |
|    value_loss           | 38.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.6      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 299       |
|    time_elapsed         | 10721     |
|    total_timesteps      | 612352    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 25.2      |
|    n_updates            | 2980      |
|    policy_gradient_loss | -1.51e-10 |
|    value_loss           | 39.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 54        |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 300       |
|    time_elapsed         | 10766     |
|    total_timesteps      | 614400    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 24.4      |
|    n_updates            | 2990      |
|    policy_gradient_loss | -1.72e-09 |
|    value_loss           | 40.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 301       |
|    time_elapsed         | 10803     |
|    total_timesteps      | 616448    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 16.5      |
|    n_updates            | 3000      |
|    policy_gradient_loss | 7.13e-10  |
|    value_loss           | 38.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 54.4      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 302       |
|    time_elapsed         | 10842     |
|    total_timesteps      | 618496    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 22.8      |
|    n_updates            | 3010      |
|    policy_gradient_loss | -2.82e-10 |
|    value_loss           | 38.3      |
---------------------------------------
Num timesteps: 620000
Best mean reward: 57.60 - Last mean reward per episode: 54.80
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 54.8      |
| time/                   |           |
|    fps                  | 57        |
|    iterations           | 303       |
|    time_elapsed         | 10879     |
|    total_timesteps      | 620544    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.5      |
|    n_updates            | 3020      |
|    policy_gradient_loss | -3.97e-10 |
|    value_loss           | 37.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 55.2      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 304       |
|    time_elapsed         | 10923     |
|    total_timesteps      | 622592    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 15.2      |
|    n_updates            | 3030      |
|    policy_gradient_loss | 1.61e-10  |
|    value_loss           | 36.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 55.6      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 305       |
|    time_elapsed         | 10963     |
|    total_timesteps      | 624640    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 17.9      |
|    n_updates            | 3040      |
|    policy_gradient_loss | -7.99e-10 |
|    value_loss           | 37.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 54.8      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 306       |
|    time_elapsed         | 11002     |
|    total_timesteps      | 626688    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 15.6      |
|    n_updates            | 3050      |
|    policy_gradient_loss | 1.24e-09  |
|    value_loss           | 38.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 54.2      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 307       |
|    time_elapsed         | 11040     |
|    total_timesteps      | 628736    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 22.2      |
|    n_updates            | 3060      |
|    policy_gradient_loss | -5.55e-10 |
|    value_loss           | 39.1      |
---------------------------------------
Num timesteps: 630000
Best mean reward: 57.60 - Last mean reward per episode: 54.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 54            |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 308           |
|    time_elapsed         | 11082         |
|    total_timesteps      | 630784        |
| train/                  |               |
|    approx_kl            | 0.00024801714 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000271     |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 12.1          |
|    n_updates            | 3070          |
|    policy_gradient_loss | -5.55e-05     |
|    value_loss           | 37.6          |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 54        |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 309       |
|    time_elapsed         | 11120     |
|    total_timesteps      | 632832    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000277 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 19.9      |
|    n_updates            | 3080      |
|    policy_gradient_loss | -4.44e-11 |
|    value_loss           | 38.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 53.8      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 310       |
|    time_elapsed         | 11156     |
|    total_timesteps      | 634880    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000277 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 17.1      |
|    n_updates            | 3090      |
|    policy_gradient_loss | 3e-10     |
|    value_loss           | 40.4      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 54            |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 311           |
|    time_elapsed         | 11192         |
|    total_timesteps      | 636928        |
| train/                  |               |
|    approx_kl            | 5.4016797e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000185     |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 20.2          |
|    n_updates            | 3100          |
|    policy_gradient_loss | -6.37e-05     |
|    value_loss           | 38            |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | 54.4      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 312       |
|    time_elapsed         | 11235     |
|    total_timesteps      | 638976    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000175 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 13.4      |
|    n_updates            | 3110      |
|    policy_gradient_loss | 5.7e-10   |
|    value_loss           | 38.6      |
---------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 287, in <module>
    model.learn(total_timesteps=1000000, callback=[WandbCallback(), save_callback])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 300, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 179, in collect_rollouts
    actions, values, log_probs = self.policy(obs_tensor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 645, in forward
    features = self.extract_features(obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/policy.py", line 45, in forward
    obj_emb = self.model(pyg_data.x_dict, pyg_data.edge_index_dict, pyg_data.batch_dict)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_gnn.py", line 104, in forward
    self.layer(x_dict, edge_index_dict)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_gnn.py", line 79, in layer
    out = self.obj_to_atom(x_dict, edge_index_dict)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_message_passing.py", line 74, in forward
    out = self._internal_forward(x, edge_index_dict[edge_type], edge_type)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_message_passing.py", line 121, in _internal_forward
    out = self.simple(x, edge_index)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/conv/simple_conv.py", line 83, in forward
    out = self.propagate(edge_index, x=x, edge_weight=edge_weight,
  File "/var/folders/my/7z0rbf091qj03p882sd10h_00000gn/T/torch_geometric.nn.conv.simple_conv_SimpleConv_propagate_ramj3us6.py", line 238, in propagate
    if not torch.jit.is_scripting() and not is_compiling():
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/_compile.py", line 9, in is_compiling
    def is_compiling() -> bool:
KeyboardInterrupt