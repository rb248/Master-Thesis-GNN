
Using cpu device
No data available for logging.
No data available for logging.
-----------------------------
| time/              |      |
|    fps             | 283  |
|    iterations      | 1    |
|    time_elapsed    | 7    |
|    total_timesteps | 2048 |
-----------------------------
Num timesteps: 3000
Best mean reward: -inf - Last mean reward per episode: -1500.00
Saving new best model at 3000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 4000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 104         |
|    iterations           | 2           |
|    time_elapsed         | 39          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.012204446 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.0414      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.315       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00659    |
|    value_loss           | 3.87        |
-----------------------------------------
Num timesteps: 5000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
Num timesteps: 6000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -1.5e+03     |
| time/                   |              |
|    fps                  | 83           |
|    iterations           | 3            |
|    time_elapsed         | 73           |
|    total_timesteps      | 6144         |
| train/                  |              |
|    approx_kl            | 0.0007190651 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | -0.00482     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0225       |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.000101    |
|    value_loss           | 12.7         |
------------------------------------------
Num timesteps: 7000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
Num timesteps: 8000
Best mean reward: -1500.00 - Last mean reward per episode: -1500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3e+03        |
|    ep_rew_mean          | -1.5e+03     |
| time/                   |              |
|    fps                  | 76           |
|    iterations           | 4            |
|    time_elapsed         | 107          |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.0014120297 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | -1.07e-06    |
|    learning_rate        | 0.0003       |
|    loss                 | 4.21         |
|    n_updates            | 30           |
|    policy_gradient_loss | 0.000339     |
|    value_loss           | 10.6         |
------------------------------------------
Num timesteps: 9000
Best mean reward: -1500.00 - Last mean reward per episode: -1365.50
Saving new best model at 8193 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 10000
Best mean reward: -1365.50 - Last mean reward per episode: -1365.50
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.73e+03   |
|    ep_rew_mean          | -1.37e+03  |
| time/                   |            |
|    fps                  | 73         |
|    iterations           | 5          |
|    time_elapsed         | 139        |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.01041491 |
|    clip_fraction        | 0.04       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | -4.77e-07  |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0176     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0027    |
|    value_loss           | 6.29       |
----------------------------------------
Num timesteps: 11000
Best mean reward: -1365.50 - Last mean reward per episode: -1365.50
Num timesteps: 12000
Best mean reward: -1365.50 - Last mean reward per episode: -1399.12
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.8e+03    |
|    ep_rew_mean          | -1.4e+03   |
| time/                   |            |
|    fps                  | 72         |
|    iterations           | 6          |
|    time_elapsed         | 169        |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.01322167 |
|    clip_fraction        | 0.0321     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | 5.96e-08   |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0234     |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.000491  |
|    value_loss           | 4.07       |
----------------------------------------
Num timesteps: 13000
Best mean reward: -1365.50 - Last mean reward per episode: -1399.12
Num timesteps: 14000
Best mean reward: -1365.50 - Last mean reward per episode: -1399.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.84e+03    |
|    ep_rew_mean          | -1.42e+03   |
| time/                   |             |
|    fps                  | 71          |
|    iterations           | 7           |
|    time_elapsed         | 199         |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.004517672 |
|    clip_fraction        | 0.0444      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.341       |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.000551   |
|    value_loss           | 6.08        |
-----------------------------------------
Num timesteps: 15000
Best mean reward: -1365.50 - Last mean reward per episode: -1419.30
Num timesteps: 16000
Best mean reward: -1365.50 - Last mean reward per episode: -1419.30
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.84e+03    |
|    ep_rew_mean          | -1.42e+03   |
| time/                   |             |
|    fps                  | 71          |
|    iterations           | 8           |
|    time_elapsed         | 228         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.008746382 |
|    clip_fraction        | 0.0193      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | -4.77e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 0.155       |
|    n_updates            | 70          |
|    policy_gradient_loss | 0.000586    |
|    value_loss           | 6.16        |
-----------------------------------------
Num timesteps: 17000
Best mean reward: -1365.50 - Last mean reward per episode: -1419.30
Num timesteps: 18000
Best mean reward: -1365.50 - Last mean reward per episode: -1432.75
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.87e+03     |
|    ep_rew_mean          | -1.43e+03    |
| time/                   |              |
|    fps                  | 71           |
|    iterations           | 9            |
|    time_elapsed         | 256          |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0048682527 |
|    clip_fraction        | 0.0238       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.02        |
|    explained_variance   | -0.000428    |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0595       |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.000892    |
|    value_loss           | 0.819        |
------------------------------------------
Num timesteps: 19000
Best mean reward: -1365.50 - Last mean reward per episode: -1432.75
Num timesteps: 20000
Best mean reward: -1365.50 - Last mean reward per episode: -1432.75
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.88e+03    |
|    ep_rew_mean          | -1.44e+03   |
| time/                   |             |
|    fps                  | 71          |
|    iterations           | 10          |
|    time_elapsed         | 284         |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.021315508 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.935      |
|    explained_variance   | -2.05e-05   |
|    learning_rate        | 0.0003      |
|    loss                 | 0.522       |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00427    |
|    value_loss           | 6.79        |
-----------------------------------------
Num timesteps: 21000
Best mean reward: -1365.50 - Last mean reward per episode: -1442.36
Num timesteps: 22000
Best mean reward: -1365.50 - Last mean reward per episode: -1442.36
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.88e+03     |
|    ep_rew_mean          | -1.44e+03    |
| time/                   |              |
|    fps                  | 71           |
|    iterations           | 11           |
|    time_elapsed         | 313          |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0051562157 |
|    clip_fraction        | 0.0199       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.9         |
|    explained_variance   | -7.15e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 2.3          |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.000517    |
|    value_loss           | 7.14         |
------------------------------------------
Num timesteps: 23000
Best mean reward: -1365.50 - Last mean reward per episode: -1442.36
Num timesteps: 24000
Best mean reward: -1365.50 - Last mean reward per episode: -1449.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.9e+03     |
|    ep_rew_mean          | -1.45e+03   |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 12          |
|    time_elapsed         | 340         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.017690994 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.805      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00931    |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00214    |
|    value_loss           | 0.204       |
-----------------------------------------
Num timesteps: 25000
Best mean reward: -1365.50 - Last mean reward per episode: -1449.56
Num timesteps: 26000
Best mean reward: -1365.50 - Last mean reward per episode: -1449.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.91e+03    |
|    ep_rew_mean          | -1.46e+03   |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 13          |
|    time_elapsed         | 368         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.010617092 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.687      |
|    explained_variance   | -1.67e-06   |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0704      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00152    |
|    value_loss           | 7.88        |
-----------------------------------------
Num timesteps: 27000
Best mean reward: -1365.50 - Last mean reward per episode: -1455.17
Num timesteps: 28000
Best mean reward: -1365.50 - Last mean reward per episode: -1455.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.91e+03     |
|    ep_rew_mean          | -1.46e+03    |
| time/                   |              |
|    fps                  | 72           |
|    iterations           | 14           |
|    time_elapsed         | 395          |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0051359413 |
|    clip_fraction        | 0.128        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.579       |
|    explained_variance   | 2.38e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 3.53         |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00373     |
|    value_loss           | 8.15         |
------------------------------------------
Num timesteps: 29000
Best mean reward: -1365.50 - Last mean reward per episode: -1455.17
Num timesteps: 30000
Best mean reward: -1365.50 - Last mean reward per episode: -1459.65
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.92e+03    |
|    ep_rew_mean          | -1.46e+03   |
| time/                   |             |
|    fps                  | 72          |
|    iterations           | 15          |
|    time_elapsed         | 424         |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.004027985 |
|    clip_fraction        | 0.0396      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.517      |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000928    |
|    n_updates            | 140         |
|    policy_gradient_loss | 0.000379    |
|    value_loss           | 0.0751      |
-----------------------------------------
Num timesteps: 31000
Best mean reward: -1365.50 - Last mean reward per episode: -1459.65
Num timesteps: 32000
Best mean reward: -1365.50 - Last mean reward per episode: -1459.65
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.93e+03     |
|    ep_rew_mean          | -1.46e+03    |
| time/                   |              |
|    fps                  | 72           |
|    iterations           | 16           |
|    time_elapsed         | 451          |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0012188386 |
|    clip_fraction        | 0.00347      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.472       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 16.4         |
|    n_updates            | 150          |
|    policy_gradient_loss | 0.000526     |
|    value_loss           | 8.65         |
------------------------------------------
Num timesteps: 33000
Best mean reward: -1365.50 - Last mean reward per episode: -1459.65
Num timesteps: 34000
Best mean reward: -1365.50 - Last mean reward per episode: -1459.65
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.93e+03     |
|    ep_rew_mean          | -1.46e+03    |
| time/                   |              |
|    fps                  | 72           |
|    iterations           | 17           |
|    time_elapsed         | 479          |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0025113604 |
|    clip_fraction        | 0.0457       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.517       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.43         |
|    n_updates            | 160          |
|    policy_gradient_loss | 0.00143      |
|    value_loss           | 8.29         |
------------------------------------------
Num timesteps: 35000
Best mean reward: -1365.50 - Last mean reward per episode: -1459.65
Num timesteps: 36000
Best mean reward: -1365.50 - Last mean reward per episode: -1459.65
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.93e+03     |
|    ep_rew_mean          | -1.47e+03    |
| time/                   |              |
|    fps                  | 72           |
|    iterations           | 18           |
|    time_elapsed         | 507          |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.0014231672 |
|    clip_fraction        | 0.0293       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.467       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00212      |
|    n_updates            | 170          |
|    policy_gradient_loss | 0.000817     |
|    value_loss           | 0.0221       |
------------------------------------------
Num timesteps: 37000
Best mean reward: -1365.50 - Last mean reward per episode: -1459.65
Num timesteps: 38000
Best mean reward: -1365.50 - Last mean reward per episode: -1459.65
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.94e+03     |
|    ep_rew_mean          | -1.47e+03    |
| time/                   |              |
|    fps                  | 72           |
|    iterations           | 19           |
|    time_elapsed         | 535          |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0026304964 |
|    clip_fraction        | 0.0825       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.447       |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 1.1          |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00225     |
|    value_loss           | 9.16         |
------------------------------------------
Num timesteps: 39000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 40000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.94e+03     |
|    ep_rew_mean          | -1.47e+03    |
| time/                   |              |
|    fps                  | 72           |
|    iterations           | 20           |
|    time_elapsed         | 562          |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0019122269 |
|    clip_fraction        | 0.0253       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.481       |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.87         |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00118     |
|    value_loss           | 9.23         |
------------------------------------------
Num timesteps: 41000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 42000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 43000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.94e+03     |
|    ep_rew_mean          | -1.47e+03    |
| time/                   |              |
|    fps                  | 72           |
|    iterations           | 21           |
|    time_elapsed         | 589          |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0043370854 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.427       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00526     |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.000154    |
|    value_loss           | 0.00881      |
------------------------------------------
Num timesteps: 44000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 45000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.95e+03     |
|    ep_rew_mean          | -1.47e+03    |
| time/                   |              |
|    fps                  | 72           |
|    iterations           | 22           |
|    time_elapsed         | 617          |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0026715223 |
|    clip_fraction        | 0.0377       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.382       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 1.53         |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.000231    |
|    value_loss           | 9.43         |
------------------------------------------
Num timesteps: 46000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 47000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.95e+03      |
|    ep_rew_mean          | -1.47e+03     |
| time/                   |               |
|    fps                  | 73            |
|    iterations           | 23            |
|    time_elapsed         | 644           |
|    total_timesteps      | 47104         |
| train/                  |               |
|    approx_kl            | 0.00076682854 |
|    clip_fraction        | 0.0104        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.34         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.14          |
|    n_updates            | 220           |
|    policy_gradient_loss | 6.39e-05      |
|    value_loss           | 9.47          |
-------------------------------------------
Num timesteps: 48000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 49000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.95e+03     |
|    ep_rew_mean          | -1.47e+03    |
| time/                   |              |
|    fps                  | 73           |
|    iterations           | 24           |
|    time_elapsed         | 672          |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0020801222 |
|    clip_fraction        | 0.0464       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.275       |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00324     |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.000739    |
|    value_loss           | 0.00664      |
------------------------------------------
Num timesteps: 50000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 51000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.95e+03      |
|    ep_rew_mean          | -1.48e+03     |
| time/                   |               |
|    fps                  | 73            |
|    iterations           | 25            |
|    time_elapsed         | 699           |
|    total_timesteps      | 51200         |
| train/                  |               |
|    approx_kl            | 0.00031939597 |
|    clip_fraction        | 0.00522       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.248        |
|    explained_variance   | 1.79e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0481        |
|    n_updates            | 240           |
|    policy_gradient_loss | 0.000266      |
|    value_loss           | 9.63          |
-------------------------------------------
Num timesteps: 52000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 53000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.96e+03     |
|    ep_rew_mean          | -1.48e+03    |
| time/                   |              |
|    fps                  | 73           |
|    iterations           | 26           |
|    time_elapsed         | 726          |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0011497628 |
|    clip_fraction        | 0.00479      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.223       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 4.18         |
|    n_updates            | 250          |
|    policy_gradient_loss | 0.000268     |
|    value_loss           | 9.62         |
------------------------------------------
Num timesteps: 54000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 55000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.96e+03    |
|    ep_rew_mean          | -1.48e+03   |
| time/                   |             |
|    fps                  | 73          |
|    iterations           | 27          |
|    time_elapsed         | 754         |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.001081137 |
|    clip_fraction        | 0.00752     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.199      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0633      |
|    n_updates            | 260         |
|    policy_gradient_loss | 1.68e-06    |
|    value_loss           | 9.61        |
-----------------------------------------
Num timesteps: 56000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 57000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.96e+03     |
|    ep_rew_mean          | -1.48e+03    |
| time/                   |              |
|    fps                  | 73           |
|    iterations           | 28           |
|    time_elapsed         | 781          |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0005712026 |
|    clip_fraction        | 0.00898      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.221       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000781     |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.000714    |
|    value_loss           | 0.00259      |
------------------------------------------
Num timesteps: 58000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 59000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.96e+03      |
|    ep_rew_mean          | -1.48e+03     |
| time/                   |               |
|    fps                  | 73            |
|    iterations           | 29            |
|    time_elapsed         | 808           |
|    total_timesteps      | 59392         |
| train/                  |               |
|    approx_kl            | 0.00016452765 |
|    clip_fraction        | 0.00317       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.218        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0239        |
|    n_updates            | 280           |
|    policy_gradient_loss | 0.000245      |
|    value_loss           | 9.71          |
-------------------------------------------
Num timesteps: 60000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 61000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.96e+03     |
|    ep_rew_mean          | -1.48e+03    |
| time/                   |              |
|    fps                  | 73           |
|    iterations           | 30           |
|    time_elapsed         | 835          |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0007768935 |
|    clip_fraction        | 0.00586      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.239       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 8.63         |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.000435    |
|    value_loss           | 9.71         |
------------------------------------------
Num timesteps: 62000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 63000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.96e+03      |
|    ep_rew_mean          | -1.48e+03     |
| time/                   |               |
|    fps                  | 73            |
|    iterations           | 31            |
|    time_elapsed         | 862           |
|    total_timesteps      | 63488         |
| train/                  |               |
|    approx_kl            | 0.00047197976 |
|    clip_fraction        | 0.0125        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.208        |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000981     |
|    n_updates            | 300           |
|    policy_gradient_loss | 0.000216      |
|    value_loss           | 0.00167       |
-------------------------------------------
Num timesteps: 64000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 65000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.96e+03     |
|    ep_rew_mean          | -1.48e+03    |
| time/                   |              |
|    fps                  | 73           |
|    iterations           | 32           |
|    time_elapsed         | 889          |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0011170276 |
|    clip_fraction        | 0.0147       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.185       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0597       |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.000408    |
|    value_loss           | 9.81         |
------------------------------------------
Num timesteps: 66000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 67000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.96e+03      |
|    ep_rew_mean          | -1.48e+03     |
| time/                   |               |
|    fps                  | 73            |
|    iterations           | 33            |
|    time_elapsed         | 917           |
|    total_timesteps      | 67584         |
| train/                  |               |
|    approx_kl            | 0.00015200657 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.174        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 2.86          |
|    n_updates            | 320           |
|    policy_gradient_loss | 0.000205      |
|    value_loss           | 9.76          |
-------------------------------------------
Num timesteps: 68000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 69000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.96e+03     |
|    ep_rew_mean          | -1.48e+03    |
| time/                   |              |
|    fps                  | 73           |
|    iterations           | 34           |
|    time_elapsed         | 944          |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 8.570432e-05 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.149       |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 5.3e-07      |
|    n_updates            | 330          |
|    policy_gradient_loss | 0.000371     |
|    value_loss           | 0.00194      |
------------------------------------------
Num timesteps: 70000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 71000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.97e+03      |
|    ep_rew_mean          | -1.48e+03     |
| time/                   |               |
|    fps                  | 73            |
|    iterations           | 35            |
|    time_elapsed         | 971           |
|    total_timesteps      | 71680         |
| train/                  |               |
|    approx_kl            | 0.00063545693 |
|    clip_fraction        | 0.0146        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.137        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 0.617         |
|    n_updates            | 340           |
|    policy_gradient_loss | -0.000451     |
|    value_loss           | 9.8           |
-------------------------------------------
Num timesteps: 72000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 73000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.97e+03      |
|    ep_rew_mean          | -1.48e+03     |
| time/                   |               |
|    fps                  | 73            |
|    iterations           | 36            |
|    time_elapsed         | 998           |
|    total_timesteps      | 73728         |
| train/                  |               |
|    approx_kl            | 0.00020676226 |
|    clip_fraction        | 0.00947       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.119        |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 9.24          |
|    n_updates            | 350           |
|    policy_gradient_loss | -3.22e-05     |
|    value_loss           | 9.74          |
-------------------------------------------
Num timesteps: 74000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 75000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.97e+03     |
|    ep_rew_mean          | -1.48e+03    |
| time/                   |              |
|    fps                  | 73           |
|    iterations           | 37           |
|    time_elapsed         | 1026         |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0005939555 |
|    clip_fraction        | 0.0112       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.119       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000325    |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.000593    |
|    value_loss           | 0.00178      |
------------------------------------------
Num timesteps: 76000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 77000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.97e+03      |
|    ep_rew_mean          | -1.48e+03     |
| time/                   |               |
|    fps                  | 73            |
|    iterations           | 38            |
|    time_elapsed         | 1053          |
|    total_timesteps      | 77824         |
| train/                  |               |
|    approx_kl            | 0.00056047254 |
|    clip_fraction        | 0.00859       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.102        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 8.72          |
|    n_updates            | 370           |
|    policy_gradient_loss | -0.000265     |
|    value_loss           | 9.81          |
-------------------------------------------
Num timesteps: 78000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 79000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.97e+03      |
|    ep_rew_mean          | -1.48e+03     |
| time/                   |               |
|    fps                  | 73            |
|    iterations           | 39            |
|    time_elapsed         | 1080          |
|    total_timesteps      | 79872         |
| train/                  |               |
|    approx_kl            | 0.00036236906 |
|    clip_fraction        | 0.00532       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.102        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.81          |
|    n_updates            | 380           |
|    policy_gradient_loss | -0.000415     |
|    value_loss           | 9.8           |
-------------------------------------------
Num timesteps: 80000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 81000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.97e+03      |
|    ep_rew_mean          | -1.49e+03     |
| time/                   |               |
|    fps                  | 73            |
|    iterations           | 40            |
|    time_elapsed         | 1107          |
|    total_timesteps      | 81920         |
| train/                  |               |
|    approx_kl            | 0.00089118816 |
|    clip_fraction        | 0.0144        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0759       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | -1.25e-05     |
|    n_updates            | 390           |
|    policy_gradient_loss | -0.00065      |
|    value_loss           | 0.00145       |
-------------------------------------------
Num timesteps: 82000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 83000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.97e+03    |
|    ep_rew_mean          | -1.49e+03   |
| time/                   |             |
|    fps                  | 73          |
|    iterations           | 41          |
|    time_elapsed         | 1135        |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.000440854 |
|    clip_fraction        | 0.00957     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0602     |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 14.6        |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.000305   |
|    value_loss           | 9.84        |
-----------------------------------------
Num timesteps: 84000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 85000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 86000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.97e+03      |
|    ep_rew_mean          | -1.49e+03     |
| time/                   |               |
|    fps                  | 74            |
|    iterations           | 42            |
|    time_elapsed         | 1161          |
|    total_timesteps      | 86016         |
| train/                  |               |
|    approx_kl            | 0.00035172165 |
|    clip_fraction        | 0.00771       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.048        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0744        |
|    n_updates            | 410           |
|    policy_gradient_loss | -0.000304     |
|    value_loss           | 9.8           |
-------------------------------------------
Num timesteps: 87000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 88000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.97e+03      |
|    ep_rew_mean          | -1.49e+03     |
| time/                   |               |
|    fps                  | 74            |
|    iterations           | 43            |
|    time_elapsed         | 1188          |
|    total_timesteps      | 88064         |
| train/                  |               |
|    approx_kl            | 0.00030743316 |
|    clip_fraction        | 0.00371       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0384       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 6.02e-05      |
|    n_updates            | 420           |
|    policy_gradient_loss | -0.000109     |
|    value_loss           | 0.0013        |
-------------------------------------------
Num timesteps: 89000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 90000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.97e+03      |
|    ep_rew_mean          | -1.49e+03     |
| time/                   |               |
|    fps                  | 74            |
|    iterations           | 44            |
|    time_elapsed         | 1215          |
|    total_timesteps      | 90112         |
| train/                  |               |
|    approx_kl            | 0.00019027892 |
|    clip_fraction        | 0.00327       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.039        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0299        |
|    n_updates            | 430           |
|    policy_gradient_loss | -0.000686     |
|    value_loss           | 9.84          |
-------------------------------------------
Num timesteps: 91000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 92000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.97e+03     |
|    ep_rew_mean          | -1.49e+03    |
| time/                   |              |
|    fps                  | 74           |
|    iterations           | 45           |
|    time_elapsed         | 1242         |
|    total_timesteps      | 92160        |
| train/                  |              |
|    approx_kl            | 0.0001350108 |
|    clip_fraction        | 0.00308      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0301      |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 3.25         |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.000129    |
|    value_loss           | 9.82         |
------------------------------------------
Num timesteps: 93000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
Num timesteps: 94000
Best mean reward: -1365.50 - Last mean reward per episode: -1500.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.97e+03      |
|    ep_rew_mean          | -1.49e+03     |
| time/                   |               |
|    fps                  | 74            |
|    iterations           | 46            |
|    time_elapsed         | 1271          |
|    total_timesteps      | 94208         |
| train/                  |               |
|    approx_kl            | 0.00024816906 |
|    clip_fraction        | 0.00298       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0239       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | -4.71e-07     |
|    n_updates            | 450           |
|    policy_gradient_loss | -5.23e-05     |
|    value_loss           | 0.0016        |
-------------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 290, in <module>
    model.learn(total_timesteps=1000000, callback=[WandbCallback(), save_callback])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 313, in learn
    self.train()
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 217, in train
    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 730, in evaluate_actions
    features = self.extract_features(obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/policy.py", line 40, in forward
    pyg_data = self.encoder.encode(observations)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/encoder/GraphEncoder.py", line 460, in encode
    return Batch.from_data_list(self.to_pyg_data(batch_data))
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/encoder/GraphEncoder.py", line 513, in to_pyg_data
    data[edge_type].edge_index = edge_tensor
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/data/hetero_data.py", line 185, in __getitem__
    return self.get_edge_store(*key)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/data/hetero_data.py", line 606, in get_edge_store
    out = EdgeStorage(_parent=self, _key=key)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/data/storage.py", line 76, in __init__
    setattr(self, key, value)
KeyboardInterrupt