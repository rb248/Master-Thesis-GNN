
Using cpu device
No data available for logging.
No data available for logging.
-----------------------------
| time/              |      |
|    fps             | 297  |
|    iterations      | 1    |
|    time_elapsed    | 6    |
|    total_timesteps | 2048 |
-----------------------------
No data available for logging.
No data available for logging.
------------------------------------------
| time/                   |              |
|    fps                  | 117          |
|    iterations           | 2            |
|    time_elapsed         | 34           |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0034975195 |
|    clip_fraction        | 0.0272       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.0909       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.326        |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.000132    |
|    value_loss           | 5.47         |
------------------------------------------
Num timesteps: 5000
Best mean reward: -inf - Last mean reward per episode: -2367.00
Saving new best model at 4814 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 6000
Best mean reward: -2367.00 - Last mean reward per episode: -2367.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.81e+03    |
|    ep_rew_mean          | -2.37e+03   |
| time/                   |             |
|    fps                  | 95          |
|    iterations           | 3           |
|    time_elapsed         | 64          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.008884757 |
|    clip_fraction        | 0.00811     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.311       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00577    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00172    |
|    value_loss           | 14.3        |
-----------------------------------------
Num timesteps: 7000
Best mean reward: -2367.00 - Last mean reward per episode: -2367.00
Num timesteps: 8000
Best mean reward: -2367.00 - Last mean reward per episode: -2367.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.81e+03    |
|    ep_rew_mean          | -2.37e+03   |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 4           |
|    time_elapsed         | 93          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.012361466 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.00283     |
|    learning_rate        | 0.0003      |
|    loss                 | 6.26        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00833    |
|    value_loss           | 16.7        |
-----------------------------------------
Num timesteps: 9000
Best mean reward: -2367.00 - Last mean reward per episode: -2367.00
Num timesteps: 10000
Best mean reward: -2367.00 - Last mean reward per episode: -2003.75
Saving new best model at 9335 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.67e+03    |
|    ep_rew_mean          | -2e+03      |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 5           |
|    time_elapsed         | 124         |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.018410895 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.985      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 9.79        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00865    |
|    value_loss           | 18          |
-----------------------------------------
Num timesteps: 11000
Best mean reward: -2003.75 - Last mean reward per episode: -2003.75
Num timesteps: 12000
Best mean reward: -2003.75 - Last mean reward per episode: -2003.75
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.67e+03   |
|    ep_rew_mean          | -2e+03     |
| time/                   |            |
|    fps                  | 80         |
|    iterations           | 6          |
|    time_elapsed         | 153        |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.01954833 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.86      |
|    explained_variance   | 5.96e-08   |
|    learning_rate        | 0.0003     |
|    loss                 | 17.5       |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 29.1       |
----------------------------------------
Num timesteps: 13000
Best mean reward: -2003.75 - Last mean reward per episode: -1561.50
Saving new best model at 12289 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 14000
Best mean reward: -1561.50 - Last mean reward per episode: -1561.50
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.1e+03     |
|    ep_rew_mean          | -1.56e+03   |
| time/                   |             |
|    fps                  | 68          |
|    iterations           | 7           |
|    time_elapsed         | 207         |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.006353127 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.777      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 9.77        |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00802    |
|    value_loss           | 29.2        |
-----------------------------------------
Num timesteps: 15000
Best mean reward: -1561.50 - Last mean reward per episode: -1561.50
Num timesteps: 16000
Best mean reward: -1561.50 - Last mean reward per episode: -1561.50
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.1e+03     |
|    ep_rew_mean          | -1.56e+03   |
| time/                   |             |
|    fps                  | 63          |
|    iterations           | 8           |
|    time_elapsed         | 256         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.006610019 |
|    clip_fraction        | 0.0954      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.7        |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 9.61        |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00742    |
|    value_loss           | 31.7        |
-----------------------------------------
Num timesteps: 17000
Best mean reward: -1561.50 - Last mean reward per episode: -1318.12
Saving new best model at 16385 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 18000
Best mean reward: -1318.12 - Last mean reward per episode: -1318.12
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 4.1e+03      |
|    ep_rew_mean          | -1.32e+03    |
| time/                   |              |
|    fps                  | 61           |
|    iterations           | 9            |
|    time_elapsed         | 301          |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0023456214 |
|    clip_fraction        | 0.0212       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.68        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 12.7         |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.000671    |
|    value_loss           | 32.8         |
------------------------------------------
Num timesteps: 19000
Best mean reward: -1318.12 - Last mean reward per episode: -1318.12
Num timesteps: 20000
Best mean reward: -1318.12 - Last mean reward per episode: -1318.12
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 4.1e+03      |
|    ep_rew_mean          | -1.32e+03    |
| time/                   |              |
|    fps                  | 59           |
|    iterations           | 10           |
|    time_elapsed         | 345          |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.0043609384 |
|    clip_fraction        | 0.0252       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.617       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 21.6         |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00179     |
|    value_loss           | 33.9         |
------------------------------------------
Num timesteps: 21000
Best mean reward: -1318.12 - Last mean reward per episode: -1156.10
Saving new best model at 20481 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 22000
Best mean reward: -1156.10 - Last mean reward per episode: -1156.10
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 4.1e+03      |
|    ep_rew_mean          | -1.16e+03    |
| time/                   |              |
|    fps                  | 57           |
|    iterations           | 11           |
|    time_elapsed         | 392          |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0034676762 |
|    clip_fraction        | 0.0283       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.523       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 21.4         |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00195     |
|    value_loss           | 33.2         |
------------------------------------------
Num timesteps: 23000
Best mean reward: -1156.10 - Last mean reward per episode: -1156.10
Num timesteps: 24000
Best mean reward: -1156.10 - Last mean reward per episode: -1156.10
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 4.1e+03      |
|    ep_rew_mean          | -1.16e+03    |
| time/                   |              |
|    fps                  | 55           |
|    iterations           | 12           |
|    time_elapsed         | 440          |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0028559766 |
|    clip_fraction        | 0.042        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.456       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 21.4         |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00268     |
|    value_loss           | 36.4         |
------------------------------------------
Num timesteps: 25000
Best mean reward: -1156.10 - Last mean reward per episode: -994.75
Saving new best model at 24577 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 26000
Best mean reward: -994.75 - Last mean reward per episode: -994.75
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 4.1e+03      |
|    ep_rew_mean          | -995         |
| time/                   |              |
|    fps                  | 54           |
|    iterations           | 13           |
|    time_elapsed         | 487          |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.0013035545 |
|    clip_fraction        | 0.0284       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.4         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 19.1         |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00185     |
|    value_loss           | 36.3         |
------------------------------------------
Num timesteps: 27000
Best mean reward: -994.75 - Last mean reward per episode: -994.75
Num timesteps: 28000
Best mean reward: -994.75 - Last mean reward per episode: -994.75
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.1e+03     |
|    ep_rew_mean          | -995        |
| time/                   |             |
|    fps                  | 53          |
|    iterations           | 14          |
|    time_elapsed         | 532         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.002125734 |
|    clip_fraction        | 0.0378      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.34       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 21.8        |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00244    |
|    value_loss           | 35.3        |
-----------------------------------------
Num timesteps: 29000
Best mean reward: -994.75 - Last mean reward per episode: -853.79
Saving new best model at 28673 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 30000
Best mean reward: -853.79 - Last mean reward per episode: -853.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 4.1e+03      |
|    ep_rew_mean          | -854         |
| time/                   |              |
|    fps                  | 53           |
|    iterations           | 15           |
|    time_elapsed         | 576          |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.0027904706 |
|    clip_fraction        | 0.0342       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.287       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 18.3         |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00272     |
|    value_loss           | 36.8         |
------------------------------------------
Num timesteps: 31000
Best mean reward: -853.79 - Last mean reward per episode: -853.79
Num timesteps: 32000
Best mean reward: -853.79 - Last mean reward per episode: -853.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 4.1e+03      |
|    ep_rew_mean          | -854         |
| time/                   |              |
|    fps                  | 52           |
|    iterations           | 16           |
|    time_elapsed         | 620          |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0014683085 |
|    clip_fraction        | 0.0147       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.273       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 20.7         |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.000274    |
|    value_loss           | 36.3         |
------------------------------------------
Num timesteps: 33000
Best mean reward: -853.79 - Last mean reward per episode: -740.56
Saving new best model at 32769 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 34000
Best mean reward: -740.56 - Last mean reward per episode: -740.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 4.1e+03      |
|    ep_rew_mean          | -741         |
| time/                   |              |
|    fps                  | 44           |
|    iterations           | 17           |
|    time_elapsed         | 791          |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0010430892 |
|    clip_fraction        | 0.01         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.251       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 15.8         |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00053     |
|    value_loss           | 37.2         |
------------------------------------------
Num timesteps: 35000
Best mean reward: -740.56 - Last mean reward per episode: -652.06
Saving new best model at 34817 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 36000
Best mean reward: -652.06 - Last mean reward per episode: -652.06
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.87e+03      |
|    ep_rew_mean          | -652          |
| time/                   |               |
|    fps                  | 43            |
|    iterations           | 18            |
|    time_elapsed         | 851           |
|    total_timesteps      | 36864         |
| train/                  |               |
|    approx_kl            | 0.00077665376 |
|    clip_fraction        | 0.00771       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.251        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 18.5          |
|    n_updates            | 170           |
|    policy_gradient_loss | -0.000126     |
|    value_loss           | 37.3          |
-------------------------------------------
Num timesteps: 37000
Best mean reward: -652.06 - Last mean reward per episode: -652.06
Num timesteps: 38000
Best mean reward: -652.06 - Last mean reward per episode: -579.35
Saving new best model at 37267 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.73e+03      |
|    ep_rew_mean          | -579          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 19            |
|    time_elapsed         | 909           |
|    total_timesteps      | 38912         |
| train/                  |               |
|    approx_kl            | 0.00043527476 |
|    clip_fraction        | 0.0124        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.235        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 15.2          |
|    n_updates            | 180           |
|    policy_gradient_loss | -0.000956     |
|    value_loss           | 37.1          |
-------------------------------------------
Num timesteps: 39000
Best mean reward: -579.35 - Last mean reward per episode: -579.35
Num timesteps: 40000
Best mean reward: -579.35 - Last mean reward per episode: -579.35
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.69e+03      |
|    ep_rew_mean          | -518          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 20            |
|    time_elapsed         | 961           |
|    total_timesteps      | 40960         |
| train/                  |               |
|    approx_kl            | 0.00031609196 |
|    clip_fraction        | 0.00977       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.225        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 19.3          |
|    n_updates            | 190           |
|    policy_gradient_loss | -0.000194     |
|    value_loss           | 36.3          |
-------------------------------------------
Num timesteps: 41000
Best mean reward: -579.35 - Last mean reward per episode: -333.05
Saving new best model at 40595 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 42000
Best mean reward: -333.05 - Last mean reward per episode: -333.05
Num timesteps: 43000
Best mean reward: -333.05 - Last mean reward per episode: -333.05
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.69e+03     |
|    ep_rew_mean          | -518         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 21           |
|    time_elapsed         | 1005         |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0023438379 |
|    clip_fraction        | 0.0411       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.221       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 19.3         |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00284     |
|    value_loss           | 36.4         |
------------------------------------------
Num timesteps: 44000
Best mean reward: -333.05 - Last mean reward per episode: -165.70
Saving new best model at 43009 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 45000
Best mean reward: -165.70 - Last mean reward per episode: -165.70
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.58e+03      |
|    ep_rew_mean          | -472          |
| time/                   |               |
|    fps                  | 43            |
|    iterations           | 22            |
|    time_elapsed         | 1038          |
|    total_timesteps      | 45056         |
| train/                  |               |
|    approx_kl            | 0.00027390913 |
|    clip_fraction        | 0.00366       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.207        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 19.4          |
|    n_updates            | 210           |
|    policy_gradient_loss | -0.000205     |
|    value_loss           | 38.6          |
-------------------------------------------
Num timesteps: 46000
Best mean reward: -165.70 - Last mean reward per episode: -165.70
Num timesteps: 47000
Best mean reward: -165.70 - Last mean reward per episode: -165.70
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.58e+03     |
|    ep_rew_mean          | -472         |
| time/                   |              |
|    fps                  | 44           |
|    iterations           | 23           |
|    time_elapsed         | 1067         |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0010663451 |
|    clip_fraction        | 0.0199       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.167       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 22.4         |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00134     |
|    value_loss           | 39.1         |
------------------------------------------
Num timesteps: 48000
Best mean reward: -165.70 - Last mean reward per episode: -100.80
Saving new best model at 47105 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 49000
Best mean reward: -100.80 - Last mean reward per episode: -100.80
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.62e+03     |
|    ep_rew_mean          | -438         |
| time/                   |              |
|    fps                  | 44           |
|    iterations           | 24           |
|    time_elapsed         | 1105         |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0010685758 |
|    clip_fraction        | 0.00571      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.194       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 19           |
|    n_updates            | 230          |
|    policy_gradient_loss | 0.000153     |
|    value_loss           | 39.3         |
------------------------------------------
Num timesteps: 50000
Best mean reward: -100.80 - Last mean reward per episode: -100.80
Num timesteps: 51000
Best mean reward: -100.80 - Last mean reward per episode: -100.80
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.62e+03     |
|    ep_rew_mean          | -438         |
| time/                   |              |
|    fps                  | 44           |
|    iterations           | 25           |
|    time_elapsed         | 1137         |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0023784246 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.186       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 21.7         |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.000943    |
|    value_loss           | 37.6         |
------------------------------------------
Num timesteps: 52000
Best mean reward: -100.80 - Last mean reward per episode: -24.80
Saving new best model at 51201 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 53000
Best mean reward: -24.80 - Last mean reward per episode: -24.80
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.66e+03     |
|    ep_rew_mean          | -394         |
| time/                   |              |
|    fps                  | 45           |
|    iterations           | 26           |
|    time_elapsed         | 1171         |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0008221256 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.161       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 20.3         |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00196     |
|    value_loss           | 37.3         |
------------------------------------------
Num timesteps: 54000
Best mean reward: -24.80 - Last mean reward per episode: -24.80
Num timesteps: 55000
Best mean reward: -24.80 - Last mean reward per episode: -24.80
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.66e+03     |
|    ep_rew_mean          | -394         |
| time/                   |              |
|    fps                  | 45           |
|    iterations           | 27           |
|    time_elapsed         | 1202         |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0011067891 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.134       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 19.7         |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00183     |
|    value_loss           | 36.2         |
------------------------------------------
Num timesteps: 56000
Best mean reward: -24.80 - Last mean reward per episode: 47.20
Saving new best model at 55297 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 57000
Best mean reward: 47.20 - Last mean reward per episode: 47.20
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.69e+03      |
|    ep_rew_mean          | -354          |
| time/                   |               |
|    fps                  | 46            |
|    iterations           | 28            |
|    time_elapsed         | 1233          |
|    total_timesteps      | 57344         |
| train/                  |               |
|    approx_kl            | 0.00071420847 |
|    clip_fraction        | 0.00854       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.113        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 21.7          |
|    n_updates            | 270           |
|    policy_gradient_loss | -0.000535     |
|    value_loss           | 38.5          |
-------------------------------------------
Num timesteps: 58000
Best mean reward: 47.20 - Last mean reward per episode: 47.20
Num timesteps: 59000
Best mean reward: 47.20 - Last mean reward per episode: 47.20
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.69e+03     |
|    ep_rew_mean          | -354         |
| time/                   |              |
|    fps                  | 46           |
|    iterations           | 29           |
|    time_elapsed         | 1267         |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0010454735 |
|    clip_fraction        | 0.0042       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.106       |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 20.8         |
|    n_updates            | 280          |
|    policy_gradient_loss | 0.000164     |
|    value_loss           | 38.6         |
------------------------------------------
Num timesteps: 60000
Best mean reward: 47.20 - Last mean reward per episode: 81.20
Saving new best model at 59393 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 61000
Best mean reward: 81.20 - Last mean reward per episode: 81.20
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.71e+03      |
|    ep_rew_mean          | -322          |
| time/                   |               |
|    fps                  | 47            |
|    iterations           | 30            |
|    time_elapsed         | 1301          |
|    total_timesteps      | 61440         |
| train/                  |               |
|    approx_kl            | 0.00025730228 |
|    clip_fraction        | 0.00439       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.117        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 18.2          |
|    n_updates            | 290           |
|    policy_gradient_loss | -0.000105     |
|    value_loss           | 38.9          |
-------------------------------------------
Num timesteps: 62000
Best mean reward: 81.20 - Last mean reward per episode: 81.20
Num timesteps: 63000
Best mean reward: 81.20 - Last mean reward per episode: 81.20
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.71e+03     |
|    ep_rew_mean          | -322         |
| time/                   |              |
|    fps                  | 47           |
|    iterations           | 31           |
|    time_elapsed         | 1337         |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.0003694637 |
|    clip_fraction        | 0.00654      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.109       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 24.6         |
|    n_updates            | 300          |
|    policy_gradient_loss | -6.33e-05    |
|    value_loss           | 39.4         |
------------------------------------------
Num timesteps: 64000
Best mean reward: 81.20 - Last mean reward per episode: 95.20
Saving new best model at 63489 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 65000
Best mean reward: 95.20 - Last mean reward per episode: 95.20
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.73e+03      |
|    ep_rew_mean          | -296          |
| time/                   |               |
|    fps                  | 47            |
|    iterations           | 32            |
|    time_elapsed         | 1371          |
|    total_timesteps      | 65536         |
| train/                  |               |
|    approx_kl            | 0.00012837816 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.1          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 14.3          |
|    n_updates            | 310           |
|    policy_gradient_loss | 8.43e-05      |
|    value_loss           | 37.4          |
-------------------------------------------
Num timesteps: 66000
Best mean reward: 95.20 - Last mean reward per episode: 95.20
Num timesteps: 67000
Best mean reward: 95.20 - Last mean reward per episode: 95.20
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.73e+03     |
|    ep_rew_mean          | -296         |
| time/                   |              |
|    fps                  | 48           |
|    iterations           | 33           |
|    time_elapsed         | 1404         |
|    total_timesteps      | 67584        |
| train/                  |              |
|    approx_kl            | 0.0004072963 |
|    clip_fraction        | 0.00537      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0875      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 27.8         |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.000242    |
|    value_loss           | 38.8         |
------------------------------------------
Num timesteps: 68000
Best mean reward: 95.20 - Last mean reward per episode: 117.20
Saving new best model at 67585 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 69000
Best mean reward: 117.20 - Last mean reward per episode: 117.20
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.75e+03     |
|    ep_rew_mean          | -264         |
| time/                   |              |
|    fps                  | 48           |
|    iterations           | 34           |
|    time_elapsed         | 1436         |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 6.581342e-05 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.087       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 25.9         |
|    n_updates            | 330          |
|    policy_gradient_loss | -4.98e-05    |
|    value_loss           | 38.5         |
------------------------------------------
Num timesteps: 70000
Best mean reward: 117.20 - Last mean reward per episode: 117.20
Num timesteps: 71000
Best mean reward: 117.20 - Last mean reward per episode: 117.20
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.75e+03     |
|    ep_rew_mean          | -264         |
| time/                   |              |
|    fps                  | 48           |
|    iterations           | 35           |
|    time_elapsed         | 1469         |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.0001164992 |
|    clip_fraction        | 0.00581      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0711      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 24.3         |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.000179    |
|    value_loss           | 37.5         |
------------------------------------------
Num timesteps: 72000
Best mean reward: 117.20 - Last mean reward per episode: 130.80
Saving new best model at 71681 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 73000
Best mean reward: 130.80 - Last mean reward per episode: 130.80
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.77e+03      |
|    ep_rew_mean          | -240          |
| time/                   |               |
|    fps                  | 49            |
|    iterations           | 36            |
|    time_elapsed         | 1500          |
|    total_timesteps      | 73728         |
| train/                  |               |
|    approx_kl            | 0.00047550726 |
|    clip_fraction        | 0.00615       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0755       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 15.2          |
|    n_updates            | 350           |
|    policy_gradient_loss | -0.000361     |
|    value_loss           | 39            |
-------------------------------------------
Num timesteps: 74000
Best mean reward: 130.80 - Last mean reward per episode: 130.80
Num timesteps: 75000
Best mean reward: 130.80 - Last mean reward per episode: 130.80
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.77e+03      |
|    ep_rew_mean          | -240          |
| time/                   |               |
|    fps                  | 49            |
|    iterations           | 37            |
|    time_elapsed         | 1533          |
|    total_timesteps      | 75776         |
| train/                  |               |
|    approx_kl            | 0.00043473885 |
|    clip_fraction        | 0.0041        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0696       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 19.4          |
|    n_updates            | 360           |
|    policy_gradient_loss | -6.48e-05     |
|    value_loss           | 37.9          |
-------------------------------------------
Num timesteps: 76000
Best mean reward: 130.80 - Last mean reward per episode: 146.50
Saving new best model at 75777 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 77000
Best mean reward: 146.50 - Last mean reward per episode: 146.50
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.79e+03     |
|    ep_rew_mean          | -216         |
| time/                   |              |
|    fps                  | 49           |
|    iterations           | 38           |
|    time_elapsed         | 1564         |
|    total_timesteps      | 77824        |
| train/                  |              |
|    approx_kl            | 6.943761e-05 |
|    clip_fraction        | 0.00386      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0574      |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 18.6         |
|    n_updates            | 370          |
|    policy_gradient_loss | -7.6e-05     |
|    value_loss           | 38.1         |
------------------------------------------
Num timesteps: 78000
Best mean reward: 146.50 - Last mean reward per episode: 146.50
Num timesteps: 79000
Best mean reward: 146.50 - Last mean reward per episode: 146.50
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.79e+03      |
|    ep_rew_mean          | -216          |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 39            |
|    time_elapsed         | 1597          |
|    total_timesteps      | 79872         |
| train/                  |               |
|    approx_kl            | 0.00031226058 |
|    clip_fraction        | 0.00264       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0589       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 20.6          |
|    n_updates            | 380           |
|    policy_gradient_loss | -0.000141     |
|    value_loss           | 37.9          |
-------------------------------------------
Num timesteps: 80000
Best mean reward: 146.50 - Last mean reward per episode: 166.10
Saving new best model at 79873 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 81000
Best mean reward: 166.10 - Last mean reward per episode: 166.10
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.8e+03       |
|    ep_rew_mean          | -192          |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 40            |
|    time_elapsed         | 1628          |
|    total_timesteps      | 81920         |
| train/                  |               |
|    approx_kl            | 0.00019413367 |
|    clip_fraction        | 0.00571       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0425       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 14.8          |
|    n_updates            | 390           |
|    policy_gradient_loss | -0.000629     |
|    value_loss           | 37.5          |
-------------------------------------------
Num timesteps: 82000
Best mean reward: 166.10 - Last mean reward per episode: 166.10
Num timesteps: 83000
Best mean reward: 166.10 - Last mean reward per episode: 166.10
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.8e+03       |
|    ep_rew_mean          | -192          |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 41            |
|    time_elapsed         | 1659          |
|    total_timesteps      | 83968         |
| train/                  |               |
|    approx_kl            | 0.00011710255 |
|    clip_fraction        | 0.00234       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0476       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 25            |
|    n_updates            | 400           |
|    policy_gradient_loss | -0.000143     |
|    value_loss           | 39.6          |
-------------------------------------------
Num timesteps: 84000
Best mean reward: 166.10 - Last mean reward per episode: 176.00
Saving new best model at 83969 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 85000
Best mean reward: 176.00 - Last mean reward per episode: 176.00
Num timesteps: 86000
Best mean reward: 176.00 - Last mean reward per episode: 176.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.82e+03      |
|    ep_rew_mean          | -177          |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 42            |
|    time_elapsed         | 1692          |
|    total_timesteps      | 86016         |
| train/                  |               |
|    approx_kl            | 0.00037222728 |
|    clip_fraction        | 0.00747       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0396       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 19.7          |
|    n_updates            | 410           |
|    policy_gradient_loss | -0.000495     |
|    value_loss           | 38.1          |
-------------------------------------------
Num timesteps: 87000
Best mean reward: 176.00 - Last mean reward per episode: 176.00
Num timesteps: 88000
Best mean reward: 176.00 - Last mean reward per episode: 176.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.82e+03      |
|    ep_rew_mean          | -177          |
| time/                   |               |
|    fps                  | 51            |
|    iterations           | 43            |
|    time_elapsed         | 1723          |
|    total_timesteps      | 88064         |
| train/                  |               |
|    approx_kl            | 0.00016971791 |
|    clip_fraction        | 0.00337       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0331       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 19            |
|    n_updates            | 420           |
|    policy_gradient_loss | -0.000286     |
|    value_loss           | 38.9          |
-------------------------------------------
Num timesteps: 89000
Best mean reward: 176.00 - Last mean reward per episode: 198.00
Saving new best model at 88065 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 90000
Best mean reward: 198.00 - Last mean reward per episode: 198.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.83e+03      |
|    ep_rew_mean          | -161          |
| time/                   |               |
|    fps                  | 51            |
|    iterations           | 44            |
|    time_elapsed         | 1755          |
|    total_timesteps      | 90112         |
| train/                  |               |
|    approx_kl            | 0.00011205886 |
|    clip_fraction        | 0.00298       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0276       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 21.1          |
|    n_updates            | 430           |
|    policy_gradient_loss | -0.000239     |
|    value_loss           | 38.9          |
-------------------------------------------
Num timesteps: 91000
Best mean reward: 198.00 - Last mean reward per episode: 198.00
Num timesteps: 92000
Best mean reward: 198.00 - Last mean reward per episode: 198.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.83e+03      |
|    ep_rew_mean          | -161          |
| time/                   |               |
|    fps                  | 51            |
|    iterations           | 45            |
|    time_elapsed         | 1785          |
|    total_timesteps      | 92160         |
| train/                  |               |
|    approx_kl            | 6.3741114e-05 |
|    clip_fraction        | 0.00166       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0232       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 15.4          |
|    n_updates            | 440           |
|    policy_gradient_loss | -0.000266     |
|    value_loss           | 37.4          |
-------------------------------------------
Num timesteps: 93000
Best mean reward: 198.00 - Last mean reward per episode: 212.00
Saving new best model at 92161 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 94000
Best mean reward: 212.00 - Last mean reward per episode: 212.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.84e+03    |
|    ep_rew_mean          | -142        |
| time/                   |             |
|    fps                  | 51          |
|    iterations           | 46          |
|    time_elapsed         | 1817        |
|    total_timesteps      | 94208       |
| train/                  |             |
|    approx_kl            | 7.48568e-05 |
|    clip_fraction        | 0.00176     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0276     |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 20          |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.000479   |
|    value_loss           | 37.5        |
-----------------------------------------
Num timesteps: 95000
Best mean reward: 212.00 - Last mean reward per episode: 212.00
Num timesteps: 96000
Best mean reward: 212.00 - Last mean reward per episode: 212.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.84e+03      |
|    ep_rew_mean          | -142          |
| time/                   |               |
|    fps                  | 52            |
|    iterations           | 47            |
|    time_elapsed         | 1849          |
|    total_timesteps      | 96256         |
| train/                  |               |
|    approx_kl            | 0.00013372637 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0236       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 18.6          |
|    n_updates            | 460           |
|    policy_gradient_loss | -0.000142     |
|    value_loss           | 38.3          |
-------------------------------------------
Num timesteps: 97000
Best mean reward: 212.00 - Last mean reward per episode: 214.00
Saving new best model at 96257 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 98000
Best mean reward: 214.00 - Last mean reward per episode: 214.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.85e+03      |
|    ep_rew_mean          | -127          |
| time/                   |               |
|    fps                  | 52            |
|    iterations           | 48            |
|    time_elapsed         | 1882          |
|    total_timesteps      | 98304         |
| train/                  |               |
|    approx_kl            | 0.00041080732 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0233       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 20.4          |
|    n_updates            | 470           |
|    policy_gradient_loss | 2.83e-05      |
|    value_loss           | 37.9          |
-------------------------------------------
Num timesteps: 99000
Best mean reward: 214.00 - Last mean reward per episode: 214.00
Num timesteps: 100000
Best mean reward: 214.00 - Last mean reward per episode: 214.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.85e+03      |
|    ep_rew_mean          | -127          |
| time/                   |               |
|    fps                  | 52            |
|    iterations           | 49            |
|    time_elapsed         | 1914          |
|    total_timesteps      | 100352        |
| train/                  |               |
|    approx_kl            | 0.00024021478 |
|    clip_fraction        | 0.00273       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0259       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 20.7          |
|    n_updates            | 480           |
|    policy_gradient_loss | 8.57e-05      |
|    value_loss           | 38.6          |
-------------------------------------------
Num timesteps: 101000
Best mean reward: 214.00 - Last mean reward per episode: 218.00
Saving new best model at 100353 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 102000
Best mean reward: 218.00 - Last mean reward per episode: 218.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.86e+03      |
|    ep_rew_mean          | -114          |
| time/                   |               |
|    fps                  | 52            |
|    iterations           | 50            |
|    time_elapsed         | 1947          |
|    total_timesteps      | 102400        |
| train/                  |               |
|    approx_kl            | 0.00014070986 |
|    clip_fraction        | 0.00225       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0203       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 23            |
|    n_updates            | 490           |
|    policy_gradient_loss | -0.000399     |
|    value_loss           | 39.8          |
-------------------------------------------
Num timesteps: 103000
Best mean reward: 218.00 - Last mean reward per episode: 218.00
Num timesteps: 104000
Best mean reward: 218.00 - Last mean reward per episode: 218.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.86e+03      |
|    ep_rew_mean          | -114          |
| time/                   |               |
|    fps                  | 52            |
|    iterations           | 51            |
|    time_elapsed         | 1978          |
|    total_timesteps      | 104448        |
| train/                  |               |
|    approx_kl            | 0.00015338464 |
|    clip_fraction        | 0.00229       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0224       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 20.3          |
|    n_updates            | 500           |
|    policy_gradient_loss | 5.61e-05      |
|    value_loss           | 37.3          |
-------------------------------------------
Num timesteps: 105000
Best mean reward: 218.00 - Last mean reward per episode: 236.00
Saving new best model at 104449 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 106000
Best mean reward: 236.00 - Last mean reward per episode: 236.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.87e+03      |
|    ep_rew_mean          | -98.7         |
| time/                   |               |
|    fps                  | 52            |
|    iterations           | 52            |
|    time_elapsed         | 2010          |
|    total_timesteps      | 106496        |
| train/                  |               |
|    approx_kl            | 3.6981073e-05 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0225       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 17            |
|    n_updates            | 510           |
|    policy_gradient_loss | -0.000157     |
|    value_loss           | 37.7          |
-------------------------------------------
Num timesteps: 107000
Best mean reward: 236.00 - Last mean reward per episode: 236.00
Num timesteps: 108000
Best mean reward: 236.00 - Last mean reward per episode: 236.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.87e+03     |
|    ep_rew_mean          | -98.7        |
| time/                   |              |
|    fps                  | 53           |
|    iterations           | 53           |
|    time_elapsed         | 2040         |
|    total_timesteps      | 108544       |
| train/                  |              |
|    approx_kl            | 7.232919e-05 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0251      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 27.7         |
|    n_updates            | 520          |
|    policy_gradient_loss | -2.75e-05    |
|    value_loss           | 40.1         |
------------------------------------------
Num timesteps: 109000
Best mean reward: 236.00 - Last mean reward per episode: 224.00
Num timesteps: 110000
Best mean reward: 236.00 - Last mean reward per episode: 224.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.88e+03     |
|    ep_rew_mean          | -89.7        |
| time/                   |              |
|    fps                  | 53           |
|    iterations           | 54           |
|    time_elapsed         | 2072         |
|    total_timesteps      | 110592       |
| train/                  |              |
|    approx_kl            | 7.373444e-05 |
|    clip_fraction        | 0.00234      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0328      |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 17.2         |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.000135    |
|    value_loss           | 40           |
------------------------------------------
Num timesteps: 111000
Best mean reward: 236.00 - Last mean reward per episode: 224.00
Num timesteps: 112000
Best mean reward: 236.00 - Last mean reward per episode: 224.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.88e+03      |
|    ep_rew_mean          | -89.7         |
| time/                   |               |
|    fps                  | 53            |
|    iterations           | 55            |
|    time_elapsed         | 2103          |
|    total_timesteps      | 112640        |
| train/                  |               |
|    approx_kl            | 0.00038823477 |
|    clip_fraction        | 0.00444       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.03         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 19.9          |
|    n_updates            | 540           |
|    policy_gradient_loss | -0.000363     |
|    value_loss           | 38            |
-------------------------------------------
Num timesteps: 113000
Best mean reward: 236.00 - Last mean reward per episode: 224.00
Num timesteps: 114000
Best mean reward: 236.00 - Last mean reward per episode: 224.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.88e+03    |
|    ep_rew_mean          | -80         |
| time/                   |             |
|    fps                  | 53          |
|    iterations           | 56          |
|    time_elapsed         | 2135        |
|    total_timesteps      | 114688      |
| train/                  |             |
|    approx_kl            | 0.000648198 |
|    clip_fraction        | 0.00361     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0247     |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 16.5        |
|    n_updates            | 550         |
|    policy_gradient_loss | 0.000155    |
|    value_loss           | 38.8        |
-----------------------------------------
Num timesteps: 115000
Best mean reward: 236.00 - Last mean reward per episode: 224.00
Num timesteps: 116000
Best mean reward: 236.00 - Last mean reward per episode: 224.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.88e+03      |
|    ep_rew_mean          | -80           |
| time/                   |               |
|    fps                  | 53            |
|    iterations           | 57            |
|    time_elapsed         | 2167          |
|    total_timesteps      | 116736        |
| train/                  |               |
|    approx_kl            | 0.00013745154 |
|    clip_fraction        | 0.00117       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0179       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 20.8          |
|    n_updates            | 560           |
|    policy_gradient_loss | -5.73e-05     |
|    value_loss           | 38.6          |
-------------------------------------------
Num timesteps: 117000
Best mean reward: 236.00 - Last mean reward per episode: 230.00
Num timesteps: 118000
Best mean reward: 236.00 - Last mean reward per episode: 230.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.89e+03      |
|    ep_rew_mean          | -67.6         |
| time/                   |               |
|    fps                  | 54            |
|    iterations           | 58            |
|    time_elapsed         | 2198          |
|    total_timesteps      | 118784        |
| train/                  |               |
|    approx_kl            | 2.1833897e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0189       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 17.4          |
|    n_updates            | 570           |
|    policy_gradient_loss | -7.04e-05     |
|    value_loss           | 37.9          |
-------------------------------------------
Num timesteps: 119000
Best mean reward: 236.00 - Last mean reward per episode: 230.00
Num timesteps: 120000
Best mean reward: 236.00 - Last mean reward per episode: 230.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.89e+03      |
|    ep_rew_mean          | -67.6         |
| time/                   |               |
|    fps                  | 54            |
|    iterations           | 59            |
|    time_elapsed         | 2230          |
|    total_timesteps      | 120832        |
| train/                  |               |
|    approx_kl            | 2.5720947e-05 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0171       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 15.2          |
|    n_updates            | 580           |
|    policy_gradient_loss | -0.000169     |
|    value_loss           | 38.6          |
-------------------------------------------
Num timesteps: 121000
Best mean reward: 236.00 - Last mean reward per episode: 218.00
Num timesteps: 122000
Best mean reward: 236.00 - Last mean reward per episode: 218.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.9e+03      |
|    ep_rew_mean          | -59.9        |
| time/                   |              |
|    fps                  | 54           |
|    iterations           | 60           |
|    time_elapsed         | 2261         |
|    total_timesteps      | 122880       |
| train/                  |              |
|    approx_kl            | 7.969822e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0176      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 20.6         |
|    n_updates            | 590          |
|    policy_gradient_loss | -1.15e-05    |
|    value_loss           | 39           |
------------------------------------------
Num timesteps: 123000
Best mean reward: 236.00 - Last mean reward per episode: 218.00
Num timesteps: 124000
Best mean reward: 236.00 - Last mean reward per episode: 218.00
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 3.9e+03  |
|    ep_rew_mean          | -59.9    |
| time/                   |          |
|    fps                  | 54       |
|    iterations           | 61       |
|    time_elapsed         | 2292     |
|    total_timesteps      | 124928   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.019   |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 22.7     |
|    n_updates            | 600      |
|    policy_gradient_loss | 1.31e-11 |
|    value_loss           | 37.2     |
--------------------------------------
Num timesteps: 125000
Best mean reward: 236.00 - Last mean reward per episode: 236.00
Num timesteps: 126000
Best mean reward: 236.00 - Last mean reward per episode: 236.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.9e+03       |
|    ep_rew_mean          | -48.3         |
| time/                   |               |
|    fps                  | 54            |
|    iterations           | 62            |
|    time_elapsed         | 2324          |
|    total_timesteps      | 126976        |
| train/                  |               |
|    approx_kl            | 6.0124847e-05 |
|    clip_fraction        | 0.0021        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0233       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 20.5          |
|    n_updates            | 610           |
|    policy_gradient_loss | -3.06e-05     |
|    value_loss           | 37.3          |
-------------------------------------------
Num timesteps: 127000
Best mean reward: 236.00 - Last mean reward per episode: 236.00
Num timesteps: 128000
Best mean reward: 236.00 - Last mean reward per episode: 236.00
Num timesteps: 129000
Best mean reward: 236.00 - Last mean reward per episode: 236.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.9e+03       |
|    ep_rew_mean          | -48.3         |
| time/                   |               |
|    fps                  | 54            |
|    iterations           | 63            |
|    time_elapsed         | 2354          |
|    total_timesteps      | 129024        |
| train/                  |               |
|    approx_kl            | 2.1982909e-05 |
|    clip_fraction        | 0.00269       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0271       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 18.5          |
|    n_updates            | 620           |
|    policy_gradient_loss | -0.000176     |
|    value_loss           | 36.5          |
-------------------------------------------
Num timesteps: 130000
Best mean reward: 236.00 - Last mean reward per episode: 250.00
Saving new best model at 129025 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
Num timesteps: 131000
Best mean reward: 250.00 - Last mean reward per episode: 250.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.91e+03      |
|    ep_rew_mean          | -36.7         |
| time/                   |               |
|    fps                  | 54            |
|    iterations           | 64            |
|    time_elapsed         | 2385          |
|    total_timesteps      | 131072        |
| train/                  |               |
|    approx_kl            | 8.8493805e-05 |
|    clip_fraction        | 0.00171       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0226       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 20.8          |
|    n_updates            | 630           |
|    policy_gradient_loss | -3.15e-05     |
|    value_loss           | 37.2          |
-------------------------------------------
Num timesteps: 132000
Best mean reward: 250.00 - Last mean reward per episode: 250.00
Num timesteps: 133000
Best mean reward: 250.00 - Last mean reward per episode: 250.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.91e+03      |
|    ep_rew_mean          | -36.7         |
| time/                   |               |
|    fps                  | 55            |
|    iterations           | 65            |
|    time_elapsed         | 2418          |
|    total_timesteps      | 133120        |
| train/                  |               |
|    approx_kl            | 0.00011441982 |
|    clip_fraction        | 0.00254       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0184       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 17.2          |
|    n_updates            | 640           |
|    policy_gradient_loss | -0.000316     |
|    value_loss           | 38.9          |
-------------------------------------------
Num timesteps: 134000
Best mean reward: 250.00 - Last mean reward per episode: 236.00
Num timesteps: 135000
Best mean reward: 250.00 - Last mean reward per episode: 236.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.92e+03      |
|    ep_rew_mean          | -30.6         |
| time/                   |               |
|    fps                  | 55            |
|    iterations           | 66            |
|    time_elapsed         | 2449          |
|    total_timesteps      | 135168        |
| train/                  |               |
|    approx_kl            | 0.00025650073 |
|    clip_fraction        | 0.00239       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0231       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 20.2          |
|    n_updates            | 650           |
|    policy_gradient_loss | -0.000491     |
|    value_loss           | 39.5          |
-------------------------------------------
Num timesteps: 136000
Best mean reward: 250.00 - Last mean reward per episode: 236.00
Num timesteps: 137000
Best mean reward: 250.00 - Last mean reward per episode: 236.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.92e+03      |
|    ep_rew_mean          | -30.6         |
| time/                   |               |
|    fps                  | 55            |
|    iterations           | 67            |
|    time_elapsed         | 2481          |
|    total_timesteps      | 137216        |
| train/                  |               |
|    approx_kl            | 0.00011154497 |
|    clip_fraction        | 0.00103       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.024        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 17.4          |
|    n_updates            | 660           |
|    policy_gradient_loss | -5.35e-05     |
|    value_loss           | 37.5          |
-------------------------------------------
Num timesteps: 138000
Best mean reward: 250.00 - Last mean reward per episode: 238.00
Num timesteps: 139000
Best mean reward: 250.00 - Last mean reward per episode: 238.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.92e+03     |
|    ep_rew_mean          | -22.5        |
| time/                   |              |
|    fps                  | 55           |
|    iterations           | 68           |
|    time_elapsed         | 2511         |
|    total_timesteps      | 139264       |
| train/                  |              |
|    approx_kl            | 0.0008457514 |
|    clip_fraction        | 0.00327      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0319      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 20.6         |
|    n_updates            | 670          |
|    policy_gradient_loss | -0.000108    |
|    value_loss           | 39.2         |
------------------------------------------
Num timesteps: 140000
Best mean reward: 250.00 - Last mean reward per episode: 238.00
Num timesteps: 141000
Best mean reward: 250.00 - Last mean reward per episode: 238.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.92e+03      |
|    ep_rew_mean          | -22.5         |
| time/                   |               |
|    fps                  | 55            |
|    iterations           | 69            |
|    time_elapsed         | 2544          |
|    total_timesteps      | 141312        |
| train/                  |               |
|    approx_kl            | 0.00012294753 |
|    clip_fraction        | 0.00244       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0321       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 22.2          |
|    n_updates            | 680           |
|    policy_gradient_loss | -0.000153     |
|    value_loss           | 38.3          |
-------------------------------------------
Num timesteps: 142000
Best mean reward: 250.00 - Last mean reward per episode: 246.00
Num timesteps: 143000
Best mean reward: 250.00 - Last mean reward per episode: 246.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.93e+03      |
|    ep_rew_mean          | -14.3         |
| time/                   |               |
|    fps                  | 55            |
|    iterations           | 70            |
|    time_elapsed         | 2575          |
|    total_timesteps      | 143360        |
| train/                  |               |
|    approx_kl            | 0.00029622216 |
|    clip_fraction        | 0.00483       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0227       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 21.2          |
|    n_updates            | 690           |
|    policy_gradient_loss | -0.000476     |
|    value_loss           | 37.7          |
-------------------------------------------
Num timesteps: 144000
Best mean reward: 250.00 - Last mean reward per episode: 246.00
Num timesteps: 145000
Best mean reward: 250.00 - Last mean reward per episode: 246.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.93e+03      |
|    ep_rew_mean          | -14.3         |
| time/                   |               |
|    fps                  | 55            |
|    iterations           | 71            |
|    time_elapsed         | 2608          |
|    total_timesteps      | 145408        |
| train/                  |               |
|    approx_kl            | 0.00017348528 |
|    clip_fraction        | 0.00288       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0172       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 21.3          |
|    n_updates            | 700           |
|    policy_gradient_loss | -0.00034      |
|    value_loss           | 38.4          |
-------------------------------------------
Num timesteps: 146000
Best mean reward: 250.00 - Last mean reward per episode: 228.00
Num timesteps: 147000
Best mean reward: 250.00 - Last mean reward per episode: 228.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.93e+03      |
|    ep_rew_mean          | -10.4         |
| time/                   |               |
|    fps                  | 55            |
|    iterations           | 72            |
|    time_elapsed         | 2640          |
|    total_timesteps      | 147456        |
| train/                  |               |
|    approx_kl            | 4.4296117e-05 |
|    clip_fraction        | 0.000635      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0172       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 16.6          |
|    n_updates            | 710           |
|    policy_gradient_loss | -0.000155     |
|    value_loss           | 39.6          |
-------------------------------------------
Num timesteps: 148000
Best mean reward: 250.00 - Last mean reward per episode: 228.00
Num timesteps: 149000
Best mean reward: 250.00 - Last mean reward per episode: 228.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.93e+03      |
|    ep_rew_mean          | -10.4         |
| time/                   |               |
|    fps                  | 55            |
|    iterations           | 73            |
|    time_elapsed         | 2677          |
|    total_timesteps      | 149504        |
| train/                  |               |
|    approx_kl            | 5.5749028e-05 |
|    clip_fraction        | 0.00166       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0122       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 19.6          |
|    n_updates            | 720           |
|    policy_gradient_loss | -0.000214     |
|    value_loss           | 38.2          |
-------------------------------------------
Num timesteps: 150000
Best mean reward: 250.00 - Last mean reward per episode: 228.00
Num timesteps: 151000
Best mean reward: 250.00 - Last mean reward per episode: 228.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.93e+03     |
|    ep_rew_mean          | -6.12        |
| time/                   |              |
|    fps                  | 55           |
|    iterations           | 74           |
|    time_elapsed         | 2713         |
|    total_timesteps      | 151552       |
| train/                  |              |
|    approx_kl            | 0.0001207334 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00911     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 14.7         |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.000223    |
|    value_loss           | 39.5         |
------------------------------------------
Num timesteps: 152000
Best mean reward: 250.00 - Last mean reward per episode: 228.00
Num timesteps: 153000
Best mean reward: 250.00 - Last mean reward per episode: 228.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.93e+03     |
|    ep_rew_mean          | -6.12        |
| time/                   |              |
|    fps                  | 55           |
|    iterations           | 75           |
|    time_elapsed         | 2745         |
|    total_timesteps      | 153600       |
| train/                  |              |
|    approx_kl            | 0.0002632681 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0124      |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 18.7         |
|    n_updates            | 740          |
|    policy_gradient_loss | -6.53e-05    |
|    value_loss           | 37           |
------------------------------------------
Num timesteps: 154000
Best mean reward: 250.00 - Last mean reward per episode: 240.00
Num timesteps: 155000
Best mean reward: 250.00 - Last mean reward per episode: 240.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.94e+03    |
|    ep_rew_mean          | 2.04        |
| time/                   |             |
|    fps                  | 56          |
|    iterations           | 76          |
|    time_elapsed         | 2777        |
|    total_timesteps      | 155648      |
| train/                  |             |
|    approx_kl            | 3.02736e-05 |
|    clip_fraction        | 0.000244    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0132     |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 16.7        |
|    n_updates            | 750         |
|    policy_gradient_loss | -3.25e-05   |
|    value_loss           | 36.6        |
-----------------------------------------
Num timesteps: 156000
Best mean reward: 250.00 - Last mean reward per episode: 240.00
Num timesteps: 157000
Best mean reward: 250.00 - Last mean reward per episode: 240.00
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 3.94e+03 |
|    ep_rew_mean          | 2.04     |
| time/                   |          |
|    fps                  | 56       |
|    iterations           | 77       |
|    time_elapsed         | 2807     |
|    total_timesteps      | 157696   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0125  |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 18.1     |
|    n_updates            | 760      |
|    policy_gradient_loss | 1.59e-10 |
|    value_loss           | 37.9     |
--------------------------------------
Num timesteps: 158000
Best mean reward: 250.00 - Last mean reward per episode: 230.00
Num timesteps: 159000
Best mean reward: 250.00 - Last mean reward per episode: 230.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.94e+03      |
|    ep_rew_mean          | 6.79          |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 78            |
|    time_elapsed         | 2838          |
|    total_timesteps      | 159744        |
| train/                  |               |
|    approx_kl            | 6.8922905e-05 |
|    clip_fraction        | 0.00161       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0104       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 19.6          |
|    n_updates            | 770           |
|    policy_gradient_loss | -0.00019      |
|    value_loss           | 40.3          |
-------------------------------------------
Num timesteps: 160000
Best mean reward: 250.00 - Last mean reward per episode: 230.00
Num timesteps: 161000
Best mean reward: 250.00 - Last mean reward per episode: 230.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.94e+03      |
|    ep_rew_mean          | 6.79          |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 79            |
|    time_elapsed         | 2868          |
|    total_timesteps      | 161792        |
| train/                  |               |
|    approx_kl            | 3.9435312e-05 |
|    clip_fraction        | 0.00083       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00818      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 21.8          |
|    n_updates            | 780           |
|    policy_gradient_loss | -0.0001       |
|    value_loss           | 37.5          |
-------------------------------------------
Num timesteps: 162000
Best mean reward: 250.00 - Last mean reward per episode: 238.00
Num timesteps: 163000
Best mean reward: 250.00 - Last mean reward per episode: 238.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.95e+03      |
|    ep_rew_mean          | 12.8          |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 80            |
|    time_elapsed         | 2905          |
|    total_timesteps      | 163840        |
| train/                  |               |
|    approx_kl            | 0.00015683571 |
|    clip_fraction        | 0.000732      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0059       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 19.6          |
|    n_updates            | 790           |
|    policy_gradient_loss | -3.09e-05     |
|    value_loss           | 37.4          |
-------------------------------------------
Num timesteps: 164000
Best mean reward: 250.00 - Last mean reward per episode: 238.00
Num timesteps: 165000
Best mean reward: 250.00 - Last mean reward per episode: 238.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.95e+03      |
|    ep_rew_mean          | 12.8          |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 81            |
|    time_elapsed         | 2937          |
|    total_timesteps      | 165888        |
| train/                  |               |
|    approx_kl            | 0.00035292108 |
|    clip_fraction        | 0.00146       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00341      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 15.7          |
|    n_updates            | 800           |
|    policy_gradient_loss | 3.89e-05      |
|    value_loss           | 38.1          |
-------------------------------------------
Num timesteps: 166000
Best mean reward: 250.00 - Last mean reward per episode: 236.00
Num timesteps: 167000
Best mean reward: 250.00 - Last mean reward per episode: 236.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.95e+03  |
|    ep_rew_mean          | 19.4      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 82        |
|    time_elapsed         | 2972      |
|    total_timesteps      | 167936    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00306  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 16.1      |
|    n_updates            | 810       |
|    policy_gradient_loss | -8.48e-10 |
|    value_loss           | 38.5      |
---------------------------------------
Num timesteps: 168000
Best mean reward: 250.00 - Last mean reward per episode: 236.00
Num timesteps: 169000
Best mean reward: 250.00 - Last mean reward per episode: 236.00
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 3.95e+03       |
|    ep_rew_mean          | 19.4           |
| time/                   |                |
|    fps                  | 56             |
|    iterations           | 83             |
|    time_elapsed         | 3005           |
|    total_timesteps      | 169984         |
| train/                  |                |
|    approx_kl            | 1.21851335e-05 |
|    clip_fraction        | 0.000439       |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00348       |
|    explained_variance   | 0              |
|    learning_rate        | 0.0003         |
|    loss                 | 19.6           |
|    n_updates            | 820            |
|    policy_gradient_loss | -6.09e-05      |
|    value_loss           | 36.3           |
--------------------------------------------
Num timesteps: 170000
Best mean reward: 250.00 - Last mean reward per episode: 232.00
Num timesteps: 171000
Best mean reward: 250.00 - Last mean reward per episode: 232.00
Num timesteps: 172000
Best mean reward: 250.00 - Last mean reward per episode: 232.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.95e+03  |
|    ep_rew_mean          | 25.8      |
| time/                   |           |
|    fps                  | 56        |
|    iterations           | 84        |
|    time_elapsed         | 3041      |
|    total_timesteps      | 172032    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00353  |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 16.5      |
|    n_updates            | 830       |
|    policy_gradient_loss | -3.78e-11 |
|    value_loss           | 37.8      |
---------------------------------------
Num timesteps: 173000
Best mean reward: 250.00 - Last mean reward per episode: 232.00
Num timesteps: 174000
Best mean reward: 250.00 - Last mean reward per episode: 232.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.95e+03      |
|    ep_rew_mean          | 25.8          |
| time/                   |               |
|    fps                  | 56            |
|    iterations           | 85            |
|    time_elapsed         | 3074          |
|    total_timesteps      | 174080        |
| train/                  |               |
|    approx_kl            | 1.9574887e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00298      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 19.6          |
|    n_updates            | 840           |
|    policy_gradient_loss | -0.000107     |
|    value_loss           | 38.9          |
-------------------------------------------
Num timesteps: 175000
Best mean reward: 250.00 - Last mean reward per episode: 236.00
Num timesteps: 176000
Best mean reward: 250.00 - Last mean reward per episode: 236.00
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3.96e+03   |
|    ep_rew_mean          | 30         |
| time/                   |            |
|    fps                  | 56         |
|    iterations           | 86         |
|    time_elapsed         | 3112       |
|    total_timesteps      | 176128     |
| train/                  |            |
|    approx_kl            | 7.4123e-05 |
|    clip_fraction        | 0.000391   |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0021    |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 16.1       |
|    n_updates            | 850        |
|    policy_gradient_loss | -3.66e-05  |
|    value_loss           | 38.7       |
----------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 290, in <module>
    model.learn(total_timesteps=1000000, callback=[WandbCallback(), save_callback])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 313, in learn
    self.train()
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 217, in train
    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 730, in evaluate_actions
    features = self.extract_features(obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/policy.py", line 40, in forward
    pyg_data = self.encoder.encode(observations)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/encoder/GraphEncoder.py", line 446, in encode
    graph.add_edge(i, atom_index, position=0)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/networkx/classes/graph.py", line 962, in add_edge
    datadict = self._adj[u].get(v, self.edge_attr_dict_factory())
KeyboardInterrupt