
Using cpu device
-----------------------------
| time/              |      |
|    fps             | 252  |
|    iterations      | 1    |
|    time_elapsed    | 8    |
|    total_timesteps | 2048 |
-----------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | -1.65e+03   |
| time/                   |             |
|    fps                  | 69          |
|    iterations           | 2           |
|    time_elapsed         | 58          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.013872579 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.0342      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.21        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0226     |
|    value_loss           | 13.1        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22e+03    |
|    ep_rew_mean          | -1.22e+03   |
| time/                   |             |
|    fps                  | 50          |
|    iterations           | 3           |
|    time_elapsed         | 121         |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.013429218 |
|    clip_fraction        | 0.0392      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 0.61        |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00653    |
|    value_loss           | 22          |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.48e+03    |
|    ep_rew_mean          | -1.32e+03   |
| time/                   |             |
|    fps                  | 47          |
|    iterations           | 4           |
|    time_elapsed         | 173         |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.015953049 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.88        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 11.6        |
-----------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -1324.00
Saving new best model at 7446 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.48e+03   |
|    ep_rew_mean          | -1.32e+03  |
| time/                   |            |
|    fps                  | 46         |
|    iterations           | 5          |
|    time_elapsed         | 222        |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.01348849 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.914     |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 0.266      |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.00632   |
|    value_loss           | 8.38       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.56e+03    |
|    ep_rew_mean          | -1.34e+03   |
| time/                   |             |
|    fps                  | 45          |
|    iterations           | 6           |
|    time_elapsed         | 271         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.014318626 |
|    clip_fraction        | 0.0984      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.797      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0283      |
|    n_updates            | 50          |
|    policy_gradient_loss | 1.68e-05    |
|    value_loss           | 3.16        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.65e+03    |
|    ep_rew_mean          | -1.37e+03   |
| time/                   |             |
|    fps                  | 43          |
|    iterations           | 7           |
|    time_elapsed         | 328         |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.008036495 |
|    clip_fraction        | 0.0542      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.716      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000988    |
|    n_updates            | 60          |
|    policy_gradient_loss | 2.14e-05    |
|    value_loss           | 2.33        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.71e+03   |
|    ep_rew_mean          | -1.4e+03   |
| time/                   |            |
|    fps                  | 42         |
|    iterations           | 8          |
|    time_elapsed         | 382        |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.00324408 |
|    clip_fraction        | 0.0364     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.626     |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0003     |
|    loss                 | 0.639      |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.00108   |
|    value_loss           | 6.28       |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.71e+03     |
|    ep_rew_mean          | -1.4e+03     |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 9            |
|    time_elapsed         | 436          |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0041970387 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.675       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.47         |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00113     |
|    value_loss           | 6.56         |
------------------------------------------
Num timesteps: 20000
Best mean reward: -1324.00 - Last mean reward per episode: -1352.64
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.63e+03    |
|    ep_rew_mean          | -1.35e+03   |
| time/                   |             |
|    fps                  | 41          |
|    iterations           | 10          |
|    time_elapsed         | 496         |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.004940877 |
|    clip_fraction        | 0.0463      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.675      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00335    |
|    n_updates            | 90          |
|    policy_gradient_loss | -3.27e-05   |
|    value_loss           | 0.461       |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.68e+03  |
|    ep_rew_mean          | -1.37e+03 |
| time/                   |           |
|    fps                  | 40        |
|    iterations           | 11        |
|    time_elapsed         | 549       |
|    total_timesteps      | 22528     |
| train/                  |           |
|    approx_kl            | 0.0068453 |
|    clip_fraction        | 0.0708    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.59     |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | -0.00785  |
|    n_updates            | 100       |
|    policy_gradient_loss | 0.00319   |
|    value_loss           | 1.09      |
---------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.5e+03      |
|    ep_rew_mean          | -1.28e+03    |
| time/                   |              |
|    fps                  | 39           |
|    iterations           | 12           |
|    time_elapsed         | 618          |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0050449055 |
|    clip_fraction        | 0.04         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.54        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.443        |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.000148    |
|    value_loss           | 7.73         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.53e+03    |
|    ep_rew_mean          | -1.29e+03   |
| time/                   |             |
|    fps                  | 39          |
|    iterations           | 13          |
|    time_elapsed         | 676         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.002814876 |
|    clip_fraction        | 0.0503      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.505      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00966    |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00268    |
|    value_loss           | 1.03        |
-----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.57e+03      |
|    ep_rew_mean          | -1.31e+03     |
| time/                   |               |
|    fps                  | 39            |
|    iterations           | 14            |
|    time_elapsed         | 732           |
|    total_timesteps      | 28672         |
| train/                  |               |
|    approx_kl            | 0.00092872465 |
|    clip_fraction        | 0.0234        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.447        |
|    explained_variance   | 2.38e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 8.83          |
|    n_updates            | 130           |
|    policy_gradient_loss | 1.55e-05      |
|    value_loss           | 8.34          |
-------------------------------------------
Num timesteps: 30000
Best mean reward: -1324.00 - Last mean reward per episode: -1217.75
Saving new best model at 28722 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.39e+03     |
|    ep_rew_mean          | -1.22e+03    |
| time/                   |              |
|    fps                  | 38           |
|    iterations           | 15           |
|    time_elapsed         | 798          |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.0006739959 |
|    clip_fraction        | 0.00913      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.502       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 26.8         |
|    n_updates            | 140          |
|    policy_gradient_loss | -3.72e-05    |
|    value_loss           | 8.54         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.39e+03     |
|    ep_rew_mean          | -1.21e+03    |
| time/                   |              |
|    fps                  | 38           |
|    iterations           | 16           |
|    time_elapsed         | 857          |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0013234152 |
|    clip_fraction        | 0.0569       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.416       |
|    explained_variance   | 2.38e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 7.72         |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00188     |
|    value_loss           | 8.76         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.34e+03     |
|    ep_rew_mean          | -1.19e+03    |
| time/                   |              |
|    fps                  | 37           |
|    iterations           | 17           |
|    time_elapsed         | 919          |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0012615724 |
|    clip_fraction        | 0.0162       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.416       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 13.7         |
|    n_updates            | 160          |
|    policy_gradient_loss | 0.000444     |
|    value_loss           | 8.86         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.38e+03     |
|    ep_rew_mean          | -1.21e+03    |
| time/                   |              |
|    fps                  | 37           |
|    iterations           | 18           |
|    time_elapsed         | 974          |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.0009422775 |
|    clip_fraction        | 0.0271       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.352       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00123      |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.000431    |
|    value_loss           | 1.07         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.37e+03    |
|    ep_rew_mean          | -1.2e+03    |
| time/                   |             |
|    fps                  | 37          |
|    iterations           | 19          |
|    time_elapsed         | 1034        |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.002526671 |
|    clip_fraction        | 0.0215      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.407      |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0387      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.000781   |
|    value_loss           | 9.19        |
-----------------------------------------
Num timesteps: 40000
Best mean reward: -1217.75 - Last mean reward per episode: -1200.94
Saving new best model at 37920 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.41e+03    |
|    ep_rew_mean          | -1.22e+03   |
| time/                   |             |
|    fps                  | 37          |
|    iterations           | 20          |
|    time_elapsed         | 1088        |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.003361707 |
|    clip_fraction        | 0.0645      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.355      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 21.1        |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00102    |
|    value_loss           | 9.28        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.41e+03     |
|    ep_rew_mean          | -1.22e+03    |
| time/                   |              |
|    fps                  | 37           |
|    iterations           | 21           |
|    time_elapsed         | 1140         |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0011565464 |
|    clip_fraction        | 0.0316       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.391       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 4.24         |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00142     |
|    value_loss           | 9.33         |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.39e+03   |
|    ep_rew_mean          | -1.21e+03  |
| time/                   |            |
|    fps                  | 37         |
|    iterations           | 22         |
|    time_elapsed         | 1194       |
|    total_timesteps      | 45056      |
| train/                  |            |
|    approx_kl            | 0.00448622 |
|    clip_fraction        | 0.0717     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.326     |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0232     |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.00265   |
|    value_loss           | 0.00854    |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.42e+03     |
|    ep_rew_mean          | -1.22e+03    |
| time/                   |              |
|    fps                  | 37           |
|    iterations           | 23           |
|    time_elapsed         | 1248         |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0013367268 |
|    clip_fraction        | 0.0101       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.307       |
|    explained_variance   | 2.38e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00223     |
|    n_updates            | 220          |
|    policy_gradient_loss | 0.000746     |
|    value_loss           | 1.12         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.45e+03     |
|    ep_rew_mean          | -1.24e+03    |
| time/                   |              |
|    fps                  | 37           |
|    iterations           | 24           |
|    time_elapsed         | 1299         |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0018405244 |
|    clip_fraction        | 0.0303       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.271       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0793       |
|    n_updates            | 230          |
|    policy_gradient_loss | 0.000127     |
|    value_loss           | 9.62         |
------------------------------------------
Num timesteps: 50000
Best mean reward: -1200.94 - Last mean reward per episode: -1237.97
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.45e+03     |
|    ep_rew_mean          | -1.24e+03    |
| time/                   |              |
|    fps                  | 37           |
|    iterations           | 25           |
|    time_elapsed         | 1355         |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0020430146 |
|    clip_fraction        | 0.02         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.227       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 9.61         |
|    n_updates            | 240          |
|    policy_gradient_loss | -8.07e-05    |
|    value_loss           | 9.61         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.44e+03     |
|    ep_rew_mean          | -1.23e+03    |
| time/                   |              |
|    fps                  | 37           |
|    iterations           | 26           |
|    time_elapsed         | 1409         |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0021394745 |
|    clip_fraction        | 0.0436       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.18        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0032      |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00135     |
|    value_loss           | 0.0033       |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.46e+03     |
|    ep_rew_mean          | -1.24e+03    |
| time/                   |              |
|    fps                  | 37           |
|    iterations           | 27           |
|    time_elapsed         | 1464         |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0008186042 |
|    clip_fraction        | 0.0128       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.168       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00113     |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.000603    |
|    value_loss           | 1.14         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.49e+03     |
|    ep_rew_mean          | -1.25e+03    |
| time/                   |              |
|    fps                  | 37           |
|    iterations           | 28           |
|    time_elapsed         | 1520         |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0005067063 |
|    clip_fraction        | 0.00923      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.127       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 14.2         |
|    n_updates            | 270          |
|    policy_gradient_loss | 2.27e-05     |
|    value_loss           | 9.79         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.49e+03     |
|    ep_rew_mean          | -1.25e+03    |
| time/                   |              |
|    fps                  | 37           |
|    iterations           | 29           |
|    time_elapsed         | 1575         |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0005142899 |
|    clip_fraction        | 0.00454      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.112       |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 16.6         |
|    n_updates            | 280          |
|    policy_gradient_loss | 0.000121     |
|    value_loss           | 9.78         |
------------------------------------------
Num timesteps: 60000
Best mean reward: -1200.94 - Last mean reward per episode: -1247.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.47e+03     |
|    ep_rew_mean          | -1.25e+03    |
| time/                   |              |
|    fps                  | 37           |
|    iterations           | 30           |
|    time_elapsed         | 1637         |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0009303549 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0852      |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | -0.001       |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.000314    |
|    value_loss           | 0.00208      |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.46e+03      |
|    ep_rew_mean          | -1.24e+03     |
| time/                   |               |
|    fps                  | 37            |
|    iterations           | 31            |
|    time_elapsed         | 1699          |
|    total_timesteps      | 63488         |
| train/                  |               |
|    approx_kl            | 0.00042490562 |
|    clip_fraction        | 0.00957       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0672       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000133      |
|    n_updates            | 300           |
|    policy_gradient_loss | -0.000219     |
|    value_loss           | 1.15          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.47e+03      |
|    ep_rew_mean          | -1.25e+03     |
| time/                   |               |
|    fps                  | 37            |
|    iterations           | 32            |
|    time_elapsed         | 1756          |
|    total_timesteps      | 65536         |
| train/                  |               |
|    approx_kl            | 0.00054117176 |
|    clip_fraction        | 0.0109        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.053        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | -4.57e-06     |
|    n_updates            | 310           |
|    policy_gradient_loss | -0.000197     |
|    value_loss           | 1.16          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.49e+03      |
|    ep_rew_mean          | -1.25e+03     |
| time/                   |               |
|    fps                  | 37            |
|    iterations           | 33            |
|    time_elapsed         | 1808          |
|    total_timesteps      | 67584         |
| train/                  |               |
|    approx_kl            | 0.00013042032 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0542       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0552        |
|    n_updates            | 320           |
|    policy_gradient_loss | -0.000389     |
|    value_loss           | 9.94          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.49e+03      |
|    ep_rew_mean          | -1.25e+03     |
| time/                   |               |
|    fps                  | 37            |
|    iterations           | 34            |
|    time_elapsed         | 1859          |
|    total_timesteps      | 69632         |
| train/                  |               |
|    approx_kl            | 0.00020712978 |
|    clip_fraction        | 0.00454       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0454       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 0.279         |
|    n_updates            | 330           |
|    policy_gradient_loss | -0.000185     |
|    value_loss           | 9.88          |
-------------------------------------------
Num timesteps: 70000
Best mean reward: -1200.94 - Last mean reward per episode: -1252.55
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.49e+03      |
|    ep_rew_mean          | -1.25e+03     |
| time/                   |               |
|    fps                  | 37            |
|    iterations           | 35            |
|    time_elapsed         | 1911          |
|    total_timesteps      | 71680         |
| train/                  |               |
|    approx_kl            | 5.9185782e-05 |
|    clip_fraction        | 0.00303       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.038        |
|    explained_variance   | 4.77e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.33e-05      |
|    n_updates            | 340           |
|    policy_gradient_loss | -0.00075      |
|    value_loss           | 0.000919      |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.47e+03      |
|    ep_rew_mean          | -1.24e+03     |
| time/                   |               |
|    fps                  | 37            |
|    iterations           | 36            |
|    time_elapsed         | 1983          |
|    total_timesteps      | 73728         |
| train/                  |               |
|    approx_kl            | 0.00010915022 |
|    clip_fraction        | 0.00254       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0313       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 2.55e-05      |
|    n_updates            | 350           |
|    policy_gradient_loss | 0.00166       |
|    value_loss           | 1.16          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.46e+03      |
|    ep_rew_mean          | -1.24e+03     |
| time/                   |               |
|    fps                  | 37            |
|    iterations           | 37            |
|    time_elapsed         | 2042          |
|    total_timesteps      | 75776         |
| train/                  |               |
|    approx_kl            | 0.00027929837 |
|    clip_fraction        | 0.00566       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.024        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.52e-05      |
|    n_updates            | 360           |
|    policy_gradient_loss | -0.000178     |
|    value_loss           | 1.17          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.48e+03      |
|    ep_rew_mean          | -1.25e+03     |
| time/                   |               |
|    fps                  | 37            |
|    iterations           | 38            |
|    time_elapsed         | 2097          |
|    total_timesteps      | 77824         |
| train/                  |               |
|    approx_kl            | 5.4137374e-05 |
|    clip_fraction        | 0.000635      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0204       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 11.7          |
|    n_updates            | 370           |
|    policy_gradient_loss | -4.42e-05     |
|    value_loss           | 10            |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.48e+03      |
|    ep_rew_mean          | -1.25e+03     |
| time/                   |               |
|    fps                  | 37            |
|    iterations           | 39            |
|    time_elapsed         | 2151          |
|    total_timesteps      | 79872         |
| train/                  |               |
|    approx_kl            | 0.00010878232 |
|    clip_fraction        | 0.00259       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0159       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 0.433         |
|    n_updates            | 380           |
|    policy_gradient_loss | -0.000102     |
|    value_loss           | 9.95          |
-------------------------------------------
Num timesteps: 80000
Best mean reward: -1200.94 - Last mean reward per episode: -1255.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.5e+03      |
|    ep_rew_mean          | -1.26e+03    |
| time/                   |              |
|    fps                  | 37           |
|    iterations           | 40           |
|    time_elapsed         | 2202         |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 9.179249e-05 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0122      |
|    explained_variance   | 5.36e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | -1.14e-05    |
|    n_updates            | 390          |
|    policy_gradient_loss | 0.00117      |
|    value_loss           | 0.000626     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.51e+03     |
|    ep_rew_mean          | -1.26e+03    |
| time/                   |              |
|    fps                  | 37           |
|    iterations           | 41           |
|    time_elapsed         | 2253         |
|    total_timesteps      | 83968        |
| train/                  |              |
|    approx_kl            | 8.294583e-05 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0093      |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00056     |
|    n_updates            | 400          |
|    policy_gradient_loss | -6.48e-05    |
|    value_loss           | 1.17         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.49e+03      |
|    ep_rew_mean          | -1.25e+03     |
| time/                   |               |
|    fps                  | 37            |
|    iterations           | 42            |
|    time_elapsed         | 2313          |
|    total_timesteps      | 86016         |
| train/                  |               |
|    approx_kl            | 3.6444457e-05 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00758      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0671        |
|    n_updates            | 410           |
|    policy_gradient_loss | -2.53e-05     |
|    value_loss           | 10            |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.51e+03      |
|    ep_rew_mean          | -1.26e+03     |
| time/                   |               |
|    fps                  | 37            |
|    iterations           | 43            |
|    time_elapsed         | 2365          |
|    total_timesteps      | 88064         |
| train/                  |               |
|    approx_kl            | 1.8814055e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0065       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 11.9          |
|    n_updates            | 420           |
|    policy_gradient_loss | -1.25e-05     |
|    value_loss           | 9.92          |
-------------------------------------------
Num timesteps: 90000
Best mean reward: -1200.94 - Last mean reward per episode: -1261.20
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.51e+03     |
|    ep_rew_mean          | -1.26e+03    |
| time/                   |              |
|    fps                  | 37           |
|    iterations           | 44           |
|    time_elapsed         | 2416         |
|    total_timesteps      | 90112        |
| train/                  |              |
|    approx_kl            | 3.035873e-05 |
|    clip_fraction        | 0.000635     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0053      |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 6.31         |
|    n_updates            | 430          |
|    policy_gradient_loss | -2.57e-05    |
|    value_loss           | 9.91         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.5e+03      |
|    ep_rew_mean          | -1.26e+03    |
| time/                   |              |
|    fps                  | 37           |
|    iterations           | 45           |
|    time_elapsed         | 2466         |
|    total_timesteps      | 92160        |
| train/                  |              |
|    approx_kl            | 6.995734e-05 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00395     |
|    explained_variance   | 2.38e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.03e-05     |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.000662    |
|    value_loss           | 0.000946     |
------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.52e+03  |
|    ep_rew_mean          | -1.27e+03 |
| time/                   |           |
|    fps                  | 37        |
|    iterations           | 46        |
|    time_elapsed         | 2516      |
|    total_timesteps      | 94208     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00378  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | -6.4e-06  |
|    n_updates            | 450       |
|    policy_gradient_loss | -1.43e-05 |
|    value_loss           | 1.16      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.53e+03  |
|    ep_rew_mean          | -1.27e+03 |
| time/                   |           |
|    fps                  | 37        |
|    iterations           | 47        |
|    time_elapsed         | 2566      |
|    total_timesteps      | 96256     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00378  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 11.4      |
|    n_updates            | 460       |
|    policy_gradient_loss | 6.51e-09  |
|    value_loss           | 9.96      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.5e+03       |
|    ep_rew_mean          | -1.26e+03     |
| time/                   |               |
|    fps                  | 37            |
|    iterations           | 48            |
|    time_elapsed         | 2629          |
|    total_timesteps      | 98304         |
| train/                  |               |
|    approx_kl            | 2.9907475e-05 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00332      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 0.498         |
|    n_updates            | 470           |
|    policy_gradient_loss | -1.19e-05     |
|    value_loss           | 9.9           |
-------------------------------------------
Num timesteps: 100000
Best mean reward: -1200.94 - Last mean reward per episode: -1255.74
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.5e+03   |
|    ep_rew_mean          | -1.26e+03 |
| time/                   |           |
|    fps                  | 37        |
|    iterations           | 49        |
|    time_elapsed         | 2684      |
|    total_timesteps      | 100352    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00288  |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.066     |
|    n_updates            | 480       |
|    policy_gradient_loss | -7.1e-09  |
|    value_loss           | 9.88      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.51e+03  |
|    ep_rew_mean          | -1.26e+03 |
| time/                   |           |
|    fps                  | 37        |
|    iterations           | 50        |
|    time_elapsed         | 2738      |
|    total_timesteps      | 102400    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00288  |
|    explained_variance   | 4.77e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.000302  |
|    n_updates            | 490       |
|    policy_gradient_loss | 0.00036   |
|    value_loss           | 0.00124   |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.51e+03  |
|    ep_rew_mean          | -1.26e+03 |
| time/                   |           |
|    fps                  | 37        |
|    iterations           | 51        |
|    time_elapsed         | 2795      |
|    total_timesteps      | 104448    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00288  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 3.58e-06  |
|    n_updates            | 500       |
|    policy_gradient_loss | -7.98e-05 |
|    value_loss           | 1.16      |
---------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.52e+03     |
|    ep_rew_mean          | -1.27e+03    |
| time/                   |              |
|    fps                  | 37           |
|    iterations           | 52           |
|    time_elapsed         | 2852         |
|    total_timesteps      | 106496       |
| train/                  |              |
|    approx_kl            | 9.907846e-05 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00219     |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.401        |
|    n_updates            | 510          |
|    policy_gradient_loss | -4.8e-05     |
|    value_loss           | 9.94         |
------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.48e+03  |
|    ep_rew_mean          | -1.25e+03 |
| time/                   |           |
|    fps                  | 37        |
|    iterations           | 53        |
|    time_elapsed         | 2916      |
|    total_timesteps      | 108544    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00211  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.806     |
|    n_updates            | 520       |
|    policy_gradient_loss | -5.65e-09 |
|    value_loss           | 9.88      |
---------------------------------------
Num timesteps: 110000
Best mean reward: -1200.94 - Last mean reward per episode: -1251.81
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.49e+03  |
|    ep_rew_mean          | -1.25e+03 |
| time/                   |           |
|    fps                  | 37        |
|    iterations           | 54        |
|    time_elapsed         | 2973      |
|    total_timesteps      | 110592    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00211  |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.455     |
|    n_updates            | 530       |
|    policy_gradient_loss | 2.92e-09  |
|    value_loss           | 9.84      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.48e+03  |
|    ep_rew_mean          | -1.25e+03 |
| time/                   |           |
|    fps                  | 37        |
|    iterations           | 55        |
|    time_elapsed         | 3032      |
|    total_timesteps      | 112640    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00211  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.462     |
|    n_updates            | 540       |
|    policy_gradient_loss | -8.46e-09 |
|    value_loss           | 9.77      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.48e+03  |
|    ep_rew_mean          | -1.25e+03 |
| time/                   |           |
|    fps                  | 37        |
|    iterations           | 56        |
|    time_elapsed         | 3091      |
|    total_timesteps      | 114688    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00211  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.439     |
|    n_updates            | 550       |
|    policy_gradient_loss | -2.65e-08 |
|    value_loss           | 9.76      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.48e+03      |
|    ep_rew_mean          | -1.25e+03     |
| time/                   |               |
|    fps                  | 37            |
|    iterations           | 57            |
|    time_elapsed         | 3148          |
|    total_timesteps      | 116736        |
| train/                  |               |
|    approx_kl            | 1.5810481e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00176      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.51          |
|    n_updates            | 560           |
|    policy_gradient_loss | -1.47e-05     |
|    value_loss           | 9.69          |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.48e+03  |
|    ep_rew_mean          | -1.25e+03 |
| time/                   |           |
|    fps                  | 37        |
|    iterations           | 58        |
|    time_elapsed         | 3202      |
|    total_timesteps      | 118784    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00174  |
|    explained_variance   | 3.58e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 5.2e-05   |
|    n_updates            | 570       |
|    policy_gradient_loss | 0.000588  |
|    value_loss           | 0.00284   |
---------------------------------------
Num timesteps: 120000
Best mean reward: -1200.94 - Last mean reward per episode: -1252.04
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.49e+03     |
|    ep_rew_mean          | -1.25e+03    |
| time/                   |              |
|    fps                  | 37           |
|    iterations           | 59           |
|    time_elapsed         | 3258         |
|    total_timesteps      | 120832       |
| train/                  |              |
|    approx_kl            | 0.0001412293 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00119     |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000593     |
|    n_updates            | 580          |
|    policy_gradient_loss | 9.14e-05     |
|    value_loss           | 1.15         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.5e+03       |
|    ep_rew_mean          | -1.26e+03     |
| time/                   |               |
|    fps                  | 37            |
|    iterations           | 60            |
|    time_elapsed         | 3308          |
|    total_timesteps      | 122880        |
| train/                  |               |
|    approx_kl            | 7.6052966e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000846     |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 19.7          |
|    n_updates            | 590           |
|    policy_gradient_loss | -1.47e-05     |
|    value_loss           | 9.84          |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.5e+03   |
|    ep_rew_mean          | -1.26e+03 |
| time/                   |           |
|    fps                  | 37        |
|    iterations           | 61        |
|    time_elapsed         | 3361      |
|    total_timesteps      | 124928    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000813 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 27.2      |
|    n_updates            | 600       |
|    policy_gradient_loss | 3.97e-08  |
|    value_loss           | 9.79      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.5e+03   |
|    ep_rew_mean          | -1.25e+03 |
| time/                   |           |
|    fps                  | 37        |
|    iterations           | 62        |
|    time_elapsed         | 3415      |
|    total_timesteps      | 126976    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00081  |
|    explained_variance   | 2.98e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.000397  |
|    n_updates            | 610       |
|    policy_gradient_loss | -0.00101  |
|    value_loss           | 0.00176   |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.49e+03  |
|    ep_rew_mean          | -1.25e+03 |
| time/                   |           |
|    fps                  | 37        |
|    iterations           | 63        |
|    time_elapsed         | 3486      |
|    total_timesteps      | 129024    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000807 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | -5.69e-06 |
|    n_updates            | 620       |
|    policy_gradient_loss | 0.00033   |
|    value_loss           | 1.15      |
---------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 290, in <module>
    model.learn(total_timesteps=160000, callback=[WandbCallback(), save_callback])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 313, in learn
    self.train()
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 217, in train
    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 730, in evaluate_actions
    features = self.extract_features(obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/policy.py", line 40, in forward
    pyg_data = self.encoder.encode(observations)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/encoder/GraphEncoder.py", line 426, in encode
    chicken_indices = [i for i in range(num_nodes) if node_features[i, -3] == 1]
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/encoder/GraphEncoder.py", line 426, in <listcomp>
    chicken_indices = [i for i in range(num_nodes) if node_features[i, -3] == 1]
KeyboardInterrupt