
Using cpu device
-----------------------------
| time/              |      |
|    fps             | 196  |
|    iterations      | 1    |
|    time_elapsed    | 10   |
|    total_timesteps | 2048 |
-----------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.73e+03    |
|    ep_rew_mean          | -1.46e+03   |
| time/                   |             |
|    fps                  | 62          |
|    iterations           | 2           |
|    time_elapsed         | 65          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.010795822 |
|    clip_fraction        | 0.0798      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | -0.0167     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.47        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00529    |
|    value_loss           | 6.78        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.87e+03    |
|    ep_rew_mean          | -1.54e+03   |
| time/                   |             |
|    fps                  | 54          |
|    iterations           | 3           |
|    time_elapsed         | 112         |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.009970261 |
|    clip_fraction        | 0.00435     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.000612    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.763       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00248    |
|    value_loss           | 21.3        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.87e+03     |
|    ep_rew_mean          | -1.54e+03    |
| time/                   |              |
|    fps                  | 49           |
|    iterations           | 4            |
|    time_elapsed         | 164          |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.0025867396 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.26         |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00282     |
|    value_loss           | 13.8         |
------------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -1443.50
Saving new best model at 8193 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.73e+03    |
|    ep_rew_mean          | -1.44e+03   |
| time/                   |             |
|    fps                  | 48          |
|    iterations           | 5           |
|    time_elapsed         | 210         |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.007081562 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.112       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00791    |
|    value_loss           | 6.49        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.71e+03    |
|    ep_rew_mean          | -1.42e+03   |
| time/                   |             |
|    fps                  | 45          |
|    iterations           | 6           |
|    time_elapsed         | 268         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.005520184 |
|    clip_fraction        | 0.0315      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.069       |
|    n_updates            | 50          |
|    policy_gradient_loss | 0.000663    |
|    value_loss           | 4.03        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.77e+03     |
|    ep_rew_mean          | -1.43e+03    |
| time/                   |              |
|    fps                  | 45           |
|    iterations           | 7            |
|    time_elapsed         | 317          |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.0077877752 |
|    clip_fraction        | 0.0516       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.02        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 2.1          |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.000528    |
|    value_loss           | 6.18         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.77e+03    |
|    ep_rew_mean          | -1.43e+03   |
| time/                   |             |
|    fps                  | 44          |
|    iterations           | 8           |
|    time_elapsed         | 367         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.016036045 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.893      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.796       |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00593    |
|    value_loss           | 6.5         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.73e+03    |
|    ep_rew_mean          | -1.41e+03   |
| time/                   |             |
|    fps                  | 44          |
|    iterations           | 9           |
|    time_elapsed         | 414         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.005677689 |
|    clip_fraction        | 0.0355      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.821      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0655      |
|    n_updates            | 80          |
|    policy_gradient_loss | 0.00134     |
|    value_loss           | 0.765       |
-----------------------------------------
Num timesteps: 20000
Best mean reward: -1443.50 - Last mean reward per episode: -1420.64
Saving new best model at 19385 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.77e+03     |
|    ep_rew_mean          | -1.42e+03    |
| time/                   |              |
|    fps                  | 43           |
|    iterations           | 10           |
|    time_elapsed         | 468          |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.0015373849 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.792       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00374     |
|    n_updates            | 90           |
|    policy_gradient_loss | 0.000559     |
|    value_loss           | 1.2          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.8e+03      |
|    ep_rew_mean          | -1.43e+03    |
| time/                   |              |
|    fps                  | 43           |
|    iterations           | 11           |
|    time_elapsed         | 522          |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0050785276 |
|    clip_fraction        | 0.067        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.686       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.123        |
|    n_updates            | 100          |
|    policy_gradient_loss | 0.000134     |
|    value_loss           | 7.33         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.8e+03      |
|    ep_rew_mean          | -1.43e+03    |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 12           |
|    time_elapsed         | 577          |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0028514736 |
|    clip_fraction        | 0.0422       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.586       |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 9.07         |
|    n_updates            | 110          |
|    policy_gradient_loss | 0.000439     |
|    value_loss           | 7.67         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.73e+03     |
|    ep_rew_mean          | -1.39e+03    |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 13           |
|    time_elapsed         | 631          |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.0016435159 |
|    clip_fraction        | 0.0628       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.523       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000534     |
|    n_updates            | 120          |
|    policy_gradient_loss | 5.82e-05     |
|    value_loss           | 0.123        |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.76e+03    |
|    ep_rew_mean          | -1.4e+03    |
| time/                   |             |
|    fps                  | 41          |
|    iterations           | 14          |
|    time_elapsed         | 684         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.002669653 |
|    clip_fraction        | 0.0341      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.456      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00201     |
|    n_updates            | 130         |
|    policy_gradient_loss | 0.000604    |
|    value_loss           | 1.04        |
-----------------------------------------
Num timesteps: 30000
Best mean reward: -1420.64 - Last mean reward per episode: -1386.18
Saving new best model at 29992 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.73e+03     |
|    ep_rew_mean          | -1.39e+03    |
| time/                   |              |
|    fps                  | 41           |
|    iterations           | 15           |
|    time_elapsed         | 741          |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.0014731128 |
|    clip_fraction        | 0.0269       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.408       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.06         |
|    n_updates            | 140          |
|    policy_gradient_loss | 1.22e-05     |
|    value_loss           | 8.6          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.73e+03     |
|    ep_rew_mean          | -1.39e+03    |
| time/                   |              |
|    fps                  | 41           |
|    iterations           | 16           |
|    time_elapsed         | 796          |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0018534401 |
|    clip_fraction        | 0.0251       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.425       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.79         |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00171     |
|    value_loss           | 8.78         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.73e+03     |
|    ep_rew_mean          | -1.39e+03    |
| time/                   |              |
|    fps                  | 41           |
|    iterations           | 17           |
|    time_elapsed         | 848          |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0026766527 |
|    clip_fraction        | 0.0596       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.353       |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00316     |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00107     |
|    value_loss           | 0.0274       |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.75e+03     |
|    ep_rew_mean          | -1.4e+03     |
| time/                   |              |
|    fps                  | 41           |
|    iterations           | 18           |
|    time_elapsed         | 894          |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.0033521114 |
|    clip_fraction        | 0.0559       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.281       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00288     |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00155     |
|    value_loss           | 1.08         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.72e+03     |
|    ep_rew_mean          | -1.38e+03    |
| time/                   |              |
|    fps                  | 40           |
|    iterations           | 19           |
|    time_elapsed         | 953          |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0017927402 |
|    clip_fraction        | 0.0347       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.232       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 0.032        |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00062     |
|    value_loss           | 9.32         |
------------------------------------------
Num timesteps: 40000
Best mean reward: -1386.18 - Last mean reward per episode: -1324.97
Saving new best model at 39245 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.62e+03      |
|    ep_rew_mean          | -1.32e+03     |
| time/                   |               |
|    fps                  | 40            |
|    iterations           | 20            |
|    time_elapsed         | 1018          |
|    total_timesteps      | 40960         |
| train/                  |               |
|    approx_kl            | 0.00079685665 |
|    clip_fraction        | 0.0146        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.234        |
|    explained_variance   | 1.79e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 3.97          |
|    n_updates            | 190           |
|    policy_gradient_loss | -0.000509     |
|    value_loss           | 9.38          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.62e+03      |
|    ep_rew_mean          | -1.33e+03     |
| time/                   |               |
|    fps                  | 40            |
|    iterations           | 21            |
|    time_elapsed         | 1075          |
|    total_timesteps      | 43008         |
| train/                  |               |
|    approx_kl            | 0.00049316755 |
|    clip_fraction        | 0.00503       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.237        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 0.425         |
|    n_updates            | 200           |
|    policy_gradient_loss | -1.33e-05     |
|    value_loss           | 9.41          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.64e+03     |
|    ep_rew_mean          | -1.34e+03    |
| time/                   |              |
|    fps                  | 39           |
|    iterations           | 22           |
|    time_elapsed         | 1127         |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0011366109 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.214       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0984       |
|    n_updates            | 210          |
|    policy_gradient_loss | 0.000155     |
|    value_loss           | 9.43         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.64e+03     |
|    ep_rew_mean          | -1.34e+03    |
| time/                   |              |
|    fps                  | 40           |
|    iterations           | 23           |
|    time_elapsed         | 1177         |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0013349501 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.207       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 3.45         |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.000598    |
|    value_loss           | 9.45         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.62e+03     |
|    ep_rew_mean          | -1.32e+03    |
| time/                   |              |
|    fps                  | 39           |
|    iterations           | 24           |
|    time_elapsed         | 1233         |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0009640486 |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.17        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000353    |
|    n_updates            | 230          |
|    policy_gradient_loss | -3.79e-05    |
|    value_loss           | 0.00629      |
------------------------------------------
Num timesteps: 50000
Best mean reward: -1324.97 - Last mean reward per episode: -1306.76
Saving new best model at 49153 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.59e+03     |
|    ep_rew_mean          | -1.31e+03    |
| time/                   |              |
|    fps                  | 39           |
|    iterations           | 25           |
|    time_elapsed         | 1295         |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0005210799 |
|    clip_fraction        | 0.00825      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.144       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00135     |
|    n_updates            | 240          |
|    policy_gradient_loss | 0.000408     |
|    value_loss           | 1.13         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.61e+03     |
|    ep_rew_mean          | -1.32e+03    |
| time/                   |              |
|    fps                  | 39           |
|    iterations           | 26           |
|    time_elapsed         | 1350         |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0005721331 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.121       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000977    |
|    n_updates            | 250          |
|    policy_gradient_loss | 2.36e-07     |
|    value_loss           | 1.14         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.63e+03      |
|    ep_rew_mean          | -1.33e+03     |
| time/                   |               |
|    fps                  | 39            |
|    iterations           | 27            |
|    time_elapsed         | 1399          |
|    total_timesteps      | 55296         |
| train/                  |               |
|    approx_kl            | 0.00080198154 |
|    clip_fraction        | 0.0161        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.116        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.58          |
|    n_updates            | 260           |
|    policy_gradient_loss | -0.000716     |
|    value_loss           | 9.79          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.63e+03      |
|    ep_rew_mean          | -1.33e+03     |
| time/                   |               |
|    fps                  | 39            |
|    iterations           | 28            |
|    time_elapsed         | 1449          |
|    total_timesteps      | 57344         |
| train/                  |               |
|    approx_kl            | 0.00039615427 |
|    clip_fraction        | 0.00703       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0985       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 0.248         |
|    n_updates            | 270           |
|    policy_gradient_loss | -0.00018      |
|    value_loss           | 9.76          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.61e+03      |
|    ep_rew_mean          | -1.31e+03     |
| time/                   |               |
|    fps                  | 39            |
|    iterations           | 29            |
|    time_elapsed         | 1499          |
|    total_timesteps      | 59392         |
| train/                  |               |
|    approx_kl            | 0.00040330322 |
|    clip_fraction        | 0.0139        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0771       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0014       |
|    n_updates            | 280           |
|    policy_gradient_loss | -0.000625     |
|    value_loss           | 0.00164       |
-------------------------------------------
Num timesteps: 60000
Best mean reward: -1306.76 - Last mean reward per episode: -1314.75
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.62e+03      |
|    ep_rew_mean          | -1.32e+03     |
| time/                   |               |
|    fps                  | 39            |
|    iterations           | 30            |
|    time_elapsed         | 1556          |
|    total_timesteps      | 61440         |
| train/                  |               |
|    approx_kl            | 0.00063914957 |
|    clip_fraction        | 0.0114        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0607       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000485     |
|    n_updates            | 290           |
|    policy_gradient_loss | -0.00023      |
|    value_loss           | 1.15          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.63e+03      |
|    ep_rew_mean          | -1.33e+03     |
| time/                   |               |
|    fps                  | 39            |
|    iterations           | 31            |
|    time_elapsed         | 1608          |
|    total_timesteps      | 63488         |
| train/                  |               |
|    approx_kl            | 0.00015062807 |
|    clip_fraction        | 0.00269       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0585       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 8.39          |
|    n_updates            | 300           |
|    policy_gradient_loss | -0.000215     |
|    value_loss           | 9.87          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.63e+03      |
|    ep_rew_mean          | -1.33e+03     |
| time/                   |               |
|    fps                  | 39            |
|    iterations           | 32            |
|    time_elapsed         | 1664          |
|    total_timesteps      | 65536         |
| train/                  |               |
|    approx_kl            | 9.5329015e-05 |
|    clip_fraction        | 0.0022        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0622       |
|    explained_variance   | 1.79e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 18.6          |
|    n_updates            | 310           |
|    policy_gradient_loss | -2.11e-05     |
|    value_loss           | 9.83          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.62e+03     |
|    ep_rew_mean          | -1.32e+03    |
| time/                   |              |
|    fps                  | 39           |
|    iterations           | 33           |
|    time_elapsed         | 1716         |
|    total_timesteps      | 67584        |
| train/                  |              |
|    approx_kl            | 0.0005028569 |
|    clip_fraction        | 0.00859      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0481      |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000516    |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.000277    |
|    value_loss           | 0.00131      |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.64e+03      |
|    ep_rew_mean          | -1.33e+03     |
| time/                   |               |
|    fps                  | 39            |
|    iterations           | 34            |
|    time_elapsed         | 1765          |
|    total_timesteps      | 69632         |
| train/                  |               |
|    approx_kl            | 0.00023465214 |
|    clip_fraction        | 0.00386       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0466       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 8.94e-05      |
|    n_updates            | 330           |
|    policy_gradient_loss | -0.000183     |
|    value_loss           | 1.16          |
-------------------------------------------
Num timesteps: 70000
Best mean reward: -1306.76 - Last mean reward per episode: -1327.71
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.65e+03      |
|    ep_rew_mean          | -1.33e+03     |
| time/                   |               |
|    fps                  | 39            |
|    iterations           | 35            |
|    time_elapsed         | 1816          |
|    total_timesteps      | 71680         |
| train/                  |               |
|    approx_kl            | 0.00022947704 |
|    clip_fraction        | 0.00566       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0396       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 0.29          |
|    n_updates            | 340           |
|    policy_gradient_loss | -0.000206     |
|    value_loss           | 9.93          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.65e+03      |
|    ep_rew_mean          | -1.33e+03     |
| time/                   |               |
|    fps                  | 39            |
|    iterations           | 36            |
|    time_elapsed         | 1866          |
|    total_timesteps      | 73728         |
| train/                  |               |
|    approx_kl            | 0.00014273394 |
|    clip_fraction        | 0.00249       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0325       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 0.492         |
|    n_updates            | 350           |
|    policy_gradient_loss | -9.1e-05      |
|    value_loss           | 9.86          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.63e+03      |
|    ep_rew_mean          | -1.33e+03     |
| time/                   |               |
|    fps                  | 39            |
|    iterations           | 37            |
|    time_elapsed         | 1917          |
|    total_timesteps      | 75776         |
| train/                  |               |
|    approx_kl            | 5.8862817e-05 |
|    clip_fraction        | 0.00137       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0257       |
|    explained_variance   | 3.58e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 3.17e-06      |
|    n_updates            | 360           |
|    policy_gradient_loss | -7.34e-05     |
|    value_loss           | 0.0012        |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.65e+03      |
|    ep_rew_mean          | -1.33e+03     |
| time/                   |               |
|    fps                  | 39            |
|    iterations           | 38            |
|    time_elapsed         | 1969          |
|    total_timesteps      | 77824         |
| train/                  |               |
|    approx_kl            | 0.00026786447 |
|    clip_fraction        | 0.0022        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0252       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000584      |
|    n_updates            | 370           |
|    policy_gradient_loss | -1.86e-05     |
|    value_loss           | 1.16          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.66e+03      |
|    ep_rew_mean          | -1.34e+03     |
| time/                   |               |
|    fps                  | 39            |
|    iterations           | 39            |
|    time_elapsed         | 2021          |
|    total_timesteps      | 79872         |
| train/                  |               |
|    approx_kl            | 0.00012577203 |
|    clip_fraction        | 0.00181       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0278       |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 0.902         |
|    n_updates            | 380           |
|    policy_gradient_loss | -0.000357     |
|    value_loss           | 9.92          |
-------------------------------------------
Num timesteps: 80000
Best mean reward: -1306.76 - Last mean reward per episode: -1337.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.66e+03     |
|    ep_rew_mean          | -1.34e+03    |
| time/                   |              |
|    fps                  | 39           |
|    iterations           | 40           |
|    time_elapsed         | 2074         |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 8.049654e-05 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0242      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 13.5         |
|    n_updates            | 390          |
|    policy_gradient_loss | -8.29e-05    |
|    value_loss           | 9.88         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.64e+03     |
|    ep_rew_mean          | -1.33e+03    |
| time/                   |              |
|    fps                  | 39           |
|    iterations           | 41           |
|    time_elapsed         | 2124         |
|    total_timesteps      | 83968        |
| train/                  |              |
|    approx_kl            | 7.439562e-05 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0197      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000561    |
|    n_updates            | 400          |
|    policy_gradient_loss | -9.11e-05    |
|    value_loss           | 0.0019       |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.65e+03      |
|    ep_rew_mean          | -1.33e+03     |
| time/                   |               |
|    fps                  | 39            |
|    iterations           | 42            |
|    time_elapsed         | 2173          |
|    total_timesteps      | 86016         |
| train/                  |               |
|    approx_kl            | 0.00020264045 |
|    clip_fraction        | 0.00337       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0146       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000105      |
|    n_updates            | 410           |
|    policy_gradient_loss | -7.29e-05     |
|    value_loss           | 1.16          |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.66e+03  |
|    ep_rew_mean          | -1.34e+03 |
| time/                   |           |
|    fps                  | 39        |
|    iterations           | 43        |
|    time_elapsed         | 2219      |
|    total_timesteps      | 88064     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0141   |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 8.02      |
|    n_updates            | 420       |
|    policy_gradient_loss | 2.78e-08  |
|    value_loss           | 9.94      |
---------------------------------------
Num timesteps: 90000
Best mean reward: -1306.76 - Last mean reward per episode: -1339.77
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.66e+03      |
|    ep_rew_mean          | -1.34e+03     |
| time/                   |               |
|    fps                  | 39            |
|    iterations           | 44            |
|    time_elapsed         | 2267          |
|    total_timesteps      | 90112         |
| train/                  |               |
|    approx_kl            | 0.00017822735 |
|    clip_fraction        | 0.00322       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0114       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0229        |
|    n_updates            | 430           |
|    policy_gradient_loss | -0.000119     |
|    value_loss           | 9.9           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.65e+03      |
|    ep_rew_mean          | -1.33e+03     |
| time/                   |               |
|    fps                  | 39            |
|    iterations           | 45            |
|    time_elapsed         | 2316          |
|    total_timesteps      | 92160         |
| train/                  |               |
|    approx_kl            | 1.8637453e-05 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00939      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 1.23e-06      |
|    n_updates            | 440           |
|    policy_gradient_loss | -0.000666     |
|    value_loss           | 0.000818      |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.66e+03     |
|    ep_rew_mean          | -1.34e+03    |
| time/                   |              |
|    fps                  | 39           |
|    iterations           | 46           |
|    time_elapsed         | 2367         |
|    total_timesteps      | 94208        |
| train/                  |              |
|    approx_kl            | 1.238042e-05 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00759     |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 5.28e-06     |
|    n_updates            | 450          |
|    policy_gradient_loss | 2.18e-05     |
|    value_loss           | 1.16         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.67e+03      |
|    ep_rew_mean          | -1.34e+03     |
| time/                   |               |
|    fps                  | 39            |
|    iterations           | 47            |
|    time_elapsed         | 2414          |
|    total_timesteps      | 96256         |
| train/                  |               |
|    approx_kl            | 4.4981425e-05 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00625      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 11.4          |
|    n_updates            | 460           |
|    policy_gradient_loss | -4.13e-05     |
|    value_loss           | 9.99          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.67e+03     |
|    ep_rew_mean          | -1.34e+03    |
| time/                   |              |
|    fps                  | 39           |
|    iterations           | 48           |
|    time_elapsed         | 2460         |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 4.250277e-05 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.005       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.499        |
|    n_updates            | 470          |
|    policy_gradient_loss | -4.54e-05    |
|    value_loss           | 9.93         |
------------------------------------------
Num timesteps: 100000
Best mean reward: -1306.76 - Last mean reward per episode: -1335.26
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.66e+03      |
|    ep_rew_mean          | -1.34e+03     |
| time/                   |               |
|    fps                  | 40            |
|    iterations           | 49            |
|    time_elapsed         | 2508          |
|    total_timesteps      | 100352        |
| train/                  |               |
|    approx_kl            | 1.8353894e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0041       |
|    explained_variance   | 7.15e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 6.07e-06      |
|    n_updates            | 480           |
|    policy_gradient_loss | -2.96e-05     |
|    value_loss           | 0.000654      |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.67e+03  |
|    ep_rew_mean          | -1.34e+03 |
| time/                   |           |
|    fps                  | 40        |
|    iterations           | 50        |
|    time_elapsed         | 2557      |
|    total_timesteps      | 102400    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00396  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.000159  |
|    n_updates            | 490       |
|    policy_gradient_loss | -0.000264 |
|    value_loss           | 1.17      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.67e+03  |
|    ep_rew_mean          | -1.34e+03 |
| time/                   |           |
|    fps                  | 40        |
|    iterations           | 51        |
|    time_elapsed         | 2603      |
|    total_timesteps      | 104448    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00395  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0539    |
|    n_updates            | 500       |
|    policy_gradient_loss | 2.73e-08  |
|    value_loss           | 9.99      |
---------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 290, in <module>
    model.learn(total_timesteps=160000, callback=[WandbCallback(), save_callback])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 313, in learn
    self.train()
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 217, in train
    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 730, in evaluate_actions
    features = self.extract_features(obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/policy.py", line 40, in forward
    pyg_data = self.encoder.encode(observations)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/encoder/GraphEncoder.py", line 443, in encode
    if abs(node_features[i, 1] - node_features[j, 1]) <= proximity_threshold:
KeyboardInterrupt