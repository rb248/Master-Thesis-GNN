
Using cpu device
-----------------------------
| time/              |      |
|    fps             | 249  |
|    iterations      | 1    |
|    time_elapsed    | 8    |
|    total_timesteps | 2048 |
-----------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | -1.58e+03   |
| time/                   |             |
|    fps                  | 80          |
|    iterations           | 2           |
|    time_elapsed         | 51          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.011777468 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.0102      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.388       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 8.09        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3e+03      |
|    ep_rew_mean          | -1.57e+03  |
| time/                   |            |
|    fps                  | 66         |
|    iterations           | 3          |
|    time_elapsed         | 92         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.00850912 |
|    clip_fraction        | 0.0284     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | -2.03e-06  |
|    learning_rate        | 0.0003     |
|    loss                 | 0.707      |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.00626   |
|    value_loss           | 20.7       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3e+03       |
|    ep_rew_mean          | -1.57e+03   |
| time/                   |             |
|    fps                  | 59          |
|    iterations           | 4           |
|    time_elapsed         | 137         |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.009466276 |
|    clip_fraction        | 0.041       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 4.78        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00411    |
|    value_loss           | 11.5        |
-----------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -1417.50
Saving new best model at 8193 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.73e+03    |
|    ep_rew_mean          | -1.42e+03   |
| time/                   |             |
|    fps                  | 57          |
|    iterations           | 5           |
|    time_elapsed         | 178         |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.013077806 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.984      |
|    explained_variance   | 4.77e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0434      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00967    |
|    value_loss           | 6.67        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.8e+03     |
|    ep_rew_mean          | -1.44e+03   |
| time/                   |             |
|    fps                  | 56          |
|    iterations           | 6           |
|    time_elapsed         | 219         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.011632663 |
|    clip_fraction        | 0.036       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.982      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0297     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.000517   |
|    value_loss           | 3.85        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.84e+03   |
|    ep_rew_mean          | -1.45e+03  |
| time/                   |            |
|    fps                  | 54         |
|    iterations           | 7          |
|    time_elapsed         | 262        |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.01222081 |
|    clip_fraction        | 0.061      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.01      |
|    explained_variance   | 1.19e-07   |
|    learning_rate        | 0.0003     |
|    loss                 | 0.396      |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.00173   |
|    value_loss           | 6.37       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.84e+03    |
|    ep_rew_mean          | -1.45e+03   |
| time/                   |             |
|    fps                  | 53          |
|    iterations           | 8           |
|    time_elapsed         | 306         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.005752597 |
|    clip_fraction        | 0.0264      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.337       |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00152    |
|    value_loss           | 6.29        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.73e+03    |
|    ep_rew_mean          | -1.4e+03    |
| time/                   |             |
|    fps                  | 53          |
|    iterations           | 9           |
|    time_elapsed         | 346         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.011007942 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.992      |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.254       |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00802    |
|    value_loss           | 1.23        |
-----------------------------------------
Num timesteps: 20000
Best mean reward: -1417.50 - Last mean reward per episode: -1412.07
Saving new best model at 19385 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.77e+03   |
|    ep_rew_mean          | -1.41e+03  |
| time/                   |            |
|    fps                  | 52         |
|    iterations           | 10         |
|    time_elapsed         | 389        |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.01743742 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.961     |
|    explained_variance   | 5.96e-08   |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0348    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0046    |
|    value_loss           | 1.26       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.8e+03   |
|    ep_rew_mean          | -1.42e+03 |
| time/                   |           |
|    fps                  | 52        |
|    iterations           | 11        |
|    time_elapsed         | 429       |
|    total_timesteps      | 22528     |
| train/                  |           |
|    approx_kl            | 0.0134762 |
|    clip_fraction        | 0.0752    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.973    |
|    explained_variance   | 3.58e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.146     |
|    n_updates            | 100       |
|    policy_gradient_loss | -0.00176  |
|    value_loss           | 7.34      |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.8e+03     |
|    ep_rew_mean          | -1.42e+03   |
| time/                   |             |
|    fps                  | 52          |
|    iterations           | 12          |
|    time_elapsed         | 470         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.010747513 |
|    clip_fraction        | 0.0181      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.955      |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 9.03        |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00174    |
|    value_loss           | 7.66        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.73e+03    |
|    ep_rew_mean          | -1.39e+03   |
| time/                   |             |
|    fps                  | 52          |
|    iterations           | 13          |
|    time_elapsed         | 510         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.013633737 |
|    clip_fraction        | 0.0583      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.93       |
|    explained_variance   | -1.67e-06   |
|    learning_rate        | 0.0003      |
|    loss                 | -0.012      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00137    |
|    value_loss           | 0.124       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.76e+03    |
|    ep_rew_mean          | -1.4e+03    |
| time/                   |             |
|    fps                  | 51          |
|    iterations           | 14          |
|    time_elapsed         | 551         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.012650793 |
|    clip_fraction        | 0.0555      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.886      |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0239     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.000836   |
|    value_loss           | 1.04        |
-----------------------------------------
Num timesteps: 30000
Best mean reward: -1412.07 - Last mean reward per episode: -1398.05
Saving new best model at 27577 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.78e+03     |
|    ep_rew_mean          | -1.41e+03    |
| time/                   |              |
|    fps                  | 51           |
|    iterations           | 15           |
|    time_elapsed         | 600          |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.0058298837 |
|    clip_fraction        | 0.00376      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.832       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.08         |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.000565    |
|    value_loss           | 8.66         |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.78e+03   |
|    ep_rew_mean          | -1.41e+03  |
| time/                   |            |
|    fps                  | 50         |
|    iterations           | 16         |
|    time_elapsed         | 647        |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.00984716 |
|    clip_fraction        | 0.046      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.864     |
|    explained_variance   | 1.19e-07   |
|    learning_rate        | 0.0003     |
|    loss                 | 12.9       |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0013    |
|    value_loss           | 8.85       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.73e+03   |
|    ep_rew_mean          | -1.38e+03  |
| time/                   |            |
|    fps                  | 50         |
|    iterations           | 17         |
|    time_elapsed         | 690        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.01369705 |
|    clip_fraction        | 0.0676     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.842     |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0133     |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.000814  |
|    value_loss           | 0.0243     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.75e+03    |
|    ep_rew_mean          | -1.39e+03   |
| time/                   |             |
|    fps                  | 50          |
|    iterations           | 18          |
|    time_elapsed         | 731         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.015586941 |
|    clip_fraction        | 0.0805      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.778      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.02       |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00329    |
|    value_loss           | 1.08        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.77e+03    |
|    ep_rew_mean          | -1.4e+03    |
| time/                   |             |
|    fps                  | 50          |
|    iterations           | 19          |
|    time_elapsed         | 774         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.004243368 |
|    clip_fraction        | 0.0294      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.733      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0178      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.000471   |
|    value_loss           | 9.39        |
-----------------------------------------
Num timesteps: 40000
Best mean reward: -1398.05 - Last mean reward per episode: -1399.18
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.77e+03    |
|    ep_rew_mean          | -1.4e+03    |
| time/                   |             |
|    fps                  | 50          |
|    iterations           | 20          |
|    time_elapsed         | 818         |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.007630741 |
|    clip_fraction        | 0.0381      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.729      |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 3.19        |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00136    |
|    value_loss           | 9.41        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.73e+03     |
|    ep_rew_mean          | -1.38e+03    |
| time/                   |              |
|    fps                  | 49           |
|    iterations           | 21           |
|    time_elapsed         | 860          |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0077917073 |
|    clip_fraction        | 0.114        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.661       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0285       |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00111     |
|    value_loss           | 0.00637      |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.75e+03     |
|    ep_rew_mean          | -1.39e+03    |
| time/                   |              |
|    fps                  | 50           |
|    iterations           | 22           |
|    time_elapsed         | 900          |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0009999792 |
|    clip_fraction        | 0.00244      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.655       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00522     |
|    n_updates            | 210          |
|    policy_gradient_loss | 0.000776     |
|    value_loss           | 1.12         |
------------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 290, in <module>
    model.learn(total_timesteps=160000, callback=[WandbCallback(), save_callback])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 313, in learn
    self.train()
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 217, in train
    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 730, in evaluate_actions
    features = self.extract_features(obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/policy.py", line 40, in forward
    pyg_data = self.encoder.encode(observations)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/encoder/GraphEncoder.py", line 460, in encode
    return Batch.from_data_list(self.to_pyg_data(batch_data))
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/encoder/GraphEncoder.py", line 477, in to_pyg_data
    features = torch.tensor(attrs['features'])
KeyboardInterrupt