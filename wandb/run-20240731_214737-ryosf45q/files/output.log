
Using cpu device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -500     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 1        |
|    time_elapsed    | 8        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 66          |
|    iterations           | 2           |
|    time_elapsed         | 62          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.007250081 |
|    clip_fraction        | 0.0359      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.00581     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.919       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00161    |
|    value_loss           | 6.54        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 53          |
|    iterations           | 3           |
|    time_elapsed         | 114         |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.008455544 |
|    clip_fraction        | 0.0196      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.000497    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.107       |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.000216    |
|    value_loss           | 12.6        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 47          |
|    iterations           | 4           |
|    time_elapsed         | 172         |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.008168174 |
|    clip_fraction        | 0.00229     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 4.12        |
|    n_updates            | 30          |
|    policy_gradient_loss | -5.77e-05   |
|    value_loss           | 12.2        |
-----------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -500.00
Saving new best model at 10000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 45          |
|    iterations           | 5           |
|    time_elapsed         | 225         |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.008028332 |
|    clip_fraction        | 0.028       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.076       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00215    |
|    value_loss           | 10.4        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 43          |
|    iterations           | 6           |
|    time_elapsed         | 280         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.010517554 |
|    clip_fraction        | 0.0682      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 6.11        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00256    |
|    value_loss           | 9.71        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 7            |
|    time_elapsed         | 336          |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.0089561995 |
|    clip_fraction        | 0.0298       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.138        |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.000568    |
|    value_loss           | 10           |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 41           |
|    iterations           | 8            |
|    time_elapsed         | 396          |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 0.0124848345 |
|    clip_fraction        | 0.0535       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 0.114        |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.0023      |
|    value_loss           | 10.6         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 40          |
|    iterations           | 9           |
|    time_elapsed         | 460         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.012004477 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.859       |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00492    |
|    value_loss           | 11.5        |
-----------------------------------------
Num timesteps: 20000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | -500       |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 10         |
|    time_elapsed         | 517        |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.00862457 |
|    clip_fraction        | 0.0526     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.07      |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 8.21       |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.00221   |
|    value_loss           | 12.3       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 39          |
|    iterations           | 11          |
|    time_elapsed         | 571         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.015755776 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 7.98        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00544    |
|    value_loss           | 13.1        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 39          |
|    iterations           | 12          |
|    time_elapsed         | 625         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.015727852 |
|    clip_fraction        | 0.0524      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 12.5        |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00101    |
|    value_loss           | 13.7        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 39           |
|    iterations           | 13           |
|    time_elapsed         | 679          |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.0028042928 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.07        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.79         |
|    n_updates            | 120          |
|    policy_gradient_loss | 0.000339     |
|    value_loss           | 14.4         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 39          |
|    iterations           | 14          |
|    time_elapsed         | 734         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.010940747 |
|    clip_fraction        | 0.0565      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 3.8         |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00436    |
|    value_loss           | 14.9        |
-----------------------------------------
Num timesteps: 30000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 38          |
|    iterations           | 15          |
|    time_elapsed         | 788         |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.006992131 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 1.73        |
|    n_updates            | 140         |
|    policy_gradient_loss | 0.000169    |
|    value_loss           | 15.3        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 38          |
|    iterations           | 16          |
|    time_elapsed         | 843         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.008145131 |
|    clip_fraction        | 0.00591     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 12.8        |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.000422   |
|    value_loss           | 15.7        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 38          |
|    iterations           | 17          |
|    time_elapsed         | 898         |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.011398654 |
|    clip_fraction        | 0.0736      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.246       |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00151    |
|    value_loss           | 16          |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 19          |
|    iterations           | 18          |
|    time_elapsed         | 1884        |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.012247084 |
|    clip_fraction        | 0.0412      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 7.34        |
|    n_updates            | 170         |
|    policy_gradient_loss | 9.65e-05    |
|    value_loss           | 16.2        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 990         |
|    ep_rew_mean          | -494        |
| time/                   |             |
|    fps                  | 20          |
|    iterations           | 19          |
|    time_elapsed         | 1942        |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.012310841 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.273       |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0054     |
|    value_loss           | 16.4        |
-----------------------------------------
Num timesteps: 40000
Best mean reward: -500.00 - Last mean reward per episode: -494.56
Saving new best model at 39625 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 991         |
|    ep_rew_mean          | -495        |
| time/                   |             |
|    fps                  | 20          |
|    iterations           | 20          |
|    time_elapsed         | 1998        |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.008200058 |
|    clip_fraction        | 0.0311      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 19.5        |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00247    |
|    value_loss           | 39          |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 963          |
|    ep_rew_mean          | -480         |
| time/                   |              |
|    fps                  | 20           |
|    iterations           | 21           |
|    time_elapsed         | 2054         |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0036129346 |
|    clip_fraction        | 0.0791       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 8.2          |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00136     |
|    value_loss           | 16.3         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 928         |
|    ep_rew_mean          | -460        |
| time/                   |             |
|    fps                  | 21          |
|    iterations           | 22          |
|    time_elapsed         | 2109        |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.009946097 |
|    clip_fraction        | 0.0178      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 50.2        |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00133    |
|    value_loss           | 53.5        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 840         |
|    ep_rew_mean          | -413        |
| time/                   |             |
|    fps                  | 21          |
|    iterations           | 23          |
|    time_elapsed         | 2163        |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.015623787 |
|    clip_fraction        | 0.0766      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.949      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 27.2        |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00346    |
|    value_loss           | 74.1        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 741         |
|    ep_rew_mean          | -360        |
| time/                   |             |
|    fps                  | 22          |
|    iterations           | 24          |
|    time_elapsed         | 2217        |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.005789158 |
|    clip_fraction        | 0.0453      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.914      |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 32.8        |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00117    |
|    value_loss           | 145         |
-----------------------------------------
Num timesteps: 50000
Best mean reward: -494.56 - Last mean reward per episode: -338.73
Saving new best model at 49779 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 674          |
|    ep_rew_mean          | -324         |
| time/                   |              |
|    fps                  | 22           |
|    iterations           | 25           |
|    time_elapsed         | 2271         |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0046325265 |
|    clip_fraction        | 0.0559       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.846       |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 99.5         |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00204     |
|    value_loss           | 185          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 578         |
|    ep_rew_mean          | -273        |
| time/                   |             |
|    fps                  | 22          |
|    iterations           | 26          |
|    time_elapsed         | 2324        |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.009202414 |
|    clip_fraction        | 0.0863      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.763      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 52.8        |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00538    |
|    value_loss           | 163         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 472         |
|    ep_rew_mean          | -217        |
| time/                   |             |
|    fps                  | 23          |
|    iterations           | 27          |
|    time_elapsed         | 2379        |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.003337617 |
|    clip_fraction        | 0.0248      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 98.4        |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00243    |
|    value_loss           | 216         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 323          |
|    ep_rew_mean          | -137         |
| time/                   |              |
|    fps                  | 23           |
|    iterations           | 28           |
|    time_elapsed         | 2434         |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 8.883231e-05 |
|    clip_fraction        | 0.0305       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.676       |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 124          |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00217     |
|    value_loss           | 185          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 208          |
|    ep_rew_mean          | -75.5        |
| time/                   |              |
|    fps                  | 23           |
|    iterations           | 29           |
|    time_elapsed         | 2489         |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0035192175 |
|    clip_fraction        | 0.0403       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.646       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 98           |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00196     |
|    value_loss           | 174          |
------------------------------------------
Num timesteps: 60000
Best mean reward: -338.73 - Last mean reward per episode: -54.82
Saving new best model at 59990 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 138          |
|    ep_rew_mean          | -39.1        |
| time/                   |              |
|    fps                  | 24           |
|    iterations           | 30           |
|    time_elapsed         | 2546         |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0019245576 |
|    clip_fraction        | 0.0505       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.549       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 53.2         |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00189     |
|    value_loss           | 132          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 118          |
|    ep_rew_mean          | -29.4        |
| time/                   |              |
|    fps                  | 24           |
|    iterations           | 31           |
|    time_elapsed         | 2600         |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.0018970289 |
|    clip_fraction        | 0.0518       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.521       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 105          |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00179     |
|    value_loss           | 171          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 111          |
|    ep_rew_mean          | -25.6        |
| time/                   |              |
|    fps                  | 24           |
|    iterations           | 32           |
|    time_elapsed         | 2655         |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0037410958 |
|    clip_fraction        | 0.0367       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.484       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 107          |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.00133     |
|    value_loss           | 154          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 109          |
|    ep_rew_mean          | -24.9        |
| time/                   |              |
|    fps                  | 24           |
|    iterations           | 33           |
|    time_elapsed         | 2711         |
|    total_timesteps      | 67584        |
| train/                  |              |
|    approx_kl            | 0.0007397408 |
|    clip_fraction        | 0.0374       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.403       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 52.1         |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00141     |
|    value_loss           | 143          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 97.8         |
|    ep_rew_mean          | -18.9        |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 34           |
|    time_elapsed         | 2772         |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.0018523699 |
|    clip_fraction        | 0.0219       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.405       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 39.7         |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.000574    |
|    value_loss           | 136          |
------------------------------------------
Num timesteps: 70000
Best mean reward: -54.82 - Last mean reward per episode: -17.61
Saving new best model at 69985 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 98           |
|    ep_rew_mean          | -19          |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 35           |
|    time_elapsed         | 2830         |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.0026008105 |
|    clip_fraction        | 0.0308       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.387       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 54.3         |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.0026      |
|    value_loss           | 140          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 96.9         |
|    ep_rew_mean          | -18.5        |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 36           |
|    time_elapsed         | 2891         |
|    total_timesteps      | 73728        |
| train/                  |              |
|    approx_kl            | 0.0024144386 |
|    clip_fraction        | 0.0456       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.322       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 41.9         |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.00237     |
|    value_loss           | 121          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 98.5          |
|    ep_rew_mean          | -19.3         |
| time/                   |               |
|    fps                  | 25            |
|    iterations           | 37            |
|    time_elapsed         | 2950          |
|    total_timesteps      | 75776         |
| train/                  |               |
|    approx_kl            | 0.00058099267 |
|    clip_fraction        | 0.0181        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.281        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 52.7          |
|    n_updates            | 360           |
|    policy_gradient_loss | -0.00122      |
|    value_loss           | 107           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 98.9          |
|    ep_rew_mean          | -19.5         |
| time/                   |               |
|    fps                  | 25            |
|    iterations           | 38            |
|    time_elapsed         | 3009          |
|    total_timesteps      | 77824         |
| train/                  |               |
|    approx_kl            | 0.00013055833 |
|    clip_fraction        | 0.00972       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.264        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 62.9          |
|    n_updates            | 370           |
|    policy_gradient_loss | -0.000291     |
|    value_loss           | 117           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 107          |
|    ep_rew_mean          | -23.4        |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 39           |
|    time_elapsed         | 3070         |
|    total_timesteps      | 79872        |
| train/                  |              |
|    approx_kl            | 0.0011884165 |
|    clip_fraction        | 0.00937      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.304       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 55.1         |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.000406    |
|    value_loss           | 111          |
------------------------------------------
Num timesteps: 80000
Best mean reward: -17.61 - Last mean reward per episode: -23.50
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 114          |
|    ep_rew_mean          | -27.2        |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 40           |
|    time_elapsed         | 3129         |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.0015893825 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.317       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 42.1         |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.000313    |
|    value_loss           | 96           |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 103          |
|    ep_rew_mean          | -21.5        |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 41           |
|    time_elapsed         | 3186         |
|    total_timesteps      | 83968        |
| train/                  |              |
|    approx_kl            | 0.0005314527 |
|    clip_fraction        | 0.00972      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.325       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 51.2         |
|    n_updates            | 400          |
|    policy_gradient_loss | 3.26e-05     |
|    value_loss           | 116          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 97.2          |
|    ep_rew_mean          | -18.6         |
| time/                   |               |
|    fps                  | 26            |
|    iterations           | 42            |
|    time_elapsed         | 3246          |
|    total_timesteps      | 86016         |
| train/                  |               |
|    approx_kl            | 0.00067578116 |
|    clip_fraction        | 0.00698       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.278        |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 69.6          |
|    n_updates            | 410           |
|    policy_gradient_loss | -0.000115     |
|    value_loss           | 138           |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 95.5        |
|    ep_rew_mean          | -17.8       |
| time/                   |             |
|    fps                  | 26          |
|    iterations           | 43          |
|    time_elapsed         | 3305        |
|    total_timesteps      | 88064       |
| train/                  |             |
|    approx_kl            | 0.000954626 |
|    clip_fraction        | 0.0189      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.27       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 69.9        |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0008     |
|    value_loss           | 124         |
-----------------------------------------
Num timesteps: 90000
Best mean reward: -17.61 - Last mean reward per episode: -16.41
Saving new best model at 89927 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 92.8         |
|    ep_rew_mean          | -16.4        |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 44           |
|    time_elapsed         | 3368         |
|    total_timesteps      | 90112        |
| train/                  |              |
|    approx_kl            | 0.0006492061 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.245       |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 49.1         |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.000262    |
|    value_loss           | 108          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 93.1          |
|    ep_rew_mean          | -16.5         |
| time/                   |               |
|    fps                  | 26            |
|    iterations           | 45            |
|    time_elapsed         | 3425          |
|    total_timesteps      | 92160         |
| train/                  |               |
|    approx_kl            | 0.00012258653 |
|    clip_fraction        | 0.00752       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.26         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 53.1          |
|    n_updates            | 440           |
|    policy_gradient_loss | 0.000165      |
|    value_loss           | 92.7          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 99.8          |
|    ep_rew_mean          | -19.9         |
| time/                   |               |
|    fps                  | 27            |
|    iterations           | 46            |
|    time_elapsed         | 3483          |
|    total_timesteps      | 94208         |
| train/                  |               |
|    approx_kl            | 0.00030568798 |
|    clip_fraction        | 0.00835       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.236        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 84.4          |
|    n_updates            | 450           |
|    policy_gradient_loss | -0.000561     |
|    value_loss           | 113           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 103           |
|    ep_rew_mean          | -21.3         |
| time/                   |               |
|    fps                  | 27            |
|    iterations           | 47            |
|    time_elapsed         | 3542          |
|    total_timesteps      | 96256         |
| train/                  |               |
|    approx_kl            | 0.00068686047 |
|    clip_fraction        | 0.00972       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.246        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 47.2          |
|    n_updates            | 460           |
|    policy_gradient_loss | 6.41e-05      |
|    value_loss           | 99.6          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 98.5         |
|    ep_rew_mean          | -19.2        |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 48           |
|    time_elapsed         | 3600         |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0006369298 |
|    clip_fraction        | 0.0136       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.228       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 76.5         |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.000733    |
|    value_loss           | 124          |
------------------------------------------
Num timesteps: 100000
Best mean reward: -16.41 - Last mean reward per episode: -14.13
Saving new best model at 99975 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 88.9          |
|    ep_rew_mean          | -14.4         |
| time/                   |               |
|    fps                  | 27            |
|    iterations           | 49            |
|    time_elapsed         | 3660          |
|    total_timesteps      | 100352        |
| train/                  |               |
|    approx_kl            | 0.00036871532 |
|    clip_fraction        | 0.00449       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.206        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 48.8          |
|    n_updates            | 480           |
|    policy_gradient_loss | -7.46e-05     |
|    value_loss           | 123           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 80.8          |
|    ep_rew_mean          | -10.4         |
| time/                   |               |
|    fps                  | 27            |
|    iterations           | 50            |
|    time_elapsed         | 3721          |
|    total_timesteps      | 102400        |
| train/                  |               |
|    approx_kl            | 0.00091266586 |
|    clip_fraction        | 0.00801       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.173        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 68.6          |
|    n_updates            | 490           |
|    policy_gradient_loss | -0.000571     |
|    value_loss           | 116           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 84.9          |
|    ep_rew_mean          | -12.4         |
| time/                   |               |
|    fps                  | 27            |
|    iterations           | 51            |
|    time_elapsed         | 3780          |
|    total_timesteps      | 104448        |
| train/                  |               |
|    approx_kl            | 0.00014989634 |
|    clip_fraction        | 0.0102        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.142        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 64            |
|    n_updates            | 500           |
|    policy_gradient_loss | -0.000743     |
|    value_loss           | 110           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 89.6         |
|    ep_rew_mean          | -14.8        |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 52           |
|    time_elapsed         | 3839         |
|    total_timesteps      | 106496       |
| train/                  |              |
|    approx_kl            | 0.0001596447 |
|    clip_fraction        | 0.00151      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.147       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 61.4         |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.000141    |
|    value_loss           | 95           |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 90.3          |
|    ep_rew_mean          | -15.1         |
| time/                   |               |
|    fps                  | 27            |
|    iterations           | 53            |
|    time_elapsed         | 3897          |
|    total_timesteps      | 108544        |
| train/                  |               |
|    approx_kl            | 3.5282457e-05 |
|    clip_fraction        | 0.00747       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.171        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 41.1          |
|    n_updates            | 520           |
|    policy_gradient_loss | -0.000248     |
|    value_loss           | 99.8          |
-------------------------------------------
Num timesteps: 110000
Best mean reward: -14.13 - Last mean reward per episode: -16.30
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 94.4         |
|    ep_rew_mean          | -17.2        |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 54           |
|    time_elapsed         | 3956         |
|    total_timesteps      | 110592       |
| train/                  |              |
|    approx_kl            | 0.0002813034 |
|    clip_fraction        | 0.00972      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.148       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 68.4         |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.000122    |
|    value_loss           | 113          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 89.3          |
|    ep_rew_mean          | -14.7         |
| time/                   |               |
|    fps                  | 28            |
|    iterations           | 55            |
|    time_elapsed         | 4016          |
|    total_timesteps      | 112640        |
| train/                  |               |
|    approx_kl            | 0.00082371046 |
|    clip_fraction        | 0.00986       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.127        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 58.9          |
|    n_updates            | 540           |
|    policy_gradient_loss | -0.000436     |
|    value_loss           | 107           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 88.9         |
|    ep_rew_mean          | -14.5        |
| time/                   |              |
|    fps                  | 28           |
|    iterations           | 56           |
|    time_elapsed         | 4077         |
|    total_timesteps      | 114688       |
| train/                  |              |
|    approx_kl            | 0.0003775141 |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.133       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 51.9         |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.000452    |
|    value_loss           | 114          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 88.5         |
|    ep_rew_mean          | -14.3        |
| time/                   |              |
|    fps                  | 28           |
|    iterations           | 57           |
|    time_elapsed         | 4139         |
|    total_timesteps      | 116736       |
| train/                  |              |
|    approx_kl            | 0.0005188441 |
|    clip_fraction        | 0.0061       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.112       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 46.4         |
|    n_updates            | 560          |
|    policy_gradient_loss | -1.74e-05    |
|    value_loss           | 103          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 82.3          |
|    ep_rew_mean          | -11.2         |
| time/                   |               |
|    fps                  | 28            |
|    iterations           | 58            |
|    time_elapsed         | 4197          |
|    total_timesteps      | 118784        |
| train/                  |               |
|    approx_kl            | 0.00021044517 |
|    clip_fraction        | 0.00386       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0968       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 103           |
|    n_updates            | 570           |
|    policy_gradient_loss | -0.000184     |
|    value_loss           | 111           |
-------------------------------------------
Num timesteps: 120000
Best mean reward: -14.13 - Last mean reward per episode: -11.26
Saving new best model at 119964 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 82.5         |
|    ep_rew_mean          | -11.2        |
| time/                   |              |
|    fps                  | 28           |
|    iterations           | 59           |
|    time_elapsed         | 4256         |
|    total_timesteps      | 120832       |
| train/                  |              |
|    approx_kl            | 0.0005137123 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0875      |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 66.1         |
|    n_updates            | 580          |
|    policy_gradient_loss | -0.000872    |
|    value_loss           | 118          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 81.4          |
|    ep_rew_mean          | -10.7         |
| time/                   |               |
|    fps                  | 28            |
|    iterations           | 60            |
|    time_elapsed         | 4313          |
|    total_timesteps      | 122880        |
| train/                  |               |
|    approx_kl            | 0.00012467225 |
|    clip_fraction        | 0.00122       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0811       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 50.4          |
|    n_updates            | 590           |
|    policy_gradient_loss | -7.97e-05     |
|    value_loss           | 104           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 87.8          |
|    ep_rew_mean          | -14.2         |
| time/                   |               |
|    fps                  | 28            |
|    iterations           | 61            |
|    time_elapsed         | 4372          |
|    total_timesteps      | 124928        |
| train/                  |               |
|    approx_kl            | 5.1906187e-05 |
|    clip_fraction        | 0.00537       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0946       |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 39.1          |
|    n_updates            | 600           |
|    policy_gradient_loss | 0.000332      |
|    value_loss           | 88.6          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 90.8          |
|    ep_rew_mean          | -15.7         |
| time/                   |               |
|    fps                  | 28            |
|    iterations           | 62            |
|    time_elapsed         | 4432          |
|    total_timesteps      | 126976        |
| train/                  |               |
|    approx_kl            | 0.00018759628 |
|    clip_fraction        | 0.00186       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.096        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 51.2          |
|    n_updates            | 610           |
|    policy_gradient_loss | -1.22e-05     |
|    value_loss           | 98.1          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 97.3          |
|    ep_rew_mean          | -19           |
| time/                   |               |
|    fps                  | 28            |
|    iterations           | 63            |
|    time_elapsed         | 4488          |
|    total_timesteps      | 129024        |
| train/                  |               |
|    approx_kl            | 0.00045458082 |
|    clip_fraction        | 0.00771       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.109        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 78.4          |
|    n_updates            | 620           |
|    policy_gradient_loss | 0.000217      |
|    value_loss           | 117           |
-------------------------------------------
Num timesteps: 130000
Best mean reward: -11.26 - Last mean reward per episode: -23.57
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 111           |
|    ep_rew_mean          | -25.6         |
| time/                   |               |
|    fps                  | 28            |
|    iterations           | 64            |
|    time_elapsed         | 4550          |
|    total_timesteps      | 131072        |
| train/                  |               |
|    approx_kl            | 0.00015368825 |
|    clip_fraction        | 0.000732      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.12         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 31.8          |
|    n_updates            | 630           |
|    policy_gradient_loss | -1.92e-05     |
|    value_loss           | 73            |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 108           |
|    ep_rew_mean          | -24.5         |
| time/                   |               |
|    fps                  | 28            |
|    iterations           | 65            |
|    time_elapsed         | 4613          |
|    total_timesteps      | 133120        |
| train/                  |               |
|    approx_kl            | 0.00034265517 |
|    clip_fraction        | 0.0043        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.155        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 73.4          |
|    n_updates            | 640           |
|    policy_gradient_loss | 0.000346      |
|    value_loss           | 100           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 99.5         |
|    ep_rew_mean          | -19.8        |
| time/                   |              |
|    fps                  | 28           |
|    iterations           | 66           |
|    time_elapsed         | 4678         |
|    total_timesteps      | 135168       |
| train/                  |              |
|    approx_kl            | 0.0012618129 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.163       |
|    explained_variance   | 2.38e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 41.2         |
|    n_updates            | 650          |
|    policy_gradient_loss | -0.000711    |
|    value_loss           | 137          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 102           |
|    ep_rew_mean          | -21           |
| time/                   |               |
|    fps                  | 28            |
|    iterations           | 67            |
|    time_elapsed         | 4737          |
|    total_timesteps      | 137216        |
| train/                  |               |
|    approx_kl            | 0.00015201993 |
|    clip_fraction        | 0.00615       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.155        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 54.1          |
|    n_updates            | 660           |
|    policy_gradient_loss | 0.000263      |
|    value_loss           | 114           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 83.9          |
|    ep_rew_mean          | -11.9         |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 68            |
|    time_elapsed         | 4794          |
|    total_timesteps      | 139264        |
| train/                  |               |
|    approx_kl            | 0.00053985365 |
|    clip_fraction        | 0.0125        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.149        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 64.9          |
|    n_updates            | 670           |
|    policy_gradient_loss | -0.000735     |
|    value_loss           | 119           |
-------------------------------------------
Num timesteps: 140000
Best mean reward: -11.26 - Last mean reward per episode: -12.08
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 86.7          |
|    ep_rew_mean          | -13.4         |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 69            |
|    time_elapsed         | 4849          |
|    total_timesteps      | 141312        |
| train/                  |               |
|    approx_kl            | 0.00064250943 |
|    clip_fraction        | 0.0109        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.136        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 82.7          |
|    n_updates            | 680           |
|    policy_gradient_loss | -0.000228     |
|    value_loss           | 113           |
-------------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 290, in <module>
    model.learn(total_timesteps=160000, callback=[WandbCallback(), save_callback])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 313, in learn
    self.train()
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 217, in train
    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 730, in evaluate_actions
    features = self.extract_features(obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/policy.py", line 40, in forward
    pyg_data = self.encoder.encode(observations)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/encoder/GraphEncoder.py", line 426, in encode
    chicken_indices = [i for i in range(num_nodes) if node_features[i, -3] == 1]
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/encoder/GraphEncoder.py", line 426, in <listcomp>
    chicken_indices = [i for i in range(num_nodes) if node_features[i, -3] == 1]
KeyboardInterrupt