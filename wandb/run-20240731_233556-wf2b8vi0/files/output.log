---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.9     |
|    ep_rew_mean     | -45.5    |
| time/              |          |
|    fps             | 282      |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 2048     |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 90.9          |
|    ep_rew_mean          | -45.5         |
| time/                   |               |
|    fps                  | 79            |
|    iterations           | 2             |
|    time_elapsed         | 51            |
|    total_timesteps      | 4096          |
| train/                  |               |
|    approx_kl            | 0.00025866838 |
|    clip_fraction        | 0.004         |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0803       |
|    explained_variance   | 0.00193       |
|    learning_rate        | 0.0003        |
|    loss                 | 71.5          |
|    n_updates            | 590           |
|    policy_gradient_loss | -9.48e-05     |
|    value_loss           | 139           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 166           |
|    ep_rew_mean          | -128          |
| time/                   |               |
|    fps                  | 62            |
|    iterations           | 3             |
|    time_elapsed         | 99            |
|    total_timesteps      | 6144          |
| train/                  |               |
|    approx_kl            | 0.00043643877 |
|    clip_fraction        | 0.00513       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0767       |
|    explained_variance   | 1.79e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 9.84          |
|    n_updates            | 600           |
|    policy_gradient_loss | -0.000111     |
|    value_loss           | 39.4          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 135          |
|    ep_rew_mean          | -92.7        |
| time/                   |              |
|    fps                  | 57           |
|    iterations           | 4            |
|    time_elapsed         | 141          |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.0007357771 |
|    clip_fraction        | 0.00503      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0867      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 144          |
|    n_updates            | 610          |
|    policy_gradient_loss | -0.000177    |
|    value_loss           | 259          |
------------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -84.30
Saving new best model at 9941 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 126          |
|    ep_rew_mean          | -83.3        |
| time/                   |              |
|    fps                  | 54           |
|    iterations           | 5            |
|    time_elapsed         | 186          |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.0006371777 |
|    clip_fraction        | 0.00273      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.116       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 138          |
|    n_updates            | 620          |
|    policy_gradient_loss | -5.97e-06    |
|    value_loss           | 272          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 116           |
|    ep_rew_mean          | -71           |
| time/                   |               |
|    fps                  | 53            |
|    iterations           | 6             |
|    time_elapsed         | 231           |
|    total_timesteps      | 12288         |
| train/                  |               |
|    approx_kl            | 0.00096096296 |
|    clip_fraction        | 0.00278       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.108        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 131           |
|    n_updates            | 630           |
|    policy_gradient_loss | -0.000384     |
|    value_loss           | 236           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 122           |
|    ep_rew_mean          | -76.2         |
| time/                   |               |
|    fps                  | 52            |
|    iterations           | 7             |
|    time_elapsed         | 274           |
|    total_timesteps      | 14336         |
| train/                  |               |
|    approx_kl            | 0.00036431142 |
|    clip_fraction        | 0.00967       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0997       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 98.7          |
|    n_updates            | 640           |
|    policy_gradient_loss | -0.000105     |
|    value_loss           | 207           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 151           |
|    ep_rew_mean          | -98.4         |
| time/                   |               |
|    fps                  | 51            |
|    iterations           | 8             |
|    time_elapsed         | 316           |
|    total_timesteps      | 16384         |
| train/                  |               |
|    approx_kl            | 0.00058361323 |
|    clip_fraction        | 0.00762       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0878       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 43.8          |
|    n_updates            | 650           |
|    policy_gradient_loss | 0.000107      |
|    value_loss           | 107           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 126          |
|    ep_rew_mean          | -70.5        |
| time/                   |              |
|    fps                  | 51           |
|    iterations           | 9            |
|    time_elapsed         | 359          |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0006190755 |
|    clip_fraction        | 0.0117       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.115       |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 3.64         |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.000523    |
|    value_loss           | 10.5         |
------------------------------------------
Num timesteps: 20000
Best mean reward: -84.30 - Last mean reward per episode: -74.34
Saving new best model at 19950 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 132          |
|    ep_rew_mean          | -74.9        |
| time/                   |              |
|    fps                  | 50           |
|    iterations           | 10           |
|    time_elapsed         | 401          |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.0002559859 |
|    clip_fraction        | 0.00713      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.127       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 163          |
|    n_updates            | 670          |
|    policy_gradient_loss | -0.000229    |
|    value_loss           | 316          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 131           |
|    ep_rew_mean          | -73.6         |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 11            |
|    time_elapsed         | 444           |
|    total_timesteps      | 22528         |
| train/                  |               |
|    approx_kl            | 0.00031774305 |
|    clip_fraction        | 0.00356       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.114        |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 76.4          |
|    n_updates            | 680           |
|    policy_gradient_loss | -0.000272     |
|    value_loss           | 214           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 131          |
|    ep_rew_mean          | -73.1        |
| time/                   |              |
|    fps                  | 50           |
|    iterations           | 12           |
|    time_elapsed         | 489          |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0002446841 |
|    clip_fraction        | 0.00459      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.111       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 111          |
|    n_updates            | 690          |
|    policy_gradient_loss | 1.95e-05     |
|    value_loss           | 305          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 96          |
|    ep_rew_mean          | -47.4       |
| time/                   |             |
|    fps                  | 49          |
|    iterations           | 13          |
|    time_elapsed         | 534         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.000412566 |
|    clip_fraction        | 0.0083      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0914     |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 119         |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.000706   |
|    value_loss           | 254         |
-----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 97.8          |
|    ep_rew_mean          | -48           |
| time/                   |               |
|    fps                  | 49            |
|    iterations           | 14            |
|    time_elapsed         | 578           |
|    total_timesteps      | 28672         |
| train/                  |               |
|    approx_kl            | 0.00034871144 |
|    clip_fraction        | 0.00342       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0797       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 169           |
|    n_updates            | 710           |
|    policy_gradient_loss | -0.00053      |
|    value_loss           | 241           |
-------------------------------------------
Num timesteps: 30000
Best mean reward: -74.34 - Last mean reward per episode: -43.88
Saving new best model at 29768 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 91.8        |
|    ep_rew_mean          | -43.9       |
| time/                   |             |
|    fps                  | 49          |
|    iterations           | 15          |
|    time_elapsed         | 619         |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.000263801 |
|    clip_fraction        | 0.00137     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0771     |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 75.4        |
|    n_updates            | 720         |
|    policy_gradient_loss | -5.18e-05   |
|    value_loss           | 207         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 121          |
|    ep_rew_mean          | -79.9        |
| time/                   |              |
|    fps                  | 49           |
|    iterations           | 16           |
|    time_elapsed         | 667          |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 8.832777e-05 |
|    clip_fraction        | 0.00532      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0712      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 135          |
|    n_updates            | 730          |
|    policy_gradient_loss | 0.000365     |
|    value_loss           | 212          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 121           |
|    ep_rew_mean          | -81.1         |
| time/                   |               |
|    fps                  | 48            |
|    iterations           | 17            |
|    time_elapsed         | 722           |
|    total_timesteps      | 34816         |
| train/                  |               |
|    approx_kl            | 0.00031902533 |
|    clip_fraction        | 0.00298       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0869       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 22.4          |
|    n_updates            | 740           |
|    policy_gradient_loss | -0.000102     |
|    value_loss           | 68.3          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 119           |
|    ep_rew_mean          | -80.2         |
| time/                   |               |
|    fps                  | 48            |
|    iterations           | 18            |
|    time_elapsed         | 765           |
|    total_timesteps      | 36864         |
| train/                  |               |
|    approx_kl            | 0.00030599206 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.082        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 209           |
|    n_updates            | 750           |
|    policy_gradient_loss | -7.58e-05     |
|    value_loss           | 426           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 117           |
|    ep_rew_mean          | -78.2         |
| time/                   |               |
|    fps                  | 48            |
|    iterations           | 19            |
|    time_elapsed         | 806           |
|    total_timesteps      | 38912         |
| train/                  |               |
|    approx_kl            | 4.3196604e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0759       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 242           |
|    n_updates            | 760           |
|    policy_gradient_loss | -1.22e-05     |
|    value_loss           | 336           |
-------------------------------------------
Num timesteps: 40000
Best mean reward: -43.88 - Last mean reward per episode: -78.30
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 116           |
|    ep_rew_mean          | -77.5         |
| time/                   |               |
|    fps                  | 48            |
|    iterations           | 20            |
|    time_elapsed         | 850           |
|    total_timesteps      | 40960         |
| train/                  |               |
|    approx_kl            | 8.4857485e-05 |
|    clip_fraction        | 0.00425       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0734       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 137           |
|    n_updates            | 770           |
|    policy_gradient_loss | -0.000135     |
|    value_loss           | 293           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 83.8          |
|    ep_rew_mean          | -37.2         |
| time/                   |               |
|    fps                  | 48            |
|    iterations           | 21            |
|    time_elapsed         | 891           |
|    total_timesteps      | 43008         |
| train/                  |               |
|    approx_kl            | 0.00010322561 |
|    clip_fraction        | 0.00918       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0758       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 95            |
|    n_updates            | 780           |
|    policy_gradient_loss | -0.000537     |
|    value_loss           | 269           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 87            |
|    ep_rew_mean          | -40.5         |
| time/                   |               |
|    fps                  | 48            |
|    iterations           | 22            |
|    time_elapsed         | 934           |
|    total_timesteps      | 45056         |
| train/                  |               |
|    approx_kl            | 0.00030731614 |
|    clip_fraction        | 0.00376       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0947       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 82.5          |
|    n_updates            | 790           |
|    policy_gradient_loss | -1.06e-05     |
|    value_loss           | 227           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 86.8          |
|    ep_rew_mean          | -39.8         |
| time/                   |               |
|    fps                  | 48            |
|    iterations           | 23            |
|    time_elapsed         | 979           |
|    total_timesteps      | 47104         |
| train/                  |               |
|    approx_kl            | 0.00026781173 |
|    clip_fraction        | 0.00449       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0803       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 97.5          |
|    n_updates            | 800           |
|    policy_gradient_loss | 0.00048       |
|    value_loss           | 239           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 85.6          |
|    ep_rew_mean          | -38.6         |
| time/                   |               |
|    fps                  | 48            |
|    iterations           | 24            |
|    time_elapsed         | 1021          |
|    total_timesteps      | 49152         |
| train/                  |               |
|    approx_kl            | 0.00021698533 |
|    clip_fraction        | 0.00669       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.051        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 94.9          |
|    n_updates            | 810           |
|    policy_gradient_loss | -0.000651     |
|    value_loss           | 207           |
-------------------------------------------
Num timesteps: 50000
Best mean reward: -43.88 - Last mean reward per episode: -38.64
Saving new best model at 48114 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 115           |
|    ep_rew_mean          | -77.1         |
| time/                   |               |
|    fps                  | 47            |
|    iterations           | 25            |
|    time_elapsed         | 1071          |
|    total_timesteps      | 51200         |
| train/                  |               |
|    approx_kl            | 2.4376466e-05 |
|    clip_fraction        | 0.00386       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0581       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 106           |
|    n_updates            | 820           |
|    policy_gradient_loss | 0.000456      |
|    value_loss           | 218           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 115           |
|    ep_rew_mean          | -76.5         |
| time/                   |               |
|    fps                  | 47            |
|    iterations           | 26            |
|    time_elapsed         | 1128          |
|    total_timesteps      | 53248         |
| train/                  |               |
|    approx_kl            | 0.00048814766 |
|    clip_fraction        | 0.00625       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0558       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 33.9          |
|    n_updates            | 830           |
|    policy_gradient_loss | -0.000627     |
|    value_loss           | 80.3          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 128          |
|    ep_rew_mean          | -86.5        |
| time/                   |              |
|    fps                  | 47           |
|    iterations           | 27           |
|    time_elapsed         | 1171         |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0002207199 |
|    clip_fraction        | 0.00269      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0519      |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 161          |
|    n_updates            | 840          |
|    policy_gradient_loss | 0.000252     |
|    value_loss           | 376          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 127           |
|    ep_rew_mean          | -86.8         |
| time/                   |               |
|    fps                  | 47            |
|    iterations           | 28            |
|    time_elapsed         | 1216          |
|    total_timesteps      | 57344         |
| train/                  |               |
|    approx_kl            | 0.00025384044 |
|    clip_fraction        | 0.00264       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0455       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 122           |
|    n_updates            | 850           |
|    policy_gradient_loss | -0.00015      |
|    value_loss           | 174           |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 127            |
|    ep_rew_mean          | -86.8          |
| time/                   |                |
|    fps                  | 46             |
|    iterations           | 29             |
|    time_elapsed         | 1265           |
|    total_timesteps      | 59392          |
| train/                  |                |
|    approx_kl            | 0.000105153245 |
|    clip_fraction        | 0.00479        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0537        |
|    explained_variance   | -1.19e-07      |
|    learning_rate        | 0.0003         |
|    loss                 | 162            |
|    n_updates            | 860            |
|    policy_gradient_loss | 0.000191       |
|    value_loss           | 369            |
--------------------------------------------
Num timesteps: 60000
Best mean reward: -38.64 - Last mean reward per episode: -120.56
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 152           |
|    ep_rew_mean          | -121          |
| time/                   |               |
|    fps                  | 46            |
|    iterations           | 30            |
|    time_elapsed         | 1318          |
|    total_timesteps      | 61440         |
| train/                  |               |
|    approx_kl            | 0.00013020707 |
|    clip_fraction        | 0.00137       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0513       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 12.4          |
|    n_updates            | 870           |
|    policy_gradient_loss | -0.000372     |
|    value_loss           | 103           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 182          |
|    ep_rew_mean          | -159         |
| time/                   |              |
|    fps                  | 46           |
|    iterations           | 31           |
|    time_elapsed         | 1364         |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 2.227805e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0536      |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 27.3         |
|    n_updates            | 880          |
|    policy_gradient_loss | -4.72e-05    |
|    value_loss           | 156          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 182           |
|    ep_rew_mean          | -160          |
| time/                   |               |
|    fps                  | 46            |
|    iterations           | 32            |
|    time_elapsed         | 1408          |
|    total_timesteps      | 65536         |
| train/                  |               |
|    approx_kl            | 0.00023669796 |
|    clip_fraction        | 0.00645       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0654       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 240           |
|    n_updates            | 890           |
|    policy_gradient_loss | 0.000294      |
|    value_loss           | 365           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 157           |
|    ep_rew_mean          | -126          |
| time/                   |               |
|    fps                  | 46            |
|    iterations           | 33            |
|    time_elapsed         | 1452          |
|    total_timesteps      | 67584         |
| train/                  |               |
|    approx_kl            | 4.8383372e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0594       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 347           |
|    n_updates            | 900           |
|    policy_gradient_loss | -0.000158     |
|    value_loss           | 636           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 158          |
|    ep_rew_mean          | -126         |
| time/                   |              |
|    fps                  | 46           |
|    iterations           | 34           |
|    time_elapsed         | 1497         |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 7.199714e-05 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0596      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 234          |
|    n_updates            | 910          |
|    policy_gradient_loss | -4.99e-05    |
|    value_loss           | 439          |
------------------------------------------
Num timesteps: 70000
Best mean reward: -38.64 - Last mean reward per episode: -125.67
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 187          |
|    ep_rew_mean          | -148         |
| time/                   |              |
|    fps                  | 46           |
|    iterations           | 35           |
|    time_elapsed         | 1541         |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.0005448143 |
|    clip_fraction        | 0.0085       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0685      |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 90.1         |
|    n_updates            | 920          |
|    policy_gradient_loss | -0.000387    |
|    value_loss           | 172          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 176           |
|    ep_rew_mean          | -141          |
| time/                   |               |
|    fps                  | 46            |
|    iterations           | 36            |
|    time_elapsed         | 1586          |
|    total_timesteps      | 73728         |
| train/                  |               |
|    approx_kl            | 0.00023224039 |
|    clip_fraction        | 0.00581       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0773       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 40.6          |
|    n_updates            | 930           |
|    policy_gradient_loss | -0.000286     |
|    value_loss           | 49.7          |
-------------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 290, in <module>
    model.learn(total_timesteps=1000000, callback=[WandbCallback(), save_callback])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 313, in learn
    self.train()
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 217, in train
    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 730, in evaluate_actions
    features = self.extract_features(obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/policy.py", line 40, in forward
    pyg_data = self.encoder.encode(observations)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/encoder/GraphEncoder.py", line 426, in encode
    chicken_indices = [i for i in range(num_nodes) if node_features[i, -3] == 1]
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/encoder/GraphEncoder.py", line 426, in <listcomp>
    chicken_indices = [i for i in range(num_nodes) if node_features[i, -3] == 1]
KeyboardInterrupt