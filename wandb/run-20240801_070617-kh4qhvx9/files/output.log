---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.9     |
|    ep_rew_mean     | -43.1    |
| time/              |          |
|    fps             | 257      |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 2048     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 88.5         |
|    ep_rew_mean          | -38.6        |
| time/                   |              |
|    fps                  | 70           |
|    iterations           | 2            |
|    time_elapsed         | 58           |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0005530617 |
|    clip_fraction        | 0.00562      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0977      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 43.4         |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.000328    |
|    value_loss           | 105          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 83.4        |
|    ep_rew_mean          | -34.2       |
| time/                   |             |
|    fps                  | 56          |
|    iterations           | 3           |
|    time_elapsed         | 109         |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.000445846 |
|    clip_fraction        | 0.00884     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0999     |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 71.8        |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.000882   |
|    value_loss           | 154         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 82.5         |
|    ep_rew_mean          | -32.2        |
| time/                   |              |
|    fps                  | 50           |
|    iterations           | 4            |
|    time_elapsed         | 163          |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.0011666185 |
|    clip_fraction        | 0.00269      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.105       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 83.9         |
|    n_updates            | 610          |
|    policy_gradient_loss | -1.24e-06    |
|    value_loss           | 161          |
------------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -35.15
Saving new best model at 9888 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 86            |
|    ep_rew_mean          | -36.5         |
| time/                   |               |
|    fps                  | 46            |
|    iterations           | 5             |
|    time_elapsed         | 220           |
|    total_timesteps      | 10240         |
| train/                  |               |
|    approx_kl            | 0.00078705547 |
|    clip_fraction        | 0.0116        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.104        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 86.6          |
|    n_updates            | 620           |
|    policy_gradient_loss | -0.00106      |
|    value_loss           | 159           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 89.3          |
|    ep_rew_mean          | -38.5         |
| time/                   |               |
|    fps                  | 44            |
|    iterations           | 6             |
|    time_elapsed         | 273           |
|    total_timesteps      | 12288         |
| train/                  |               |
|    approx_kl            | 0.00046108686 |
|    clip_fraction        | 0.00405       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0959       |
|    explained_variance   | 1.79e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 83            |
|    n_updates            | 630           |
|    policy_gradient_loss | 0.000147      |
|    value_loss           | 158           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 90.5          |
|    ep_rew_mean          | -38           |
| time/                   |               |
|    fps                  | 44            |
|    iterations           | 7             |
|    time_elapsed         | 323           |
|    total_timesteps      | 14336         |
| train/                  |               |
|    approx_kl            | 0.00035368357 |
|    clip_fraction        | 0.00776       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0735       |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 83.8          |
|    n_updates            | 640           |
|    policy_gradient_loss | -0.000585     |
|    value_loss           | 174           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 93.4          |
|    ep_rew_mean          | -41.6         |
| time/                   |               |
|    fps                  | 43            |
|    iterations           | 8             |
|    time_elapsed         | 374           |
|    total_timesteps      | 16384         |
| train/                  |               |
|    approx_kl            | 0.00038490276 |
|    clip_fraction        | 0.00547       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0631       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 127           |
|    n_updates            | 650           |
|    policy_gradient_loss | -0.0003       |
|    value_loss           | 189           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 90.2         |
|    ep_rew_mean          | -38.6        |
| time/                   |              |
|    fps                  | 43           |
|    iterations           | 9            |
|    time_elapsed         | 423          |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 6.087101e-05 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0528      |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 74.6         |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.000107    |
|    value_loss           | 173          |
------------------------------------------
Num timesteps: 20000
Best mean reward: -35.15 - Last mean reward per episode: -34.77
Saving new best model at 19962 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 87.1         |
|    ep_rew_mean          | -35.2        |
| time/                   |              |
|    fps                  | 43           |
|    iterations           | 10           |
|    time_elapsed         | 474          |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.0001008597 |
|    clip_fraction        | 0.00171      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0523      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 79.6         |
|    n_updates            | 670          |
|    policy_gradient_loss | -1.26e-05    |
|    value_loss           | 174          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 86            |
|    ep_rew_mean          | -34.1         |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 11            |
|    time_elapsed         | 524           |
|    total_timesteps      | 22528         |
| train/                  |               |
|    approx_kl            | 0.00013913136 |
|    clip_fraction        | 0.00186       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0495       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 95.7          |
|    n_updates            | 680           |
|    policy_gradient_loss | -0.000123     |
|    value_loss           | 157           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 87.1         |
|    ep_rew_mean          | -34.4        |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 12           |
|    time_elapsed         | 573          |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 8.337328e-05 |
|    clip_fraction        | 0.00244      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0441      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 111          |
|    n_updates            | 690          |
|    policy_gradient_loss | 0.000108     |
|    value_loss           | 187          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 89            |
|    ep_rew_mean          | -35.5         |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 13            |
|    time_elapsed         | 622           |
|    total_timesteps      | 26624         |
| train/                  |               |
|    approx_kl            | 9.8582095e-05 |
|    clip_fraction        | 0.00371       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.057        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 101           |
|    n_updates            | 700           |
|    policy_gradient_loss | -0.000308     |
|    value_loss           | 155           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 116           |
|    ep_rew_mean          | -54.8         |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 14            |
|    time_elapsed         | 671           |
|    total_timesteps      | 28672         |
| train/                  |               |
|    approx_kl            | 4.0858984e-05 |
|    clip_fraction        | 0.00356       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0661       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 64.9          |
|    n_updates            | 710           |
|    policy_gradient_loss | -0.000187     |
|    value_loss           | 103           |
-------------------------------------------
Num timesteps: 30000
Best mean reward: -34.77 - Last mean reward per episode: -56.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 117           |
|    ep_rew_mean          | -55.3         |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 15            |
|    time_elapsed         | 718           |
|    total_timesteps      | 30720         |
| train/                  |               |
|    approx_kl            | 0.00025745516 |
|    clip_fraction        | 0.00127       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0585       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 33            |
|    n_updates            | 720           |
|    policy_gradient_loss | -5.17e-05     |
|    value_loss           | 46.6          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 114           |
|    ep_rew_mean          | -53.7         |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 16            |
|    time_elapsed         | 768           |
|    total_timesteps      | 32768         |
| train/                  |               |
|    approx_kl            | 0.00011002127 |
|    clip_fraction        | 0.00566       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0456       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 114           |
|    n_updates            | 730           |
|    policy_gradient_loss | 1.14e-05      |
|    value_loss           | 246           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 117           |
|    ep_rew_mean          | -57.2         |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 17            |
|    time_elapsed         | 820           |
|    total_timesteps      | 34816         |
| train/                  |               |
|    approx_kl            | 0.00076732656 |
|    clip_fraction        | 0.00132       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0544       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 125           |
|    n_updates            | 740           |
|    policy_gradient_loss | -3.54e-05     |
|    value_loss           | 252           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 120           |
|    ep_rew_mean          | -60.2         |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 18            |
|    time_elapsed         | 872           |
|    total_timesteps      | 36864         |
| train/                  |               |
|    approx_kl            | 3.6074867e-05 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0724       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 181           |
|    n_updates            | 750           |
|    policy_gradient_loss | 4.37e-05      |
|    value_loss           | 217           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 94.5          |
|    ep_rew_mean          | -43.3         |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 19            |
|    time_elapsed         | 924           |
|    total_timesteps      | 38912         |
| train/                  |               |
|    approx_kl            | 0.00033309325 |
|    clip_fraction        | 0.00659       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.076        |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 70.5          |
|    n_updates            | 760           |
|    policy_gradient_loss | -0.000264     |
|    value_loss           | 202           |
-------------------------------------------
Num timesteps: 40000
Best mean reward: -34.77 - Last mean reward per episode: -41.94
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 98.2           |
|    ep_rew_mean          | -46.3          |
| time/                   |                |
|    fps                  | 41             |
|    iterations           | 20             |
|    time_elapsed         | 975            |
|    total_timesteps      | 40960          |
| train/                  |                |
|    approx_kl            | 0.000118898344 |
|    clip_fraction        | 0.00483        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0721        |
|    explained_variance   | -1.19e-07      |
|    learning_rate        | 0.0003         |
|    loss                 | 80.7           |
|    n_updates            | 770            |
|    policy_gradient_loss | -0.000393      |
|    value_loss           | 243            |
--------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 99.2          |
|    ep_rew_mean          | -46.6         |
| time/                   |               |
|    fps                  | 41            |
|    iterations           | 21            |
|    time_elapsed         | 1026          |
|    total_timesteps      | 43008         |
| train/                  |               |
|    approx_kl            | 0.00024626794 |
|    clip_fraction        | 0.0042        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0707       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 110           |
|    n_updates            | 780           |
|    policy_gradient_loss | 0.000132      |
|    value_loss           | 209           |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 128         |
|    ep_rew_mean          | -69.1       |
| time/                   |             |
|    fps                  | 41          |
|    iterations           | 22          |
|    time_elapsed         | 1073        |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.001152825 |
|    clip_fraction        | 0.00854     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0932     |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 43.4        |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.000184   |
|    value_loss           | 119         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 124          |
|    ep_rew_mean          | -64.2        |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 23           |
|    time_elapsed         | 1120         |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0005455034 |
|    clip_fraction        | 0.00522      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.103       |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 21.7         |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.000152    |
|    value_loss           | 46.4         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 122          |
|    ep_rew_mean          | -63          |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 24           |
|    time_elapsed         | 1169         |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0019027211 |
|    clip_fraction        | 0.00542      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.121       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 195          |
|    n_updates            | 810          |
|    policy_gradient_loss | -2.89e-05    |
|    value_loss           | 281          |
------------------------------------------
Num timesteps: 50000
Best mean reward: -34.77 - Last mean reward per episode: -60.40
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 121          |
|    ep_rew_mean          | -62.2        |
| time/                   |              |
|    fps                  | 41           |
|    iterations           | 25           |
|    time_elapsed         | 1220         |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0018906409 |
|    clip_fraction        | 0.0115       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.172       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 137          |
|    n_updates            | 820          |
|    policy_gradient_loss | -1.07e-05    |
|    value_loss           | 264          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 119           |
|    ep_rew_mean          | -60.9         |
| time/                   |               |
|    fps                  | 41            |
|    iterations           | 26            |
|    time_elapsed         | 1268          |
|    total_timesteps      | 53248         |
| train/                  |               |
|    approx_kl            | 0.00022964278 |
|    clip_fraction        | 0.00659       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.17         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 107           |
|    n_updates            | 830           |
|    policy_gradient_loss | 0.000137      |
|    value_loss           | 233           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 89.3          |
|    ep_rew_mean          | -38.5         |
| time/                   |               |
|    fps                  | 41            |
|    iterations           | 27            |
|    time_elapsed         | 1317          |
|    total_timesteps      | 55296         |
| train/                  |               |
|    approx_kl            | 0.00091504504 |
|    clip_fraction        | 0.0154        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.16         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 72.2          |
|    n_updates            | 840           |
|    policy_gradient_loss | -0.000524     |
|    value_loss           | 198           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 89.3          |
|    ep_rew_mean          | -38.1         |
| time/                   |               |
|    fps                  | 41            |
|    iterations           | 28            |
|    time_elapsed         | 1366          |
|    total_timesteps      | 57344         |
| train/                  |               |
|    approx_kl            | 8.1552105e-05 |
|    clip_fraction        | 0.00254       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.152        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 130           |
|    n_updates            | 850           |
|    policy_gradient_loss | -0.000244     |
|    value_loss           | 207           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 91.6         |
|    ep_rew_mean          | -39.9        |
| time/                   |              |
|    fps                  | 41           |
|    iterations           | 29           |
|    time_elapsed         | 1416         |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 8.577114e-05 |
|    clip_fraction        | 0.00244      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.145       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 94           |
|    n_updates            | 860          |
|    policy_gradient_loss | 3.38e-05     |
|    value_loss           | 178          |
------------------------------------------
Num timesteps: 60000
Best mean reward: -34.77 - Last mean reward per episode: -47.33
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 97.7          |
|    ep_rew_mean          | -47           |
| time/                   |               |
|    fps                  | 41            |
|    iterations           | 30            |
|    time_elapsed         | 1469          |
|    total_timesteps      | 61440         |
| train/                  |               |
|    approx_kl            | 0.00049163104 |
|    clip_fraction        | 0.00801       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.121        |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 119           |
|    n_updates            | 870           |
|    policy_gradient_loss | -0.000873     |
|    value_loss           | 202           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 96.4         |
|    ep_rew_mean          | -46.9        |
| time/                   |              |
|    fps                  | 41           |
|    iterations           | 31           |
|    time_elapsed         | 1517         |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.0005294734 |
|    clip_fraction        | 0.00454      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.118       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 94.2         |
|    n_updates            | 880          |
|    policy_gradient_loss | -0.000601    |
|    value_loss           | 234          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 99.1          |
|    ep_rew_mean          | -50.3         |
| time/                   |               |
|    fps                  | 41            |
|    iterations           | 32            |
|    time_elapsed         | 1566          |
|    total_timesteps      | 65536         |
| train/                  |               |
|    approx_kl            | 0.00057355233 |
|    clip_fraction        | 0.0108        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0878       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 115           |
|    n_updates            | 890           |
|    policy_gradient_loss | -0.000667     |
|    value_loss           | 201           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 95.7          |
|    ep_rew_mean          | -47.9         |
| time/                   |               |
|    fps                  | 41            |
|    iterations           | 33            |
|    time_elapsed         | 1616          |
|    total_timesteps      | 67584         |
| train/                  |               |
|    approx_kl            | 0.00033518544 |
|    clip_fraction        | 0.0082        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.1          |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 98.4          |
|    n_updates            | 900           |
|    policy_gradient_loss | -0.000784     |
|    value_loss           | 228           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 88.9         |
|    ep_rew_mean          | -39.5        |
| time/                   |              |
|    fps                  | 41           |
|    iterations           | 34           |
|    time_elapsed         | 1665         |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.0009384849 |
|    clip_fraction        | 0.00708      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 93.6         |
|    n_updates            | 910          |
|    policy_gradient_loss | 0.000205     |
|    value_loss           | 211          |
------------------------------------------
Num timesteps: 70000
Best mean reward: -34.77 - Last mean reward per episode: -39.97
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 89.2         |
|    ep_rew_mean          | -38          |
| time/                   |              |
|    fps                  | 35           |
|    iterations           | 35           |
|    time_elapsed         | 2022         |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.0006959019 |
|    clip_fraction        | 0.0136       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.16        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 144          |
|    n_updates            | 920          |
|    policy_gradient_loss | 0.000356     |
|    value_loss           | 191          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 87.7          |
|    ep_rew_mean          | -35.4         |
| time/                   |               |
|    fps                  | 32            |
|    iterations           | 36            |
|    time_elapsed         | 2272          |
|    total_timesteps      | 73728         |
| train/                  |               |
|    approx_kl            | 0.00026201375 |
|    clip_fraction        | 0.00576       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.182        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 87.9          |
|    n_updates            | 930           |
|    policy_gradient_loss | 0.000471      |
|    value_loss           | 170           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 90           |
|    ep_rew_mean          | -35.7        |
| time/                   |              |
|    fps                  | 16           |
|    iterations           | 37           |
|    time_elapsed         | 4657         |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0014837487 |
|    clip_fraction        | 0.00596      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.168       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 94.6         |
|    n_updates            | 940          |
|    policy_gradient_loss | -0.000242    |
|    value_loss           | 163          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 88.9          |
|    ep_rew_mean          | -34.5         |
| time/                   |               |
|    fps                  | 14            |
|    iterations           | 38            |
|    time_elapsed         | 5353          |
|    total_timesteps      | 77824         |
| train/                  |               |
|    approx_kl            | 0.00088413805 |
|    clip_fraction        | 0.00615       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.197        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 63.1          |
|    n_updates            | 950           |
|    policy_gradient_loss | -0.000399     |
|    value_loss           | 160           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 89.6         |
|    ep_rew_mean          | -36          |
| time/                   |              |
|    fps                  | 14           |
|    iterations           | 39           |
|    time_elapsed         | 5411         |
|    total_timesteps      | 79872        |
| train/                  |              |
|    approx_kl            | 0.0014945079 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.218       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 105          |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.000842    |
|    value_loss           | 201          |
------------------------------------------
Num timesteps: 80000
Best mean reward: -34.77 - Last mean reward per episode: -35.33
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 89.9          |
|    ep_rew_mean          | -37.9         |
| time/                   |               |
|    fps                  | 14            |
|    iterations           | 40            |
|    time_elapsed         | 5463          |
|    total_timesteps      | 81920         |
| train/                  |               |
|    approx_kl            | 0.00088631373 |
|    clip_fraction        | 0.0158        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.208        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 164           |
|    n_updates            | 970           |
|    policy_gradient_loss | -0.000741     |
|    value_loss           | 181           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 92.3         |
|    ep_rew_mean          | -39.6        |
| time/                   |              |
|    fps                  | 15           |
|    iterations           | 41           |
|    time_elapsed         | 5513         |
|    total_timesteps      | 83968        |
| train/                  |              |
|    approx_kl            | 0.0015861581 |
|    clip_fraction        | 0.0161       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.208       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 99.9         |
|    n_updates            | 980          |
|    policy_gradient_loss | -0.000764    |
|    value_loss           | 199          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 92.3         |
|    ep_rew_mean          | -40          |
| time/                   |              |
|    fps                  | 15           |
|    iterations           | 42           |
|    time_elapsed         | 5562         |
|    total_timesteps      | 86016        |
| train/                  |              |
|    approx_kl            | 0.0029877338 |
|    clip_fraction        | 0.00962      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.214       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 43.4         |
|    n_updates            | 990          |
|    policy_gradient_loss | 8.83e-05     |
|    value_loss           | 149          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 109           |
|    ep_rew_mean          | -51.4         |
| time/                   |               |
|    fps                  | 15            |
|    iterations           | 43            |
|    time_elapsed         | 5614          |
|    total_timesteps      | 88064         |
| train/                  |               |
|    approx_kl            | 9.1871945e-05 |
|    clip_fraction        | 0.0161        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.223        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 36.1          |
|    n_updates            | 1000          |
|    policy_gradient_loss | -0.000672     |
|    value_loss           | 61            |
-------------------------------------------
Num timesteps: 90000
Best mean reward: -34.77 - Last mean reward per episode: -53.05
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 127          |
|    ep_rew_mean          | -64.4        |
| time/                   |              |
|    fps                  | 15           |
|    iterations           | 44           |
|    time_elapsed         | 5666         |
|    total_timesteps      | 90112        |
| train/                  |              |
|    approx_kl            | 0.0006944125 |
|    clip_fraction        | 0.00933      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.225       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 123          |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.000297    |
|    value_loss           | 231          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 130           |
|    ep_rew_mean          | -66.7         |
| time/                   |               |
|    fps                  | 16            |
|    iterations           | 45            |
|    time_elapsed         | 5717          |
|    total_timesteps      | 92160         |
| train/                  |               |
|    approx_kl            | 0.00016400765 |
|    clip_fraction        | 0.00454       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.187        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 3.01          |
|    n_updates            | 1020          |
|    policy_gradient_loss | 4.76e-05      |
|    value_loss           | 55.4          |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 135            |
|    ep_rew_mean          | -71.4          |
| time/                   |                |
|    fps                  | 16             |
|    iterations           | 46             |
|    time_elapsed         | 5772           |
|    total_timesteps      | 94208          |
| train/                  |                |
|    approx_kl            | 0.000102978374 |
|    clip_fraction        | 0.00596        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.233         |
|    explained_variance   | 0              |
|    learning_rate        | 0.0003         |
|    loss                 | 182            |
|    n_updates            | 1030           |
|    policy_gradient_loss | 0.000569       |
|    value_loss           | 233            |
--------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 131         |
|    ep_rew_mean          | -69.2       |
| time/                   |             |
|    fps                  | 16          |
|    iterations           | 47          |
|    time_elapsed         | 5832        |
|    total_timesteps      | 96256       |
| train/                  |             |
|    approx_kl            | 0.001714648 |
|    clip_fraction        | 0.0174      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.203      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 116         |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.00168    |
|    value_loss           | 259         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 139          |
|    ep_rew_mean          | -77.4        |
| time/                   |              |
|    fps                  | 16           |
|    iterations           | 48           |
|    time_elapsed         | 5892         |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0005055944 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.205       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 95.2         |
|    n_updates            | 1050         |
|    policy_gradient_loss | 0.000605     |
|    value_loss           | 269          |
------------------------------------------
Num timesteps: 100000
Best mean reward: -34.77 - Last mean reward per episode: -64.68
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 123          |
|    ep_rew_mean          | -66.1        |
| time/                   |              |
|    fps                  | 16           |
|    iterations           | 49           |
|    time_elapsed         | 5952         |
|    total_timesteps      | 100352       |
| train/                  |              |
|    approx_kl            | 0.0009683842 |
|    clip_fraction        | 0.00723      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.202       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 120          |
|    n_updates            | 1060         |
|    policy_gradient_loss | -0.000491    |
|    value_loss           | 248          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 103         |
|    ep_rew_mean          | -51         |
| time/                   |             |
|    fps                  | 17          |
|    iterations           | 50          |
|    time_elapsed         | 6017        |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.001572551 |
|    clip_fraction        | 0.0138      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.184      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 105         |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.000139   |
|    value_loss           | 237         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -49.1        |
| time/                   |              |
|    fps                  | 17           |
|    iterations           | 51           |
|    time_elapsed         | 6071         |
|    total_timesteps      | 104448       |
| train/                  |              |
|    approx_kl            | 0.0002277687 |
|    clip_fraction        | 0.00659      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.184       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 119          |
|    n_updates            | 1080         |
|    policy_gradient_loss | 0.000171     |
|    value_loss           | 231          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 98.9          |
|    ep_rew_mean          | -46.6         |
| time/                   |               |
|    fps                  | 17            |
|    iterations           | 52            |
|    time_elapsed         | 6125          |
|    total_timesteps      | 106496        |
| train/                  |               |
|    approx_kl            | 0.00072613335 |
|    clip_fraction        | 0.0192        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.193        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 114           |
|    n_updates            | 1090          |
|    policy_gradient_loss | -0.000809     |
|    value_loss           | 212           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 96.7          |
|    ep_rew_mean          | -44.1         |
| time/                   |               |
|    fps                  | 17            |
|    iterations           | 53            |
|    time_elapsed         | 6179          |
|    total_timesteps      | 108544        |
| train/                  |               |
|    approx_kl            | 0.00069200277 |
|    clip_fraction        | 0.0115        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.166        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 157           |
|    n_updates            | 1100          |
|    policy_gradient_loss | -0.000327     |
|    value_loss           | 245           |
-------------------------------------------
Num timesteps: 110000
Best mean reward: -34.77 - Last mean reward per episode: -44.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 97.4         |
|    ep_rew_mean          | -45          |
| time/                   |              |
|    fps                  | 17           |
|    iterations           | 54           |
|    time_elapsed         | 6235         |
|    total_timesteps      | 110592       |
| train/                  |              |
|    approx_kl            | 0.0004516603 |
|    clip_fraction        | 0.00532      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.169       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 114          |
|    n_updates            | 1110         |
|    policy_gradient_loss | 9.63e-05     |
|    value_loss           | 200          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 102          |
|    ep_rew_mean          | -47.4        |
| time/                   |              |
|    fps                  | 17           |
|    iterations           | 55           |
|    time_elapsed         | 6293         |
|    total_timesteps      | 112640       |
| train/                  |              |
|    approx_kl            | 0.0010381822 |
|    clip_fraction        | 0.0129       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.143       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 97.7         |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.000597    |
|    value_loss           | 219          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -44.9        |
| time/                   |              |
|    fps                  | 18           |
|    iterations           | 56           |
|    time_elapsed         | 6344         |
|    total_timesteps      | 114688       |
| train/                  |              |
|    approx_kl            | 0.0004490334 |
|    clip_fraction        | 0.00791      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.134       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 59.8         |
|    n_updates            | 1130         |
|    policy_gradient_loss | -0.000315    |
|    value_loss           | 142          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 102           |
|    ep_rew_mean          | -46.1         |
| time/                   |               |
|    fps                  | 18            |
|    iterations           | 57            |
|    time_elapsed         | 6396          |
|    total_timesteps      | 116736        |
| train/                  |               |
|    approx_kl            | 0.00043431623 |
|    clip_fraction        | 0.0114        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.13         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 74.6          |
|    n_updates            | 1140          |
|    policy_gradient_loss | -0.000803     |
|    value_loss           | 187           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 102          |
|    ep_rew_mean          | -45.5        |
| time/                   |              |
|    fps                  | 18           |
|    iterations           | 58           |
|    time_elapsed         | 6448         |
|    total_timesteps      | 118784       |
| train/                  |              |
|    approx_kl            | 0.0015499569 |
|    clip_fraction        | 0.00947      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.15        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 83.9         |
|    n_updates            | 1150         |
|    policy_gradient_loss | 8.35e-06     |
|    value_loss           | 206          |
------------------------------------------
Num timesteps: 120000
Best mean reward: -34.77 - Last mean reward per episode: -44.23
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -43.9        |
| time/                   |              |
|    fps                  | 18           |
|    iterations           | 59           |
|    time_elapsed         | 6502         |
|    total_timesteps      | 120832       |
| train/                  |              |
|    approx_kl            | 0.0007292826 |
|    clip_fraction        | 0.028        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.155       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 119          |
|    n_updates            | 1160         |
|    policy_gradient_loss | -0.00207     |
|    value_loss           | 200          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 120          |
|    ep_rew_mean          | -67          |
| time/                   |              |
|    fps                  | 18           |
|    iterations           | 60           |
|    time_elapsed         | 6557         |
|    total_timesteps      | 122880       |
| train/                  |              |
|    approx_kl            | 0.0003926513 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.213       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 124          |
|    n_updates            | 1170         |
|    policy_gradient_loss | -0.000604    |
|    value_loss           | 208          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 116           |
|    ep_rew_mean          | -65.2         |
| time/                   |               |
|    fps                  | 18            |
|    iterations           | 61            |
|    time_elapsed         | 6621          |
|    total_timesteps      | 124928        |
| train/                  |               |
|    approx_kl            | 0.00060909113 |
|    clip_fraction        | 0.0062        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.224        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 43.6          |
|    n_updates            | 1180          |
|    policy_gradient_loss | -1.99e-05     |
|    value_loss           | 90.2          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 116           |
|    ep_rew_mean          | -64.7         |
| time/                   |               |
|    fps                  | 19            |
|    iterations           | 62            |
|    time_elapsed         | 6673          |
|    total_timesteps      | 126976        |
| train/                  |               |
|    approx_kl            | 0.00031303137 |
|    clip_fraction        | 0.00576       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.242        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 102           |
|    n_updates            | 1190          |
|    policy_gradient_loss | 4.42e-05      |
|    value_loss           | 311           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 117           |
|    ep_rew_mean          | -65.7         |
| time/                   |               |
|    fps                  | 19            |
|    iterations           | 63            |
|    time_elapsed         | 6727          |
|    total_timesteps      | 129024        |
| train/                  |               |
|    approx_kl            | 0.00092602574 |
|    clip_fraction        | 0.0149        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.216        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 215           |
|    n_updates            | 1200          |
|    policy_gradient_loss | -0.000979     |
|    value_loss           | 256           |
-------------------------------------------
Num timesteps: 130000
Best mean reward: -34.77 - Last mean reward per episode: -65.08
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 118          |
|    ep_rew_mean          | -68.4        |
| time/                   |              |
|    fps                  | 19           |
|    iterations           | 64           |
|    time_elapsed         | 6780         |
|    total_timesteps      | 131072       |
| train/                  |              |
|    approx_kl            | 5.822355e-05 |
|    clip_fraction        | 0.0123       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.223       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 85.1         |
|    n_updates            | 1210         |
|    policy_gradient_loss | -8.03e-06    |
|    value_loss           | 249          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 99.2         |
|    ep_rew_mean          | -44.8        |
| time/                   |              |
|    fps                  | 19           |
|    iterations           | 65           |
|    time_elapsed         | 6834         |
|    total_timesteps      | 133120       |
| train/                  |              |
|    approx_kl            | 0.0009369594 |
|    clip_fraction        | 0.0126       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.252       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 62.7         |
|    n_updates            | 1220         |
|    policy_gradient_loss | 0.000395     |
|    value_loss           | 243          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 99.3          |
|    ep_rew_mean          | -45           |
| time/                   |               |
|    fps                  | 19            |
|    iterations           | 66            |
|    time_elapsed         | 6886          |
|    total_timesteps      | 135168        |
| train/                  |               |
|    approx_kl            | 0.00090962334 |
|    clip_fraction        | 0.012         |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.243        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 130           |
|    n_updates            | 1230          |
|    policy_gradient_loss | -0.000148     |
|    value_loss           | 204           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 98.1         |
|    ep_rew_mean          | -44.9        |
| time/                   |              |
|    fps                  | 19           |
|    iterations           | 67           |
|    time_elapsed         | 6936         |
|    total_timesteps      | 137216       |
| train/                  |              |
|    approx_kl            | 0.0023211422 |
|    clip_fraction        | 0.00728      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.256       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 94.7         |
|    n_updates            | 1240         |
|    policy_gradient_loss | -3.37e-05    |
|    value_loss           | 198          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 94.6        |
|    ep_rew_mean          | -39.9       |
| time/                   |             |
|    fps                  | 19          |
|    iterations           | 68          |
|    time_elapsed         | 6987        |
|    total_timesteps      | 139264      |
| train/                  |             |
|    approx_kl            | 0.001084266 |
|    clip_fraction        | 0.00776     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.227      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 135         |
|    n_updates            | 1250        |
|    policy_gradient_loss | 0.000502    |
|    value_loss           | 217         |
-----------------------------------------
Num timesteps: 140000
Best mean reward: -34.77 - Last mean reward per episode: -40.38
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 99.3          |
|    ep_rew_mean          | -45           |
| time/                   |               |
|    fps                  | 20            |
|    iterations           | 69            |
|    time_elapsed         | 7039          |
|    total_timesteps      | 141312        |
| train/                  |               |
|    approx_kl            | 0.00022595996 |
|    clip_fraction        | 0.00713       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.236        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 96.1          |
|    n_updates            | 1260          |
|    policy_gradient_loss | -0.000307     |
|    value_loss           | 196           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 99.2         |
|    ep_rew_mean          | -46.6        |
| time/                   |              |
|    fps                  | 20           |
|    iterations           | 70           |
|    time_elapsed         | 7092         |
|    total_timesteps      | 143360       |
| train/                  |              |
|    approx_kl            | 0.0004031811 |
|    clip_fraction        | 0.0061       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.211       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 90.6         |
|    n_updates            | 1270         |
|    policy_gradient_loss | -0.000427    |
|    value_loss           | 175          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 97.2          |
|    ep_rew_mean          | -46.5         |
| time/                   |               |
|    fps                  | 20            |
|    iterations           | 71            |
|    time_elapsed         | 7146          |
|    total_timesteps      | 145408        |
| train/                  |               |
|    approx_kl            | 0.00057055557 |
|    clip_fraction        | 0.0111        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.213        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 93.2          |
|    n_updates            | 1280          |
|    policy_gradient_loss | -0.000769     |
|    value_loss           | 243           |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 96.4        |
|    ep_rew_mean          | -44.6       |
| time/                   |             |
|    fps                  | 20          |
|    iterations           | 72          |
|    time_elapsed         | 7199        |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.002065036 |
|    clip_fraction        | 0.0159      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.196      |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 77.1        |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.000368   |
|    value_loss           | 221         |
-----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 97            |
|    ep_rew_mean          | -45.7         |
| time/                   |               |
|    fps                  | 20            |
|    iterations           | 73            |
|    time_elapsed         | 7251          |
|    total_timesteps      | 149504        |
| train/                  |               |
|    approx_kl            | 0.00035566694 |
|    clip_fraction        | 0.0104        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.172        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 90.9          |
|    n_updates            | 1300          |
|    policy_gradient_loss | -0.000427     |
|    value_loss           | 204           |
-------------------------------------------
Num timesteps: 150000
Best mean reward: -34.77 - Last mean reward per episode: -40.99
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 87.9          |
|    ep_rew_mean          | -36.3         |
| time/                   |               |
|    fps                  | 20            |
|    iterations           | 74            |
|    time_elapsed         | 7301          |
|    total_timesteps      | 151552        |
| train/                  |               |
|    approx_kl            | 0.00077099324 |
|    clip_fraction        | 0.0114        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.177        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 104           |
|    n_updates            | 1310          |
|    policy_gradient_loss | -0.000659     |
|    value_loss           | 191           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 91.3          |
|    ep_rew_mean          | -39.8         |
| time/                   |               |
|    fps                  | 20            |
|    iterations           | 75            |
|    time_elapsed         | 7354          |
|    total_timesteps      | 153600        |
| train/                  |               |
|    approx_kl            | 0.00019934724 |
|    clip_fraction        | 0.0041        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.16         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 102           |
|    n_updates            | 1320          |
|    policy_gradient_loss | -0.000231     |
|    value_loss           | 200           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 91.9          |
|    ep_rew_mean          | -39.6         |
| time/                   |               |
|    fps                  | 21            |
|    iterations           | 76            |
|    time_elapsed         | 7405          |
|    total_timesteps      | 155648        |
| train/                  |               |
|    approx_kl            | 0.00038647192 |
|    clip_fraction        | 0.00571       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.171        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 122           |
|    n_updates            | 1330          |
|    policy_gradient_loss | -4.34e-06     |
|    value_loss           | 193           |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 89.7        |
|    ep_rew_mean          | -38.1       |
| time/                   |             |
|    fps                  | 21          |
|    iterations           | 77          |
|    time_elapsed         | 7456        |
|    total_timesteps      | 157696      |
| train/                  |             |
|    approx_kl            | 0.000348703 |
|    clip_fraction        | 0.00283     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.144      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 111         |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.000168   |
|    value_loss           | 204         |
-----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 91.3          |
|    ep_rew_mean          | -41.3         |
| time/                   |               |
|    fps                  | 21            |
|    iterations           | 78            |
|    time_elapsed         | 7506          |
|    total_timesteps      | 159744        |
| train/                  |               |
|    approx_kl            | 0.00028357795 |
|    clip_fraction        | 0.00391       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.133        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 102           |
|    n_updates            | 1350          |
|    policy_gradient_loss | 0.000134      |
|    value_loss           | 192           |
-------------------------------------------
Num timesteps: 160000
Best mean reward: -34.77 - Last mean reward per episode: -41.03
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 92            |
|    ep_rew_mean          | -42.1         |
| time/                   |               |
|    fps                  | 21            |
|    iterations           | 79            |
|    time_elapsed         | 7557          |
|    total_timesteps      | 161792        |
| train/                  |               |
|    approx_kl            | 0.00040064444 |
|    clip_fraction        | 0.00791       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.134        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 71.5          |
|    n_updates            | 1360          |
|    policy_gradient_loss | 5.04e-05      |
|    value_loss           | 194           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 93.9          |
|    ep_rew_mean          | -42.7         |
| time/                   |               |
|    fps                  | 21            |
|    iterations           | 80            |
|    time_elapsed         | 7610          |
|    total_timesteps      | 163840        |
| train/                  |               |
|    approx_kl            | 0.00034378967 |
|    clip_fraction        | 0.00439       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.151        |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 102           |
|    n_updates            | 1370          |
|    policy_gradient_loss | 0.000165      |
|    value_loss           | 202           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 94.5          |
|    ep_rew_mean          | -41.8         |
| time/                   |               |
|    fps                  | 21            |
|    iterations           | 81            |
|    time_elapsed         | 7667          |
|    total_timesteps      | 165888        |
| train/                  |               |
|    approx_kl            | 0.00043667702 |
|    clip_fraction        | 0.0102        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.15         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 116           |
|    n_updates            | 1380          |
|    policy_gradient_loss | -0.000985     |
|    value_loss           | 164           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 92.7          |
|    ep_rew_mean          | -38.5         |
| time/                   |               |
|    fps                  | 21            |
|    iterations           | 82            |
|    time_elapsed         | 7725          |
|    total_timesteps      | 167936        |
| train/                  |               |
|    approx_kl            | 0.00018154376 |
|    clip_fraction        | 0.00605       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.152        |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 67.2          |
|    n_updates            | 1390          |
|    policy_gradient_loss | -0.000351     |
|    value_loss           | 207           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 94.7         |
|    ep_rew_mean          | -40.2        |
| time/                   |              |
|    fps                  | 21           |
|    iterations           | 83           |
|    time_elapsed         | 7782         |
|    total_timesteps      | 169984       |
| train/                  |              |
|    approx_kl            | 0.0004820293 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.167       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 103          |
|    n_updates            | 1400         |
|    policy_gradient_loss | -0.000326    |
|    value_loss           | 169          |
------------------------------------------
Num timesteps: 170000
Best mean reward: -34.77 - Last mean reward per episode: -39.84
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 93           |
|    ep_rew_mean          | -38.6        |
| time/                   |              |
|    fps                  | 21           |
|    iterations           | 84           |
|    time_elapsed         | 7842         |
|    total_timesteps      | 172032       |
| train/                  |              |
|    approx_kl            | 0.0009451511 |
|    clip_fraction        | 0.00732      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.173       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 81.5         |
|    n_updates            | 1410         |
|    policy_gradient_loss | 0.000252     |
|    value_loss           | 167          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 119           |
|    ep_rew_mean          | -57.9         |
| time/                   |               |
|    fps                  | 22            |
|    iterations           | 85            |
|    time_elapsed         | 7898          |
|    total_timesteps      | 174080        |
| train/                  |               |
|    approx_kl            | 0.00056610885 |
|    clip_fraction        | 0.00713       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.133        |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 31.7          |
|    n_updates            | 1420          |
|    policy_gradient_loss | -0.000483     |
|    value_loss           | 98.6          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 116           |
|    ep_rew_mean          | -56.9         |
| time/                   |               |
|    fps                  | 22            |
|    iterations           | 86            |
|    time_elapsed         | 7946          |
|    total_timesteps      | 176128        |
| train/                  |               |
|    approx_kl            | 0.00030800735 |
|    clip_fraction        | 0.000928      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.12         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 66.6          |
|    n_updates            | 1430          |
|    policy_gradient_loss | 4.29e-06      |
|    value_loss           | 111           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 115          |
|    ep_rew_mean          | -55.3        |
| time/                   |              |
|    fps                  | 22           |
|    iterations           | 87           |
|    time_elapsed         | 7995         |
|    total_timesteps      | 178176       |
| train/                  |              |
|    approx_kl            | 0.0006647282 |
|    clip_fraction        | 0.00786      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.11        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 130          |
|    n_updates            | 1440         |
|    policy_gradient_loss | -0.00048     |
|    value_loss           | 265          |
------------------------------------------
Num timesteps: 180000
Best mean reward: -34.77 - Last mean reward per episode: -55.26
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 143           |
|    ep_rew_mean          | -76.4         |
| time/                   |               |
|    fps                  | 22            |
|    iterations           | 88            |
|    time_elapsed         | 8043          |
|    total_timesteps      | 180224        |
| train/                  |               |
|    approx_kl            | 4.1412393e-05 |
|    clip_fraction        | 0.00688       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0924       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 88.4          |
|    n_updates            | 1450          |
|    policy_gradient_loss | -0.000176     |
|    value_loss           | 166           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 146           |
|    ep_rew_mean          | -79.7         |
| time/                   |               |
|    fps                  | 22            |
|    iterations           | 89            |
|    time_elapsed         | 8093          |
|    total_timesteps      | 182272        |
| train/                  |               |
|    approx_kl            | 0.00047394374 |
|    clip_fraction        | 0.00474       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.109        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 13            |
|    n_updates            | 1460          |
|    policy_gradient_loss | -4.15e-05     |
|    value_loss           | 10.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 144           |
|    ep_rew_mean          | -75.4         |
| time/                   |               |
|    fps                  | 22            |
|    iterations           | 90            |
|    time_elapsed         | 8142          |
|    total_timesteps      | 184320        |
| train/                  |               |
|    approx_kl            | 0.00015140456 |
|    clip_fraction        | 0.00244       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.109        |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 102           |
|    n_updates            | 1470          |
|    policy_gradient_loss | 2.74e-05      |
|    value_loss           | 234           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 121          |
|    ep_rew_mean          | -60.4        |
| time/                   |              |
|    fps                  | 22           |
|    iterations           | 91           |
|    time_elapsed         | 8197         |
|    total_timesteps      | 186368       |
| train/                  |              |
|    approx_kl            | 0.0004348498 |
|    clip_fraction        | 0.00796      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 98.5         |
|    n_updates            | 1480         |
|    policy_gradient_loss | -0.000301    |
|    value_loss           | 268          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 121           |
|    ep_rew_mean          | -60.4         |
| time/                   |               |
|    fps                  | 22            |
|    iterations           | 92            |
|    time_elapsed         | 8246          |
|    total_timesteps      | 188416        |
| train/                  |               |
|    approx_kl            | 0.00042984134 |
|    clip_fraction        | 0.00815       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.103        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 85.2          |
|    n_updates            | 1490          |
|    policy_gradient_loss | -0.000667     |
|    value_loss           | 171           |
-------------------------------------------
Num timesteps: 190000
Best mean reward: -34.77 - Last mean reward per episode: -79.57
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 154          |
|    ep_rew_mean          | -83.3        |
| time/                   |              |
|    fps                  | 22           |
|    iterations           | 93           |
|    time_elapsed         | 8294         |
|    total_timesteps      | 190464       |
| train/                  |              |
|    approx_kl            | 0.0010610215 |
|    clip_fraction        | 0.00771      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.114       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.357        |
|    n_updates            | 1500         |
|    policy_gradient_loss | 0.000235     |
|    value_loss           | 5.31         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 158          |
|    ep_rew_mean          | -87.7        |
| time/                   |              |
|    fps                  | 23           |
|    iterations           | 94           |
|    time_elapsed         | 8342         |
|    total_timesteps      | 192512       |
| train/                  |              |
|    approx_kl            | 9.331957e-05 |
|    clip_fraction        | 0.0189       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.097       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 92.7         |
|    n_updates            | 1510         |
|    policy_gradient_loss | -0.000365    |
|    value_loss           | 243          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 129           |
|    ep_rew_mean          | -67.7         |
| time/                   |               |
|    fps                  | 23            |
|    iterations           | 95            |
|    time_elapsed         | 8396          |
|    total_timesteps      | 194560        |
| train/                  |               |
|    approx_kl            | 0.00022029714 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.109        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 124           |
|    n_updates            | 1520          |
|    policy_gradient_loss | 9.14e-05      |
|    value_loss           | 305           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 127           |
|    ep_rew_mean          | -66.3         |
| time/                   |               |
|    fps                  | 23            |
|    iterations           | 96            |
|    time_elapsed         | 8451          |
|    total_timesteps      | 196608        |
| train/                  |               |
|    approx_kl            | 0.00041496454 |
|    clip_fraction        | 0.0112        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.125        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 165           |
|    n_updates            | 1530          |
|    policy_gradient_loss | 0.000184      |
|    value_loss           | 269           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 155           |
|    ep_rew_mean          | -86.6         |
| time/                   |               |
|    fps                  | 23            |
|    iterations           | 97            |
|    time_elapsed         | 8503          |
|    total_timesteps      | 198656        |
| train/                  |               |
|    approx_kl            | 0.00064684846 |
|    clip_fraction        | 0.00811       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.114        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 35.3          |
|    n_updates            | 1540          |
|    policy_gradient_loss | -0.000421     |
|    value_loss           | 66.9          |
-------------------------------------------
Num timesteps: 200000
Best mean reward: -34.77 - Last mean reward per episode: -94.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 158           |
|    ep_rew_mean          | -88.8         |
| time/                   |               |
|    fps                  | 23            |
|    iterations           | 98            |
|    time_elapsed         | 8557          |
|    total_timesteps      | 200704        |
| train/                  |               |
|    approx_kl            | 7.3690986e-05 |
|    clip_fraction        | 0.00269       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.107        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 64.3          |
|    n_updates            | 1550          |
|    policy_gradient_loss | -5.76e-05     |
|    value_loss           | 101           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 124           |
|    ep_rew_mean          | -64.1         |
| time/                   |               |
|    fps                  | 23            |
|    iterations           | 99            |
|    time_elapsed         | 8610          |
|    total_timesteps      | 202752        |
| train/                  |               |
|    approx_kl            | 0.00038930823 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.103        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 93.2          |
|    n_updates            | 1560          |
|    policy_gradient_loss | -0.000191     |
|    value_loss           | 247           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 127           |
|    ep_rew_mean          | -67           |
| time/                   |               |
|    fps                  | 23            |
|    iterations           | 100           |
|    time_elapsed         | 8667          |
|    total_timesteps      | 204800        |
| train/                  |               |
|    approx_kl            | 2.1610816e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0909       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 186           |
|    n_updates            | 1570          |
|    policy_gradient_loss | -3.34e-06     |
|    value_loss           | 308           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 130          |
|    ep_rew_mean          | -69.6        |
| time/                   |              |
|    fps                  | 23           |
|    iterations           | 101          |
|    time_elapsed         | 8728         |
|    total_timesteps      | 206848       |
| train/                  |              |
|    approx_kl            | 0.0010268809 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0871      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 107          |
|    n_updates            | 1580         |
|    policy_gradient_loss | -0.000625    |
|    value_loss           | 245          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 92.9          |
|    ep_rew_mean          | -41.4         |
| time/                   |               |
|    fps                  | 23            |
|    iterations           | 102           |
|    time_elapsed         | 8785          |
|    total_timesteps      | 208896        |
| train/                  |               |
|    approx_kl            | 0.00013831677 |
|    clip_fraction        | 0.00483       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.096        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 116           |
|    n_updates            | 1590          |
|    policy_gradient_loss | -4.56e-05     |
|    value_loss           | 248           |
-------------------------------------------
Num timesteps: 210000
Best mean reward: -34.77 - Last mean reward per episode: -42.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 94            |
|    ep_rew_mean          | -42.8         |
| time/                   |               |
|    fps                  | 23            |
|    iterations           | 103           |
|    time_elapsed         | 8839          |
|    total_timesteps      | 210944        |
| train/                  |               |
|    approx_kl            | 0.00016915673 |
|    clip_fraction        | 0.00347       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.092        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 141           |
|    n_updates            | 1600          |
|    policy_gradient_loss | 0.000101      |
|    value_loss           | 254           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 93.4          |
|    ep_rew_mean          | -40.6         |
| time/                   |               |
|    fps                  | 23            |
|    iterations           | 104           |
|    time_elapsed         | 8894          |
|    total_timesteps      | 212992        |
| train/                  |               |
|    approx_kl            | 0.00050302723 |
|    clip_fraction        | 0.00254       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.096        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 125           |
|    n_updates            | 1610          |
|    policy_gradient_loss | -3.72e-05     |
|    value_loss           | 213           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 93.4         |
|    ep_rew_mean          | -40.6        |
| time/                   |              |
|    fps                  | 24           |
|    iterations           | 105          |
|    time_elapsed         | 8945         |
|    total_timesteps      | 215040       |
| train/                  |              |
|    approx_kl            | 0.0005414692 |
|    clip_fraction        | 0.00493      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0728      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 111          |
|    n_updates            | 1620         |
|    policy_gradient_loss | -0.000398    |
|    value_loss           | 197          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 112           |
|    ep_rew_mean          | -54.9         |
| time/                   |               |
|    fps                  | 24            |
|    iterations           | 106           |
|    time_elapsed         | 8994          |
|    total_timesteps      | 217088        |
| train/                  |               |
|    approx_kl            | 0.00015126413 |
|    clip_fraction        | 0.0041        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0784       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 0.616         |
|    n_updates            | 1630          |
|    policy_gradient_loss | 7.35e-05      |
|    value_loss           | 13.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 111           |
|    ep_rew_mean          | -53.5         |
| time/                   |               |
|    fps                  | 24            |
|    iterations           | 107           |
|    time_elapsed         | 9043          |
|    total_timesteps      | 219136        |
| train/                  |               |
|    approx_kl            | 0.00029843432 |
|    clip_fraction        | 0.00356       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0651       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 90.3          |
|    n_updates            | 1640          |
|    policy_gradient_loss | -0.000197     |
|    value_loss           | 243           |
-------------------------------------------
Num timesteps: 220000
Best mean reward: -34.77 - Last mean reward per episode: -55.09
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 112          |
|    ep_rew_mean          | -54.2        |
| time/                   |              |
|    fps                  | 24           |
|    iterations           | 108          |
|    time_elapsed         | 9094         |
|    total_timesteps      | 221184       |
| train/                  |              |
|    approx_kl            | 0.0007530548 |
|    clip_fraction        | 0.00225      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0637      |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 115          |
|    n_updates            | 1650         |
|    policy_gradient_loss | 0.000112     |
|    value_loss           | 220          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 114          |
|    ep_rew_mean          | -55.7        |
| time/                   |              |
|    fps                  | 24           |
|    iterations           | 109          |
|    time_elapsed         | 9145         |
|    total_timesteps      | 223232       |
| train/                  |              |
|    approx_kl            | 0.0016236657 |
|    clip_fraction        | 0.00742      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0951      |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 101          |
|    n_updates            | 1660         |
|    policy_gradient_loss | 0.000257     |
|    value_loss           | 227          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 91.6          |
|    ep_rew_mean          | -37.4         |
| time/                   |               |
|    fps                  | 24            |
|    iterations           | 110           |
|    time_elapsed         | 9194          |
|    total_timesteps      | 225280        |
| train/                  |               |
|    approx_kl            | 0.00015350225 |
|    clip_fraction        | 0.00166       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.111        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 92.4          |
|    n_updates            | 1670          |
|    policy_gradient_loss | -6.62e-05     |
|    value_loss           | 210           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 97.6          |
|    ep_rew_mean          | -43.3         |
| time/                   |               |
|    fps                  | 24            |
|    iterations           | 111           |
|    time_elapsed         | 9247          |
|    total_timesteps      | 227328        |
| train/                  |               |
|    approx_kl            | 5.0121656e-05 |
|    clip_fraction        | 0.00889       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.116        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 88.2          |
|    n_updates            | 1680          |
|    policy_gradient_loss | -0.000517     |
|    value_loss           | 177           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 102           |
|    ep_rew_mean          | -46.8         |
| time/                   |               |
|    fps                  | 24            |
|    iterations           | 112           |
|    time_elapsed         | 9302          |
|    total_timesteps      | 229376        |
| train/                  |               |
|    approx_kl            | 0.00035826076 |
|    clip_fraction        | 0.00298       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.104        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 84.7          |
|    n_updates            | 1690          |
|    policy_gradient_loss | -0.000279     |
|    value_loss           | 193           |
-------------------------------------------
Num timesteps: 230000
Best mean reward: -34.77 - Last mean reward per episode: -49.33
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 104           |
|    ep_rew_mean          | -49.3         |
| time/                   |               |
|    fps                  | 24            |
|    iterations           | 113           |
|    time_elapsed         | 9355          |
|    total_timesteps      | 231424        |
| train/                  |               |
|    approx_kl            | 0.00035666462 |
|    clip_fraction        | 0.002         |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0899       |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 89            |
|    n_updates            | 1700          |
|    policy_gradient_loss | 0.000191      |
|    value_loss           | 200           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 104          |
|    ep_rew_mean          | -49.4        |
| time/                   |              |
|    fps                  | 24           |
|    iterations           | 114          |
|    time_elapsed         | 9409         |
|    total_timesteps      | 233472       |
| train/                  |              |
|    approx_kl            | 0.0001452876 |
|    clip_fraction        | 0.0041       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0916      |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 144          |
|    n_updates            | 1710         |
|    policy_gradient_loss | -2.75e-05    |
|    value_loss           | 237          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 93.8         |
|    ep_rew_mean          | -41.7        |
| time/                   |              |
|    fps                  | 24           |
|    iterations           | 115          |
|    time_elapsed         | 9461         |
|    total_timesteps      | 235520       |
| train/                  |              |
|    approx_kl            | 0.0005418435 |
|    clip_fraction        | 0.0085       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.103       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 127          |
|    n_updates            | 1720         |
|    policy_gradient_loss | -0.00123     |
|    value_loss           | 237          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 98.5          |
|    ep_rew_mean          | -48.2         |
| time/                   |               |
|    fps                  | 24            |
|    iterations           | 116           |
|    time_elapsed         | 9512          |
|    total_timesteps      | 237568        |
| train/                  |               |
|    approx_kl            | 0.00012377716 |
|    clip_fraction        | 0.00493       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.118        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 130           |
|    n_updates            | 1730          |
|    policy_gradient_loss | 3.49e-05      |
|    value_loss           | 236           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 110           |
|    ep_rew_mean          | -61.6         |
| time/                   |               |
|    fps                  | 25            |
|    iterations           | 117           |
|    time_elapsed         | 9565          |
|    total_timesteps      | 239616        |
| train/                  |               |
|    approx_kl            | 0.00039616437 |
|    clip_fraction        | 0.00771       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.135        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 81.5          |
|    n_updates            | 1740          |
|    policy_gradient_loss | -6.43e-05     |
|    value_loss           | 238           |
-------------------------------------------
Num timesteps: 240000
Best mean reward: -34.77 - Last mean reward per episode: -63.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.9         |
| time/                   |               |
|    fps                  | 25            |
|    iterations           | 118           |
|    time_elapsed         | 9622          |
|    total_timesteps      | 241664        |
| train/                  |               |
|    approx_kl            | 0.00023970244 |
|    clip_fraction        | 0.00361       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.123        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 91.4          |
|    n_updates            | 1750          |
|    policy_gradient_loss | -0.000204     |
|    value_loss           | 189           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 102          |
|    ep_rew_mean          | -54.8        |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 119          |
|    time_elapsed         | 9675         |
|    total_timesteps      | 243712       |
| train/                  |              |
|    approx_kl            | 0.0012184406 |
|    clip_fraction        | 0.00986      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.164       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 148          |
|    n_updates            | 1760         |
|    policy_gradient_loss | 0.000291     |
|    value_loss           | 296          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 103          |
|    ep_rew_mean          | -55.4        |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 120          |
|    time_elapsed         | 9725         |
|    total_timesteps      | 245760       |
| train/                  |              |
|    approx_kl            | 0.0011109307 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.165       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 91.8         |
|    n_updates            | 1770         |
|    policy_gradient_loss | -9.41e-05    |
|    value_loss           | 228          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 102          |
|    ep_rew_mean          | -52.2        |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 121          |
|    time_elapsed         | 9776         |
|    total_timesteps      | 247808       |
| train/                  |              |
|    approx_kl            | 0.0020119373 |
|    clip_fraction        | 0.00894      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.168       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 116          |
|    n_updates            | 1780         |
|    policy_gradient_loss | -1.32e-05    |
|    value_loss           | 227          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 90.3         |
|    ep_rew_mean          | -38.6        |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 122          |
|    time_elapsed         | 9828         |
|    total_timesteps      | 249856       |
| train/                  |              |
|    approx_kl            | 0.0005149591 |
|    clip_fraction        | 0.00825      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.138       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 169          |
|    n_updates            | 1790         |
|    policy_gradient_loss | -0.000158    |
|    value_loss           | 213          |
------------------------------------------
Num timesteps: 250000
Best mean reward: -34.77 - Last mean reward per episode: -38.47
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 94.7         |
|    ep_rew_mean          | -42.9        |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 123          |
|    time_elapsed         | 9881         |
|    total_timesteps      | 251904       |
| train/                  |              |
|    approx_kl            | 0.0004741696 |
|    clip_fraction        | 0.0042       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.141       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 96.9         |
|    n_updates            | 1800         |
|    policy_gradient_loss | -0.000432    |
|    value_loss           | 217          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 94.9         |
|    ep_rew_mean          | -42.5        |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 124          |
|    time_elapsed         | 9934         |
|    total_timesteps      | 253952       |
| train/                  |              |
|    approx_kl            | 0.0009968348 |
|    clip_fraction        | 0.00977      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.165       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 75.4         |
|    n_updates            | 1810         |
|    policy_gradient_loss | -0.000233    |
|    value_loss           | 187          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 98.1          |
|    ep_rew_mean          | -45.6         |
| time/                   |               |
|    fps                  | 25            |
|    iterations           | 125           |
|    time_elapsed         | 9985          |
|    total_timesteps      | 256000        |
| train/                  |               |
|    approx_kl            | 0.00019612539 |
|    clip_fraction        | 0.00601       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.124        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 142           |
|    n_updates            | 1820          |
|    policy_gradient_loss | -4.41e-05     |
|    value_loss           | 203           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 95.3          |
|    ep_rew_mean          | -41.8         |
| time/                   |               |
|    fps                  | 25            |
|    iterations           | 126           |
|    time_elapsed         | 10037         |
|    total_timesteps      | 258048        |
| train/                  |               |
|    approx_kl            | 0.00075633416 |
|    clip_fraction        | 0.0125        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.166        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 112           |
|    n_updates            | 1830          |
|    policy_gradient_loss | -2.69e-05     |
|    value_loss           | 196           |
-------------------------------------------
Num timesteps: 260000
Best mean reward: -34.77 - Last mean reward per episode: -36.30
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 91.3         |
|    ep_rew_mean          | -36.7        |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 127          |
|    time_elapsed         | 10094        |
|    total_timesteps      | 260096       |
| train/                  |              |
|    approx_kl            | 0.0007045198 |
|    clip_fraction        | 0.00845      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.147       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 110          |
|    n_updates            | 1840         |
|    policy_gradient_loss | -0.000308    |
|    value_loss           | 204          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 89.8          |
|    ep_rew_mean          | -35.5         |
| time/                   |               |
|    fps                  | 25            |
|    iterations           | 128           |
|    time_elapsed         | 10148         |
|    total_timesteps      | 262144        |
| train/                  |               |
|    approx_kl            | 0.00041053028 |
|    clip_fraction        | 0.00449       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.153        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 93.5          |
|    n_updates            | 1850          |
|    policy_gradient_loss | -0.000301     |
|    value_loss           | 171           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 88.2         |
|    ep_rew_mean          | -35.8        |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 129          |
|    time_elapsed         | 10205        |
|    total_timesteps      | 264192       |
| train/                  |              |
|    approx_kl            | 0.0014134352 |
|    clip_fraction        | 0.0197       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.209       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 60.7         |
|    n_updates            | 1860         |
|    policy_gradient_loss | -0.00102     |
|    value_loss           | 188          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 93.7          |
|    ep_rew_mean          | -40.5         |
| time/                   |               |
|    fps                  | 25            |
|    iterations           | 130           |
|    time_elapsed         | 10262         |
|    total_timesteps      | 266240        |
| train/                  |               |
|    approx_kl            | 0.00018962502 |
|    clip_fraction        | 0.0173        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.237        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 85.2          |
|    n_updates            | 1870          |
|    policy_gradient_loss | 0.000627      |
|    value_loss           | 198           |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 90.3        |
|    ep_rew_mean          | -37.5       |
| time/                   |             |
|    fps                  | 26          |
|    iterations           | 131         |
|    time_elapsed         | 10316       |
|    total_timesteps      | 268288      |
| train/                  |             |
|    approx_kl            | 0.000774593 |
|    clip_fraction        | 0.00947     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.22       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 99.4        |
|    n_updates            | 1880        |
|    policy_gradient_loss | 0.000149    |
|    value_loss           | 168         |
-----------------------------------------
Num timesteps: 270000
Best mean reward: -34.77 - Last mean reward per episode: -37.37
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 91.3          |
|    ep_rew_mean          | -38.4         |
| time/                   |               |
|    fps                  | 26            |
|    iterations           | 132           |
|    time_elapsed         | 10368         |
|    total_timesteps      | 270336        |
| train/                  |               |
|    approx_kl            | 0.00014825008 |
|    clip_fraction        | 0.00532       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.228        |
|    explained_variance   | 1.79e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 99.3          |
|    n_updates            | 1890          |
|    policy_gradient_loss | 0.000218      |
|    value_loss           | 195           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 92.3          |
|    ep_rew_mean          | -40.3         |
| time/                   |               |
|    fps                  | 26            |
|    iterations           | 133           |
|    time_elapsed         | 10419         |
|    total_timesteps      | 272384        |
| train/                  |               |
|    approx_kl            | 0.00097244163 |
|    clip_fraction        | 0.00937       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.215        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 142           |
|    n_updates            | 1900          |
|    policy_gradient_loss | 0.000306      |
|    value_loss           | 187           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 99.2         |
|    ep_rew_mean          | -47          |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 134          |
|    time_elapsed         | 10473        |
|    total_timesteps      | 274432       |
| train/                  |              |
|    approx_kl            | 0.0006753098 |
|    clip_fraction        | 0.00967      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.224       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 97.9         |
|    n_updates            | 1910         |
|    policy_gradient_loss | -0.00102     |
|    value_loss           | 207          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 94.3         |
|    ep_rew_mean          | -42.7        |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 135          |
|    time_elapsed         | 10524        |
|    total_timesteps      | 276480       |
| train/                  |              |
|    approx_kl            | 0.0004977984 |
|    clip_fraction        | 0.0101       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.245       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 89.7         |
|    n_updates            | 1920         |
|    policy_gradient_loss | -0.000353    |
|    value_loss           | 181          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -49.4         |
| time/                   |               |
|    fps                  | 26            |
|    iterations           | 136           |
|    time_elapsed         | 10577         |
|    total_timesteps      | 278528        |
| train/                  |               |
|    approx_kl            | 0.00088839605 |
|    clip_fraction        | 0.00864       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.256        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 96.9          |
|    n_updates            | 1930          |
|    policy_gradient_loss | -0.000237     |
|    value_loss           | 214           |
-------------------------------------------
Num timesteps: 280000
Best mean reward: -34.77 - Last mean reward per episode: -48.97
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 99.2          |
|    ep_rew_mean          | -47.5         |
| time/                   |               |
|    fps                  | 26            |
|    iterations           | 137           |
|    time_elapsed         | 10633         |
|    total_timesteps      | 280576        |
| train/                  |               |
|    approx_kl            | 0.00086502195 |
|    clip_fraction        | 0.0041        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.261        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 115           |
|    n_updates            | 1940          |
|    policy_gradient_loss | 4.47e-05      |
|    value_loss           | 200           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 94.8         |
|    ep_rew_mean          | -41          |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 138          |
|    time_elapsed         | 10686        |
|    total_timesteps      | 282624       |
| train/                  |              |
|    approx_kl            | 0.0015859337 |
|    clip_fraction        | 0.0347       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.217       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 115          |
|    n_updates            | 1950         |
|    policy_gradient_loss | -0.00213     |
|    value_loss           | 190          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 96.2         |
|    ep_rew_mean          | -42.7        |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 139          |
|    time_elapsed         | 10740        |
|    total_timesteps      | 284672       |
| train/                  |              |
|    approx_kl            | 0.0005607472 |
|    clip_fraction        | 0.00986      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.213       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 66.1         |
|    n_updates            | 1960         |
|    policy_gradient_loss | -0.000663    |
|    value_loss           | 186          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 99.3          |
|    ep_rew_mean          | -44.3         |
| time/                   |               |
|    fps                  | 26            |
|    iterations           | 140           |
|    time_elapsed         | 10796         |
|    total_timesteps      | 286720        |
| train/                  |               |
|    approx_kl            | 0.00029200807 |
|    clip_fraction        | 0.00518       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.202        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 67.2          |
|    n_updates            | 1970          |
|    policy_gradient_loss | 0.000197      |
|    value_loss           | 180           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 94.7          |
|    ep_rew_mean          | -39.7         |
| time/                   |               |
|    fps                  | 26            |
|    iterations           | 141           |
|    time_elapsed         | 10849         |
|    total_timesteps      | 288768        |
| train/                  |               |
|    approx_kl            | 1.6106467e-05 |
|    clip_fraction        | 0.0122        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.23         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 77.4          |
|    n_updates            | 1980          |
|    policy_gradient_loss | -0.000343     |
|    value_loss           | 180           |
-------------------------------------------
Num timesteps: 290000
Best mean reward: -34.77 - Last mean reward per episode: -40.71
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 96.9         |
|    ep_rew_mean          | -42.6        |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 142          |
|    time_elapsed         | 10903        |
|    total_timesteps      | 290816       |
| train/                  |              |
|    approx_kl            | 0.0004828775 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.203       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 90           |
|    n_updates            | 1990         |
|    policy_gradient_loss | -0.000843    |
|    value_loss           | 207          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 95.8         |
|    ep_rew_mean          | -42.3        |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 143          |
|    time_elapsed         | 10952        |
|    total_timesteps      | 292864       |
| train/                  |              |
|    approx_kl            | 0.0005406769 |
|    clip_fraction        | 0.0064       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.168       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 113          |
|    n_updates            | 2000         |
|    policy_gradient_loss | -0.000443    |
|    value_loss           | 183          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 94.2         |
|    ep_rew_mean          | -41.5        |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 144          |
|    time_elapsed         | 11004        |
|    total_timesteps      | 294912       |
| train/                  |              |
|    approx_kl            | 0.0002794851 |
|    clip_fraction        | 0.00718      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.166       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 110          |
|    n_updates            | 2010         |
|    policy_gradient_loss | 0.000232     |
|    value_loss           | 203          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 95.8         |
|    ep_rew_mean          | -43.3        |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 145          |
|    time_elapsed         | 11057        |
|    total_timesteps      | 296960       |
| train/                  |              |
|    approx_kl            | 0.0005199126 |
|    clip_fraction        | 0.00913      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.13        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 117          |
|    n_updates            | 2020         |
|    policy_gradient_loss | -0.000232    |
|    value_loss           | 218          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 93.2         |
|    ep_rew_mean          | -39.6        |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 146          |
|    time_elapsed         | 11108        |
|    total_timesteps      | 299008       |
| train/                  |              |
|    approx_kl            | 0.0003713621 |
|    clip_fraction        | 0.00342      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.125       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 66.4         |
|    n_updates            | 2030         |
|    policy_gradient_loss | -0.000158    |
|    value_loss           | 184          |
------------------------------------------
Num timesteps: 300000
Best mean reward: -34.77 - Last mean reward per episode: -38.73
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 102          |
|    ep_rew_mean          | -45.9        |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 147          |
|    time_elapsed         | 11165        |
|    total_timesteps      | 301056       |
| train/                  |              |
|    approx_kl            | 0.0010757938 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.107       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 127          |
|    n_updates            | 2040         |
|    policy_gradient_loss | -0.0004      |
|    value_loss           | 189          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 93.7        |
|    ep_rew_mean          | -38.5       |
| time/                   |             |
|    fps                  | 27          |
|    iterations           | 148         |
|    time_elapsed         | 11218       |
|    total_timesteps      | 303104      |
| train/                  |             |
|    approx_kl            | 0.000489044 |
|    clip_fraction        | 0.0117      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.097      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 43.5        |
|    n_updates            | 2050        |
|    policy_gradient_loss | -0.000625   |
|    value_loss           | 121         |
-----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 95.2          |
|    ep_rew_mean          | -38.7         |
| time/                   |               |
|    fps                  | 27            |
|    iterations           | 149           |
|    time_elapsed         | 11272         |
|    total_timesteps      | 305152        |
| train/                  |               |
|    approx_kl            | 0.00037945525 |
|    clip_fraction        | 0.00415       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.113        |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 125           |
|    n_updates            | 2060          |
|    policy_gradient_loss | -0.000158     |
|    value_loss           | 204           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 95.4          |
|    ep_rew_mean          | -39.5         |
| time/                   |               |
|    fps                  | 27            |
|    iterations           | 150           |
|    time_elapsed         | 11327         |
|    total_timesteps      | 307200        |
| train/                  |               |
|    approx_kl            | 0.00029257778 |
|    clip_fraction        | 0.0064        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0934       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 77            |
|    n_updates            | 2070          |
|    policy_gradient_loss | -0.000301     |
|    value_loss           | 168           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 84.3          |
|    ep_rew_mean          | -31.2         |
| time/                   |               |
|    fps                  | 27            |
|    iterations           | 151           |
|    time_elapsed         | 11386         |
|    total_timesteps      | 309248        |
| train/                  |               |
|    approx_kl            | 0.00019329591 |
|    clip_fraction        | 0.00195       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0811       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 96.3          |
|    n_updates            | 2080          |
|    policy_gradient_loss | -6.75e-05     |
|    value_loss           | 185           |
-------------------------------------------
Num timesteps: 310000
Best mean reward: -34.77 - Last mean reward per episode: -32.47
Saving new best model at 309943 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 88.1          |
|    ep_rew_mean          | -35.7         |
| time/                   |               |
|    fps                  | 27            |
|    iterations           | 152           |
|    time_elapsed         | 11446         |
|    total_timesteps      | 311296        |
| train/                  |               |
|    approx_kl            | 3.5266887e-05 |
|    clip_fraction        | 0.00244       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0833       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 56.5          |
|    n_updates            | 2090          |
|    policy_gradient_loss | 0.000152      |
|    value_loss           | 169           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 89.7          |
|    ep_rew_mean          | -37.6         |
| time/                   |               |
|    fps                  | 27            |
|    iterations           | 153           |
|    time_elapsed         | 11503         |
|    total_timesteps      | 313344        |
| train/                  |               |
|    approx_kl            | 0.00019950556 |
|    clip_fraction        | 0.00303       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0781       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 85.9          |
|    n_updates            | 2100          |
|    policy_gradient_loss | -3.1e-05      |
|    value_loss           | 203           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 88           |
|    ep_rew_mean          | -37          |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 154          |
|    time_elapsed         | 11565        |
|    total_timesteps      | 315392       |
| train/                  |              |
|    approx_kl            | 0.0004903648 |
|    clip_fraction        | 0.00591      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0792      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 103          |
|    n_updates            | 2110         |
|    policy_gradient_loss | -0.000203    |
|    value_loss           | 187          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 87.6         |
|    ep_rew_mean          | -38.4        |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 155          |
|    time_elapsed         | 11617        |
|    total_timesteps      | 317440       |
| train/                  |              |
|    approx_kl            | 0.0002471783 |
|    clip_fraction        | 0.00757      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0688      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 146          |
|    n_updates            | 2120         |
|    policy_gradient_loss | -0.00038     |
|    value_loss           | 220          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 89.4         |
|    ep_rew_mean          | -40.2        |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 156          |
|    time_elapsed         | 11666        |
|    total_timesteps      | 319488       |
| train/                  |              |
|    approx_kl            | 0.0016038219 |
|    clip_fraction        | 0.0041       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0814      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 98           |
|    n_updates            | 2130         |
|    policy_gradient_loss | 4.73e-05     |
|    value_loss           | 183          |
------------------------------------------
Num timesteps: 320000
Best mean reward: -32.47 - Last mean reward per episode: -37.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 85            |
|    ep_rew_mean          | -35           |
| time/                   |               |
|    fps                  | 27            |
|    iterations           | 157           |
|    time_elapsed         | 11717         |
|    total_timesteps      | 321536        |
| train/                  |               |
|    approx_kl            | 0.00042265115 |
|    clip_fraction        | 0.00625       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.111        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 114           |
|    n_updates            | 2140          |
|    policy_gradient_loss | -0.000467     |
|    value_loss           | 195           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 85           |
|    ep_rew_mean          | -34.8        |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 158          |
|    time_elapsed         | 11769        |
|    total_timesteps      | 323584       |
| train/                  |              |
|    approx_kl            | 0.0010939741 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.136       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 111          |
|    n_updates            | 2150         |
|    policy_gradient_loss | -0.000141    |
|    value_loss           | 189          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 98.6         |
|    ep_rew_mean          | -43.7        |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 159          |
|    time_elapsed         | 11818        |
|    total_timesteps      | 325632       |
| train/                  |              |
|    approx_kl            | 0.0013540187 |
|    clip_fraction        | 0.0117       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.173       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 30.9         |
|    n_updates            | 2160         |
|    policy_gradient_loss | -0.000922    |
|    value_loss           | 137          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 99.8         |
|    ep_rew_mean          | -44.5        |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 160          |
|    time_elapsed         | 11867        |
|    total_timesteps      | 327680       |
| train/                  |              |
|    approx_kl            | 0.0008192194 |
|    clip_fraction        | 0.0115       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.136       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 93.3         |
|    n_updates            | 2170         |
|    policy_gradient_loss | -0.000527    |
|    value_loss           | 161          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 97            |
|    ep_rew_mean          | -41.9         |
| time/                   |               |
|    fps                  | 27            |
|    iterations           | 161           |
|    time_elapsed         | 11917         |
|    total_timesteps      | 329728        |
| train/                  |               |
|    approx_kl            | 4.1926076e-05 |
|    clip_fraction        | 0.00376       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.14         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 133           |
|    n_updates            | 2180          |
|    policy_gradient_loss | 0.000154      |
|    value_loss           | 228           |
-------------------------------------------
Num timesteps: 330000
Best mean reward: -32.47 - Last mean reward per episode: -42.26
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 102           |
|    ep_rew_mean          | -46.5         |
| time/                   |               |
|    fps                  | 27            |
|    iterations           | 162           |
|    time_elapsed         | 11968         |
|    total_timesteps      | 331776        |
| train/                  |               |
|    approx_kl            | 0.00040758806 |
|    clip_fraction        | 0.00786       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.119        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 75.3          |
|    n_updates            | 2190          |
|    policy_gradient_loss | -0.000474     |
|    value_loss           | 175           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 106          |
|    ep_rew_mean          | -49.5        |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 163          |
|    time_elapsed         | 12019        |
|    total_timesteps      | 333824       |
| train/                  |              |
|    approx_kl            | 0.0005812865 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.149       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 64.1         |
|    n_updates            | 2200         |
|    policy_gradient_loss | -0.000251    |
|    value_loss           | 178          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 134           |
|    ep_rew_mean          | -69.1         |
| time/                   |               |
|    fps                  | 27            |
|    iterations           | 164           |
|    time_elapsed         | 12070         |
|    total_timesteps      | 335872        |
| train/                  |               |
|    approx_kl            | 0.00027518944 |
|    clip_fraction        | 0.00874       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.168        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 18.4          |
|    n_updates            | 2210          |
|    policy_gradient_loss | 0.000155      |
|    value_loss           | 70.5          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 123          |
|    ep_rew_mean          | -62          |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 165          |
|    time_elapsed         | 12121        |
|    total_timesteps      | 337920       |
| train/                  |              |
|    approx_kl            | 0.0022034333 |
|    clip_fraction        | 0.00747      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.178       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 26.5         |
|    n_updates            | 2220         |
|    policy_gradient_loss | -0.000298    |
|    value_loss           | 90.1         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 130          |
|    ep_rew_mean          | -66.7        |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 166          |
|    time_elapsed         | 12176        |
|    total_timesteps      | 339968       |
| train/                  |              |
|    approx_kl            | 0.0012356377 |
|    clip_fraction        | 0.0102       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.203       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 208          |
|    n_updates            | 2230         |
|    policy_gradient_loss | -0.000132    |
|    value_loss           | 207          |
------------------------------------------
Num timesteps: 340000
Best mean reward: -32.47 - Last mean reward per episode: -67.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 134          |
|    ep_rew_mean          | -69.7        |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 167          |
|    time_elapsed         | 12230        |
|    total_timesteps      | 342016       |
| train/                  |              |
|    approx_kl            | 0.0001825112 |
|    clip_fraction        | 0.00654      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.21        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 88.7         |
|    n_updates            | 2240         |
|    policy_gradient_loss | -8.4e-06     |
|    value_loss           | 245          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 134          |
|    ep_rew_mean          | -69.7        |
| time/                   |              |
|    fps                  | 28           |
|    iterations           | 168          |
|    time_elapsed         | 12285        |
|    total_timesteps      | 344064       |
| train/                  |              |
|    approx_kl            | 0.0020153655 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.248       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 95.8         |
|    n_updates            | 2250         |
|    policy_gradient_loss | -0.000155    |
|    value_loss           | 230          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 105          |
|    ep_rew_mean          | -48.2        |
| time/                   |              |
|    fps                  | 28           |
|    iterations           | 169          |
|    time_elapsed         | 12338        |
|    total_timesteps      | 346112       |
| train/                  |              |
|    approx_kl            | 0.0006772696 |
|    clip_fraction        | 0.00522      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.288       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 77.7         |
|    n_updates            | 2260         |
|    policy_gradient_loss | -0.000556    |
|    value_loss           | 214          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 106          |
|    ep_rew_mean          | -50.1        |
| time/                   |              |
|    fps                  | 28           |
|    iterations           | 170          |
|    time_elapsed         | 12392        |
|    total_timesteps      | 348160       |
| train/                  |              |
|    approx_kl            | 0.0017155324 |
|    clip_fraction        | 0.0063       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.293       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 65.5         |
|    n_updates            | 2270         |
|    policy_gradient_loss | -0.000218    |
|    value_loss           | 202          |
------------------------------------------
Num timesteps: 350000
Best mean reward: -32.47 - Last mean reward per episode: -47.73
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 102          |
|    ep_rew_mean          | -47.7        |
| time/                   |              |
|    fps                  | 28           |
|    iterations           | 171          |
|    time_elapsed         | 12447        |
|    total_timesteps      | 350208       |
| train/                  |              |
|    approx_kl            | 0.0009704934 |
|    clip_fraction        | 0.0136       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.286       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 78.8         |
|    n_updates            | 2280         |
|    policy_gradient_loss | -0.000146    |
|    value_loss           | 215          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 108          |
|    ep_rew_mean          | -53.2        |
| time/                   |              |
|    fps                  | 28           |
|    iterations           | 172          |
|    time_elapsed         | 12498        |
|    total_timesteps      | 352256       |
| train/                  |              |
|    approx_kl            | 0.0006001005 |
|    clip_fraction        | 0.0151       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.276       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 82.8         |
|    n_updates            | 2290         |
|    policy_gradient_loss | 0.000738     |
|    value_loss           | 176          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 108          |
|    ep_rew_mean          | -52.2        |
| time/                   |              |
|    fps                  | 28           |
|    iterations           | 173          |
|    time_elapsed         | 12553        |
|    total_timesteps      | 354304       |
| train/                  |              |
|    approx_kl            | 0.0013524189 |
|    clip_fraction        | 0.0187       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.294       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 86.5         |
|    n_updates            | 2300         |
|    policy_gradient_loss | -0.00044     |
|    value_loss           | 227          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 108           |
|    ep_rew_mean          | -53.5         |
| time/                   |               |
|    fps                  | 28            |
|    iterations           | 174           |
|    time_elapsed         | 12604         |
|    total_timesteps      | 356352        |
| train/                  |               |
|    approx_kl            | 0.00070997264 |
|    clip_fraction        | 0.0164        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.321        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 107           |
|    n_updates            | 2310          |
|    policy_gradient_loss | -0.000413     |
|    value_loss           | 228           |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 104         |
|    ep_rew_mean          | -49.5       |
| time/                   |             |
|    fps                  | 28          |
|    iterations           | 175         |
|    time_elapsed         | 12656       |
|    total_timesteps      | 358400      |
| train/                  |             |
|    approx_kl            | 0.000827673 |
|    clip_fraction        | 0.00327     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.308      |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 77          |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.000116   |
|    value_loss           | 209         |
-----------------------------------------
Num timesteps: 360000
Best mean reward: -32.47 - Last mean reward per episode: -44.72
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 96.3         |
|    ep_rew_mean          | -43.2        |
| time/                   |              |
|    fps                  | 28           |
|    iterations           | 176          |
|    time_elapsed         | 12709        |
|    total_timesteps      | 360448       |
| train/                  |              |
|    approx_kl            | 0.0033365553 |
|    clip_fraction        | 0.0131       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.297       |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 111          |
|    n_updates            | 2330         |
|    policy_gradient_loss | 2.51e-05     |
|    value_loss           | 219          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 93.5         |
|    ep_rew_mean          | -39.4        |
| time/                   |              |
|    fps                  | 28           |
|    iterations           | 177          |
|    time_elapsed         | 12761        |
|    total_timesteps      | 362496       |
| train/                  |              |
|    approx_kl            | 0.0017756561 |
|    clip_fraction        | 0.0229       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.253       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 91           |
|    n_updates            | 2340         |
|    policy_gradient_loss | -0.00156     |
|    value_loss           | 204          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 124           |
|    ep_rew_mean          | -61.5         |
| time/                   |               |
|    fps                  | 28            |
|    iterations           | 178           |
|    time_elapsed         | 12808         |
|    total_timesteps      | 364544        |
| train/                  |               |
|    approx_kl            | 0.00033217043 |
|    clip_fraction        | 0.0111        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.262        |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 17.6          |
|    n_updates            | 2350          |
|    policy_gradient_loss | -0.000281     |
|    value_loss           | 35.5          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 123           |
|    ep_rew_mean          | -61.7         |
| time/                   |               |
|    fps                  | 28            |
|    iterations           | 179           |
|    time_elapsed         | 12856         |
|    total_timesteps      | 366592        |
| train/                  |               |
|    approx_kl            | 0.00019794132 |
|    clip_fraction        | 0.00356       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.276        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 108           |
|    n_updates            | 2360          |
|    policy_gradient_loss | -0.000303     |
|    value_loss           | 121           |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 125         |
|    ep_rew_mean          | -63.5       |
| time/                   |             |
|    fps                  | 28          |
|    iterations           | 180         |
|    time_elapsed         | 12912       |
|    total_timesteps      | 368640      |
| train/                  |             |
|    approx_kl            | 0.002141417 |
|    clip_fraction        | 0.0211      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.228      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 137         |
|    n_updates            | 2370        |
|    policy_gradient_loss | -0.000373   |
|    value_loss           | 274         |
-----------------------------------------
Num timesteps: 370000
Best mean reward: -32.47 - Last mean reward per episode: -62.18
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 125        |
|    ep_rew_mean          | -63.5      |
| time/                   |            |
|    fps                  | 28         |
|    iterations           | 181        |
|    time_elapsed         | 12961      |
|    total_timesteps      | 370688     |
| train/                  |            |
|    approx_kl            | 0.00052824 |
|    clip_fraction        | 0.0106     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.243     |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 126        |
|    n_updates            | 2380       |
|    policy_gradient_loss | -0.000198  |
|    value_loss           | 244        |
----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 123           |
|    ep_rew_mean          | -61.4         |
| time/                   |               |
|    fps                  | 28            |
|    iterations           | 182           |
|    time_elapsed         | 13011         |
|    total_timesteps      | 372736        |
| train/                  |               |
|    approx_kl            | 0.00029776918 |
|    clip_fraction        | 0.00986       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.259        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 157           |
|    n_updates            | 2390          |
|    policy_gradient_loss | 0.000305      |
|    value_loss           | 225           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 95.7         |
|    ep_rew_mean          | -43          |
| time/                   |              |
|    fps                  | 28           |
|    iterations           | 183          |
|    time_elapsed         | 13060        |
|    total_timesteps      | 374784       |
| train/                  |              |
|    approx_kl            | 0.0014945806 |
|    clip_fraction        | 0.019        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.236       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 92.1         |
|    n_updates            | 2400         |
|    policy_gradient_loss | -0.00147     |
|    value_loss           | 198          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 92.4         |
|    ep_rew_mean          | -38.2        |
| time/                   |              |
|    fps                  | 28           |
|    iterations           | 184          |
|    time_elapsed         | 13111        |
|    total_timesteps      | 376832       |
| train/                  |              |
|    approx_kl            | 0.0005309181 |
|    clip_fraction        | 0.0164       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.261       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 129          |
|    n_updates            | 2410         |
|    policy_gradient_loss | -0.000152    |
|    value_loss           | 206          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 92.7          |
|    ep_rew_mean          | -38.9         |
| time/                   |               |
|    fps                  | 28            |
|    iterations           | 185           |
|    time_elapsed         | 13163         |
|    total_timesteps      | 378880        |
| train/                  |               |
|    approx_kl            | 0.00038941792 |
|    clip_fraction        | 0.0169        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.23         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 133           |
|    n_updates            | 2420          |
|    policy_gradient_loss | -0.0011       |
|    value_loss           | 199           |
-------------------------------------------
Num timesteps: 380000
Best mean reward: -32.47 - Last mean reward per episode: -39.68
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 95.4        |
|    ep_rew_mean          | -42.5       |
| time/                   |             |
|    fps                  | 28          |
|    iterations           | 186         |
|    time_elapsed         | 13217       |
|    total_timesteps      | 380928      |
| train/                  |             |
|    approx_kl            | 0.001659437 |
|    clip_fraction        | 0.018       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.27       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 148         |
|    n_updates            | 2430        |
|    policy_gradient_loss | -0.00144    |
|    value_loss           | 208         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 97.5         |
|    ep_rew_mean          | -43.1        |
| time/                   |              |
|    fps                  | 28           |
|    iterations           | 187          |
|    time_elapsed         | 13271        |
|    total_timesteps      | 382976       |
| train/                  |              |
|    approx_kl            | 9.929002e-05 |
|    clip_fraction        | 0.0042       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.275       |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 118          |
|    n_updates            | 2440         |
|    policy_gradient_loss | -0.000308    |
|    value_loss           | 192          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -48.7        |
| time/                   |              |
|    fps                  | 28           |
|    iterations           | 188          |
|    time_elapsed         | 13325        |
|    total_timesteps      | 385024       |
| train/                  |              |
|    approx_kl            | 0.0010015966 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.267       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 135          |
|    n_updates            | 2450         |
|    policy_gradient_loss | -0.000374    |
|    value_loss           | 200          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 99.6         |
|    ep_rew_mean          | -47.5        |
| time/                   |              |
|    fps                  | 28           |
|    iterations           | 189          |
|    time_elapsed         | 13381        |
|    total_timesteps      | 387072       |
| train/                  |              |
|    approx_kl            | 0.0020058607 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.251       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 88.4         |
|    n_updates            | 2460         |
|    policy_gradient_loss | -0.000839    |
|    value_loss           | 226          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 96.9          |
|    ep_rew_mean          | -46.2         |
| time/                   |               |
|    fps                  | 28            |
|    iterations           | 190           |
|    time_elapsed         | 13433         |
|    total_timesteps      | 389120        |
| train/                  |               |
|    approx_kl            | 5.5191398e-05 |
|    clip_fraction        | 0.000732      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.235        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 97.5          |
|    n_updates            | 2470          |
|    policy_gradient_loss | 0.000281      |
|    value_loss           | 221           |
-------------------------------------------
Num timesteps: 390000
Best mean reward: -32.47 - Last mean reward per episode: -49.30
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -48.4         |
| time/                   |               |
|    fps                  | 28            |
|    iterations           | 191           |
|    time_elapsed         | 13492         |
|    total_timesteps      | 391168        |
| train/                  |               |
|    approx_kl            | 0.00011832823 |
|    clip_fraction        | 0.00605       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.228        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 93.5          |
|    n_updates            | 2480          |
|    policy_gradient_loss | 0.000115      |
|    value_loss           | 237           |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 100         |
|    ep_rew_mean          | -49.1       |
| time/                   |             |
|    fps                  | 29          |
|    iterations           | 192         |
|    time_elapsed         | 13544       |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 9.27815e-05 |
|    clip_fraction        | 0.000781    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.212      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 75.2        |
|    n_updates            | 2490        |
|    policy_gradient_loss | 0.000178    |
|    value_loss           | 183         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 92.9         |
|    ep_rew_mean          | -40.9        |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 193          |
|    time_elapsed         | 13595        |
|    total_timesteps      | 395264       |
| train/                  |              |
|    approx_kl            | 0.0002182003 |
|    clip_fraction        | 0.00645      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.233       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 129          |
|    n_updates            | 2500         |
|    policy_gradient_loss | 0.000411     |
|    value_loss           | 212          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 91.9          |
|    ep_rew_mean          | -40.8         |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 194           |
|    time_elapsed         | 13645         |
|    total_timesteps      | 397312        |
| train/                  |               |
|    approx_kl            | 0.00039040862 |
|    clip_fraction        | 0.00181       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.215        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 109           |
|    n_updates            | 2510          |
|    policy_gradient_loss | -3.52e-05     |
|    value_loss           | 231           |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 87.8        |
|    ep_rew_mean          | -36.8       |
| time/                   |             |
|    fps                  | 29          |
|    iterations           | 195         |
|    time_elapsed         | 13699       |
|    total_timesteps      | 399360      |
| train/                  |             |
|    approx_kl            | 0.001093297 |
|    clip_fraction        | 0.0196      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.249      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 100         |
|    n_updates            | 2520        |
|    policy_gradient_loss | -0.00202    |
|    value_loss           | 209         |
-----------------------------------------
Num timesteps: 400000
Best mean reward: -32.47 - Last mean reward per episode: -35.30
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 85.9         |
|    ep_rew_mean          | -34.7        |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 196          |
|    time_elapsed         | 13750        |
|    total_timesteps      | 401408       |
| train/                  |              |
|    approx_kl            | 0.0009732794 |
|    clip_fraction        | 0.0064       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.23        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 73.1         |
|    n_updates            | 2530         |
|    policy_gradient_loss | 0.000214     |
|    value_loss           | 182          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 87            |
|    ep_rew_mean          | -36.2         |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 197           |
|    time_elapsed         | 13803         |
|    total_timesteps      | 403456        |
| train/                  |               |
|    approx_kl            | 0.00096396485 |
|    clip_fraction        | 0.00874       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.23         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 105           |
|    n_updates            | 2540          |
|    policy_gradient_loss | 0.00037       |
|    value_loss           | 201           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 98.2         |
|    ep_rew_mean          | -49.1        |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 198          |
|    time_elapsed         | 13856        |
|    total_timesteps      | 405504       |
| train/                  |              |
|    approx_kl            | 0.0009752433 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.242       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 97.8         |
|    n_updates            | 2550         |
|    policy_gradient_loss | 6.27e-05     |
|    value_loss           | 207          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 95.6          |
|    ep_rew_mean          | -45.5         |
| time/                   |               |
|    fps                  | 28            |
|    iterations           | 199           |
|    time_elapsed         | 14100         |
|    total_timesteps      | 407552        |
| train/                  |               |
|    approx_kl            | 0.00025912398 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.271        |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 103           |
|    n_updates            | 2560          |
|    policy_gradient_loss | -0.000123     |
|    value_loss           | 237           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 99.6         |
|    ep_rew_mean          | -48.2        |
| time/                   |              |
|    fps                  | 28           |
|    iterations           | 200          |
|    time_elapsed         | 14150        |
|    total_timesteps      | 409600       |
| train/                  |              |
|    approx_kl            | 0.0015510544 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.252       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 66.1         |
|    n_updates            | 2570         |
|    policy_gradient_loss | -0.00133     |
|    value_loss           | 201          |
------------------------------------------
Num timesteps: 410000
Best mean reward: -32.47 - Last mean reward per episode: -49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 106          |
|    ep_rew_mean          | -53.6        |
| time/                   |              |
|    fps                  | 28           |
|    iterations           | 201          |
|    time_elapsed         | 14201        |
|    total_timesteps      | 411648       |
| train/                  |              |
|    approx_kl            | 4.661578e-05 |
|    clip_fraction        | 0.0117       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.228       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 77           |
|    n_updates            | 2580         |
|    policy_gradient_loss | -0.000372    |
|    value_loss           | 176          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 95            |
|    ep_rew_mean          | -41.1         |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 202           |
|    time_elapsed         | 14255         |
|    total_timesteps      | 413696        |
| train/                  |               |
|    approx_kl            | 6.3277665e-05 |
|    clip_fraction        | 0.00635       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.25         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 99.1          |
|    n_updates            | 2590          |
|    policy_gradient_loss | 0.000464      |
|    value_loss           | 210           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 97.7         |
|    ep_rew_mean          | -44.9        |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 203          |
|    time_elapsed         | 14307        |
|    total_timesteps      | 415744       |
| train/                  |              |
|    approx_kl            | 0.0017878216 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.235       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 154          |
|    n_updates            | 2600         |
|    policy_gradient_loss | -1.49e-05    |
|    value_loss           | 208          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 93.8          |
|    ep_rew_mean          | -40.9         |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 204           |
|    time_elapsed         | 14358         |
|    total_timesteps      | 417792        |
| train/                  |               |
|    approx_kl            | 0.00096557103 |
|    clip_fraction        | 0.0134        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.222        |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 141           |
|    n_updates            | 2610          |
|    policy_gradient_loss | 0.000182      |
|    value_loss           | 225           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 92.3          |
|    ep_rew_mean          | -39.2         |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 205           |
|    time_elapsed         | 14409         |
|    total_timesteps      | 419840        |
| train/                  |               |
|    approx_kl            | 0.00030235513 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.181        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 83.1          |
|    n_updates            | 2620          |
|    policy_gradient_loss | 1.1e-05       |
|    value_loss           | 184           |
-------------------------------------------
Num timesteps: 420000
Best mean reward: -32.47 - Last mean reward per episode: -36.90
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 91.5         |
|    ep_rew_mean          | -39.1        |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 206          |
|    time_elapsed         | 14459        |
|    total_timesteps      | 421888       |
| train/                  |              |
|    approx_kl            | 0.0008833038 |
|    clip_fraction        | 0.00586      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.175       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 95.6         |
|    n_updates            | 2630         |
|    policy_gradient_loss | -0.000382    |
|    value_loss           | 164          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 88.3          |
|    ep_rew_mean          | -35.5         |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 207           |
|    time_elapsed         | 14509         |
|    total_timesteps      | 423936        |
| train/                  |               |
|    approx_kl            | 0.00023595389 |
|    clip_fraction        | 0.0117        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.185        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 101           |
|    n_updates            | 2640          |
|    policy_gradient_loss | -0.000402     |
|    value_loss           | 199           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 89.5         |
|    ep_rew_mean          | -37.2        |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 208          |
|    time_elapsed         | 14562        |
|    total_timesteps      | 425984       |
| train/                  |              |
|    approx_kl            | 0.0004044351 |
|    clip_fraction        | 0.00586      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.184       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 87           |
|    n_updates            | 2650         |
|    policy_gradient_loss | 3.12e-05     |
|    value_loss           | 207          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 89.8         |
|    ep_rew_mean          | -37.4        |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 209          |
|    time_elapsed         | 14614        |
|    total_timesteps      | 428032       |
| train/                  |              |
|    approx_kl            | 0.0006910546 |
|    clip_fraction        | 0.00654      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.179       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 84.4         |
|    n_updates            | 2660         |
|    policy_gradient_loss | -0.000123    |
|    value_loss           | 202          |
------------------------------------------
Num timesteps: 430000
Best mean reward: -32.47 - Last mean reward per episode: -38.28
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 90.8         |
|    ep_rew_mean          | -38.6        |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 210          |
|    time_elapsed         | 14664        |
|    total_timesteps      | 430080       |
| train/                  |              |
|    approx_kl            | 0.0008808032 |
|    clip_fraction        | 0.00986      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.189       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 100          |
|    n_updates            | 2670         |
|    policy_gradient_loss | 0.000178     |
|    value_loss           | 166          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 88.3         |
|    ep_rew_mean          | -35.2        |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 211          |
|    time_elapsed         | 14713        |
|    total_timesteps      | 432128       |
| train/                  |              |
|    approx_kl            | 0.0004038424 |
|    clip_fraction        | 0.00879      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.136       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 45.4         |
|    n_updates            | 2680         |
|    policy_gradient_loss | -0.000371    |
|    value_loss           | 163          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 89.4          |
|    ep_rew_mean          | -35.6         |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 212           |
|    time_elapsed         | 14763         |
|    total_timesteps      | 434176        |
| train/                  |               |
|    approx_kl            | 0.00015773435 |
|    clip_fraction        | 0.00483       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.156        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 86.2          |
|    n_updates            | 2690          |
|    policy_gradient_loss | 0.000142      |
|    value_loss           | 155           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 89.5          |
|    ep_rew_mean          | -36.1         |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 213           |
|    time_elapsed         | 14813         |
|    total_timesteps      | 436224        |
| train/                  |               |
|    approx_kl            | 0.00050353474 |
|    clip_fraction        | 0.00259       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.118        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 91.6          |
|    n_updates            | 2700          |
|    policy_gradient_loss | -6.7e-05      |
|    value_loss           | 182           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 89.5          |
|    ep_rew_mean          | -36.1         |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 214           |
|    time_elapsed         | 14861         |
|    total_timesteps      | 438272        |
| train/                  |               |
|    approx_kl            | 0.00055089453 |
|    clip_fraction        | 0.0144        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.134        |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 69.3          |
|    n_updates            | 2710          |
|    policy_gradient_loss | -0.000868     |
|    value_loss           | 129           |
-------------------------------------------
Num timesteps: 440000
Best mean reward: -32.47 - Last mean reward per episode: -58.47
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 120          |
|    ep_rew_mean          | -58.5        |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 215          |
|    time_elapsed         | 14910        |
|    total_timesteps      | 440320       |
| train/                  |              |
|    approx_kl            | 0.0008571735 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.146       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.437        |
|    n_updates            | 2720         |
|    policy_gradient_loss | -0.000355    |
|    value_loss           | 7.92         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 122          |
|    ep_rew_mean          | -59.6        |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 216          |
|    time_elapsed         | 14965        |
|    total_timesteps      | 442368       |
| train/                  |              |
|    approx_kl            | 0.0006639502 |
|    clip_fraction        | 0.00571      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.146       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 123          |
|    n_updates            | 2730         |
|    policy_gradient_loss | -0.00105     |
|    value_loss           | 229          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 122           |
|    ep_rew_mean          | -60.6         |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 217           |
|    time_elapsed         | 15015         |
|    total_timesteps      | 444416        |
| train/                  |               |
|    approx_kl            | 0.00028110744 |
|    clip_fraction        | 0.00347       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.152        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 96.2          |
|    n_updates            | 2740          |
|    policy_gradient_loss | -0.000133     |
|    value_loss           | 224           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 124           |
|    ep_rew_mean          | -62.5         |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 218           |
|    time_elapsed         | 15065         |
|    total_timesteps      | 446464        |
| train/                  |               |
|    approx_kl            | 0.00030850706 |
|    clip_fraction        | 0.00952       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.175        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 83.5          |
|    n_updates            | 2750          |
|    policy_gradient_loss | -0.000747     |
|    value_loss           | 202           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 97.6         |
|    ep_rew_mean          | -44.1        |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 219          |
|    time_elapsed         | 15119        |
|    total_timesteps      | 448512       |
| train/                  |              |
|    approx_kl            | 0.0002877147 |
|    clip_fraction        | 0.00693      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.18        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 56.8         |
|    n_updates            | 2760         |
|    policy_gradient_loss | 0.000569     |
|    value_loss           | 212          |
------------------------------------------
Num timesteps: 450000
Best mean reward: -32.47 - Last mean reward per episode: -41.58
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 93.9         |
|    ep_rew_mean          | -40.7        |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 220          |
|    time_elapsed         | 15170        |
|    total_timesteps      | 450560       |
| train/                  |              |
|    approx_kl            | 0.0010922751 |
|    clip_fraction        | 0.00337      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.177       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 111          |
|    n_updates            | 2770         |
|    policy_gradient_loss | -9.95e-05    |
|    value_loss           | 238          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 96.5         |
|    ep_rew_mean          | -44.2        |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 221          |
|    time_elapsed         | 15221        |
|    total_timesteps      | 452608       |
| train/                  |              |
|    approx_kl            | 0.0001014678 |
|    clip_fraction        | 0.00513      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.154       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 142          |
|    n_updates            | 2780         |
|    policy_gradient_loss | -0.000128    |
|    value_loss           | 193          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 102          |
|    ep_rew_mean          | -49.6        |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 222          |
|    time_elapsed         | 15273        |
|    total_timesteps      | 454656       |
| train/                  |              |
|    approx_kl            | 0.0002603673 |
|    clip_fraction        | 0.00552      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.17        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 81.4         |
|    n_updates            | 2790         |
|    policy_gradient_loss | 0.000155     |
|    value_loss           | 183          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 98           |
|    ep_rew_mean          | -46.4        |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 223          |
|    time_elapsed         | 15324        |
|    total_timesteps      | 456704       |
| train/                  |              |
|    approx_kl            | 0.0003017908 |
|    clip_fraction        | 0.00586      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.138       |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 108          |
|    n_updates            | 2800         |
|    policy_gradient_loss | -0.000542    |
|    value_loss           | 233          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 98.2          |
|    ep_rew_mean          | -45.8         |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 224           |
|    time_elapsed         | 15373         |
|    total_timesteps      | 458752        |
| train/                  |               |
|    approx_kl            | 0.00027421024 |
|    clip_fraction        | 0.00112       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.136        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 107           |
|    n_updates            | 2810          |
|    policy_gradient_loss | -0.000155     |
|    value_loss           | 229           |
-------------------------------------------
Num timesteps: 460000
Best mean reward: -32.47 - Last mean reward per episode: -43.85
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 96.5         |
|    ep_rew_mean          | -44.1        |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 225          |
|    time_elapsed         | 15422        |
|    total_timesteps      | 460800       |
| train/                  |              |
|    approx_kl            | 0.0003341963 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.141       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 129          |
|    n_updates            | 2820         |
|    policy_gradient_loss | -8.88e-05    |
|    value_loss           | 202          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 88.9         |
|    ep_rew_mean          | -36.8        |
| time/                   |              |
|    fps                  | 29           |
|    iterations           | 226          |
|    time_elapsed         | 15471        |
|    total_timesteps      | 462848       |
| train/                  |              |
|    approx_kl            | 0.0009725394 |
|    clip_fraction        | 0.0241       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.135       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 108          |
|    n_updates            | 2830         |
|    policy_gradient_loss | -0.00202     |
|    value_loss           | 193          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 89            |
|    ep_rew_mean          | -36.5         |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 227           |
|    time_elapsed         | 15521         |
|    total_timesteps      | 464896        |
| train/                  |               |
|    approx_kl            | 0.00012970294 |
|    clip_fraction        | 0.000928      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.139        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 91.3          |
|    n_updates            | 2840          |
|    policy_gradient_loss | 1.61e-05      |
|    value_loss           | 175           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 87.4          |
|    ep_rew_mean          | -34.5         |
| time/                   |               |
|    fps                  | 29            |
|    iterations           | 228           |
|    time_elapsed         | 15573         |
|    total_timesteps      | 466944        |
| train/                  |               |
|    approx_kl            | 0.00038739084 |
|    clip_fraction        | 0.00415       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.109        |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 103           |
|    n_updates            | 2850          |
|    policy_gradient_loss | -0.000294     |
|    value_loss           | 227           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 89.7         |
|    ep_rew_mean          | -37.4        |
| time/                   |              |
|    fps                  | 30           |
|    iterations           | 229          |
|    time_elapsed         | 15621        |
|    total_timesteps      | 468992       |
| train/                  |              |
|    approx_kl            | 0.0005205611 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.124       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 89.4         |
|    n_updates            | 2860         |
|    policy_gradient_loss | -0.000269    |
|    value_loss           | 169          |
------------------------------------------
Num timesteps: 470000
Best mean reward: -32.47 - Last mean reward per episode: -36.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 88.5          |
|    ep_rew_mean          | -36.1         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 230           |
|    time_elapsed         | 15673         |
|    total_timesteps      | 471040        |
| train/                  |               |
|    approx_kl            | 0.00049696164 |
|    clip_fraction        | 0.00771       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.108        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 109           |
|    n_updates            | 2870          |
|    policy_gradient_loss | -0.000742     |
|    value_loss           | 215           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -46.4         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 231           |
|    time_elapsed         | 15725         |
|    total_timesteps      | 473088        |
| train/                  |               |
|    approx_kl            | 0.00013281679 |
|    clip_fraction        | 0.00298       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.115        |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 60.2          |
|    n_updates            | 2880          |
|    policy_gradient_loss | 9.83e-06      |
|    value_loss           | 165           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 107           |
|    ep_rew_mean          | -52.9         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 232           |
|    time_elapsed         | 15784         |
|    total_timesteps      | 475136        |
| train/                  |               |
|    approx_kl            | 1.6890262e-05 |
|    clip_fraction        | 0.00474       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.127        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 105           |
|    n_updates            | 2890          |
|    policy_gradient_loss | -0.000126     |
|    value_loss           | 185           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 105           |
|    ep_rew_mean          | -50.8         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 233           |
|    time_elapsed         | 15842         |
|    total_timesteps      | 477184        |
| train/                  |               |
|    approx_kl            | 0.00055447465 |
|    clip_fraction        | 0.0129        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.141        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 110           |
|    n_updates            | 2900          |
|    policy_gradient_loss | -0.000669     |
|    value_loss           | 215           |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 118         |
|    ep_rew_mean          | -59         |
| time/                   |             |
|    fps                  | 30          |
|    iterations           | 234         |
|    time_elapsed         | 15895       |
|    total_timesteps      | 479232      |
| train/                  |             |
|    approx_kl            | 0.000808505 |
|    clip_fraction        | 0.0124      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.121      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 104         |
|    n_updates            | 2910        |
|    policy_gradient_loss | -0.00107    |
|    value_loss           | 121         |
-----------------------------------------
Num timesteps: 480000
Best mean reward: -32.47 - Last mean reward per episode: -60.44
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 119           |
|    ep_rew_mean          | -60.9         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 235           |
|    time_elapsed         | 15950         |
|    total_timesteps      | 481280        |
| train/                  |               |
|    approx_kl            | 0.00064033584 |
|    clip_fraction        | 0.0019        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.113        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 137           |
|    n_updates            | 2920          |
|    policy_gradient_loss | 3.58e-05      |
|    value_loss           | 251           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 109           |
|    ep_rew_mean          | -53           |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 236           |
|    time_elapsed         | 16009         |
|    total_timesteps      | 483328        |
| train/                  |               |
|    approx_kl            | 0.00046992005 |
|    clip_fraction        | 0.00537       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.105        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 152           |
|    n_updates            | 2930          |
|    policy_gradient_loss | 0.000292      |
|    value_loss           | 272           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 114           |
|    ep_rew_mean          | -59           |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 237           |
|    time_elapsed         | 16058         |
|    total_timesteps      | 485376        |
| train/                  |               |
|    approx_kl            | 0.00056401716 |
|    clip_fraction        | 0.00757       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.104        |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 107           |
|    n_updates            | 2940          |
|    policy_gradient_loss | 6.09e-05      |
|    value_loss           | 221           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 114           |
|    ep_rew_mean          | -59.4         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 238           |
|    time_elapsed         | 16112         |
|    total_timesteps      | 487424        |
| train/                  |               |
|    approx_kl            | 7.4046635e-05 |
|    clip_fraction        | 0.00767       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.143        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 110           |
|    n_updates            | 2950          |
|    policy_gradient_loss | 0.0003        |
|    value_loss           | 213           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 140          |
|    ep_rew_mean          | -76.5        |
| time/                   |              |
|    fps                  | 30           |
|    iterations           | 239          |
|    time_elapsed         | 16159        |
|    total_timesteps      | 489472       |
| train/                  |              |
|    approx_kl            | 0.0006894091 |
|    clip_fraction        | 0.00918      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.134       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.17         |
|    n_updates            | 2960         |
|    policy_gradient_loss | -0.000375    |
|    value_loss           | 41.9         |
------------------------------------------
Num timesteps: 490000
Best mean reward: -32.47 - Last mean reward per episode: -65.78
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 127           |
|    ep_rew_mean          | -66.7         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 240           |
|    time_elapsed         | 16208         |
|    total_timesteps      | 491520        |
| train/                  |               |
|    approx_kl            | 0.00029773285 |
|    clip_fraction        | 0.00972       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.122        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 35.8          |
|    n_updates            | 2970          |
|    policy_gradient_loss | 0.000146      |
|    value_loss           | 130           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 137          |
|    ep_rew_mean          | -76.8        |
| time/                   |              |
|    fps                  | 30           |
|    iterations           | 241          |
|    time_elapsed         | 16258        |
|    total_timesteps      | 493568       |
| train/                  |              |
|    approx_kl            | 0.0002069815 |
|    clip_fraction        | 0.00303      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.121       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 141          |
|    n_updates            | 2980         |
|    policy_gradient_loss | -0.000165    |
|    value_loss           | 269          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 135          |
|    ep_rew_mean          | -74.6        |
| time/                   |              |
|    fps                  | 30           |
|    iterations           | 242          |
|    time_elapsed         | 16309        |
|    total_timesteps      | 495616       |
| train/                  |              |
|    approx_kl            | 0.0002902514 |
|    clip_fraction        | 0.00532      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.115       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 121          |
|    n_updates            | 2990         |
|    policy_gradient_loss | -7.34e-05    |
|    value_loss           | 214          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 141           |
|    ep_rew_mean          | -79.2         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 243           |
|    time_elapsed         | 16359         |
|    total_timesteps      | 497664        |
| train/                  |               |
|    approx_kl            | 0.00046584534 |
|    clip_fraction        | 0.00396       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.11         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 117           |
|    n_updates            | 3000          |
|    policy_gradient_loss | -0.000236     |
|    value_loss           | 260           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 106           |
|    ep_rew_mean          | -52.9         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 244           |
|    time_elapsed         | 16411         |
|    total_timesteps      | 499712        |
| train/                  |               |
|    approx_kl            | 0.00034749534 |
|    clip_fraction        | 0.00303       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.123        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 144           |
|    n_updates            | 3010          |
|    policy_gradient_loss | 0.000249      |
|    value_loss           | 235           |
-------------------------------------------
Num timesteps: 500000
Best mean reward: -32.47 - Last mean reward per episode: -51.80
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 103           |
|    ep_rew_mean          | -52.2         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 245           |
|    time_elapsed         | 16462         |
|    total_timesteps      | 501760        |
| train/                  |               |
|    approx_kl            | 0.00052905583 |
|    clip_fraction        | 0.0082        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.107        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 126           |
|    n_updates            | 3020          |
|    policy_gradient_loss | -0.000506     |
|    value_loss           | 248           |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 90.3        |
|    ep_rew_mean          | -39.4       |
| time/                   |             |
|    fps                  | 30          |
|    iterations           | 246         |
|    time_elapsed         | 16511       |
|    total_timesteps      | 503808      |
| train/                  |             |
|    approx_kl            | 0.000649686 |
|    clip_fraction        | 0.0061      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0917     |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 92.9        |
|    n_updates            | 3030        |
|    policy_gradient_loss | -0.000619   |
|    value_loss           | 220         |
-----------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 89.2           |
|    ep_rew_mean          | -38.5          |
| time/                   |                |
|    fps                  | 30             |
|    iterations           | 247            |
|    time_elapsed         | 16559          |
|    total_timesteps      | 505856         |
| train/                  |                |
|    approx_kl            | 0.000114600145 |
|    clip_fraction        | 0.00562        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.107         |
|    explained_variance   | 0              |
|    learning_rate        | 0.0003         |
|    loss                 | 111            |
|    n_updates            | 3040           |
|    policy_gradient_loss | -0.000123      |
|    value_loss           | 187            |
--------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 83.5          |
|    ep_rew_mean          | -31.3         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 248           |
|    time_elapsed         | 16607         |
|    total_timesteps      | 507904        |
| train/                  |               |
|    approx_kl            | 0.00038641866 |
|    clip_fraction        | 0.00732       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.091        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 110           |
|    n_updates            | 3050          |
|    policy_gradient_loss | -0.000527     |
|    value_loss           | 183           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 85.9          |
|    ep_rew_mean          | -34.6         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 249           |
|    time_elapsed         | 16657         |
|    total_timesteps      | 509952        |
| train/                  |               |
|    approx_kl            | 0.00017087517 |
|    clip_fraction        | 0.00488       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0946       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 58.9          |
|    n_updates            | 3060          |
|    policy_gradient_loss | -0.000427     |
|    value_loss           | 186           |
-------------------------------------------
Num timesteps: 510000
Best mean reward: -32.47 - Last mean reward per episode: -34.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 89          |
|    ep_rew_mean          | -36.6       |
| time/                   |             |
|    fps                  | 30          |
|    iterations           | 250         |
|    time_elapsed         | 16706       |
|    total_timesteps      | 512000      |
| train/                  |             |
|    approx_kl            | 0.000516176 |
|    clip_fraction        | 0.00698     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0926     |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 131         |
|    n_updates            | 3070        |
|    policy_gradient_loss | -0.000494   |
|    value_loss           | 188         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 91           |
|    ep_rew_mean          | -37.9        |
| time/                   |              |
|    fps                  | 30           |
|    iterations           | 251          |
|    time_elapsed         | 16758        |
|    total_timesteps      | 514048       |
| train/                  |              |
|    approx_kl            | 9.470037e-06 |
|    clip_fraction        | 0.0021       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0923      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 116          |
|    n_updates            | 3080         |
|    policy_gradient_loss | -0.000155    |
|    value_loss           | 207          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 88            |
|    ep_rew_mean          | -37.1         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 252           |
|    time_elapsed         | 16808         |
|    total_timesteps      | 516096        |
| train/                  |               |
|    approx_kl            | 0.00060669065 |
|    clip_fraction        | 0.00542       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0841       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 79.6          |
|    n_updates            | 3090          |
|    policy_gradient_loss | -0.00023      |
|    value_loss           | 197           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 90.8          |
|    ep_rew_mean          | -40.2         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 253           |
|    time_elapsed         | 16859         |
|    total_timesteps      | 518144        |
| train/                  |               |
|    approx_kl            | 0.00029007962 |
|    clip_fraction        | 0.00215       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0728       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 87.2          |
|    n_updates            | 3100          |
|    policy_gradient_loss | -0.000202     |
|    value_loss           | 208           |
-------------------------------------------
Num timesteps: 520000
Best mean reward: -32.47 - Last mean reward per episode: -42.21
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 92.3          |
|    ep_rew_mean          | -42           |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 254           |
|    time_elapsed         | 16908         |
|    total_timesteps      | 520192        |
| train/                  |               |
|    approx_kl            | 0.00068718195 |
|    clip_fraction        | 0.00693       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0907       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 114           |
|    n_updates            | 3110          |
|    policy_gradient_loss | 9.45e-05      |
|    value_loss           | 200           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 90.8         |
|    ep_rew_mean          | -40.1        |
| time/                   |              |
|    fps                  | 30           |
|    iterations           | 255          |
|    time_elapsed         | 16959        |
|    total_timesteps      | 522240       |
| train/                  |              |
|    approx_kl            | 0.0001386258 |
|    clip_fraction        | 0.00591      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0863      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 70.3         |
|    n_updates            | 3120         |
|    policy_gradient_loss | 6.75e-05     |
|    value_loss           | 175          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 91.5          |
|    ep_rew_mean          | -39.4         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 256           |
|    time_elapsed         | 17007         |
|    total_timesteps      | 524288        |
| train/                  |               |
|    approx_kl            | 0.00035959348 |
|    clip_fraction        | 0.00137       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0941       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 103           |
|    n_updates            | 3130          |
|    policy_gradient_loss | -0.000155     |
|    value_loss           | 184           |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 91.2           |
|    ep_rew_mean          | -38.4          |
| time/                   |                |
|    fps                  | 30             |
|    iterations           | 257            |
|    time_elapsed         | 17059          |
|    total_timesteps      | 526336         |
| train/                  |                |
|    approx_kl            | 0.000120829965 |
|    clip_fraction        | 0.00483        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0972        |
|    explained_variance   | 0              |
|    learning_rate        | 0.0003         |
|    loss                 | 117            |
|    n_updates            | 3140           |
|    policy_gradient_loss | -3.11e-05      |
|    value_loss           | 172            |
--------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 86.7          |
|    ep_rew_mean          | -33.2         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 258           |
|    time_elapsed         | 17109         |
|    total_timesteps      | 528384        |
| train/                  |               |
|    approx_kl            | 0.00027435584 |
|    clip_fraction        | 0.00537       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0786       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 72.7          |
|    n_updates            | 3150          |
|    policy_gradient_loss | -0.000153     |
|    value_loss           | 163           |
-------------------------------------------
Num timesteps: 530000
Best mean reward: -32.47 - Last mean reward per episode: -35.94
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 87            |
|    ep_rew_mean          | -34.3         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 259           |
|    time_elapsed         | 17160         |
|    total_timesteps      | 530432        |
| train/                  |               |
|    approx_kl            | 0.00014612274 |
|    clip_fraction        | 0.00435       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0864       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 72.8          |
|    n_updates            | 3160          |
|    policy_gradient_loss | 2.57e-05      |
|    value_loss           | 167           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 86.8          |
|    ep_rew_mean          | -35.2         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 260           |
|    time_elapsed         | 17217         |
|    total_timesteps      | 532480        |
| train/                  |               |
|    approx_kl            | 0.00070323446 |
|    clip_fraction        | 0.0063        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0835       |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 94.2          |
|    n_updates            | 3170          |
|    policy_gradient_loss | 0.000132      |
|    value_loss           | 191           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 85.5         |
|    ep_rew_mean          | -34.3        |
| time/                   |              |
|    fps                  | 30           |
|    iterations           | 261          |
|    time_elapsed         | 17429        |
|    total_timesteps      | 534528       |
| train/                  |              |
|    approx_kl            | 0.0007829135 |
|    clip_fraction        | 0.00532      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0612      |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 92.4         |
|    n_updates            | 3180         |
|    policy_gradient_loss | -0.000158    |
|    value_loss           | 172          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 87.3          |
|    ep_rew_mean          | -35.7         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 262           |
|    time_elapsed         | 17481         |
|    total_timesteps      | 536576        |
| train/                  |               |
|    approx_kl            | 0.00019317449 |
|    clip_fraction        | 0.00381       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0713       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 46.4          |
|    n_updates            | 3190          |
|    policy_gradient_loss | -0.000158     |
|    value_loss           | 176           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 94.7         |
|    ep_rew_mean          | -41.8        |
| time/                   |              |
|    fps                  | 30           |
|    iterations           | 263          |
|    time_elapsed         | 17529        |
|    total_timesteps      | 538624       |
| train/                  |              |
|    approx_kl            | 7.261694e-05 |
|    clip_fraction        | 0.002        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0547      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 84.5         |
|    n_updates            | 3200         |
|    policy_gradient_loss | 8.62e-05     |
|    value_loss           | 170          |
------------------------------------------
Num timesteps: 540000
Best mean reward: -32.47 - Last mean reward per episode: -41.37
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 95.7         |
|    ep_rew_mean          | -42.1        |
| time/                   |              |
|    fps                  | 30           |
|    iterations           | 264          |
|    time_elapsed         | 17583        |
|    total_timesteps      | 540672       |
| train/                  |              |
|    approx_kl            | 0.0002757508 |
|    clip_fraction        | 0.00664      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0743      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 139          |
|    n_updates            | 3210         |
|    policy_gradient_loss | -0.000862    |
|    value_loss           | 165          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 97.3          |
|    ep_rew_mean          | -43.6         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 265           |
|    time_elapsed         | 17634         |
|    total_timesteps      | 542720        |
| train/                  |               |
|    approx_kl            | 0.00039937877 |
|    clip_fraction        | 0.011         |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0913       |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 60.5          |
|    n_updates            | 3220          |
|    policy_gradient_loss | -0.000433     |
|    value_loss           | 208           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 99            |
|    ep_rew_mean          | -44.6         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 266           |
|    time_elapsed         | 17687         |
|    total_timesteps      | 544768        |
| train/                  |               |
|    approx_kl            | 0.00030806052 |
|    clip_fraction        | 0.00605       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0938       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 92.8          |
|    n_updates            | 3230          |
|    policy_gradient_loss | -0.000336     |
|    value_loss           | 213           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 88.2         |
|    ep_rew_mean          | -36          |
| time/                   |              |
|    fps                  | 30           |
|    iterations           | 267          |
|    time_elapsed         | 17741        |
|    total_timesteps      | 546816       |
| train/                  |              |
|    approx_kl            | 0.0007335867 |
|    clip_fraction        | 0.00713      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0874      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 112          |
|    n_updates            | 3240         |
|    policy_gradient_loss | -0.000498    |
|    value_loss           | 197          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 89.9          |
|    ep_rew_mean          | -37.9         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 268           |
|    time_elapsed         | 17793         |
|    total_timesteps      | 548864        |
| train/                  |               |
|    approx_kl            | 0.00037801985 |
|    clip_fraction        | 0.00806       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0822       |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 132           |
|    n_updates            | 3250          |
|    policy_gradient_loss | -0.000581     |
|    value_loss           | 207           |
-------------------------------------------
Num timesteps: 550000
Best mean reward: -32.47 - Last mean reward per episode: -35.04
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 85.7          |
|    ep_rew_mean          | -34.7         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 269           |
|    time_elapsed         | 17847         |
|    total_timesteps      | 550912        |
| train/                  |               |
|    approx_kl            | 0.00021302555 |
|    clip_fraction        | 0.00439       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.076        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 116           |
|    n_updates            | 3260          |
|    policy_gradient_loss | -0.000524     |
|    value_loss           | 202           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 79.2          |
|    ep_rew_mean          | -29.1         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 270           |
|    time_elapsed         | 17895         |
|    total_timesteps      | 552960        |
| train/                  |               |
|    approx_kl            | 0.00044538267 |
|    clip_fraction        | 0.0061        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.071        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 79.9          |
|    n_updates            | 3270          |
|    policy_gradient_loss | -0.000802     |
|    value_loss           | 176           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 87.2          |
|    ep_rew_mean          | -37.3         |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 271           |
|    time_elapsed         | 17947         |
|    total_timesteps      | 555008        |
| train/                  |               |
|    approx_kl            | 0.00017042703 |
|    clip_fraction        | 0.00381       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0542       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 84.4          |
|    n_updates            | 3280          |
|    policy_gradient_loss | -0.000231     |
|    value_loss           | 185           |
-------------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 290, in <module>
    model.learn(total_timesteps=1000000, callback=[WandbCallback(), save_callback])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 313, in learn
    self.train()
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 279, in train
    loss.backward()
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt