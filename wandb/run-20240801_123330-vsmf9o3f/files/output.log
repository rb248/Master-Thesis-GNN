
Using cpu device
-----------------------------
| time/              |      |
|    fps             | 121  |
|    iterations      | 1    |
|    time_elapsed    | 16   |
|    total_timesteps | 2048 |
-----------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.4e+03     |
|    ep_rew_mean          | -1.17e+03   |
| time/                   |             |
|    fps                  | 55          |
|    iterations           | 2           |
|    time_elapsed         | 73          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.013150215 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | -0.000817   |
|    learning_rate        | 0.0003      |
|    loss                 | 0.391       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00437    |
|    value_loss           | 3.44        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.45e+03    |
|    ep_rew_mean          | -1.13e+03   |
| time/                   |             |
|    fps                  | 47          |
|    iterations           | 3           |
|    time_elapsed         | 130         |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.014310615 |
|    clip_fraction        | 0.0846      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | -1.31e-05   |
|    learning_rate        | 0.0003      |
|    loss                 | 13.7        |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00574    |
|    value_loss           | 25.6        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.34e+03    |
|    ep_rew_mean          | -998        |
| time/                   |             |
|    fps                  | 43          |
|    iterations           | 4           |
|    time_elapsed         | 190         |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.014361962 |
|    clip_fraction        | 0.0839      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 7.09e-06    |
|    learning_rate        | 0.0003      |
|    loss                 | 11.5        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0054     |
|    value_loss           | 28.6        |
-----------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -939.62
Saving new best model at 9137 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.28e+03    |
|    ep_rew_mean          | -940        |
| time/                   |             |
|    fps                  | 41          |
|    iterations           | 5           |
|    time_elapsed         | 249         |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.015473464 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.914      |
|    explained_variance   | 5.48e-06    |
|    learning_rate        | 0.0003      |
|    loss                 | 21.3        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00567    |
|    value_loss           | 30.2        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.22e+03    |
|    ep_rew_mean          | -907        |
| time/                   |             |
|    fps                  | 39          |
|    iterations           | 6           |
|    time_elapsed         | 310         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.010526899 |
|    clip_fraction        | 0.0952      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.829      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 27.1        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00543    |
|    value_loss           | 38.9        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.16e+03     |
|    ep_rew_mean          | -872         |
| time/                   |              |
|    fps                  | 38           |
|    iterations           | 7            |
|    time_elapsed         | 371          |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.0051869135 |
|    clip_fraction        | 0.0194       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.779       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 8.55         |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00139     |
|    value_loss           | 32.5         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.21e+03    |
|    ep_rew_mean          | -858        |
| time/                   |             |
|    fps                  | 38          |
|    iterations           | 8           |
|    time_elapsed         | 427         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.004633611 |
|    clip_fraction        | 0.0185      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.767      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 17.7        |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.000332   |
|    value_loss           | 44.9        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.25e+03     |
|    ep_rew_mean          | -874         |
| time/                   |              |
|    fps                  | 38           |
|    iterations           | 9            |
|    time_elapsed         | 484          |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0069089774 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.687       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 12.5         |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00051     |
|    value_loss           | 39.2         |
------------------------------------------
Num timesteps: 20000
Best mean reward: -939.62 - Last mean reward per episode: -873.88
Saving new best model at 18002 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.23e+03     |
|    ep_rew_mean          | -831         |
| time/                   |              |
|    fps                  | 37           |
|    iterations           | 10           |
|    time_elapsed         | 544          |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.0014279815 |
|    clip_fraction        | 0.0188       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.669       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 12.9         |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.000164    |
|    value_loss           | 31.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.21e+03     |
|    ep_rew_mean          | -820         |
| time/                   |              |
|    fps                  | 37           |
|    iterations           | 11           |
|    time_elapsed         | 604          |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0037645623 |
|    clip_fraction        | 0.0412       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.631       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 39.7         |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00259     |
|    value_loss           | 58.4         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.18e+03    |
|    ep_rew_mean          | -800        |
| time/                   |             |
|    fps                  | 36          |
|    iterations           | 12          |
|    time_elapsed         | 665         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.002808591 |
|    clip_fraction        | 0.0288      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.549      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 24.2        |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.000782   |
|    value_loss           | 37.7        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.16e+03     |
|    ep_rew_mean          | -758         |
| time/                   |              |
|    fps                  | 36           |
|    iterations           | 13           |
|    time_elapsed         | 726          |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.0029408112 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.548       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 25.4         |
|    n_updates            | 120          |
|    policy_gradient_loss | 5.19e-05     |
|    value_loss           | 55.8         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.13e+03     |
|    ep_rew_mean          | -739         |
| time/                   |              |
|    fps                  | 36           |
|    iterations           | 14           |
|    time_elapsed         | 789          |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0014287549 |
|    clip_fraction        | 0.00566      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.564       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 27.9         |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.000425    |
|    value_loss           | 61.6         |
------------------------------------------
Num timesteps: 30000
Best mean reward: -873.88 - Last mean reward per episode: -717.93
Saving new best model at 29582 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.11e+03     |
|    ep_rew_mean          | -718         |
| time/                   |              |
|    fps                  | 36           |
|    iterations           | 15           |
|    time_elapsed         | 850          |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.0035796897 |
|    clip_fraction        | 0.00747      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.554       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 18.7         |
|    n_updates            | 140          |
|    policy_gradient_loss | 0.000231     |
|    value_loss           | 56.7         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.1e+03     |
|    ep_rew_mean          | -702        |
| time/                   |             |
|    fps                  | 35          |
|    iterations           | 16          |
|    time_elapsed         | 911         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.003584924 |
|    clip_fraction        | 0.0333      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.467      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 18.6        |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00149    |
|    value_loss           | 42.9        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.08e+03    |
|    ep_rew_mean          | -693        |
| time/                   |             |
|    fps                  | 35          |
|    iterations           | 17          |
|    time_elapsed         | 974         |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.001248621 |
|    clip_fraction        | 0.0377      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.398      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 29.3        |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.000867   |
|    value_loss           | 58.6        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -678         |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 18           |
|    time_elapsed         | 1418         |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.0029345583 |
|    clip_fraction        | 0.0325       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.342       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 30.6         |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.000642    |
|    value_loss           | 48.1         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -666         |
| time/                   |              |
|    fps                  | 16           |
|    iterations           | 19           |
|    time_elapsed         | 2429         |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0012199831 |
|    clip_fraction        | 0.00977      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.307       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 27.1         |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.000781    |
|    value_loss           | 60.5         |
------------------------------------------
Num timesteps: 40000
Best mean reward: -717.93 - Last mean reward per episode: -663.11
Saving new best model at 38938 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -663         |
| time/                   |              |
|    fps                  | 11           |
|    iterations           | 20           |
|    time_elapsed         | 3417         |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0002678501 |
|    clip_fraction        | 0.00264      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.297       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 33.3         |
|    n_updates            | 190          |
|    policy_gradient_loss | 0.000141     |
|    value_loss           | 50.4         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -660         |
| time/                   |              |
|    fps                  | 12           |
|    iterations           | 21           |
|    time_elapsed         | 3478         |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0013345093 |
|    clip_fraction        | 0.0248       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.259       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 29.8         |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00087     |
|    value_loss           | 52.2         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -658         |
| time/                   |              |
|    fps                  | 12           |
|    iterations           | 22           |
|    time_elapsed         | 3539         |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0017615766 |
|    clip_fraction        | 0.0389       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.229       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 15.7         |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00187     |
|    value_loss           | 49.3         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -652        |
| time/                   |             |
|    fps                  | 13          |
|    iterations           | 23          |
|    time_elapsed         | 3600        |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 5.78365e-05 |
|    clip_fraction        | 0.00811     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.237      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 13.8        |
|    n_updates            | 220         |
|    policy_gradient_loss | -4.8e-05    |
|    value_loss           | 62.2        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -650         |
| time/                   |              |
|    fps                  | 13           |
|    iterations           | 24           |
|    time_elapsed         | 3663         |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0005645244 |
|    clip_fraction        | 0.014        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.22        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 29.2         |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.000493    |
|    value_loss           | 45.4         |
------------------------------------------
Num timesteps: 50000
Best mean reward: -663.11 - Last mean reward per episode: -648.17
Saving new best model at 49172 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -648         |
| time/                   |              |
|    fps                  | 13           |
|    iterations           | 25           |
|    time_elapsed         | 3723         |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0005487565 |
|    clip_fraction        | 0.00396      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.196       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 25.5         |
|    n_updates            | 240          |
|    policy_gradient_loss | 0.000214     |
|    value_loss           | 51.4         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -650         |
| time/                   |              |
|    fps                  | 14           |
|    iterations           | 26           |
|    time_elapsed         | 3785         |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0010187153 |
|    clip_fraction        | 0.00918      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.179       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 17.9         |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.000513    |
|    value_loss           | 39           |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -648         |
| time/                   |              |
|    fps                  | 14           |
|    iterations           | 27           |
|    time_elapsed         | 3845         |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0006124028 |
|    clip_fraction        | 0.0131       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.153       |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 22.1         |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.000714    |
|    value_loss           | 49.6         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -643          |
| time/                   |               |
|    fps                  | 14            |
|    iterations           | 28            |
|    time_elapsed         | 3908          |
|    total_timesteps      | 57344         |
| train/                  |               |
|    approx_kl            | 0.00086339354 |
|    clip_fraction        | 0.00854       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.134        |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 25.3          |
|    n_updates            | 270           |
|    policy_gradient_loss | -0.000117     |
|    value_loss           | 57.2          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -647         |
| time/                   |              |
|    fps                  | 14           |
|    iterations           | 29           |
|    time_elapsed         | 3967         |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0006007096 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.126       |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 21.3         |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00159     |
|    value_loss           | 45           |
------------------------------------------
Num timesteps: 60000
Best mean reward: -648.17 - Last mean reward per episode: -653.53
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.06e+03      |
|    ep_rew_mean          | -654          |
| time/                   |               |
|    fps                  | 15            |
|    iterations           | 30            |
|    time_elapsed         | 4026          |
|    total_timesteps      | 61440         |
| train/                  |               |
|    approx_kl            | 8.2033075e-05 |
|    clip_fraction        | 0.00317       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.132        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 21            |
|    n_updates            | 290           |
|    policy_gradient_loss | 5.01e-05      |
|    value_loss           | 37.7          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -641          |
| time/                   |               |
|    fps                  | 15            |
|    iterations           | 31            |
|    time_elapsed         | 4088          |
|    total_timesteps      | 63488         |
| train/                  |               |
|    approx_kl            | 0.00037267111 |
|    clip_fraction        | 0.00415       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0994       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 21            |
|    n_updates            | 300           |
|    policy_gradient_loss | -0.000272     |
|    value_loss           | 62.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -637          |
| time/                   |               |
|    fps                  | 15            |
|    iterations           | 32            |
|    time_elapsed         | 4148          |
|    total_timesteps      | 65536         |
| train/                  |               |
|    approx_kl            | 0.00012276272 |
|    clip_fraction        | 0.00205       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.098        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 29.6          |
|    n_updates            | 310           |
|    policy_gradient_loss | -4.56e-05     |
|    value_loss           | 58.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -639          |
| time/                   |               |
|    fps                  | 16            |
|    iterations           | 33            |
|    time_elapsed         | 4215          |
|    total_timesteps      | 67584         |
| train/                  |               |
|    approx_kl            | 0.00045348416 |
|    clip_fraction        | 0.00562       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0971       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 15.5          |
|    n_updates            | 320           |
|    policy_gradient_loss | -0.000247     |
|    value_loss           | 43.6          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -635         |
| time/                   |              |
|    fps                  | 16           |
|    iterations           | 34           |
|    time_elapsed         | 4284         |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.0001582857 |
|    clip_fraction        | 0.00303      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0816      |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 45.6         |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.000266    |
|    value_loss           | 59.1         |
------------------------------------------
Num timesteps: 70000
Best mean reward: -648.17 - Last mean reward per episode: -629.85
Saving new best model at 69650 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -630          |
| time/                   |               |
|    fps                  | 16            |
|    iterations           | 35            |
|    time_elapsed         | 4345          |
|    total_timesteps      | 71680         |
| train/                  |               |
|    approx_kl            | 0.00011319024 |
|    clip_fraction        | 0.00195       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0804       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 21.2          |
|    n_updates            | 340           |
|    policy_gradient_loss | -7.16e-06     |
|    value_loss           | 62.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -628          |
| time/                   |               |
|    fps                  | 16            |
|    iterations           | 36            |
|    time_elapsed         | 4408          |
|    total_timesteps      | 73728         |
| train/                  |               |
|    approx_kl            | 0.00015495889 |
|    clip_fraction        | 0.00225       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0877       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 18.4          |
|    n_updates            | 350           |
|    policy_gradient_loss | -0.000479     |
|    value_loss           | 55.6          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -621          |
| time/                   |               |
|    fps                  | 16            |
|    iterations           | 37            |
|    time_elapsed         | 4470          |
|    total_timesteps      | 75776         |
| train/                  |               |
|    approx_kl            | 0.00018144498 |
|    clip_fraction        | 0.00308       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0993       |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 25.6          |
|    n_updates            | 360           |
|    policy_gradient_loss | -0.000238     |
|    value_loss           | 67.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -615          |
| time/                   |               |
|    fps                  | 17            |
|    iterations           | 38            |
|    time_elapsed         | 4532          |
|    total_timesteps      | 77824         |
| train/                  |               |
|    approx_kl            | 0.00046841023 |
|    clip_fraction        | 0.00708       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0844       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 28.1          |
|    n_updates            | 370           |
|    policy_gradient_loss | -0.00034      |
|    value_loss           | 72            |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -613          |
| time/                   |               |
|    fps                  | 17            |
|    iterations           | 39            |
|    time_elapsed         | 4597          |
|    total_timesteps      | 79872         |
| train/                  |               |
|    approx_kl            | 0.00034929224 |
|    clip_fraction        | 0.00918       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.117        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 49.7          |
|    n_updates            | 380           |
|    policy_gradient_loss | 0.000191      |
|    value_loss           | 56.2          |
-------------------------------------------
Num timesteps: 80000
Best mean reward: -629.85 - Last mean reward per episode: -610.94
Saving new best model at 79873 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -611          |
| time/                   |               |
|    fps                  | 17            |
|    iterations           | 40            |
|    time_elapsed         | 4658          |
|    total_timesteps      | 81920         |
| train/                  |               |
|    approx_kl            | 0.00062100054 |
|    clip_fraction        | 0.00859       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.114        |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 32.7          |
|    n_updates            | 390           |
|    policy_gradient_loss | -9.41e-05     |
|    value_loss           | 53.7          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -611          |
| time/                   |               |
|    fps                  | 17            |
|    iterations           | 41            |
|    time_elapsed         | 4718          |
|    total_timesteps      | 83968         |
| train/                  |               |
|    approx_kl            | 0.00088627904 |
|    clip_fraction        | 0.00498       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.115        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 25.8          |
|    n_updates            | 400           |
|    policy_gradient_loss | 1.93e-05      |
|    value_loss           | 52.3          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -614          |
| time/                   |               |
|    fps                  | 17            |
|    iterations           | 42            |
|    time_elapsed         | 4779          |
|    total_timesteps      | 86016         |
| train/                  |               |
|    approx_kl            | 0.00021431941 |
|    clip_fraction        | 0.00322       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0897       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 28.3          |
|    n_updates            | 410           |
|    policy_gradient_loss | 0.000153      |
|    value_loss           | 35.8          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -614          |
| time/                   |               |
|    fps                  | 18            |
|    iterations           | 43            |
|    time_elapsed         | 4840          |
|    total_timesteps      | 88064         |
| train/                  |               |
|    approx_kl            | 0.00017313877 |
|    clip_fraction        | 0.0041        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0732       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 36.1          |
|    n_updates            | 420           |
|    policy_gradient_loss | -9.75e-05     |
|    value_loss           | 54.9          |
-------------------------------------------
Num timesteps: 90000
Best mean reward: -610.94 - Last mean reward per episode: -608.90
Saving new best model at 88065 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -609         |
| time/                   |              |
|    fps                  | 18           |
|    iterations           | 44           |
|    time_elapsed         | 4902         |
|    total_timesteps      | 90112        |
| train/                  |              |
|    approx_kl            | 3.631579e-05 |
|    clip_fraction        | 0.00161      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0653      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 32           |
|    n_updates            | 430          |
|    policy_gradient_loss | -8.98e-05    |
|    value_loss           | 65.8         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -608          |
| time/                   |               |
|    fps                  | 18            |
|    iterations           | 45            |
|    time_elapsed         | 4963          |
|    total_timesteps      | 92160         |
| train/                  |               |
|    approx_kl            | 0.00030608417 |
|    clip_fraction        | 0.00571       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0612       |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 35            |
|    n_updates            | 440           |
|    policy_gradient_loss | -0.000359     |
|    value_loss           | 54.1          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -604          |
| time/                   |               |
|    fps                  | 18            |
|    iterations           | 46            |
|    time_elapsed         | 5024          |
|    total_timesteps      | 94208         |
| train/                  |               |
|    approx_kl            | 1.6612263e-05 |
|    clip_fraction        | 0.00229       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0562       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 31.8          |
|    n_updates            | 450           |
|    policy_gradient_loss | -0.000113     |
|    value_loss           | 63.1          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -607          |
| time/                   |               |
|    fps                  | 18            |
|    iterations           | 47            |
|    time_elapsed         | 5083          |
|    total_timesteps      | 96256         |
| train/                  |               |
|    approx_kl            | 0.00031928997 |
|    clip_fraction        | 0.00317       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0639       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 26.3          |
|    n_updates            | 460           |
|    policy_gradient_loss | -6.02e-05     |
|    value_loss           | 54.6          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -606         |
| time/                   |              |
|    fps                  | 19           |
|    iterations           | 48           |
|    time_elapsed         | 5144         |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0002880248 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0586      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 19           |
|    n_updates            | 470          |
|    policy_gradient_loss | -9.87e-05    |
|    value_loss           | 45.7         |
------------------------------------------
Num timesteps: 100000
Best mean reward: -608.90 - Last mean reward per episode: -605.29
Saving new best model at 98308 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -605          |
| time/                   |               |
|    fps                  | 19            |
|    iterations           | 49            |
|    time_elapsed         | 5206          |
|    total_timesteps      | 100352        |
| train/                  |               |
|    approx_kl            | 0.00017585515 |
|    clip_fraction        | 0.00317       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0478       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 41.2          |
|    n_updates            | 480           |
|    policy_gradient_loss | -0.000329     |
|    value_loss           | 55.8          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -605          |
| time/                   |               |
|    fps                  | 19            |
|    iterations           | 50            |
|    time_elapsed         | 5269          |
|    total_timesteps      | 102400        |
| train/                  |               |
|    approx_kl            | 0.00019271663 |
|    clip_fraction        | 0.00298       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0434       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 18.6          |
|    n_updates            | 490           |
|    policy_gradient_loss | -0.000219     |
|    value_loss           | 49.3          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -605          |
| time/                   |               |
|    fps                  | 19            |
|    iterations           | 51            |
|    time_elapsed         | 5331          |
|    total_timesteps      | 104448        |
| train/                  |               |
|    approx_kl            | 0.00010870671 |
|    clip_fraction        | 0.00381       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0373       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 36.3          |
|    n_updates            | 500           |
|    policy_gradient_loss | -0.000177     |
|    value_loss           | 53.7          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -604          |
| time/                   |               |
|    fps                  | 19            |
|    iterations           | 52            |
|    time_elapsed         | 5394          |
|    total_timesteps      | 106496        |
| train/                  |               |
|    approx_kl            | 0.00020985081 |
|    clip_fraction        | 0.00435       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0321       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 20.6          |
|    n_updates            | 510           |
|    policy_gradient_loss | -0.000326     |
|    value_loss           | 56.8          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -604          |
| time/                   |               |
|    fps                  | 19            |
|    iterations           | 53            |
|    time_elapsed         | 5458          |
|    total_timesteps      | 108544        |
| train/                  |               |
|    approx_kl            | 1.1314871e-05 |
|    clip_fraction        | 0.00146       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.026        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 22.2          |
|    n_updates            | 520           |
|    policy_gradient_loss | -2.99e-05     |
|    value_loss           | 53.9          |
-------------------------------------------
Num timesteps: 110000
Best mean reward: -605.29 - Last mean reward per episode: -597.50
Saving new best model at 108575 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -598          |
| time/                   |               |
|    fps                  | 20            |
|    iterations           | 54            |
|    time_elapsed         | 5522          |
|    total_timesteps      | 110592        |
| train/                  |               |
|    approx_kl            | 0.00011347799 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0227       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 31.3          |
|    n_updates            | 530           |
|    policy_gradient_loss | 3.16e-05      |
|    value_loss           | 74.3          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -600         |
| time/                   |              |
|    fps                  | 20           |
|    iterations           | 55           |
|    time_elapsed         | 5585         |
|    total_timesteps      | 112640       |
| train/                  |              |
|    approx_kl            | 7.345778e-05 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0224      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 25.5         |
|    n_updates            | 540          |
|    policy_gradient_loss | 0.000102     |
|    value_loss           | 35.8         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -603          |
| time/                   |               |
|    fps                  | 20            |
|    iterations           | 56            |
|    time_elapsed         | 5649          |
|    total_timesteps      | 114688        |
| train/                  |               |
|    approx_kl            | 0.00021394712 |
|    clip_fraction        | 0.00127       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0142       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 10.1          |
|    n_updates            | 550           |
|    policy_gradient_loss | -0.000145     |
|    value_loss           | 34            |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -605          |
| time/                   |               |
|    fps                  | 20            |
|    iterations           | 57            |
|    time_elapsed         | 5713          |
|    total_timesteps      | 116736        |
| train/                  |               |
|    approx_kl            | 4.8605434e-05 |
|    clip_fraction        | 0.00107       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0157       |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 30.6          |
|    n_updates            | 560           |
|    policy_gradient_loss | -9e-05        |
|    value_loss           | 42.8          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -605          |
| time/                   |               |
|    fps                  | 20            |
|    iterations           | 58            |
|    time_elapsed         | 5780          |
|    total_timesteps      | 118784        |
| train/                  |               |
|    approx_kl            | 4.2053434e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0146       |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 37.3          |
|    n_updates            | 570           |
|    policy_gradient_loss | -2.36e-05     |
|    value_loss           | 55.3          |
-------------------------------------------
Num timesteps: 120000
Best mean reward: -597.50 - Last mean reward per episode: -606.09
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -606          |
| time/                   |               |
|    fps                  | 20            |
|    iterations           | 59            |
|    time_elapsed         | 5845          |
|    total_timesteps      | 120832        |
| train/                  |               |
|    approx_kl            | 8.4377825e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0147       |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 30.1          |
|    n_updates            | 580           |
|    policy_gradient_loss | -8.16e-06     |
|    value_loss           | 43.7          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -599          |
| time/                   |               |
|    fps                  | 20            |
|    iterations           | 60            |
|    time_elapsed         | 5915          |
|    total_timesteps      | 122880        |
| train/                  |               |
|    approx_kl            | 1.1635246e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0116       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 33            |
|    n_updates            | 590           |
|    policy_gradient_loss | 1.38e-06      |
|    value_loss           | 77.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -596          |
| time/                   |               |
|    fps                  | 20            |
|    iterations           | 61            |
|    time_elapsed         | 5984          |
|    total_timesteps      | 124928        |
| train/                  |               |
|    approx_kl            | 1.4094665e-05 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0123       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 22.5          |
|    n_updates            | 600           |
|    policy_gradient_loss | 5.67e-06      |
|    value_loss           | 66.7          |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -597        |
| time/                   |             |
|    fps                  | 20          |
|    iterations           | 62          |
|    time_elapsed         | 6047        |
|    total_timesteps      | 126976      |
| train/                  |             |
|    approx_kl            | 0.009221764 |
|    clip_fraction        | 0.0105      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0801     |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 32.7        |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00244    |
|    value_loss           | 45.8        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -591        |
| time/                   |             |
|    fps                  | 21          |
|    iterations           | 63          |
|    time_elapsed         | 6118        |
|    total_timesteps      | 129024      |
| train/                  |             |
|    approx_kl            | 0.003864578 |
|    clip_fraction        | 0.0292      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.285      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 48.4        |
|    n_updates            | 620         |
|    policy_gradient_loss | 0.00194     |
|    value_loss           | 77.6        |
-----------------------------------------
Num timesteps: 130000
Best mean reward: -597.50 - Last mean reward per episode: -587.47
Saving new best model at 129041 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -587        |
| time/                   |             |
|    fps                  | 21          |
|    iterations           | 64          |
|    time_elapsed         | 6187        |
|    total_timesteps      | 131072      |
| train/                  |             |
|    approx_kl            | 0.012298549 |
|    clip_fraction        | 0.00947     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.201      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 28.2        |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.000131   |
|    value_loss           | 65.3        |
-----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -588          |
| time/                   |               |
|    fps                  | 21            |
|    iterations           | 65            |
|    time_elapsed         | 6256          |
|    total_timesteps      | 133120        |
| train/                  |               |
|    approx_kl            | 0.00032842887 |
|    clip_fraction        | 0.0174        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.28         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 26.5          |
|    n_updates            | 640           |
|    policy_gradient_loss | 0.000609      |
|    value_loss           | 51.6          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -588         |
| time/                   |              |
|    fps                  | 21           |
|    iterations           | 66           |
|    time_elapsed         | 6321         |
|    total_timesteps      | 135168       |
| train/                  |              |
|    approx_kl            | 0.0012720982 |
|    clip_fraction        | 0.0211       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.264       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 25.5         |
|    n_updates            | 650          |
|    policy_gradient_loss | 0.000175     |
|    value_loss           | 49.8         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -590         |
| time/                   |              |
|    fps                  | 21           |
|    iterations           | 67           |
|    time_elapsed         | 6385         |
|    total_timesteps      | 137216       |
| train/                  |              |
|    approx_kl            | 0.0026828228 |
|    clip_fraction        | 0.0387       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.27        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 19.2         |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.00196     |
|    value_loss           | 42.2         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -590          |
| time/                   |               |
|    fps                  | 21            |
|    iterations           | 68            |
|    time_elapsed         | 6449          |
|    total_timesteps      | 139264        |
| train/                  |               |
|    approx_kl            | 0.00061298895 |
|    clip_fraction        | 0.0173        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.307        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 21.6          |
|    n_updates            | 670           |
|    policy_gradient_loss | 0.000695      |
|    value_loss           | 50.2          |
-------------------------------------------
Num timesteps: 140000
Best mean reward: -587.47 - Last mean reward per episode: -588.60
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -589         |
| time/                   |              |
|    fps                  | 21           |
|    iterations           | 69           |
|    time_elapsed         | 6512         |
|    total_timesteps      | 141312       |
| train/                  |              |
|    approx_kl            | 0.0018019264 |
|    clip_fraction        | 0.031        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.235       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 30.4         |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.00167     |
|    value_loss           | 60.5         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -589         |
| time/                   |              |
|    fps                  | 21           |
|    iterations           | 70           |
|    time_elapsed         | 6572         |
|    total_timesteps      | 143360       |
| train/                  |              |
|    approx_kl            | 0.0009871599 |
|    clip_fraction        | 0.0173       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.218       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 34.6         |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.000313    |
|    value_loss           | 52.2         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -589         |
| time/                   |              |
|    fps                  | 21           |
|    iterations           | 71           |
|    time_elapsed         | 6635         |
|    total_timesteps      | 145408       |
| train/                  |              |
|    approx_kl            | 0.0004312006 |
|    clip_fraction        | 0.0126       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.167       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 42.2         |
|    n_updates            | 700          |
|    policy_gradient_loss | -3.66e-05    |
|    value_loss           | 60           |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -589          |
| time/                   |               |
|    fps                  | 22            |
|    iterations           | 72            |
|    time_elapsed         | 6698          |
|    total_timesteps      | 147456        |
| train/                  |               |
|    approx_kl            | 0.00061681576 |
|    clip_fraction        | 0.0229        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.271        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 12.5          |
|    n_updates            | 710           |
|    policy_gradient_loss | -0.000588     |
|    value_loss           | 49.9          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -584         |
| time/                   |              |
|    fps                  | 22           |
|    iterations           | 73           |
|    time_elapsed         | 6767         |
|    total_timesteps      | 149504       |
| train/                  |              |
|    approx_kl            | 0.0016987423 |
|    clip_fraction        | 0.0157       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.227       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 47.9         |
|    n_updates            | 720          |
|    policy_gradient_loss | -0.00015     |
|    value_loss           | 78.3         |
------------------------------------------
Num timesteps: 150000
Best mean reward: -587.47 - Last mean reward per episode: -584.77
Saving new best model at 149517 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -585         |
| time/                   |              |
|    fps                  | 22           |
|    iterations           | 74           |
|    time_elapsed         | 6835         |
|    total_timesteps      | 151552       |
| train/                  |              |
|    approx_kl            | 0.0008243259 |
|    clip_fraction        | 0.0209       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.291       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 22.2         |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.000554    |
|    value_loss           | 44.5         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -588         |
| time/                   |              |
|    fps                  | 22           |
|    iterations           | 75           |
|    time_elapsed         | 6900         |
|    total_timesteps      | 153600       |
| train/                  |              |
|    approx_kl            | 0.0018778024 |
|    clip_fraction        | 0.0229       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.289       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 13.9         |
|    n_updates            | 740          |
|    policy_gradient_loss | -0.000199    |
|    value_loss           | 33           |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -585          |
| time/                   |               |
|    fps                  | 22            |
|    iterations           | 76            |
|    time_elapsed         | 6963          |
|    total_timesteps      | 155648        |
| train/                  |               |
|    approx_kl            | 0.00054104207 |
|    clip_fraction        | 0.00728       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.231        |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 43            |
|    n_updates            | 750           |
|    policy_gradient_loss | 0.000355      |
|    value_loss           | 67.8          |
-------------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 290, in <module>
    model.learn(total_timesteps=1000000, callback=[WandbCallback(), save_callback])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 313, in learn
    self.train()
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 217, in train
    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 730, in evaluate_actions
    features = self.extract_features(obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/policy.py", line 45, in forward
    obj_emb = self.model(pyg_data.x_dict, pyg_data.edge_index_dict, pyg_data.batch_dict)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_gnn.py", line 104, in forward
    self.layer(x_dict, edge_index_dict)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_gnn.py", line 79, in layer
    out = self.obj_to_atom(x_dict, edge_index_dict)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_message_passing.py", line 74, in forward
    out = self._internal_forward(x, edge_index_dict[edge_type], edge_type)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_message_passing.py", line 121, in _internal_forward
    out = self.simple(x, edge_index)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/conv/simple_conv.py", line 83, in forward
    out = self.propagate(edge_index, x=x, edge_weight=edge_weight,
  File "/var/folders/my/7z0rbf091qj03p882sd10h_00000gn/T/torch_geometric.nn.conv.simple_conv_SimpleConv_propagate_l04xnb0o.py", line 230, in propagate
    out = self.aggregate(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py", line 625, in aggregate
    return self.aggr_module(inputs, index, ptr=ptr, dim_size=dim_size,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/experimental.py", line 117, in wrapper
    return func(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/aggr/base.py", line 128, in __call__
    return super().__call__(x, index=index, ptr=ptr, dim_size=dim_size,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/aggr/basic.py", line 22, in forward
    return self.reduce(x, index, ptr, dim_size, dim, reduce='sum')
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/aggr/base.py", line 182, in reduce
    return scatter(x, index, dim, dim_size, reduce)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/utils/_scatter.py", line 75, in scatter
    return src.new_zeros(size).scatter_add_(dim, index, src)
KeyboardInterrupt