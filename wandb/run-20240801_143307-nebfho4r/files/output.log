
Using cpu device
-----------------------------
| time/              |      |
|    fps             | 119  |
|    iterations      | 1    |
|    time_elapsed    | 17   |
|    total_timesteps | 2048 |
-----------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -1.02e+03   |
| time/                   |             |
|    fps                  | 49          |
|    iterations           | 2           |
|    time_elapsed         | 82          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.011150947 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | -0.000153   |
|    learning_rate        | 0.0003      |
|    loss                 | 0.374       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.003      |
|    value_loss           | 3.19        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -964        |
| time/                   |             |
|    fps                  | 41          |
|    iterations           | 3           |
|    time_elapsed         | 146         |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.011879912 |
|    clip_fraction        | 0.0214      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 9.12        |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00141    |
|    value_loss           | 23.2        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -924        |
| time/                   |             |
|    fps                  | 38          |
|    iterations           | 4           |
|    time_elapsed         | 210         |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.016032599 |
|    clip_fraction        | 0.0505      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 14.5        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00328    |
|    value_loss           | 25.9        |
-----------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -844.12
Saving new best model at 8193 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -844        |
| time/                   |             |
|    fps                  | 37          |
|    iterations           | 5           |
|    time_elapsed         | 275         |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.018864011 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.912      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 27.6        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00725    |
|    value_loss           | 42.2        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -796        |
| time/                   |             |
|    fps                  | 36          |
|    iterations           | 6           |
|    time_elapsed         | 339         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.005126774 |
|    clip_fraction        | 0.0503      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.811      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 31.4        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00182    |
|    value_loss           | 43.4        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -779         |
| time/                   |              |
|    fps                  | 35           |
|    iterations           | 7            |
|    time_elapsed         | 402          |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.0072632395 |
|    clip_fraction        | 0.0903       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.744       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 6.31         |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00561     |
|    value_loss           | 36.9         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -737         |
| time/                   |              |
|    fps                  | 35           |
|    iterations           | 8            |
|    time_elapsed         | 466          |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 0.0025797593 |
|    clip_fraction        | 0.0326       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.664       |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 18.3         |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00196     |
|    value_loss           | 53.1         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -690         |
| time/                   |              |
|    fps                  | 34           |
|    iterations           | 9            |
|    time_elapsed         | 534          |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0027132179 |
|    clip_fraction        | 0.017        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.674       |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 20.9         |
|    n_updates            | 80           |
|    policy_gradient_loss | 2.27e-05     |
|    value_loss           | 57.6         |
------------------------------------------
Num timesteps: 20000
Best mean reward: -844.12 - Last mean reward per episode: -647.39
Saving new best model at 18433 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -647         |
| time/                   |              |
|    fps                  | 34           |
|    iterations           | 10           |
|    time_elapsed         | 595          |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.0034004755 |
|    clip_fraction        | 0.0433       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.615       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 29.6         |
|    n_updates            | 90           |
|    policy_gradient_loss | 0.000432     |
|    value_loss           | 65.5         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -622         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 11           |
|    time_elapsed         | 664          |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0041790223 |
|    clip_fraction        | 0.0447       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.536       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 43.4         |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00175     |
|    value_loss           | 59           |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -574         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 12           |
|    time_elapsed         | 727          |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0036103951 |
|    clip_fraction        | 0.0523       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.452       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 45.7         |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00328     |
|    value_loss           | 75.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -552         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 13           |
|    time_elapsed         | 788          |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.0016533469 |
|    clip_fraction        | 0.0254       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.401       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 46.8         |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00215     |
|    value_loss           | 65.6         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -519         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 14           |
|    time_elapsed         | 848          |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0018854889 |
|    clip_fraction        | 0.0273       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.339       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 41.3         |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00204     |
|    value_loss           | 74.8         |
------------------------------------------
Num timesteps: 30000
Best mean reward: -647.39 - Last mean reward per episode: -504.18
Saving new best model at 29177 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.08e+03     |
|    ep_rew_mean          | -504         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 15           |
|    time_elapsed         | 904          |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.0025216339 |
|    clip_fraction        | 0.0251       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.296       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 30.2         |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00146     |
|    value_loss           | 66.4         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.08e+03     |
|    ep_rew_mean          | -484         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 16           |
|    time_elapsed         | 965          |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0015724732 |
|    clip_fraction        | 0.0219       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.234       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 33.2         |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00151     |
|    value_loss           | 77.9         |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.07e+03   |
|    ep_rew_mean          | -460       |
| time/                   |            |
|    fps                  | 33         |
|    iterations           | 17         |
|    time_elapsed         | 1025       |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.00096672 |
|    clip_fraction        | 0.0202     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.224     |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 36.6       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.000842  |
|    value_loss           | 70         |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.06e+03     |
|    ep_rew_mean          | -453         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 18           |
|    time_elapsed         | 1086         |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.0026308335 |
|    clip_fraction        | 0.00937      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.221       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 30.8         |
|    n_updates            | 170          |
|    policy_gradient_loss | 0.00014      |
|    value_loss           | 69.8         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -442          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 19            |
|    time_elapsed         | 1149          |
|    total_timesteps      | 38912         |
| train/                  |               |
|    approx_kl            | 0.00061305636 |
|    clip_fraction        | 0.00742       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.214        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 56.6          |
|    n_updates            | 180           |
|    policy_gradient_loss | -0.000539     |
|    value_loss           | 68.9          |
-------------------------------------------
Num timesteps: 40000
Best mean reward: -504.18 - Last mean reward per episode: -436.66
Saving new best model at 38913 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -437          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 20            |
|    time_elapsed         | 1212          |
|    total_timesteps      | 40960         |
| train/                  |               |
|    approx_kl            | 0.00050342537 |
|    clip_fraction        | 0.0189        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.229        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 34.7          |
|    n_updates            | 190           |
|    policy_gradient_loss | -0.000696     |
|    value_loss           | 68            |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -430         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 21           |
|    time_elapsed         | 1274         |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0011993821 |
|    clip_fraction        | 0.0188       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.2         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 29.2         |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.000903    |
|    value_loss           | 69.5         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -420          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 22            |
|    time_elapsed         | 1335          |
|    total_timesteps      | 45056         |
| train/                  |               |
|    approx_kl            | 0.00086382625 |
|    clip_fraction        | 0.00937       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.178        |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 34.6          |
|    n_updates            | 210           |
|    policy_gradient_loss | -0.00078      |
|    value_loss           | 70.6          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -405         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 23           |
|    time_elapsed         | 1395         |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0003436759 |
|    clip_fraction        | 0.00342      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.155       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 43.5         |
|    n_updates            | 220          |
|    policy_gradient_loss | -4.24e-05    |
|    value_loss           | 77.9         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -393          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 24            |
|    time_elapsed         | 1455          |
|    total_timesteps      | 49152         |
| train/                  |               |
|    approx_kl            | 0.00060819875 |
|    clip_fraction        | 0.00688       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.155        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 28.1          |
|    n_updates            | 230           |
|    policy_gradient_loss | 0.000253      |
|    value_loss           | 78            |
-------------------------------------------
Num timesteps: 50000
Best mean reward: -436.66 - Last mean reward per episode: -390.27
Saving new best model at 49153 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -390         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 25           |
|    time_elapsed         | 1517         |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 9.152916e-05 |
|    clip_fraction        | 0.00669      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.188       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 47.5         |
|    n_updates            | 240          |
|    policy_gradient_loss | 8.46e-05     |
|    value_loss           | 66.5         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -377          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 26            |
|    time_elapsed         | 1578          |
|    total_timesteps      | 53248         |
| train/                  |               |
|    approx_kl            | 0.00024469598 |
|    clip_fraction        | 0.00933       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.142        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 45.9          |
|    n_updates            | 250           |
|    policy_gradient_loss | -0.000152     |
|    value_loss           | 78.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -376          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 27            |
|    time_elapsed         | 1638          |
|    total_timesteps      | 55296         |
| train/                  |               |
|    approx_kl            | 0.00040071522 |
|    clip_fraction        | 0.0112        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.169        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 37.3          |
|    n_updates            | 260           |
|    policy_gradient_loss | -0.000135     |
|    value_loss           | 65.6          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -366          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 28            |
|    time_elapsed         | 1699          |
|    total_timesteps      | 57344         |
| train/                  |               |
|    approx_kl            | 0.00021779825 |
|    clip_fraction        | 0.00684       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.171        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 40.5          |
|    n_updates            | 270           |
|    policy_gradient_loss | 0.00011       |
|    value_loss           | 79.2          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -359         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 29           |
|    time_elapsed         | 1761         |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0008905225 |
|    clip_fraction        | 0.0226       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.14        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 35.7         |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00162     |
|    value_loss           | 76.2         |
------------------------------------------
Num timesteps: 60000
Best mean reward: -390.27 - Last mean reward per episode: -359.88
Saving new best model at 59393 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -360          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 30            |
|    time_elapsed         | 1824          |
|    total_timesteps      | 61440         |
| train/                  |               |
|    approx_kl            | 0.00040798206 |
|    clip_fraction        | 0.0147        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.119        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 25.7          |
|    n_updates            | 290           |
|    policy_gradient_loss | -0.000901     |
|    value_loss           | 61.1          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -350         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 31           |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.0003775918 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.115       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 25.9         |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.000154    |
|    value_loss           | 81.2         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -347          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 32            |
|    time_elapsed         | 1945          |
|    total_timesteps      | 65536         |
| train/                  |               |
|    approx_kl            | 0.00023680099 |
|    clip_fraction        | 0.0061        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.12         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 39.4          |
|    n_updates            | 310           |
|    policy_gradient_loss | -0.000332     |
|    value_loss           | 69.8          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -341          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 33            |
|    time_elapsed         | 2008          |
|    total_timesteps      | 67584         |
| train/                  |               |
|    approx_kl            | 0.00046634788 |
|    clip_fraction        | 0.00762       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.108        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 54.6          |
|    n_updates            | 320           |
|    policy_gradient_loss | -0.000471     |
|    value_loss           | 74.1          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -336          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 34            |
|    time_elapsed         | 2069          |
|    total_timesteps      | 69632         |
| train/                  |               |
|    approx_kl            | 0.00046917898 |
|    clip_fraction        | 0.00557       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0973       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 44.7          |
|    n_updates            | 330           |
|    policy_gradient_loss | -9.6e-05      |
|    value_loss           | 80.8          |
-------------------------------------------
Num timesteps: 70000
Best mean reward: -359.88 - Last mean reward per episode: -328.72
Saving new best model at 69633 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -329          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 35            |
|    time_elapsed         | 2130          |
|    total_timesteps      | 71680         |
| train/                  |               |
|    approx_kl            | 0.00083168014 |
|    clip_fraction        | 0.0104        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.106        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 43.6          |
|    n_updates            | 340           |
|    policy_gradient_loss | -0.000101     |
|    value_loss           | 78.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -325          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 36            |
|    time_elapsed         | 2191          |
|    total_timesteps      | 73728         |
| train/                  |               |
|    approx_kl            | 0.00012589945 |
|    clip_fraction        | 0.00391       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.105        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 29.3          |
|    n_updates            | 350           |
|    policy_gradient_loss | -7.4e-05      |
|    value_loss           | 69.1          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -320          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 37            |
|    time_elapsed         | 2252          |
|    total_timesteps      | 75776         |
| train/                  |               |
|    approx_kl            | 0.00032678212 |
|    clip_fraction        | 0.00547       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0921       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 32.8          |
|    n_updates            | 360           |
|    policy_gradient_loss | -0.000326     |
|    value_loss           | 80.3          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -317          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 38            |
|    time_elapsed         | 2313          |
|    total_timesteps      | 77824         |
| train/                  |               |
|    approx_kl            | 0.00078232965 |
|    clip_fraction        | 0.00605       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0793       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 35.2          |
|    n_updates            | 370           |
|    policy_gradient_loss | -0.000118     |
|    value_loss           | 71.6          |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 2.05e+03       |
|    ep_rew_mean          | -312           |
| time/                   |                |
|    fps                  | 33             |
|    iterations           | 39             |
|    time_elapsed         | 2375           |
|    total_timesteps      | 79872          |
| train/                  |                |
|    approx_kl            | 0.000119386474 |
|    clip_fraction        | 0.00459        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0806        |
|    explained_variance   | 0              |
|    learning_rate        | 0.0003         |
|    loss                 | 33.7           |
|    n_updates            | 380            |
|    policy_gradient_loss | -1.35e-05      |
|    value_loss           | 84             |
--------------------------------------------
Num timesteps: 80000
Best mean reward: -328.72 - Last mean reward per episode: -310.17
Saving new best model at 79873 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -310          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 40            |
|    time_elapsed         | 2437          |
|    total_timesteps      | 81920         |
| train/                  |               |
|    approx_kl            | 0.00015333909 |
|    clip_fraction        | 0.00464       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.058        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 33.6          |
|    n_updates            | 390           |
|    policy_gradient_loss | -6.46e-06     |
|    value_loss           | 67.8          |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -306        |
| time/                   |             |
|    fps                  | 33          |
|    iterations           | 41          |
|    time_elapsed         | 2497        |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.000224532 |
|    clip_fraction        | 0.00547     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.051      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 42.7        |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.000471   |
|    value_loss           | 76.1        |
-----------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 2.05e+03       |
|    ep_rew_mean          | -306           |
| time/                   |                |
|    fps                  | 33             |
|    iterations           | 42             |
|    time_elapsed         | 2560           |
|    total_timesteps      | 86016          |
| train/                  |                |
|    approx_kl            | 0.000113267946 |
|    clip_fraction        | 0.000439       |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0475        |
|    explained_variance   | 0              |
|    learning_rate        | 0.0003         |
|    loss                 | 21.5           |
|    n_updates            | 410            |
|    policy_gradient_loss | 5.75e-05       |
|    value_loss           | 69.9           |
--------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -302          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 43            |
|    time_elapsed         | 2621          |
|    total_timesteps      | 88064         |
| train/                  |               |
|    approx_kl            | 7.1923976e-05 |
|    clip_fraction        | 0.00352       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0375       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 40            |
|    n_updates            | 420           |
|    policy_gradient_loss | -0.000411     |
|    value_loss           | 82.1          |
-------------------------------------------
Num timesteps: 90000
Best mean reward: -310.17 - Last mean reward per episode: -301.22
Saving new best model at 88065 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -301          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 44            |
|    time_elapsed         | 2681          |
|    total_timesteps      | 90112         |
| train/                  |               |
|    approx_kl            | 0.00010320361 |
|    clip_fraction        | 0.00239       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0317       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 22            |
|    n_updates            | 430           |
|    policy_gradient_loss | -7.9e-05      |
|    value_loss           | 69.7          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -301          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 45            |
|    time_elapsed         | 2744          |
|    total_timesteps      | 92160         |
| train/                  |               |
|    approx_kl            | 0.00020998379 |
|    clip_fraction        | 0.00205       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0276       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 34.5          |
|    n_updates            | 440           |
|    policy_gradient_loss | -0.000135     |
|    value_loss           | 72.1          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -293         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 46           |
|    time_elapsed         | 2806         |
|    total_timesteps      | 94208        |
| train/                  |              |
|    approx_kl            | 4.488675e-05 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0234      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 42.7         |
|    n_updates            | 450          |
|    policy_gradient_loss | -2.5e-05     |
|    value_loss           | 87.4         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -293         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 47           |
|    time_elapsed         | 2870         |
|    total_timesteps      | 96256        |
| train/                  |              |
|    approx_kl            | 0.0006495168 |
|    clip_fraction        | 0.00244      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0293      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 35.6         |
|    n_updates            | 460          |
|    policy_gradient_loss | 2.39e-05     |
|    value_loss           | 71.5         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -287        |
| time/                   |             |
|    fps                  | 33          |
|    iterations           | 48          |
|    time_elapsed         | 2931        |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 9.26122e-05 |
|    clip_fraction        | 0.00166     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0336     |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 42.3        |
|    n_updates            | 470         |
|    policy_gradient_loss | 0.000157    |
|    value_loss           | 82.2        |
-----------------------------------------
Num timesteps: 100000
Best mean reward: -301.22 - Last mean reward per episode: -285.89
Saving new best model at 98305 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -286         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 49           |
|    time_elapsed         | 2995         |
|    total_timesteps      | 100352       |
| train/                  |              |
|    approx_kl            | 7.902307e-05 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0304      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 50.2         |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.000127    |
|    value_loss           | 71.2         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -281          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 50            |
|    time_elapsed         | 3056          |
|    total_timesteps      | 102400        |
| train/                  |               |
|    approx_kl            | 1.8470077e-05 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0389       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 41.1          |
|    n_updates            | 490           |
|    policy_gradient_loss | 6.69e-06      |
|    value_loss           | 80            |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -277          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 51            |
|    time_elapsed         | 3117          |
|    total_timesteps      | 104448        |
| train/                  |               |
|    approx_kl            | 0.00033023217 |
|    clip_fraction        | 0.00527       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0225       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 36.9          |
|    n_updates            | 500           |
|    policy_gradient_loss | -0.000393     |
|    value_loss           | 77.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -278          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 52            |
|    time_elapsed         | 3182          |
|    total_timesteps      | 106496        |
| train/                  |               |
|    approx_kl            | 0.00023443229 |
|    clip_fraction        | 0.00215       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0287       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 34.8          |
|    n_updates            | 510           |
|    policy_gradient_loss | -8.14e-05     |
|    value_loss           | 67.7          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -276          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 53            |
|    time_elapsed         | 3245          |
|    total_timesteps      | 108544        |
| train/                  |               |
|    approx_kl            | 0.00010954877 |
|    clip_fraction        | 0.0021        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0364       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 38.5          |
|    n_updates            | 520           |
|    policy_gradient_loss | -6.67e-05     |
|    value_loss           | 75.2          |
-------------------------------------------
Num timesteps: 110000
Best mean reward: -285.89 - Last mean reward per episode: -274.58
Saving new best model at 108545 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -275        |
| time/                   |             |
|    fps                  | 33          |
|    iterations           | 54          |
|    time_elapsed         | 3308        |
|    total_timesteps      | 110592      |
| train/                  |             |
|    approx_kl            | 9.91924e-05 |
|    clip_fraction        | 0.00132     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0383     |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 37.5        |
|    n_updates            | 530         |
|    policy_gradient_loss | 2.36e-05    |
|    value_loss           | 70.5        |
-----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 55            |
|    time_elapsed         | 3370          |
|    total_timesteps      | 112640        |
| train/                  |               |
|    approx_kl            | 0.00025383517 |
|    clip_fraction        | 0.0043        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.044        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 46.3          |
|    n_updates            | 540           |
|    policy_gradient_loss | -4.92e-05     |
|    value_loss           | 73.7          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -273          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 56            |
|    time_elapsed         | 3432          |
|    total_timesteps      | 114688        |
| train/                  |               |
|    approx_kl            | 0.00024844613 |
|    clip_fraction        | 0.00537       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0309       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 48.1          |
|    n_updates            | 550           |
|    policy_gradient_loss | -0.000278     |
|    value_loss           | 73.9          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -271         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 57           |
|    time_elapsed         | 3496         |
|    total_timesteps      | 116736       |
| train/                  |              |
|    approx_kl            | 0.0002059261 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0278      |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 37.2         |
|    n_updates            | 560          |
|    policy_gradient_loss | -0.00025     |
|    value_loss           | 78.4         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -268          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 58            |
|    time_elapsed         | 3559          |
|    total_timesteps      | 118784        |
| train/                  |               |
|    approx_kl            | 2.1115673e-05 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0257       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 37.4          |
|    n_updates            | 570           |
|    policy_gradient_loss | 1.77e-05      |
|    value_loss           | 75.4          |
-------------------------------------------
Num timesteps: 120000
Best mean reward: -274.58 - Last mean reward per episode: -269.35
Saving new best model at 118785 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -269          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 59            |
|    time_elapsed         | 3621          |
|    total_timesteps      | 120832        |
| train/                  |               |
|    approx_kl            | 0.00019607315 |
|    clip_fraction        | 0.00146       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0374       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 36.4          |
|    n_updates            | 580           |
|    policy_gradient_loss | -0.000107     |
|    value_loss           | 67            |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -266         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 60           |
|    time_elapsed         | 3683         |
|    total_timesteps      | 122880       |
| train/                  |              |
|    approx_kl            | 7.824146e-05 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0329      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 55.8         |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.000114    |
|    value_loss           | 80.7         |
------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 2.05e+03       |
|    ep_rew_mean          | -266           |
| time/                   |                |
|    fps                  | 33             |
|    iterations           | 61             |
|    time_elapsed         | 3745           |
|    total_timesteps      | 124928         |
| train/                  |                |
|    approx_kl            | 0.000111151894 |
|    clip_fraction        | 0.00195        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0374        |
|    explained_variance   | 0              |
|    learning_rate        | 0.0003         |
|    loss                 | 38.3           |
|    n_updates            | 600            |
|    policy_gradient_loss | -0.000215      |
|    value_loss           | 74.7           |
--------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -263          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 62            |
|    time_elapsed         | 3806          |
|    total_timesteps      | 126976        |
| train/                  |               |
|    approx_kl            | 0.00010622837 |
|    clip_fraction        | 0.00181       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0337       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 50.8          |
|    n_updates            | 610           |
|    policy_gradient_loss | 0.000108      |
|    value_loss           | 79.1          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -260          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 63            |
|    time_elapsed         | 3870          |
|    total_timesteps      | 129024        |
| train/                  |               |
|    approx_kl            | 0.00011883769 |
|    clip_fraction        | 0.00264       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0342       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 36.8          |
|    n_updates            | 620           |
|    policy_gradient_loss | 3.5e-05       |
|    value_loss           | 78.5          |
-------------------------------------------
Num timesteps: 130000
Best mean reward: -269.35 - Last mean reward per episode: -258.29
Saving new best model at 129025 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -258          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 64            |
|    time_elapsed         | 3931          |
|    total_timesteps      | 131072        |
| train/                  |               |
|    approx_kl            | 0.00024501845 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0276       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 42.6          |
|    n_updates            | 630           |
|    policy_gradient_loss | -0.000407     |
|    value_loss           | 82.3          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -256          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 65            |
|    time_elapsed         | 3994          |
|    total_timesteps      | 133120        |
| train/                  |               |
|    approx_kl            | 3.6448357e-05 |
|    clip_fraction        | 0.00195       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0228       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 66.8          |
|    n_updates            | 640           |
|    policy_gradient_loss | -0.000137     |
|    value_loss           | 81.8          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -254          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 66            |
|    time_elapsed         | 4056          |
|    total_timesteps      | 135168        |
| train/                  |               |
|    approx_kl            | 6.5345084e-05 |
|    clip_fraction        | 0.00229       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0177       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 42.9          |
|    n_updates            | 650           |
|    policy_gradient_loss | -0.000253     |
|    value_loss           | 79.5          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -252          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 67            |
|    time_elapsed         | 4117          |
|    total_timesteps      | 137216        |
| train/                  |               |
|    approx_kl            | 6.8166526e-05 |
|    clip_fraction        | 0.00146       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0225       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 41.1          |
|    n_updates            | 660           |
|    policy_gradient_loss | 0.000103      |
|    value_loss           | 80.4          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -252         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 68           |
|    time_elapsed         | 4178         |
|    total_timesteps      | 139264       |
| train/                  |              |
|    approx_kl            | 1.093332e-05 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0246      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 26.8         |
|    n_updates            | 670          |
|    policy_gradient_loss | -1.51e-05    |
|    value_loss           | 70.5         |
------------------------------------------
Num timesteps: 140000
Best mean reward: -258.29 - Last mean reward per episode: -251.51
Saving new best model at 139265 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -252          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 69            |
|    time_elapsed         | 4243          |
|    total_timesteps      | 141312        |
| train/                  |               |
|    approx_kl            | 0.00017993592 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0216       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 31.6          |
|    n_updates            | 680           |
|    policy_gradient_loss | -0.000101     |
|    value_loss           | 73.5          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -250         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 70           |
|    time_elapsed         | 4305         |
|    total_timesteps      | 143360       |
| train/                  |              |
|    approx_kl            | 7.251697e-05 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0197      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 47.3         |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.000184    |
|    value_loss           | 74.5         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -252          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 71            |
|    time_elapsed         | 4365          |
|    total_timesteps      | 145408        |
| train/                  |               |
|    approx_kl            | 0.00027954616 |
|    clip_fraction        | 0.00186       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0187       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 30.8          |
|    n_updates            | 700           |
|    policy_gradient_loss | 2.58e-05      |
|    value_loss           | 65.5          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -251          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 72            |
|    time_elapsed         | 4426          |
|    total_timesteps      | 147456        |
| train/                  |               |
|    approx_kl            | 3.7915306e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.02         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 40.9          |
|    n_updates            | 710           |
|    policy_gradient_loss | -8.99e-07     |
|    value_loss           | 74.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -250          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 73            |
|    time_elapsed         | 4487          |
|    total_timesteps      | 149504        |
| train/                  |               |
|    approx_kl            | 2.6634167e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0186       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 47.2          |
|    n_updates            | 720           |
|    policy_gradient_loss | 2.43e-05      |
|    value_loss           | 76.1          |
-------------------------------------------
Num timesteps: 150000
Best mean reward: -251.51 - Last mean reward per episode: -248.12
Saving new best model at 149505 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -248          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 74            |
|    time_elapsed         | 4551          |
|    total_timesteps      | 151552        |
| train/                  |               |
|    approx_kl            | 0.00028840388 |
|    clip_fraction        | 0.00269       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0221       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 35.2          |
|    n_updates            | 730           |
|    policy_gradient_loss | -9.09e-05     |
|    value_loss           | 77.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -246          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 75            |
|    time_elapsed         | 4614          |
|    total_timesteps      | 153600        |
| train/                  |               |
|    approx_kl            | 3.6408019e-06 |
|    clip_fraction        | 0.00117       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0239       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 29.7          |
|    n_updates            | 740           |
|    policy_gradient_loss | -9.21e-05     |
|    value_loss           | 83.2          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -246         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 76           |
|    time_elapsed         | 4678         |
|    total_timesteps      | 155648       |
| train/                  |              |
|    approx_kl            | 7.686604e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0288      |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 48.2         |
|    n_updates            | 750          |
|    policy_gradient_loss | -0.00013     |
|    value_loss           | 73.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -244         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 77           |
|    time_elapsed         | 4741         |
|    total_timesteps      | 157696       |
| train/                  |              |
|    approx_kl            | 6.656963e-05 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0382      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 39.7         |
|    n_updates            | 760          |
|    policy_gradient_loss | -0.000146    |
|    value_loss           | 79.2         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -243         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 78           |
|    time_elapsed         | 4804         |
|    total_timesteps      | 159744       |
| train/                  |              |
|    approx_kl            | 3.849095e-05 |
|    clip_fraction        | 0.00229      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0397      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 40           |
|    n_updates            | 770          |
|    policy_gradient_loss | 1.91e-05     |
|    value_loss           | 75.7         |
------------------------------------------
Num timesteps: 160000
Best mean reward: -248.12 - Last mean reward per episode: -243.24
Saving new best model at 159745 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -243          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 79            |
|    time_elapsed         | 4866          |
|    total_timesteps      | 161792        |
| train/                  |               |
|    approx_kl            | 0.00028175642 |
|    clip_fraction        | 0.00161       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0331       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 38.6          |
|    n_updates            | 780           |
|    policy_gradient_loss | -8.86e-06     |
|    value_loss           | 71            |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -244          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 80            |
|    time_elapsed         | 4930          |
|    total_timesteps      | 163840        |
| train/                  |               |
|    approx_kl            | 0.00022472418 |
|    clip_fraction        | 0.00264       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0348       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 34.3          |
|    n_updates            | 790           |
|    policy_gradient_loss | 0.000111      |
|    value_loss           | 68.1          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -241         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 81           |
|    time_elapsed         | 4992         |
|    total_timesteps      | 165888       |
| train/                  |              |
|    approx_kl            | 2.717704e-05 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0359      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 24.2         |
|    n_updates            | 800          |
|    policy_gradient_loss | 3.01e-05     |
|    value_loss           | 81.9         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -244         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 82           |
|    time_elapsed         | 5053         |
|    total_timesteps      | 167936       |
| train/                  |              |
|    approx_kl            | 0.0003455048 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0295      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 27.7         |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.000237    |
|    value_loss           | 59.5         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -245        |
| time/                   |             |
|    fps                  | 33          |
|    iterations           | 83          |
|    time_elapsed         | 5115        |
|    total_timesteps      | 169984      |
| train/                  |             |
|    approx_kl            | 0.000127746 |
|    clip_fraction        | 0.00234     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0254     |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 23.5        |
|    n_updates            | 820         |
|    policy_gradient_loss | 0.000151    |
|    value_loss           | 66.6        |
-----------------------------------------
Num timesteps: 170000
Best mean reward: -243.24 - Last mean reward per episode: -242.92
Saving new best model at 169985 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -243          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 84            |
|    time_elapsed         | 5179          |
|    total_timesteps      | 172032        |
| train/                  |               |
|    approx_kl            | 3.5796547e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0197       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 39.1          |
|    n_updates            | 830           |
|    policy_gradient_loss | -6.36e-05     |
|    value_loss           | 80.6          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -240          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 85            |
|    time_elapsed         | 5242          |
|    total_timesteps      | 174080        |
| train/                  |               |
|    approx_kl            | 1.6044127e-05 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0196       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 47.5          |
|    n_updates            | 840           |
|    policy_gradient_loss | -2.28e-06     |
|    value_loss           | 83.5          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -242         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 86           |
|    time_elapsed         | 5303         |
|    total_timesteps      | 176128       |
| train/                  |              |
|    approx_kl            | 6.723218e-05 |
|    clip_fraction        | 0.00259      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.018       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 36.5         |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.000157    |
|    value_loss           | 64.2         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -242         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 87           |
|    time_elapsed         | 5365         |
|    total_timesteps      | 178176       |
| train/                  |              |
|    approx_kl            | 7.174566e-05 |
|    clip_fraction        | 0.00229      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0145      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 41.6         |
|    n_updates            | 860          |
|    policy_gradient_loss | -0.000122    |
|    value_loss           | 70.5         |
------------------------------------------
Num timesteps: 180000
Best mean reward: -242.92 - Last mean reward per episode: -242.63
Saving new best model at 178177 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -243          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 88            |
|    time_elapsed         | 5426          |
|    total_timesteps      | 180224        |
| train/                  |               |
|    approx_kl            | 4.4529326e-05 |
|    clip_fraction        | 0.00117       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0163       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 44.5          |
|    n_updates            | 870           |
|    policy_gradient_loss | -8.55e-05     |
|    value_loss           | 68            |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -240          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 89            |
|    time_elapsed         | 5489          |
|    total_timesteps      | 182272        |
| train/                  |               |
|    approx_kl            | 0.00020981446 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0163       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 52.7          |
|    n_updates            | 880           |
|    policy_gradient_loss | -7.89e-05     |
|    value_loss           | 86            |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -242          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 90            |
|    time_elapsed         | 5551          |
|    total_timesteps      | 184320        |
| train/                  |               |
|    approx_kl            | 3.5571982e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0185       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 22.2          |
|    n_updates            | 890           |
|    policy_gradient_loss | -0.000103     |
|    value_loss           | 64.1          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -240          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 91            |
|    time_elapsed         | 5612          |
|    total_timesteps      | 186368        |
| train/                  |               |
|    approx_kl            | 0.00038643807 |
|    clip_fraction        | 0.00117       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00896      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 41.3          |
|    n_updates            | 900           |
|    policy_gradient_loss | -0.00014      |
|    value_loss           | 77.6          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -239         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 92           |
|    time_elapsed         | 5675         |
|    total_timesteps      | 188416       |
| train/                  |              |
|    approx_kl            | 6.625778e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00745     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 41.3         |
|    n_updates            | 910          |
|    policy_gradient_loss | -1.16e-05    |
|    value_loss           | 77.6         |
------------------------------------------
Num timesteps: 190000
Best mean reward: -242.63 - Last mean reward per episode: -238.79
Saving new best model at 188417 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -239          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 93            |
|    time_elapsed         | 5736          |
|    total_timesteps      | 190464        |
| train/                  |               |
|    approx_kl            | 2.6614347e-05 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00804      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 33.1          |
|    n_updates            | 920           |
|    policy_gradient_loss | -0.000244     |
|    value_loss           | 68.1          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -238          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 94            |
|    time_elapsed         | 5798          |
|    total_timesteps      | 192512        |
| train/                  |               |
|    approx_kl            | 0.00011130457 |
|    clip_fraction        | 0.000635      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00904      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 29.1          |
|    n_updates            | 930           |
|    policy_gradient_loss | 0.00014       |
|    value_loss           | 77.6          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -237          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 95            |
|    time_elapsed         | 5859          |
|    total_timesteps      | 194560        |
| train/                  |               |
|    approx_kl            | 0.00011852439 |
|    clip_fraction        | 0.00142       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00822      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 28.7          |
|    n_updates            | 940           |
|    policy_gradient_loss | -0.00017      |
|    value_loss           | 82.1          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -235          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 96            |
|    time_elapsed         | 5922          |
|    total_timesteps      | 196608        |
| train/                  |               |
|    approx_kl            | 0.00018321417 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0138       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 38.4          |
|    n_updates            | 950           |
|    policy_gradient_loss | -0.000214     |
|    value_loss           | 84.5          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -235         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 97           |
|    time_elapsed         | 5985         |
|    total_timesteps      | 198656       |
| train/                  |              |
|    approx_kl            | 5.288949e-05 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0166      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 25.5         |
|    n_updates            | 960          |
|    policy_gradient_loss | 4.32e-05     |
|    value_loss           | 75.4         |
------------------------------------------
Num timesteps: 200000
Best mean reward: -238.79 - Last mean reward per episode: -235.65
Saving new best model at 198657 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -236          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 98            |
|    time_elapsed         | 6049          |
|    total_timesteps      | 200704        |
| train/                  |               |
|    approx_kl            | 3.8174534e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0159       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 29.3          |
|    n_updates            | 970           |
|    policy_gradient_loss | 7.38e-05      |
|    value_loss           | 69.6          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -235          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 99            |
|    time_elapsed         | 6110          |
|    total_timesteps      | 202752        |
| train/                  |               |
|    approx_kl            | 0.00020195369 |
|    clip_fraction        | 0.00166       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0122       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 38.5          |
|    n_updates            | 980           |
|    policy_gradient_loss | -8.03e-05     |
|    value_loss           | 74.6          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -236          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 100           |
|    time_elapsed         | 6171          |
|    total_timesteps      | 204800        |
| train/                  |               |
|    approx_kl            | 5.1979267e-05 |
|    clip_fraction        | 0.000928      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00799      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 31.4          |
|    n_updates            | 990           |
|    policy_gradient_loss | -0.0001       |
|    value_loss           | 65.7          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -236         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 101          |
|    time_elapsed         | 6231         |
|    total_timesteps      | 206848       |
| train/                  |              |
|    approx_kl            | 6.472244e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0112      |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 29.8         |
|    n_updates            | 1000         |
|    policy_gradient_loss | -7.82e-05    |
|    value_loss           | 76.7         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -230          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 102           |
|    time_elapsed         | 6293          |
|    total_timesteps      | 208896        |
| train/                  |               |
|    approx_kl            | 9.4062445e-05 |
|    clip_fraction        | 0.00132       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0134       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 27.7          |
|    n_updates            | 1010          |
|    policy_gradient_loss | 0.000109      |
|    value_loss           | 60.6          |
-------------------------------------------
Num timesteps: 210000
Best mean reward: -235.65 - Last mean reward per episode: -222.40
Saving new best model at 208897 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -222      |
| time/                   |           |
|    fps                  | 33        |
|    iterations           | 103       |
|    time_elapsed         | 6357      |
|    total_timesteps      | 210944    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00872  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 42.1      |
|    n_updates            | 1020      |
|    policy_gradient_loss | -2.78e-10 |
|    value_loss           | 75.9      |
---------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -215         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 104          |
|    time_elapsed         | 6418         |
|    total_timesteps      | 212992       |
| train/                  |              |
|    approx_kl            | 2.624339e-05 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00897     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 31.1         |
|    n_updates            | 1030         |
|    policy_gradient_loss | 3.82e-05     |
|    value_loss           | 77.6         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -210          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 105           |
|    time_elapsed         | 6480          |
|    total_timesteps      | 215040        |
| train/                  |               |
|    approx_kl            | 0.00043402423 |
|    clip_fraction        | 0.00171       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0147       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 41.6          |
|    n_updates            | 1040          |
|    policy_gradient_loss | -0.000359     |
|    value_loss           | 78.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -206          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 106           |
|    time_elapsed         | 6542          |
|    total_timesteps      | 217088        |
| train/                  |               |
|    approx_kl            | 8.6965214e-05 |
|    clip_fraction        | 0.00181       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0154       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 27.6          |
|    n_updates            | 1050          |
|    policy_gradient_loss | 5.9e-05       |
|    value_loss           | 78.3          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -201         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 107          |
|    time_elapsed         | 6603         |
|    total_timesteps      | 219136       |
| train/                  |              |
|    approx_kl            | 3.824744e-05 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0133      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 34.6         |
|    n_updates            | 1060         |
|    policy_gradient_loss | 2.28e-05     |
|    value_loss           | 73.6         |
------------------------------------------
Num timesteps: 220000
Best mean reward: -222.40 - Last mean reward per episode: -197.50
Saving new best model at 219137 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -198          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 108           |
|    time_elapsed         | 6667          |
|    total_timesteps      | 221184        |
| train/                  |               |
|    approx_kl            | 5.8567035e-05 |
|    clip_fraction        | 0.00083       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0103       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 35.5          |
|    n_updates            | 1070          |
|    policy_gradient_loss | -0.000118     |
|    value_loss           | 85.2          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -195          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 109           |
|    time_elapsed         | 6728          |
|    total_timesteps      | 223232        |
| train/                  |               |
|    approx_kl            | 3.0843075e-06 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0104       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 46.3          |
|    n_updates            | 1080          |
|    policy_gradient_loss | -1.3e-05      |
|    value_loss           | 74            |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -194         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 110          |
|    time_elapsed         | 6789         |
|    total_timesteps      | 225280       |
| train/                  |              |
|    approx_kl            | 7.602741e-05 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0112      |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 41.9         |
|    n_updates            | 1090         |
|    policy_gradient_loss | 9.73e-05     |
|    value_loss           | 78.8         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -189          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 111           |
|    time_elapsed         | 6851          |
|    total_timesteps      | 227328        |
| train/                  |               |
|    approx_kl            | 5.5529876e-05 |
|    clip_fraction        | 0.00122       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00627      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 48.2          |
|    n_updates            | 1100          |
|    policy_gradient_loss | -0.000171     |
|    value_loss           | 82.8          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -189         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 112          |
|    time_elapsed         | 6915         |
|    total_timesteps      | 229376       |
| train/                  |              |
|    approx_kl            | 3.468877e-05 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00511     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 58.2         |
|    n_updates            | 1110         |
|    policy_gradient_loss | -4.76e-05    |
|    value_loss           | 78.1         |
------------------------------------------
Num timesteps: 230000
Best mean reward: -197.50 - Last mean reward per episode: -188.50
Saving new best model at 229377 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -188          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 113           |
|    time_elapsed         | 6978          |
|    total_timesteps      | 231424        |
| train/                  |               |
|    approx_kl            | 2.2817636e-05 |
|    clip_fraction        | 0.00083       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00548      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 66.4          |
|    n_updates            | 1120          |
|    policy_gradient_loss | -0.000105     |
|    value_loss           | 72.2          |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -188      |
| time/                   |           |
|    fps                  | 33        |
|    iterations           | 114       |
|    time_elapsed         | 7041      |
|    total_timesteps      | 233472    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00524  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 30        |
|    n_updates            | 1130      |
|    policy_gradient_loss | 6.59e-10  |
|    value_loss           | 85.1      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 2.04e+03 |
|    ep_rew_mean          | -186     |
| time/                   |          |
|    fps                  | 33       |
|    iterations           | 115      |
|    time_elapsed         | 7103     |
|    total_timesteps      | 235520   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00524 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 43.6     |
|    n_updates            | 1140     |
|    policy_gradient_loss | 2.1e-09  |
|    value_loss           | 79.7     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.04e+03  |
|    ep_rew_mean          | -187      |
| time/                   |           |
|    fps                  | 33        |
|    iterations           | 116       |
|    time_elapsed         | 7164      |
|    total_timesteps      | 237568    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00524  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 20.2      |
|    n_updates            | 1150      |
|    policy_gradient_loss | -3.46e-10 |
|    value_loss           | 61.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.04e+03  |
|    ep_rew_mean          | -188      |
| time/                   |           |
|    fps                  | 33        |
|    iterations           | 117       |
|    time_elapsed         | 7226      |
|    total_timesteps      | 239616    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00524  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 36.4      |
|    n_updates            | 1160      |
|    policy_gradient_loss | -4.28e-10 |
|    value_loss           | 74.6      |
---------------------------------------
Num timesteps: 240000
Best mean reward: -188.50 - Last mean reward per episode: -186.26
Saving new best model at 239617 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -186          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 118           |
|    time_elapsed         | 7288          |
|    total_timesteps      | 241664        |
| train/                  |               |
|    approx_kl            | 0.00029799814 |
|    clip_fraction        | 0.000732      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00404      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 33.3          |
|    n_updates            | 1170          |
|    policy_gradient_loss | -5.09e-05     |
|    value_loss           | 74.2          |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -185      |
| time/                   |           |
|    fps                  | 33        |
|    iterations           | 119       |
|    time_elapsed         | 7349      |
|    total_timesteps      | 243712    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00252  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 31.3      |
|    n_updates            | 1180      |
|    policy_gradient_loss | -9.75e-10 |
|    value_loss           | 79        |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -184          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 120           |
|    time_elapsed         | 7410          |
|    total_timesteps      | 245760        |
| train/                  |               |
|    approx_kl            | 0.00048062956 |
|    clip_fraction        | 0.000732      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00444      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 37.3          |
|    n_updates            | 1190          |
|    policy_gradient_loss | -0.000233     |
|    value_loss           | 74.7          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -181          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 121           |
|    time_elapsed         | 7473          |
|    total_timesteps      | 247808        |
| train/                  |               |
|    approx_kl            | 2.2435794e-05 |
|    clip_fraction        | 0.00083       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00642      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 47.6          |
|    n_updates            | 1200          |
|    policy_gradient_loss | -0.000179     |
|    value_loss           | 79.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -180          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 122           |
|    time_elapsed         | 7533          |
|    total_timesteps      | 249856        |
| train/                  |               |
|    approx_kl            | 4.5286317e-05 |
|    clip_fraction        | 0.00117       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00547      |
|    explained_variance   | 1.79e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 38.9          |
|    n_updates            | 1210          |
|    policy_gradient_loss | 5.44e-05      |
|    value_loss           | 76.4          |
-------------------------------------------
Num timesteps: 250000
Best mean reward: -186.26 - Last mean reward per episode: -179.50
Saving new best model at 249857 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -180      |
| time/                   |           |
|    fps                  | 33        |
|    iterations           | 123       |
|    time_elapsed         | 7594      |
|    total_timesteps      | 251904    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00561  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 37.5      |
|    n_updates            | 1220      |
|    policy_gradient_loss | -5.54e-10 |
|    value_loss           | 84.5      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -177          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 124           |
|    time_elapsed         | 7655          |
|    total_timesteps      | 253952        |
| train/                  |               |
|    approx_kl            | 7.3706586e-05 |
|    clip_fraction        | 0.000635      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00433      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 36.2          |
|    n_updates            | 1230          |
|    policy_gradient_loss | -0.000142     |
|    value_loss           | 85.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -176          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 125           |
|    time_elapsed         | 7717          |
|    total_timesteps      | 256000        |
| train/                  |               |
|    approx_kl            | 4.0749117e-05 |
|    clip_fraction        | 0.000928      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00334      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 46.7          |
|    n_updates            | 1240          |
|    policy_gradient_loss | -8.32e-05     |
|    value_loss           | 80.1          |
-------------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 2.05e+03 |
|    ep_rew_mean          | -178     |
| time/                   |          |
|    fps                  | 33       |
|    iterations           | 126      |
|    time_elapsed         | 7779     |
|    total_timesteps      | 258048   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00319 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 42.4     |
|    n_updates            | 1250     |
|    policy_gradient_loss | 1.46e-10 |
|    value_loss           | 73       |
--------------------------------------
Num timesteps: 260000
Best mean reward: -179.50 - Last mean reward per episode: -175.00
Saving new best model at 258049 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -175         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 127          |
|    time_elapsed         | 7842         |
|    total_timesteps      | 260096       |
| train/                  |              |
|    approx_kl            | 3.141162e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00237     |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 39.7         |
|    n_updates            | 1260         |
|    policy_gradient_loss | -2.63e-05    |
|    value_loss           | 79.5         |
------------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 2.05e+03 |
|    ep_rew_mean          | -177     |
| time/                   |          |
|    fps                  | 33       |
|    iterations           | 128      |
|    time_elapsed         | 7903     |
|    total_timesteps      | 262144   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0023  |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 32.8     |
|    n_updates            | 1270     |
|    policy_gradient_loss | 2.97e-10 |
|    value_loss           | 65.5     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 2.05e+03 |
|    ep_rew_mean          | -178     |
| time/                   |          |
|    fps                  | 33       |
|    iterations           | 129      |
|    time_elapsed         | 7964     |
|    total_timesteps      | 264192   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0023  |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 45       |
|    n_updates            | 1280     |
|    policy_gradient_loss | 1.64e-09 |
|    value_loss           | 71.7     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -175      |
| time/                   |           |
|    fps                  | 33        |
|    iterations           | 130       |
|    time_elapsed         | 8026      |
|    total_timesteps      | 266240    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0023   |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 37.2      |
|    n_updates            | 1290      |
|    policy_gradient_loss | -8.18e-10 |
|    value_loss           | 80.8      |
---------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -177         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 131          |
|    time_elapsed         | 8089         |
|    total_timesteps      | 268288       |
| train/                  |              |
|    approx_kl            | 8.434395e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00357     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 30           |
|    n_updates            | 1300         |
|    policy_gradient_loss | -0.000187    |
|    value_loss           | 62.9         |
------------------------------------------
Num timesteps: 270000
Best mean reward: -175.00 - Last mean reward per episode: -176.50
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -176          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 132           |
|    time_elapsed         | 8151          |
|    total_timesteps      | 270336        |
| train/                  |               |
|    approx_kl            | 1.2078934e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00306      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 44.2          |
|    n_updates            | 1310          |
|    policy_gradient_loss | -6.36e-05     |
|    value_loss           | 77.9          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -175         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 133          |
|    time_elapsed         | 8214         |
|    total_timesteps      | 272384       |
| train/                  |              |
|    approx_kl            | 3.547952e-05 |
|    clip_fraction        | 0.000635     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00253     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 34.8         |
|    n_updates            | 1320         |
|    policy_gradient_loss | -4.49e-05    |
|    value_loss           | 85.9         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -172          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 134           |
|    time_elapsed         | 8275          |
|    total_timesteps      | 274432        |
| train/                  |               |
|    approx_kl            | 1.2970297e-05 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00268      |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 45            |
|    n_updates            | 1330          |
|    policy_gradient_loss | -3.56e-06     |
|    value_loss           | 91.5          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -173         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 135          |
|    time_elapsed         | 8338         |
|    total_timesteps      | 276480       |
| train/                  |              |
|    approx_kl            | 0.0002497288 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00129     |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 53.4         |
|    n_updates            | 1340         |
|    policy_gradient_loss | -6.8e-05     |
|    value_loss           | 75           |
------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -171      |
| time/                   |           |
|    fps                  | 33        |
|    iterations           | 136       |
|    time_elapsed         | 8400      |
|    total_timesteps      | 278528    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00103  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 38        |
|    n_updates            | 1350      |
|    policy_gradient_loss | -2.91e-12 |
|    value_loss           | 76.1      |
---------------------------------------
Num timesteps: 280000
Best mean reward: -175.00 - Last mean reward per episode: -168.10
Saving new best model at 278529 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 2.05e+03 |
|    ep_rew_mean          | -168     |
| time/                   |          |
|    fps                  | 33       |
|    iterations           | 137      |
|    time_elapsed         | 8464     |
|    total_timesteps      | 280576   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00103 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 49.1     |
|    n_updates            | 1360     |
|    policy_gradient_loss | 1.39e-09 |
|    value_loss           | 93.6     |
--------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -168          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 138           |
|    time_elapsed         | 8530          |
|    total_timesteps      | 282624        |
| train/                  |               |
|    approx_kl            | 2.5037822e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000811     |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 30.5          |
|    n_updates            | 1370          |
|    policy_gradient_loss | -7.01e-05     |
|    value_loss           | 78.7          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -168          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 139           |
|    time_elapsed         | 8593          |
|    total_timesteps      | 284672        |
| train/                  |               |
|    approx_kl            | 1.5388505e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000655     |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 25.7          |
|    n_updates            | 1380          |
|    policy_gradient_loss | -3.86e-05     |
|    value_loss           | 75.6          |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -166      |
| time/                   |           |
|    fps                  | 33        |
|    iterations           | 140       |
|    time_elapsed         | 8665      |
|    total_timesteps      | 286720    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000617 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 32.5      |
|    n_updates            | 1390      |
|    policy_gradient_loss | 8.25e-10  |
|    value_loss           | 83.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -167      |
| time/                   |           |
|    fps                  | 33        |
|    iterations           | 141       |
|    time_elapsed         | 8731      |
|    total_timesteps      | 288768    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000617 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 37.5      |
|    n_updates            | 1400      |
|    policy_gradient_loss | 1.51e-10  |
|    value_loss           | 71.2      |
---------------------------------------
Num timesteps: 290000
Best mean reward: -168.10 - Last mean reward per episode: -166.00
Saving new best model at 288769 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -166      |
| time/                   |           |
|    fps                  | 33        |
|    iterations           | 142       |
|    time_elapsed         | 8798      |
|    total_timesteps      | 290816    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000617 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 49.8      |
|    n_updates            | 1410      |
|    policy_gradient_loss | -4.58e-10 |
|    value_loss           | 73.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -168      |
| time/                   |           |
|    fps                  | 33        |
|    iterations           | 143       |
|    time_elapsed         | 8862      |
|    total_timesteps      | 292864    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000617 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 34        |
|    n_updates            | 1420      |
|    policy_gradient_loss | -3.8e-10  |
|    value_loss           | 65.6      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -168          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 144           |
|    time_elapsed         | 8932          |
|    total_timesteps      | 294912        |
| train/                  |               |
|    approx_kl            | 0.00021830134 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000286     |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 37.6          |
|    n_updates            | 1430          |
|    policy_gradient_loss | -5e-05        |
|    value_loss           | 73.5          |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -168      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 145       |
|    time_elapsed         | 9002      |
|    total_timesteps      | 296960    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000241 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 39.8      |
|    n_updates            | 1440      |
|    policy_gradient_loss | 5.08e-10  |
|    value_loss           | 68.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -170      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 146       |
|    time_elapsed         | 9068      |
|    total_timesteps      | 299008    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000241 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 26.5      |
|    n_updates            | 1450      |
|    policy_gradient_loss | 2.14e-10  |
|    value_loss           | 74.8      |
---------------------------------------
Num timesteps: 300000
Best mean reward: -166.00 - Last mean reward per episode: -170.50
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -170      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 147       |
|    time_elapsed         | 9139      |
|    total_timesteps      | 301056    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000241 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 40.7      |
|    n_updates            | 1460      |
|    policy_gradient_loss | 1.08e-09  |
|    value_loss           | 69        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -172      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 148       |
|    time_elapsed         | 9206      |
|    total_timesteps      | 303104    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000241 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 34.9      |
|    n_updates            | 1470      |
|    policy_gradient_loss | -8.51e-11 |
|    value_loss           | 80.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -170      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 149       |
|    time_elapsed         | 9273      |
|    total_timesteps      | 305152    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000241 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 35.6      |
|    n_updates            | 1480      |
|    policy_gradient_loss | 1.16e-09  |
|    value_loss           | 84.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -171      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 150       |
|    time_elapsed         | 9343      |
|    total_timesteps      | 307200    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000241 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 32        |
|    n_updates            | 1490      |
|    policy_gradient_loss | -2.31e-09 |
|    value_loss           | 72.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -172      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 151       |
|    time_elapsed         | 9407      |
|    total_timesteps      | 309248    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000241 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 48.7      |
|    n_updates            | 1500      |
|    policy_gradient_loss | 1.99e-10  |
|    value_loss           | 72.9      |
---------------------------------------
Num timesteps: 310000
Best mean reward: -166.00 - Last mean reward per episode: -171.10
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -171      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 152       |
|    time_elapsed         | 9474      |
|    total_timesteps      | 311296    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000241 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 40        |
|    n_updates            | 1510      |
|    policy_gradient_loss | 5.49e-10  |
|    value_loss           | 76.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -168      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 153       |
|    time_elapsed         | 9540      |
|    total_timesteps      | 313344    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000241 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0003    |
|    loss                 | 57.9      |
|    n_updates            | 1520      |
|    policy_gradient_loss | -1.56e-09 |
|    value_loss           | 88.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -168      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 154       |
|    time_elapsed         | 9604      |
|    total_timesteps      | 315392    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000241 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 45.3      |
|    n_updates            | 1530      |
|    policy_gradient_loss | 3.45e-10  |
|    value_loss           | 83.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -165      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 155       |
|    time_elapsed         | 9675      |
|    total_timesteps      | 317440    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000241 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 42.5      |
|    n_updates            | 1540      |
|    policy_gradient_loss | -2.26e-11 |
|    value_loss           | 84.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -162      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 156       |
|    time_elapsed         | 9742      |
|    total_timesteps      | 319488    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000241 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 42.6      |
|    n_updates            | 1550      |
|    policy_gradient_loss | -5.25e-10 |
|    value_loss           | 82.7      |
---------------------------------------
Num timesteps: 320000
Best mean reward: -166.00 - Last mean reward per episode: -161.80
Saving new best model at 319489 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -162      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 157       |
|    time_elapsed         | 9810      |
|    total_timesteps      | 321536    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000241 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 50.8      |
|    n_updates            | 1560      |
|    policy_gradient_loss | -3.14e-10 |
|    value_loss           | 76.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -162      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 158       |
|    time_elapsed         | 9885      |
|    total_timesteps      | 323584    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000241 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 35.4      |
|    n_updates            | 1570      |
|    policy_gradient_loss | -2.14e-10 |
|    value_loss           | 81.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -161      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 159       |
|    time_elapsed         | 9953      |
|    total_timesteps      | 325632    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000241 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 42.4      |
|    n_updates            | 1580      |
|    policy_gradient_loss | 9.34e-10  |
|    value_loss           | 75.2      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -163          |
| time/                   |               |
|    fps                  | 32            |
|    iterations           | 160           |
|    time_elapsed         | 10019         |
|    total_timesteps      | 327680        |
| train/                  |               |
|    approx_kl            | 0.00028073986 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000479     |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 36.9          |
|    n_updates            | 1590          |
|    policy_gradient_loss | -0.000172     |
|    value_loss           | 69.5          |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -163      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 161       |
|    time_elapsed         | 10087     |
|    total_timesteps      | 329728    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000516 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 32.6      |
|    n_updates            | 1600      |
|    policy_gradient_loss | -1.42e-10 |
|    value_loss           | 73.2      |
---------------------------------------
Num timesteps: 330000
Best mean reward: -161.80 - Last mean reward per episode: -165.10
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -165      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 162       |
|    time_elapsed         | 10149     |
|    total_timesteps      | 331776    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000516 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 28.7      |
|    n_updates            | 1610      |
|    policy_gradient_loss | -6.16e-10 |
|    value_loss           | 68.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -166      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 163       |
|    time_elapsed         | 10216     |
|    total_timesteps      | 333824    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000516 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 40        |
|    n_updates            | 1620      |
|    policy_gradient_loss | -1.28e-09 |
|    value_loss           | 72.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -165      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 164       |
|    time_elapsed         | 10288     |
|    total_timesteps      | 335872    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000516 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 34.9      |
|    n_updates            | 1630      |
|    policy_gradient_loss | 5.31e-10  |
|    value_loss           | 81.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -166      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 165       |
|    time_elapsed         | 10354     |
|    total_timesteps      | 337920    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000516 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 38.4      |
|    n_updates            | 1640      |
|    policy_gradient_loss | -4.11e-10 |
|    value_loss           | 76.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -167      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 166       |
|    time_elapsed         | 10423     |
|    total_timesteps      | 339968    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000516 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 39.7      |
|    n_updates            | 1650      |
|    policy_gradient_loss | 4.73e-10  |
|    value_loss           | 72.8      |
---------------------------------------
Num timesteps: 340000
Best mean reward: -161.80 - Last mean reward per episode: -166.30
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -166      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 167       |
|    time_elapsed         | 10488     |
|    total_timesteps      | 342016    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000516 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 49.8      |
|    n_updates            | 1660      |
|    policy_gradient_loss | 2.21e-09  |
|    value_loss           | 81.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -164      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 168       |
|    time_elapsed         | 10555     |
|    total_timesteps      | 344064    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000516 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 37.3      |
|    n_updates            | 1670      |
|    policy_gradient_loss | 5.09e-10  |
|    value_loss           | 86.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -162      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 169       |
|    time_elapsed         | 10620     |
|    total_timesteps      | 346112    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000516 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 32        |
|    n_updates            | 1680      |
|    policy_gradient_loss | 1.1e-09   |
|    value_loss           | 78.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 2.05e+03  |
|    ep_rew_mean          | -160      |
| time/                   |           |
|    fps                  | 32        |
|    iterations           | 170       |
|    time_elapsed         | 10685     |
|    total_timesteps      | 348160    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000516 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 39.9      |
|    n_updates            | 1690      |
|    policy_gradient_loss | -5.39e-10 |
|    value_loss           | 88.1      |
---------------------------------------
Num timesteps: 350000
Best mean reward: -161.80 - Last mean reward per episode: -156.10
Saving new best model at 348161 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.05e+03      |
|    ep_rew_mean          | -156          |
| time/                   |               |
|    fps                  | 32            |
|    iterations           | 171           |
|    time_elapsed         | 10747         |
|    total_timesteps      | 350208        |
| train/                  |               |
|    approx_kl            | 7.6493394e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000322     |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 56.1          |
|    n_updates            | 1700          |
|    policy_gradient_loss | -6.04e-05     |
|    value_loss           | 80.9          |
-------------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 290, in <module>
    model.learn(total_timesteps=1000000, callback=[WandbCallback(), save_callback])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 313, in learn
    self.train()
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 217, in train
    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 730, in evaluate_actions
    features = self.extract_features(obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/policy.py", line 45, in forward
    obj_emb = self.model(pyg_data.x_dict, pyg_data.edge_index_dict, pyg_data.batch_dict)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_gnn.py", line 104, in forward
    self.layer(x_dict, edge_index_dict)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_gnn.py", line 82, in layer
    out = self.atom_to_obj(x_dict, edge_index_dict)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_message_passing.py", line 74, in forward
    out = self._internal_forward(x, edge_index_dict[edge_type], edge_type)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_message_passing.py", line 149, in _internal_forward
    return self.select(x, edges_index, int(edge_type[1]))
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_message_passing.py", line 171, in forward
    return self.propagate(edge_index, x=x, position=position)
  File "/var/folders/my/7z0rbf091qj03p882sd10h_00000gn/T/games.model.hetero_message_passing_SelectMP_propagate__ca87zcu.py", line 135, in propagate
    kwargs = self.collect(
  File "/var/folders/my/7z0rbf091qj03p882sd10h_00000gn/T/games.model.hetero_message_passing_SelectMP_propagate__ca87zcu.py", line 78, in collect
    x_j = self._index_select(_x_0, edge_index_j)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py", line 313, in _index_select
    return self._index_select_safe(src, index)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py", line 317, in _index_select_safe
    return src.index_select(self.node_dim, index)
KeyboardInterrupt