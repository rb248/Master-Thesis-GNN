
Using cpu device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -500     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 1        |
|    time_elapsed    | 16       |
|    total_timesteps | 2048     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 44           |
|    iterations           | 2            |
|    time_elapsed         | 92           |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0079250485 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | -0.00133     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.951        |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00365     |
|    value_loss           | 4.52         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -492        |
| time/                   |             |
|    fps                  | 35          |
|    iterations           | 3           |
|    time_elapsed         | 173         |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.011632482 |
|    clip_fraction        | 0.0133      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.000167    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.132       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0014     |
|    value_loss           | 15.7        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -489        |
| time/                   |             |
|    fps                  | 32          |
|    iterations           | 4           |
|    time_elapsed         | 254         |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.007728357 |
|    clip_fraction        | 0.0368      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 7.93e-06    |
|    learning_rate        | 0.0003      |
|    loss                 | 8.09        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00222    |
|    value_loss           | 16.4        |
-----------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -487.00
Saving new best model at 10000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | -487       |
| time/                   |            |
|    fps                  | 30         |
|    iterations           | 5          |
|    time_elapsed         | 337        |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.00777697 |
|    clip_fraction        | 0.0228     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.000797   |
|    learning_rate        | 0.0003     |
|    loss                 | 1.34       |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.00123   |
|    value_loss           | 12.6       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -488        |
| time/                   |             |
|    fps                  | 28          |
|    iterations           | 6           |
|    time_elapsed         | 426         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.008369118 |
|    clip_fraction        | 0.0474      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 6.3         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.000245   |
|    value_loss           | 10.8        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -483        |
| time/                   |             |
|    fps                  | 28          |
|    iterations           | 7           |
|    time_elapsed         | 507         |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.015509665 |
|    clip_fraction        | 0.067       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.967      |
|    explained_variance   | 0.0899      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.82        |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.000205    |
|    value_loss           | 16.5        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -477        |
| time/                   |             |
|    fps                  | 27          |
|    iterations           | 8           |
|    time_elapsed         | 588         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.004777206 |
|    clip_fraction        | 0.0312      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.871      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 2.38        |
|    n_updates            | 70          |
|    policy_gradient_loss | 1.28e-05    |
|    value_loss           | 16          |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | -471       |
| time/                   |            |
|    fps                  | 27         |
|    iterations           | 9          |
|    time_elapsed         | 676        |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.01475811 |
|    clip_fraction        | 0.0957     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.865     |
|    explained_variance   | 5.96e-08   |
|    learning_rate        | 0.0003     |
|    loss                 | 15.9       |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.00347   |
|    value_loss           | 22.9       |
----------------------------------------
Num timesteps: 20000
Best mean reward: -487.00 - Last mean reward per episode: -468.00
Saving new best model at 20000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -468         |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 10           |
|    time_elapsed         | 761          |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.0029107647 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.815       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 13.1         |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.000724    |
|    value_loss           | 23.9         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -465         |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 11           |
|    time_elapsed         | 849          |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0063701947 |
|    clip_fraction        | 0.0895       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.743       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 9.67         |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00392     |
|    value_loss           | 17           |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -453        |
| time/                   |             |
|    fps                  | 26          |
|    iterations           | 12          |
|    time_elapsed         | 925         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.008114476 |
|    clip_fraction        | 0.091       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.662      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 12.6        |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0048     |
|    value_loss           | 33          |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -448         |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 13           |
|    time_elapsed         | 1007         |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.0024808561 |
|    clip_fraction        | 0.0225       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.626       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 21.9         |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00119     |
|    value_loss           | 35.6         |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | -443       |
| time/                   |            |
|    fps                  | 26         |
|    iterations           | 14         |
|    time_elapsed         | 1095       |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.00529048 |
|    clip_fraction        | 0.0407     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.583     |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0003     |
|    loss                 | 5.73       |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.000488  |
|    value_loss           | 27.1       |
----------------------------------------
Num timesteps: 30000
Best mean reward: -468.00 - Last mean reward per episode: -436.67
Saving new best model at 30000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -437        |
| time/                   |             |
|    fps                  | 26          |
|    iterations           | 15          |
|    time_elapsed         | 1175        |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.004417224 |
|    clip_fraction        | 0.0569      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.522      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 14.8        |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00296    |
|    value_loss           | 40.7        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -425         |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 16           |
|    time_elapsed         | 1258         |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0022427936 |
|    clip_fraction        | 0.0185       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.46        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 18.8         |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.000932    |
|    value_loss           | 37.9         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -420         |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 17           |
|    time_elapsed         | 1350         |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0019264133 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.475       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 16.5         |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.000256    |
|    value_loss           | 54.7         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -416        |
| time/                   |             |
|    fps                  | 25          |
|    iterations           | 18          |
|    time_elapsed         | 1426        |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.002061534 |
|    clip_fraction        | 0.018       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.438      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 28.3        |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.000368   |
|    value_loss           | 40.8        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -416        |
| time/                   |             |
|    fps                  | 25          |
|    iterations           | 19          |
|    time_elapsed         | 1516        |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.000488497 |
|    clip_fraction        | 0.0241      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.39       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 9.93        |
|    n_updates            | 180         |
|    policy_gradient_loss | 0.000204    |
|    value_loss           | 23.7        |
-----------------------------------------
Num timesteps: 40000
Best mean reward: -436.67 - Last mean reward per episode: -406.75
Saving new best model at 40000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -407         |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 20           |
|    time_elapsed         | 1601         |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0016732651 |
|    clip_fraction        | 0.0175       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.385       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 14.7         |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.000439    |
|    value_loss           | 43.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -402         |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 21           |
|    time_elapsed         | 1676         |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0018447482 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.387       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 28.1         |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00188     |
|    value_loss           | 49           |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -397         |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 22           |
|    time_elapsed         | 1752         |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0029491787 |
|    clip_fraction        | 0.0394       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.321       |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 16.1         |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00248     |
|    value_loss           | 35.6         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -393         |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 23           |
|    time_elapsed         | 1838         |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0016231906 |
|    clip_fraction        | 0.00796      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.303       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 25.5         |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.000547    |
|    value_loss           | 39.2         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -393        |
| time/                   |             |
|    fps                  | 25          |
|    iterations           | 24          |
|    time_elapsed         | 1921        |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.000633094 |
|    clip_fraction        | 0.0147      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.248      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 21.3        |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00084    |
|    value_loss           | 38.3        |
-----------------------------------------
Num timesteps: 50000
Best mean reward: -406.75 - Last mean reward per episode: -389.60
Saving new best model at 50000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -391         |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 25           |
|    time_elapsed         | 2004         |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0003502871 |
|    clip_fraction        | 0.00996      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.247       |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 20.5         |
|    n_updates            | 240          |
|    policy_gradient_loss | 0.000275     |
|    value_loss           | 20.8         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -389         |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 26           |
|    time_elapsed         | 2097         |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0014881366 |
|    clip_fraction        | 0.0237       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.212       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 15.6         |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00102     |
|    value_loss           | 43.3         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -388         |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 27           |
|    time_elapsed         | 2182         |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0012552463 |
|    clip_fraction        | 0.0209       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.181       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 15.2         |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00127     |
|    value_loss           | 43.5         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -385          |
| time/                   |               |
|    fps                  | 25            |
|    iterations           | 28            |
|    time_elapsed         | 2261          |
|    total_timesteps      | 57344         |
| train/                  |               |
|    approx_kl            | 0.00020142598 |
|    clip_fraction        | 0.0082        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.197        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 17.1          |
|    n_updates            | 270           |
|    policy_gradient_loss | -0.000534     |
|    value_loss           | 33.1          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -384          |
| time/                   |               |
|    fps                  | 25            |
|    iterations           | 29            |
|    time_elapsed         | 2340          |
|    total_timesteps      | 59392         |
| train/                  |               |
|    approx_kl            | 0.00041271836 |
|    clip_fraction        | 0.00781       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.181        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 27.5          |
|    n_updates            | 280           |
|    policy_gradient_loss | -0.000421     |
|    value_loss           | 39.3          |
-------------------------------------------
Num timesteps: 60000
Best mean reward: -389.60 - Last mean reward per episode: -383.67
Saving new best model at 60000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -386         |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 30           |
|    time_elapsed         | 2420         |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0013753653 |
|    clip_fraction        | 0.00928      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.171       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 12           |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.000241    |
|    value_loss           | 29.9         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -385          |
| time/                   |               |
|    fps                  | 25            |
|    iterations           | 31            |
|    time_elapsed         | 2504          |
|    total_timesteps      | 63488         |
| train/                  |               |
|    approx_kl            | 0.00026574446 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.155        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 9.03          |
|    n_updates            | 300           |
|    policy_gradient_loss | 0.000209      |
|    value_loss           | 19.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -382          |
| time/                   |               |
|    fps                  | 25            |
|    iterations           | 32            |
|    time_elapsed         | 2588          |
|    total_timesteps      | 65536         |
| train/                  |               |
|    approx_kl            | 0.00048740295 |
|    clip_fraction        | 0.0144        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.16         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 23.6          |
|    n_updates            | 310           |
|    policy_gradient_loss | -0.0014       |
|    value_loss           | 39.6          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -380          |
| time/                   |               |
|    fps                  | 25            |
|    iterations           | 33            |
|    time_elapsed         | 2668          |
|    total_timesteps      | 67584         |
| train/                  |               |
|    approx_kl            | 0.00049061095 |
|    clip_fraction        | 0.0156        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.136        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 17.5          |
|    n_updates            | 320           |
|    policy_gradient_loss | -0.00105      |
|    value_loss           | 41.8          |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -377        |
| time/                   |             |
|    fps                  | 25          |
|    iterations           | 34          |
|    time_elapsed         | 2751        |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.000792769 |
|    clip_fraction        | 0.0134      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.122      |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 30.7        |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.000825   |
|    value_loss           | 49.4        |
-----------------------------------------
Num timesteps: 70000
Best mean reward: -383.67 - Last mean reward per episode: -376.43
Saving new best model at 70000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -375          |
| time/                   |               |
|    fps                  | 25            |
|    iterations           | 35            |
|    time_elapsed         | 2827          |
|    total_timesteps      | 71680         |
| train/                  |               |
|    approx_kl            | 0.00059825083 |
|    clip_fraction        | 0.00747       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.106        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 18.4          |
|    n_updates            | 340           |
|    policy_gradient_loss | -0.000525     |
|    value_loss           | 42            |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -376          |
| time/                   |               |
|    fps                  | 25            |
|    iterations           | 36            |
|    time_elapsed         | 2905          |
|    total_timesteps      | 73728         |
| train/                  |               |
|    approx_kl            | 0.00057467737 |
|    clip_fraction        | 0.0106        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0912       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 7.31          |
|    n_updates            | 350           |
|    policy_gradient_loss | -0.000686     |
|    value_loss           | 34.8          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -376         |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 37           |
|    time_elapsed         | 2986         |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0002356593 |
|    clip_fraction        | 0.0043       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0893      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 11.6         |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.000374    |
|    value_loss           | 23.5         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -375          |
| time/                   |               |
|    fps                  | 25            |
|    iterations           | 38            |
|    time_elapsed         | 3068          |
|    total_timesteps      | 77824         |
| train/                  |               |
|    approx_kl            | 0.00028188864 |
|    clip_fraction        | 0.00186       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0858       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 23.7          |
|    n_updates            | 370           |
|    policy_gradient_loss | -8.09e-05     |
|    value_loss           | 39.4          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -376         |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 39           |
|    time_elapsed         | 3161         |
|    total_timesteps      | 79872        |
| train/                  |              |
|    approx_kl            | 4.622218e-05 |
|    clip_fraction        | 0.00337      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0858      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 7.66         |
|    n_updates            | 380          |
|    policy_gradient_loss | -6.5e-05     |
|    value_loss           | 22.8         |
------------------------------------------
Num timesteps: 80000
Best mean reward: -376.43 - Last mean reward per episode: -374.38
Saving new best model at 80000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -374          |
| time/                   |               |
|    fps                  | 25            |
|    iterations           | 40            |
|    time_elapsed         | 3245          |
|    total_timesteps      | 81920         |
| train/                  |               |
|    approx_kl            | 7.7470235e-05 |
|    clip_fraction        | 0.0064        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0934       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 18.1          |
|    n_updates            | 390           |
|    policy_gradient_loss | -0.00012      |
|    value_loss           | 40.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -373          |
| time/                   |               |
|    fps                  | 25            |
|    iterations           | 41            |
|    time_elapsed         | 3323          |
|    total_timesteps      | 83968         |
| train/                  |               |
|    approx_kl            | 0.00031909798 |
|    clip_fraction        | 0.00625       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0765       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 27.3          |
|    n_updates            | 400           |
|    policy_gradient_loss | -0.000449     |
|    value_loss           | 36.9          |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 1e+03          |
|    ep_rew_mean          | -371           |
| time/                   |                |
|    fps                  | 25             |
|    iterations           | 42             |
|    time_elapsed         | 3408           |
|    total_timesteps      | 86016          |
| train/                  |                |
|    approx_kl            | 1.41222845e-05 |
|    clip_fraction        | 0.00103        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0725        |
|    explained_variance   | 0              |
|    learning_rate        | 0.0003         |
|    loss                 | 17.9           |
|    n_updates            | 410            |
|    policy_gradient_loss | 2.15e-05       |
|    value_loss           | 28.4           |
--------------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 290, in <module>
    model.learn(total_timesteps=1000000, callback=[WandbCallback(), save_callback])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 300, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 179, in collect_rollouts
    actions, values, log_probs = self.policy(obs_tensor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 645, in forward
    features = self.extract_features(obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/policy.py", line 45, in forward
    obj_emb = self.model(pyg_data.x_dict, pyg_data.edge_index_dict, pyg_data.batch_dict)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_gnn.py", line 104, in forward
    self.layer(x_dict, edge_index_dict)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_gnn.py", line 79, in layer
    out = self.obj_to_atom(x_dict, edge_index_dict)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_message_passing.py", line 80, in forward
    return self._group_out(out_dict)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_message_passing.py", line 128, in _group_out
    out_dict[dst] = self.update_mlp_by_dst[dst](stacked)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/models/mlp.py", line 231, in forward
    x = lin(x)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/dense/linear.py", line 147, in forward
    return F.linear(x, self.weight, self.bias)
KeyboardInterrupt