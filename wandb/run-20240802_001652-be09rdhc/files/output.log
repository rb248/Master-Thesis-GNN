/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.
  logger.warn(
Using cpu device
-----------------------------
| time/              |      |
|    fps             | 239  |
|    iterations      | 1    |
|    time_elapsed    | 8    |
|    total_timesteps | 2048 |
-----------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.51e+03     |
|    ep_rew_mean          | -1.79e+03    |
| time/                   |              |
|    fps                  | 65           |
|    iterations           | 2            |
|    time_elapsed         | 62           |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0118455235 |
|    clip_fraction        | 0.169        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | -0.1         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.534        |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.0121      |
|    value_loss           | 6.51         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.51e+03    |
|    ep_rew_mean          | -1.79e+03   |
| time/                   |             |
|    fps                  | 55          |
|    iterations           | 3           |
|    time_elapsed         | 110         |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.005828678 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0927      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.000199   |
|    value_loss           | 14.4        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.07e+03    |
|    ep_rew_mean          | -1.57e+03   |
| time/                   |             |
|    fps                  | 51          |
|    iterations           | 4           |
|    time_elapsed         | 159         |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.010578055 |
|    clip_fraction        | 0.0153      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.219       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00204    |
|    value_loss           | 11.2        |
-----------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -1568.75
Saving new best model at 6145 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.07e+03    |
|    ep_rew_mean          | -1.57e+03   |
| time/                   |             |
|    fps                  | 48          |
|    iterations           | 5           |
|    time_elapsed         | 209         |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.006162376 |
|    clip_fraction        | 0.0192      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0492      |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.000101    |
|    value_loss           | 6.61        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.41e+03    |
|    ep_rew_mean          | -1.73e+03   |
| time/                   |             |
|    fps                  | 47          |
|    iterations           | 6           |
|    time_elapsed         | 257         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.013098741 |
|    clip_fraction        | 0.0317      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.971      |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00221     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.000359   |
|    value_loss           | 3.67        |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.41e+03  |
|    ep_rew_mean          | -1.73e+03 |
| time/                   |           |
|    fps                  | 46        |
|    iterations           | 7         |
|    time_elapsed         | 306       |
|    total_timesteps      | 14336     |
| train/                  |           |
|    approx_kl            | 0.0083427 |
|    clip_fraction        | 0.0475    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.891    |
|    explained_variance   | 1.79e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0268    |
|    n_updates            | 60        |
|    policy_gradient_loss | -0.00185  |
|    value_loss           | 2.55      |
---------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.58e+03     |
|    ep_rew_mean          | -1.81e+03    |
| time/                   |              |
|    fps                  | 46           |
|    iterations           | 8            |
|    time_elapsed         | 354          |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 0.0023030909 |
|    clip_fraction        | 0.0369       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.802       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000629     |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.000348    |
|    value_loss           | 1.19         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.58e+03     |
|    ep_rew_mean          | -1.81e+03    |
| time/                   |              |
|    fps                  | 45           |
|    iterations           | 9            |
|    time_elapsed         | 403          |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0076858164 |
|    clip_fraction        | 0.124        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.736       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0412       |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00226     |
|    value_loss           | 1.36         |
------------------------------------------
Num timesteps: 20000
Best mean reward: -1568.75 - Last mean reward per episode: -1858.30
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.69e+03     |
|    ep_rew_mean          | -1.86e+03    |
| time/                   |              |
|    fps                  | 45           |
|    iterations           | 10           |
|    time_elapsed         | 452          |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.0020170799 |
|    clip_fraction        | 0.0202       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.662       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00137      |
|    n_updates            | 90           |
|    policy_gradient_loss | 0.00142      |
|    value_loss           | 0.405        |
------------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 101, in <module>
    model.learn(total_timesteps=500000, callback=[callback, WandbCallback()] )
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 300, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 179, in collect_rollouts
    actions, values, log_probs = self.policy(obs_tensor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 645, in forward
    features = self.extract_features(obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/policy.py", line 45, in forward
    obj_emb = self.model(pyg_data.x_dict, pyg_data.edge_index_dict, pyg_data.batch_dict)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_gnn.py", line 104, in forward
    self.layer(x_dict, edge_index_dict)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_gnn.py", line 79, in layer
    out = self.obj_to_atom(x_dict, edge_index_dict)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_message_passing.py", line 74, in forward
    out = self._internal_forward(x, edge_index_dict[edge_type], edge_type)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_message_passing.py", line 121, in _internal_forward
    out = self.simple(x, edge_index)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/conv/simple_conv.py", line 83, in forward
    out = self.propagate(edge_index, x=x, edge_weight=edge_weight,
  File "/var/folders/my/7z0rbf091qj03p882sd10h_00000gn/T/torch_geometric.nn.conv.simple_conv_SimpleConv_propagate_vlprcqb5.py", line 230, in propagate
    out = self.aggregate(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py", line 625, in aggregate
    return self.aggr_module(inputs, index, ptr=ptr, dim_size=dim_size,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/experimental.py", line 117, in wrapper
    return func(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/aggr/base.py", line 128, in __call__
    return super().__call__(x, index=index, ptr=ptr, dim_size=dim_size,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/aggr/basic.py", line 22, in forward
    return self.reduce(x, index, ptr, dim_size, dim, reduce='sum')
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/aggr/base.py", line 182, in reduce
    return scatter(x, index, dim, dim_size, reduce)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/utils/_scatter.py", line 65, in scatter
    size = src.size()[:dim] + (dim_size, ) + src.size()[dim + 1:]
KeyboardInterrupt