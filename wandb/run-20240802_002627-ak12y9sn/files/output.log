/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.
  logger.warn(
Using cpu device
-----------------------------
| time/              |      |
|    fps             | 218  |
|    iterations      | 1    |
|    time_elapsed    | 9    |
|    total_timesteps | 2048 |
-----------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.2e+03   |
|    ep_rew_mean          | -1.57e+03 |
| time/                   |           |
|    fps                  | 63        |
|    iterations           | 2         |
|    time_elapsed         | 64        |
|    total_timesteps      | 4096      |
| train/                  |           |
|    approx_kl            | 0.0107159 |
|    clip_fraction        | 0.0805    |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.09     |
|    explained_variance   | 0.0493    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.838     |
|    n_updates            | 10        |
|    policy_gradient_loss | -0.00255  |
|    value_loss           | 7.82      |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.2e+03     |
|    ep_rew_mean          | -1.57e+03   |
| time/                   |             |
|    fps                  | 53          |
|    iterations           | 3           |
|    time_elapsed         | 114         |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.011773248 |
|    clip_fraction        | 0.0108      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 2.38e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.847       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00069    |
|    value_loss           | 13.6        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.07e+03    |
|    ep_rew_mean          | -1.39e+03   |
| time/                   |             |
|    fps                  | 49          |
|    iterations           | 4           |
|    time_elapsed         | 164         |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.018235296 |
|    clip_fraction        | 0.097       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 13.9        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0064     |
|    value_loss           | 32.2        |
-----------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -1149.67
Saving new best model at 8818 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.94e+03   |
|    ep_rew_mean          | -1.15e+03  |
| time/                   |            |
|    fps                  | 46         |
|    iterations           | 5          |
|    time_elapsed         | 221        |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.01600981 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.891     |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 16.8       |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.00932   |
|    value_loss           | 46.1       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.94e+03    |
|    ep_rew_mean          | -1.15e+03   |
| time/                   |             |
|    fps                  | 44          |
|    iterations           | 6           |
|    time_elapsed         | 273         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.004454811 |
|    clip_fraction        | 0.0684      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.807      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 32.2        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00416    |
|    value_loss           | 57.8        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.07e+03     |
|    ep_rew_mean          | -1.09e+03    |
| time/                   |              |
|    fps                  | 43           |
|    iterations           | 7            |
|    time_elapsed         | 326          |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.0005506392 |
|    clip_fraction        | 0.0408       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.726       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 10.4         |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.000453    |
|    value_loss           | 41.3         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.22e+03    |
|    ep_rew_mean          | -1.06e+03   |
| time/                   |             |
|    fps                  | 43          |
|    iterations           | 8           |
|    time_elapsed         | 378         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.003454793 |
|    clip_fraction        | 0.0173      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.776      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 15.7        |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00111    |
|    value_loss           | 44.6        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.22e+03     |
|    ep_rew_mean          | -1.06e+03    |
| time/                   |              |
|    fps                  | 43           |
|    iterations           | 9            |
|    time_elapsed         | 427          |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0072758244 |
|    clip_fraction        | 0.0273       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.743       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 36.3         |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.002       |
|    value_loss           | 61.5         |
------------------------------------------
Num timesteps: 20000
Best mean reward: -1149.67 - Last mean reward per episode: -966.08
Saving new best model at 18433 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3.07e+03   |
|    ep_rew_mean          | -966       |
| time/                   |            |
|    fps                  | 43         |
|    iterations           | 10         |
|    time_elapsed         | 476        |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.00554361 |
|    clip_fraction        | 0.0327     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.658     |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 21.2       |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.000973  |
|    value_loss           | 57.4       |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.07e+03     |
|    ep_rew_mean          | -966         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 11           |
|    time_elapsed         | 524          |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0023978753 |
|    clip_fraction        | 0.0396       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.545       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 51.4         |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00144     |
|    value_loss           | 61.5         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.22e+03     |
|    ep_rew_mean          | -898         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 12           |
|    time_elapsed         | 574          |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0038900143 |
|    clip_fraction        | 0.0303       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.478       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 46.9         |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00191     |
|    value_loss           | 72.3         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.22e+03     |
|    ep_rew_mean          | -898         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 13           |
|    time_elapsed         | 624          |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.0007920235 |
|    clip_fraction        | 0.0229       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.417       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 49.7         |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00195     |
|    value_loss           | 77.4         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.33e+03     |
|    ep_rew_mean          | -820         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 14           |
|    time_elapsed         | 671          |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0010019398 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.418       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 28.2         |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.000691    |
|    value_loss           | 74.8         |
------------------------------------------
Num timesteps: 30000
Best mean reward: -966.08 - Last mean reward per episode: -820.31
Saving new best model at 26625 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.33e+03     |
|    ep_rew_mean          | -820         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 15           |
|    time_elapsed         | 722          |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.0024119252 |
|    clip_fraction        | 0.0399       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.358       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 46.2         |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00213     |
|    value_loss           | 79.6         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.41e+03     |
|    ep_rew_mean          | -747         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 16           |
|    time_elapsed         | 771          |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0024861987 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.328       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 37.9         |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.0011      |
|    value_loss           | 79.4         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.41e+03     |
|    ep_rew_mean          | -747         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 17           |
|    time_elapsed         | 818          |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0017436579 |
|    clip_fraction        | 0.0235       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.28        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 36.3         |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00123     |
|    value_loss           | 78.5         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.48e+03     |
|    ep_rew_mean          | -721         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 18           |
|    time_elapsed         | 866          |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.0018892774 |
|    clip_fraction        | 0.0177       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.242       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 32.4         |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00025     |
|    value_loss           | 65.5         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.48e+03     |
|    ep_rew_mean          | -721         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 19           |
|    time_elapsed         | 914          |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0007690571 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.195       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 42.9         |
|    n_updates            | 180          |
|    policy_gradient_loss | -2.22e-06    |
|    value_loss           | 85.2         |
------------------------------------------
Num timesteps: 40000
Best mean reward: -820.31 - Last mean reward per episode: -664.23
Saving new best model at 38913 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.54e+03      |
|    ep_rew_mean          | -664          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 20            |
|    time_elapsed         | 962           |
|    total_timesteps      | 40960         |
| train/                  |               |
|    approx_kl            | 0.00055279315 |
|    clip_fraction        | 0.00811       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.211        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 49.7          |
|    n_updates            | 190           |
|    policy_gradient_loss | -7.57e-05     |
|    value_loss           | 80.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.54e+03      |
|    ep_rew_mean          | -664          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 21            |
|    time_elapsed         | 1010          |
|    total_timesteps      | 43008         |
| train/                  |               |
|    approx_kl            | 0.00045682178 |
|    clip_fraction        | 0.00742       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.189        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 61.5          |
|    n_updates            | 200           |
|    policy_gradient_loss | -0.000191     |
|    value_loss           | 77.2          |
-------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3.58e+03   |
|    ep_rew_mean          | -617       |
| time/                   |            |
|    fps                  | 42         |
|    iterations           | 22         |
|    time_elapsed         | 1060       |
|    total_timesteps      | 45056      |
| train/                  |            |
|    approx_kl            | 0.00244322 |
|    clip_fraction        | 0.0162     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.174     |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0003     |
|    loss                 | 43.4       |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.000431  |
|    value_loss           | 87.6       |
----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.58e+03      |
|    ep_rew_mean          | -617          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 23            |
|    time_elapsed         | 1107          |
|    total_timesteps      | 47104         |
| train/                  |               |
|    approx_kl            | 0.00046864062 |
|    clip_fraction        | 0.00415       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.166        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 45.6          |
|    n_updates            | 220           |
|    policy_gradient_loss | -0.000372     |
|    value_loss           | 82.1          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.62e+03      |
|    ep_rew_mean          | -593          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 24            |
|    time_elapsed         | 1153          |
|    total_timesteps      | 49152         |
| train/                  |               |
|    approx_kl            | 0.00031419858 |
|    clip_fraction        | 0.011         |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.149        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 45.2          |
|    n_updates            | 230           |
|    policy_gradient_loss | -0.000145     |
|    value_loss           | 78.2          |
-------------------------------------------
Num timesteps: 50000
Best mean reward: -664.23 - Last mean reward per episode: -593.27
Saving new best model at 47105 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.62e+03      |
|    ep_rew_mean          | -593          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 25            |
|    time_elapsed         | 1200          |
|    total_timesteps      | 51200         |
| train/                  |               |
|    approx_kl            | 0.00031006048 |
|    clip_fraction        | 0.00752       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.129        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 44            |
|    n_updates            | 240           |
|    policy_gradient_loss | -0.000437     |
|    value_loss           | 83.4          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.66e+03     |
|    ep_rew_mean          | -562         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 26           |
|    time_elapsed         | 1248         |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0002850912 |
|    clip_fraction        | 0.00493      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.135       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 33.3         |
|    n_updates            | 250          |
|    policy_gradient_loss | -3.14e-05    |
|    value_loss           | 79.1         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.66e+03     |
|    ep_rew_mean          | -562         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 27           |
|    time_elapsed         | 1296         |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0005200672 |
|    clip_fraction        | 0.00693      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.123       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 54.8         |
|    n_updates            | 260          |
|    policy_gradient_loss | 0.000621     |
|    value_loss           | 85.6         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.69e+03     |
|    ep_rew_mean          | -537         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 28           |
|    time_elapsed         | 1344         |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0011072375 |
|    clip_fraction        | 0.0126       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.136       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 43.2         |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.000331    |
|    value_loss           | 80.5         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.69e+03      |
|    ep_rew_mean          | -537          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 29            |
|    time_elapsed         | 1392          |
|    total_timesteps      | 59392         |
| train/                  |               |
|    approx_kl            | 0.00011967993 |
|    clip_fraction        | 0.0061        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.142        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 41.4          |
|    n_updates            | 280           |
|    policy_gradient_loss | -0.000204     |
|    value_loss           | 81            |
-------------------------------------------
Num timesteps: 60000
Best mean reward: -593.27 - Last mean reward per episode: -511.66
Saving new best model at 59393 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.71e+03      |
|    ep_rew_mean          | -512          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 30            |
|    time_elapsed         | 1441          |
|    total_timesteps      | 61440         |
| train/                  |               |
|    approx_kl            | 5.9846003e-05 |
|    clip_fraction        | 0.0019        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.126        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 32.9          |
|    n_updates            | 290           |
|    policy_gradient_loss | 0.000173      |
|    value_loss           | 86.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.71e+03      |
|    ep_rew_mean          | -512          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 31            |
|    time_elapsed         | 1490          |
|    total_timesteps      | 63488         |
| train/                  |               |
|    approx_kl            | 4.3884735e-05 |
|    clip_fraction        | 0.00591       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.116        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 55.3          |
|    n_updates            | 300           |
|    policy_gradient_loss | 0.000153      |
|    value_loss           | 77.6          |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.73e+03    |
|    ep_rew_mean          | -498        |
| time/                   |             |
|    fps                  | 42          |
|    iterations           | 32          |
|    time_elapsed         | 1539        |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.000608213 |
|    clip_fraction        | 0.017       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.102      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 24.6        |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00111    |
|    value_loss           | 80.2        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.73e+03     |
|    ep_rew_mean          | -498         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 33           |
|    time_elapsed         | 1588         |
|    total_timesteps      | 67584        |
| train/                  |              |
|    approx_kl            | 0.0004068895 |
|    clip_fraction        | 0.00874      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0915      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 51.5         |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.000181    |
|    value_loss           | 81.5         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.75e+03      |
|    ep_rew_mean          | -494          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 34            |
|    time_elapsed         | 1637          |
|    total_timesteps      | 69632         |
| train/                  |               |
|    approx_kl            | 0.00015147415 |
|    clip_fraction        | 0.00264       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0924       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 39.1          |
|    n_updates            | 330           |
|    policy_gradient_loss | 0.000158      |
|    value_loss           | 75.9          |
-------------------------------------------
Num timesteps: 70000
Best mean reward: -511.66 - Last mean reward per episode: -494.03
Saving new best model at 67585 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.75e+03      |
|    ep_rew_mean          | -494          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 35            |
|    time_elapsed         | 1684          |
|    total_timesteps      | 71680         |
| train/                  |               |
|    approx_kl            | 0.00010981076 |
|    clip_fraction        | 0.00293       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0963       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 46            |
|    n_updates            | 340           |
|    policy_gradient_loss | 0.000107      |
|    value_loss           | 86.2          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.77e+03      |
|    ep_rew_mean          | -481          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 36            |
|    time_elapsed         | 1732          |
|    total_timesteps      | 73728         |
| train/                  |               |
|    approx_kl            | 0.00042818516 |
|    clip_fraction        | 0.00767       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0712       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 40.6          |
|    n_updates            | 350           |
|    policy_gradient_loss | -0.000643     |
|    value_loss           | 76            |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.77e+03     |
|    ep_rew_mean          | -481         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 37           |
|    time_elapsed         | 1780         |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0010733716 |
|    clip_fraction        | 0.00542      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0601      |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 39.8         |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.000183    |
|    value_loss           | 79.1         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.79e+03      |
|    ep_rew_mean          | -472          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 38            |
|    time_elapsed         | 1827          |
|    total_timesteps      | 77824         |
| train/                  |               |
|    approx_kl            | 0.00016920042 |
|    clip_fraction        | 0.00254       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0451       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 36.6          |
|    n_updates            | 370           |
|    policy_gradient_loss | -0.000109     |
|    value_loss           | 78.5          |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 3.79e+03       |
|    ep_rew_mean          | -472           |
| time/                   |                |
|    fps                  | 42             |
|    iterations           | 39             |
|    time_elapsed         | 1875           |
|    total_timesteps      | 79872          |
| train/                  |                |
|    approx_kl            | 0.000112202135 |
|    clip_fraction        | 0.00254        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0391        |
|    explained_variance   | 0              |
|    learning_rate        | 0.0003         |
|    loss                 | 36.8           |
|    n_updates            | 380            |
|    policy_gradient_loss | -0.000222      |
|    value_loss           | 73.9           |
--------------------------------------------
Num timesteps: 80000
Best mean reward: -494.03 - Last mean reward per episode: -463.17
Saving new best model at 79873 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.8e+03       |
|    ep_rew_mean          | -463          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 40            |
|    time_elapsed         | 1923          |
|    total_timesteps      | 81920         |
| train/                  |               |
|    approx_kl            | 0.00015411156 |
|    clip_fraction        | 0.00376       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0311       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 38.5          |
|    n_updates            | 390           |
|    policy_gradient_loss | -0.000287     |
|    value_loss           | 87.3          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.8e+03       |
|    ep_rew_mean          | -463          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 41            |
|    time_elapsed         | 1971          |
|    total_timesteps      | 83968         |
| train/                  |               |
|    approx_kl            | 0.00015614784 |
|    clip_fraction        | 0.00298       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0311       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 39.9          |
|    n_updates            | 400           |
|    policy_gradient_loss | -0.000379     |
|    value_loss           | 81            |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.82e+03      |
|    ep_rew_mean          | -449          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 42            |
|    time_elapsed         | 2017          |
|    total_timesteps      | 86016         |
| train/                  |               |
|    approx_kl            | 7.7816774e-05 |
|    clip_fraction        | 0.000732      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0277       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 26.6          |
|    n_updates            | 410           |
|    policy_gradient_loss | -5.84e-05     |
|    value_loss           | 78.8          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.82e+03      |
|    ep_rew_mean          | -449          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 43            |
|    time_elapsed         | 2065          |
|    total_timesteps      | 88064         |
| train/                  |               |
|    approx_kl            | 7.5460586e-05 |
|    clip_fraction        | 0.00181       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0222       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 18.7          |
|    n_updates            | 420           |
|    policy_gradient_loss | -0.000201     |
|    value_loss           | 66.9          |
-------------------------------------------
Num timesteps: 90000
Best mean reward: -463.17 - Last mean reward per episode: -445.76
Saving new best model at 88065 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.83e+03      |
|    ep_rew_mean          | -446          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 44            |
|    time_elapsed         | 2112          |
|    total_timesteps      | 90112         |
| train/                  |               |
|    approx_kl            | 0.00048471778 |
|    clip_fraction        | 0.00439       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0142       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 44.7          |
|    n_updates            | 430           |
|    policy_gradient_loss | -0.00033      |
|    value_loss           | 84.6          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.83e+03      |
|    ep_rew_mean          | -446          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 45            |
|    time_elapsed         | 2160          |
|    total_timesteps      | 92160         |
| train/                  |               |
|    approx_kl            | 4.7141133e-05 |
|    clip_fraction        | 0.00112       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0177       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 27.9          |
|    n_updates            | 440           |
|    policy_gradient_loss | -3.82e-05     |
|    value_loss           | 77.7          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.84e+03     |
|    ep_rew_mean          | -446         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 46           |
|    time_elapsed         | 2208         |
|    total_timesteps      | 94208        |
| train/                  |              |
|    approx_kl            | 5.529978e-05 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0174      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 40           |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.000221    |
|    value_loss           | 74.2         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.84e+03     |
|    ep_rew_mean          | -446         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 47           |
|    time_elapsed         | 2257         |
|    total_timesteps      | 96256        |
| train/                  |              |
|    approx_kl            | 5.971291e-06 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0179      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 40.3         |
|    n_updates            | 460          |
|    policy_gradient_loss | 1.65e-05     |
|    value_loss           | 78.6         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.85e+03      |
|    ep_rew_mean          | -435          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 48            |
|    time_elapsed         | 2306          |
|    total_timesteps      | 98304         |
| train/                  |               |
|    approx_kl            | 4.9316674e-05 |
|    clip_fraction        | 0.00103       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0228       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 23.8          |
|    n_updates            | 470           |
|    policy_gradient_loss | 4.07e-05      |
|    value_loss           | 85.1          |
-------------------------------------------
Num timesteps: 100000
Best mean reward: -445.76 - Last mean reward per episode: -434.74
Saving new best model at 96257 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.85e+03      |
|    ep_rew_mean          | -435          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 49            |
|    time_elapsed         | 2353          |
|    total_timesteps      | 100352        |
| train/                  |               |
|    approx_kl            | 0.00011147588 |
|    clip_fraction        | 0.0019        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0187       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 35.9          |
|    n_updates            | 480           |
|    policy_gradient_loss | -0.000167     |
|    value_loss           | 65.1          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.86e+03     |
|    ep_rew_mean          | -431         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 50           |
|    time_elapsed         | 2401         |
|    total_timesteps      | 102400       |
| train/                  |              |
|    approx_kl            | 9.321296e-05 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0222      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 53.6         |
|    n_updates            | 490          |
|    policy_gradient_loss | -7.59e-05    |
|    value_loss           | 84.8         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.86e+03      |
|    ep_rew_mean          | -431          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 51            |
|    time_elapsed         | 2449          |
|    total_timesteps      | 104448        |
| train/                  |               |
|    approx_kl            | 5.1214418e-05 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0136       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 52.2          |
|    n_updates            | 500           |
|    policy_gradient_loss | -0.000139     |
|    value_loss           | 75.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.87e+03      |
|    ep_rew_mean          | -430          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 52            |
|    time_elapsed         | 2497          |
|    total_timesteps      | 106496        |
| train/                  |               |
|    approx_kl            | 2.9632967e-05 |
|    clip_fraction        | 0.000537      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0115       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 48.1          |
|    n_updates            | 510           |
|    policy_gradient_loss | -5.14e-05     |
|    value_loss           | 75.6          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.87e+03     |
|    ep_rew_mean          | -430         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 53           |
|    time_elapsed         | 2543         |
|    total_timesteps      | 108544       |
| train/                  |              |
|    approx_kl            | 1.000351e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0119      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 55.4         |
|    n_updates            | 520          |
|    policy_gradient_loss | -1.39e-05    |
|    value_loss           | 85.2         |
------------------------------------------
Num timesteps: 110000
Best mean reward: -434.74 - Last mean reward per episode: -421.16
Saving new best model at 108545 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.88e+03      |
|    ep_rew_mean          | -421          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 54            |
|    time_elapsed         | 2591          |
|    total_timesteps      | 110592        |
| train/                  |               |
|    approx_kl            | 0.00051626784 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0168       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 61            |
|    n_updates            | 530           |
|    policy_gradient_loss | 0.00021       |
|    value_loss           | 77.1          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.88e+03      |
|    ep_rew_mean          | -421          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 55            |
|    time_elapsed         | 2639          |
|    total_timesteps      | 112640        |
| train/                  |               |
|    approx_kl            | 0.00012268484 |
|    clip_fraction        | 0.00151       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0173       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 50.8          |
|    n_updates            | 540           |
|    policy_gradient_loss | -4.14e-05     |
|    value_loss           | 90.3          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.88e+03      |
|    ep_rew_mean          | -411          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 56            |
|    time_elapsed         | 2686          |
|    total_timesteps      | 114688        |
| train/                  |               |
|    approx_kl            | 0.00021058248 |
|    clip_fraction        | 0.00239       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0133       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 30            |
|    n_updates            | 550           |
|    policy_gradient_loss | -0.000101     |
|    value_loss           | 73.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.88e+03      |
|    ep_rew_mean          | -411          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 57            |
|    time_elapsed         | 2733          |
|    total_timesteps      | 116736        |
| train/                  |               |
|    approx_kl            | 7.4643176e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0111       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 15.4          |
|    n_updates            | 560           |
|    policy_gradient_loss | -2.06e-05     |
|    value_loss           | 67.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.89e+03      |
|    ep_rew_mean          | -410          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 58            |
|    time_elapsed         | 2781          |
|    total_timesteps      | 118784        |
| train/                  |               |
|    approx_kl            | 3.9723964e-05 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0106       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 41.4          |
|    n_updates            | 570           |
|    policy_gradient_loss | -5.56e-05     |
|    value_loss           | 83.7          |
-------------------------------------------
Num timesteps: 120000
Best mean reward: -421.16 - Last mean reward per episode: -409.62
Saving new best model at 116737 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.89e+03      |
|    ep_rew_mean          | -410          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 59            |
|    time_elapsed         | 2829          |
|    total_timesteps      | 120832        |
| train/                  |               |
|    approx_kl            | 3.3011107e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0109       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 58.8          |
|    n_updates            | 580           |
|    policy_gradient_loss | -0.000104     |
|    value_loss           | 80.6          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.9e+03       |
|    ep_rew_mean          | -407          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 60            |
|    time_elapsed         | 2878          |
|    total_timesteps      | 122880        |
| train/                  |               |
|    approx_kl            | 2.6204594e-05 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00795      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 30.7          |
|    n_updates            | 590           |
|    policy_gradient_loss | -5.24e-05     |
|    value_loss           | 73.5          |
-------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3.9e+03    |
|    ep_rew_mean          | -407       |
| time/                   |            |
|    fps                  | 42         |
|    iterations           | 61         |
|    time_elapsed         | 2925       |
|    total_timesteps      | 124928     |
| train/                  |            |
|    approx_kl            | 4.9739e-05 |
|    clip_fraction        | 0.00137    |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00547   |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 29.9       |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.000106  |
|    value_loss           | 72.8       |
----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.9e+03       |
|    ep_rew_mean          | -406          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 62            |
|    time_elapsed         | 2973          |
|    total_timesteps      | 126976        |
| train/                  |               |
|    approx_kl            | 2.5090965e-05 |
|    clip_fraction        | 0.00083       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00489      |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 39.7          |
|    n_updates            | 610           |
|    policy_gradient_loss | -0.000119     |
|    value_loss           | 81.3          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.9e+03       |
|    ep_rew_mean          | -406          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 63            |
|    time_elapsed         | 3021          |
|    total_timesteps      | 129024        |
| train/                  |               |
|    approx_kl            | 0.00026441857 |
|    clip_fraction        | 0.00103       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00657      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 32.6          |
|    n_updates            | 620           |
|    policy_gradient_loss | 1.64e-06      |
|    value_loss           | 77.8          |
-------------------------------------------
Num timesteps: 130000
Best mean reward: -409.62 - Last mean reward per episode: -403.11
Saving new best model at 129025 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.91e+03      |
|    ep_rew_mean          | -403          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 64            |
|    time_elapsed         | 3068          |
|    total_timesteps      | 131072        |
| train/                  |               |
|    approx_kl            | 1.7454877e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00596      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 26.3          |
|    n_updates            | 630           |
|    policy_gradient_loss | -6.51e-05     |
|    value_loss           | 73.5          |
-------------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 3.91e+03 |
|    ep_rew_mean          | -403     |
| time/                   |          |
|    fps                  | 42       |
|    iterations           | 65       |
|    time_elapsed         | 3116     |
|    total_timesteps      | 133120   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00579 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 37.3     |
|    n_updates            | 640      |
|    policy_gradient_loss | 3.22e-10 |
|    value_loss           | 86.1     |
--------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.92e+03      |
|    ep_rew_mean          | -394          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 66            |
|    time_elapsed         | 3165          |
|    total_timesteps      | 135168        |
| train/                  |               |
|    approx_kl            | 2.4545094e-05 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00514      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 56.4          |
|    n_updates            | 650           |
|    policy_gradient_loss | -6.48e-05     |
|    value_loss           | 82            |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.92e+03      |
|    ep_rew_mean          | -394          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 67            |
|    time_elapsed         | 3212          |
|    total_timesteps      | 137216        |
| train/                  |               |
|    approx_kl            | 4.0910003e-05 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00385      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 37.8          |
|    n_updates            | 660           |
|    policy_gradient_loss | -0.000148     |
|    value_loss           | 84.4          |
-------------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 3.92e+03 |
|    ep_rew_mean          | -383     |
| time/                   |          |
|    fps                  | 42       |
|    iterations           | 68       |
|    time_elapsed         | 3259     |
|    total_timesteps      | 139264   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00369 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 34.2     |
|    n_updates            | 670      |
|    policy_gradient_loss | 1.11e-09 |
|    value_loss           | 86.5     |
--------------------------------------
Num timesteps: 140000
Best mean reward: -403.11 - Last mean reward per episode: -383.10
Saving new best model at 137217 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 3.92e+03 |
|    ep_rew_mean          | -383     |
| time/                   |          |
|    fps                  | 42       |
|    iterations           | 69       |
|    time_elapsed         | 3306     |
|    total_timesteps      | 141312   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00369 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 39       |
|    n_updates            | 680      |
|    policy_gradient_loss | 6.05e-10 |
|    value_loss           | 71.8     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.93e+03  |
|    ep_rew_mean          | -381      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 70        |
|    time_elapsed         | 3352      |
|    total_timesteps      | 143360    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00369  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 42        |
|    n_updates            | 690       |
|    policy_gradient_loss | 1.09e-09  |
|    value_loss           | 85.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.93e+03  |
|    ep_rew_mean          | -381      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 71        |
|    time_elapsed         | 3398      |
|    total_timesteps      | 145408    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00369  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 38.6      |
|    n_updates            | 700       |
|    policy_gradient_loss | 1.69e-10  |
|    value_loss           | 86.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.93e+03  |
|    ep_rew_mean          | -369      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 72        |
|    time_elapsed         | 3445      |
|    total_timesteps      | 147456    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00369  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 32        |
|    n_updates            | 710       |
|    policy_gradient_loss | -3.59e-10 |
|    value_loss           | 83.3      |
---------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.93e+03     |
|    ep_rew_mean          | -369         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 73           |
|    time_elapsed         | 3493         |
|    total_timesteps      | 149504       |
| train/                  |              |
|    approx_kl            | 7.705824e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00238     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 57.4         |
|    n_updates            | 720          |
|    policy_gradient_loss | -2.27e-05    |
|    value_loss           | 80.6         |
------------------------------------------
Num timesteps: 150000
Best mean reward: -383.10 - Last mean reward per episode: -368.49
Saving new best model at 149505 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.93e+03  |
|    ep_rew_mean          | -368      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 74        |
|    time_elapsed         | 3540      |
|    total_timesteps      | 151552    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0023   |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 28.6      |
|    n_updates            | 730       |
|    policy_gradient_loss | -2.55e-09 |
|    value_loss           | 81.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.93e+03  |
|    ep_rew_mean          | -368      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 75        |
|    time_elapsed         | 3587      |
|    total_timesteps      | 153600    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0023   |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 39.4      |
|    n_updates            | 740       |
|    policy_gradient_loss | 4.65e-10  |
|    value_loss           | 81.2      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 3.94e+03 |
|    ep_rew_mean          | -366     |
| time/                   |          |
|    fps                  | 42       |
|    iterations           | 76       |
|    time_elapsed         | 3635     |
|    total_timesteps      | 155648   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0023  |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.0003   |
|    loss                 | 55.1     |
|    n_updates            | 750      |
|    policy_gradient_loss | 9.78e-10 |
|    value_loss           | 82.5     |
--------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 3.94e+03       |
|    ep_rew_mean          | -366           |
| time/                   |                |
|    fps                  | 42             |
|    iterations           | 77             |
|    time_elapsed         | 3682           |
|    total_timesteps      | 157696         |
| train/                  |                |
|    approx_kl            | 0.000104927516 |
|    clip_fraction        | 0.000488       |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0022        |
|    explained_variance   | 0              |
|    learning_rate        | 0.0003         |
|    loss                 | 51.1           |
|    n_updates            | 760            |
|    policy_gradient_loss | 5.8e-05        |
|    value_loss           | 84.4           |
--------------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 3.94e+03 |
|    ep_rew_mean          | -365     |
| time/                   |          |
|    fps                  | 42       |
|    iterations           | 78       |
|    time_elapsed         | 3729     |
|    total_timesteps      | 159744   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00306 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 33       |
|    n_updates            | 770      |
|    policy_gradient_loss | 1.91e-09 |
|    value_loss           | 73.8     |
--------------------------------------
Num timesteps: 160000
Best mean reward: -368.49 - Last mean reward per episode: -365.46
Saving new best model at 157697 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.94e+03     |
|    ep_rew_mean          | -365         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 79           |
|    time_elapsed         | 3778         |
|    total_timesteps      | 161792       |
| train/                  |              |
|    approx_kl            | 8.380346e-05 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00312     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 47.6         |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.000207    |
|    value_loss           | 76.9         |
------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.95e+03  |
|    ep_rew_mean          | -366      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 80        |
|    time_elapsed         | 3826      |
|    total_timesteps      | 163840    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00286  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 42.4      |
|    n_updates            | 790       |
|    policy_gradient_loss | -1.92e-10 |
|    value_loss           | 77.7      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 3.95e+03 |
|    ep_rew_mean          | -366     |
| time/                   |          |
|    fps                  | 42       |
|    iterations           | 81       |
|    time_elapsed         | 3874     |
|    total_timesteps      | 165888   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00286 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 49.2     |
|    n_updates            | 800      |
|    policy_gradient_loss | 4.73e-11 |
|    value_loss           | 76.2     |
--------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.95e+03      |
|    ep_rew_mean          | -368          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 82            |
|    time_elapsed         | 3922          |
|    total_timesteps      | 167936        |
| train/                  |               |
|    approx_kl            | 4.1476946e-05 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00221      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 30.2          |
|    n_updates            | 810           |
|    policy_gradient_loss | -0.000119     |
|    value_loss           | 70.6          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.95e+03     |
|    ep_rew_mean          | -368         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 83           |
|    time_elapsed         | 3969         |
|    total_timesteps      | 169984       |
| train/                  |              |
|    approx_kl            | 0.0001769414 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00331     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 43.4         |
|    n_updates            | 820          |
|    policy_gradient_loss | -0.000154    |
|    value_loss           | 80.5         |
------------------------------------------
Num timesteps: 170000
Best mean reward: -365.46 - Last mean reward per episode: -369.83
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.95e+03     |
|    ep_rew_mean          | -370         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 84           |
|    time_elapsed         | 4018         |
|    total_timesteps      | 172032       |
| train/                  |              |
|    approx_kl            | 3.394432e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00279     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 29.1         |
|    n_updates            | 830          |
|    policy_gradient_loss | -3.79e-05    |
|    value_loss           | 68           |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.95e+03     |
|    ep_rew_mean          | -370         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 85           |
|    time_elapsed         | 4066         |
|    total_timesteps      | 174080       |
| train/                  |              |
|    approx_kl            | 3.366056e-05 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0022      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 54           |
|    n_updates            | 840          |
|    policy_gradient_loss | -6.86e-05    |
|    value_loss           | 82.3         |
------------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 3.96e+03 |
|    ep_rew_mean          | -370     |
| time/                   |          |
|    fps                  | 42       |
|    iterations           | 86       |
|    time_elapsed         | 4113     |
|    total_timesteps      | 176128   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00213 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 42.7     |
|    n_updates            | 850      |
|    policy_gradient_loss | 3.59e-10 |
|    value_loss           | 74.3     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 3.96e+03 |
|    ep_rew_mean          | -370     |
| time/                   |          |
|    fps                  | 42       |
|    iterations           | 87       |
|    time_elapsed         | 4162     |
|    total_timesteps      | 178176   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00213 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 49.5     |
|    n_updates            | 860      |
|    policy_gradient_loss | 8.71e-10 |
|    value_loss           | 88.4     |
--------------------------------------
Num timesteps: 180000
Best mean reward: -365.46 - Last mean reward per episode: -361.08
Saving new best model at 178177 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.96e+03      |
|    ep_rew_mean          | -361          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 88            |
|    time_elapsed         | 4210          |
|    total_timesteps      | 180224        |
| train/                  |               |
|    approx_kl            | 6.4312946e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00295      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 44.2          |
|    n_updates            | 870           |
|    policy_gradient_loss | -6.5e-05      |
|    value_loss           | 83.3          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.96e+03      |
|    ep_rew_mean          | -361          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 89            |
|    time_elapsed         | 4258          |
|    total_timesteps      | 182272        |
| train/                  |               |
|    approx_kl            | 1.7115031e-05 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00306      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 32.4          |
|    n_updates            | 880           |
|    policy_gradient_loss | 7.43e-07      |
|    value_loss           | 81.8          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.96e+03     |
|    ep_rew_mean          | -359         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 90           |
|    time_elapsed         | 4305         |
|    total_timesteps      | 184320       |
| train/                  |              |
|    approx_kl            | 9.440223e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00247     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 34.8         |
|    n_updates            | 890          |
|    policy_gradient_loss | -6.83e-05    |
|    value_loss           | 83.3         |
------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.96e+03  |
|    ep_rew_mean          | -359      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 91        |
|    time_elapsed         | 4351      |
|    total_timesteps      | 186368    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00243  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 35.1      |
|    n_updates            | 900       |
|    policy_gradient_loss | 1.66e-10  |
|    value_loss           | 70.8      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.97e+03      |
|    ep_rew_mean          | -357          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 92            |
|    time_elapsed         | 4397          |
|    total_timesteps      | 188416        |
| train/                  |               |
|    approx_kl            | 4.7381967e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00322      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 32.4          |
|    n_updates            | 910           |
|    policy_gradient_loss | -1.84e-05     |
|    value_loss           | 86.5          |
-------------------------------------------
Num timesteps: 190000
Best mean reward: -361.08 - Last mean reward per episode: -356.90
Saving new best model at 186369 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.97e+03    |
|    ep_rew_mean          | -357        |
| time/                   |             |
|    fps                  | 42          |
|    iterations           | 93          |
|    time_elapsed         | 4446        |
|    total_timesteps      | 190464      |
| train/                  |             |
|    approx_kl            | 0.000318836 |
|    clip_fraction        | 0.00142     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00513    |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 35.9        |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.000341   |
|    value_loss           | 87          |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.97e+03     |
|    ep_rew_mean          | -352         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 94           |
|    time_elapsed         | 4494         |
|    total_timesteps      | 192512       |
| train/                  |              |
|    approx_kl            | 0.0010052497 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.013       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 22.3         |
|    n_updates            | 930          |
|    policy_gradient_loss | -0.000262    |
|    value_loss           | 77.6         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.97e+03      |
|    ep_rew_mean          | -352          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 95            |
|    time_elapsed         | 4541          |
|    total_timesteps      | 194560        |
| train/                  |               |
|    approx_kl            | 5.9139013e-05 |
|    clip_fraction        | 0.000928      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0104       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 40.4          |
|    n_updates            | 940           |
|    policy_gradient_loss | -2.19e-05     |
|    value_loss           | 80.5          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.97e+03      |
|    ep_rew_mean          | -351          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 96            |
|    time_elapsed         | 4587          |
|    total_timesteps      | 196608        |
| train/                  |               |
|    approx_kl            | 0.00034946072 |
|    clip_fraction        | 0.00151       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00877      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 44            |
|    n_updates            | 950           |
|    policy_gradient_loss | -1.66e-05     |
|    value_loss           | 79            |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.97e+03      |
|    ep_rew_mean          | -351          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 97            |
|    time_elapsed         | 4635          |
|    total_timesteps      | 198656        |
| train/                  |               |
|    approx_kl            | 0.00023606053 |
|    clip_fraction        | 0.0022        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00897      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 39            |
|    n_updates            | 960           |
|    policy_gradient_loss | -0.000168     |
|    value_loss           | 75.5          |
-------------------------------------------
Num timesteps: 200000
Best mean reward: -356.90 - Last mean reward per episode: -346.77
Saving new best model at 198657 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.97e+03      |
|    ep_rew_mean          | -347          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 98            |
|    time_elapsed         | 4683          |
|    total_timesteps      | 200704        |
| train/                  |               |
|    approx_kl            | 0.00012417557 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00612      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 44            |
|    n_updates            | 970           |
|    policy_gradient_loss | -7.82e-05     |
|    value_loss           | 84.4          |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.97e+03  |
|    ep_rew_mean          | -347      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 99        |
|    time_elapsed         | 4731      |
|    total_timesteps      | 202752    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00536  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 49.9      |
|    n_updates            | 980       |
|    policy_gradient_loss | -1.36e-09 |
|    value_loss           | 85.1      |
---------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.98e+03     |
|    ep_rew_mean          | -340         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 100          |
|    time_elapsed         | 4778         |
|    total_timesteps      | 204800       |
| train/                  |              |
|    approx_kl            | 0.0003347437 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00355     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 52.3         |
|    n_updates            | 990          |
|    policy_gradient_loss | -0.000102    |
|    value_loss           | 85.6         |
------------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 3.98e+03 |
|    ep_rew_mean          | -340     |
| time/                   |          |
|    fps                  | 42       |
|    iterations           | 101      |
|    time_elapsed         | 4824     |
|    total_timesteps      | 206848   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0026  |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 29.3     |
|    n_updates            | 1000     |
|    policy_gradient_loss | 3.67e-10 |
|    value_loss           | 74.2     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 3.98e+03 |
|    ep_rew_mean          | -338     |
| time/                   |          |
|    fps                  | 42       |
|    iterations           | 102      |
|    time_elapsed         | 4870     |
|    total_timesteps      | 208896   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0026  |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 35.4     |
|    n_updates            | 1010     |
|    policy_gradient_loss | 1.22e-09 |
|    value_loss           | 80.4     |
--------------------------------------
Num timesteps: 210000
Best mean reward: -346.77 - Last mean reward per episode: -338.36
Saving new best model at 206849 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.98e+03      |
|    ep_rew_mean          | -338          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 103           |
|    time_elapsed         | 4917          |
|    total_timesteps      | 210944        |
| train/                  |               |
|    approx_kl            | 2.1366315e-05 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00227      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 35.8          |
|    n_updates            | 1020          |
|    policy_gradient_loss | -8.17e-06     |
|    value_loss           | 86.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.98e+03      |
|    ep_rew_mean          | -333          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 104           |
|    time_elapsed         | 4963          |
|    total_timesteps      | 212992        |
| train/                  |               |
|    approx_kl            | 2.8162933e-05 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00184      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 42            |
|    n_updates            | 1030          |
|    policy_gradient_loss | -1.83e-05     |
|    value_loss           | 83.3          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.98e+03      |
|    ep_rew_mean          | -333          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 105           |
|    time_elapsed         | 5010          |
|    total_timesteps      | 215040        |
| train/                  |               |
|    approx_kl            | 1.3924029e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00137      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 38.6          |
|    n_updates            | 1040          |
|    policy_gradient_loss | -2.27e-05     |
|    value_loss           | 79.3          |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.98e+03  |
|    ep_rew_mean          | -333      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 106       |
|    time_elapsed         | 5056      |
|    total_timesteps      | 217088    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00134  |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 34        |
|    n_updates            | 1050      |
|    policy_gradient_loss | -9.23e-10 |
|    value_loss           | 74.5      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.98e+03      |
|    ep_rew_mean          | -333          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 107           |
|    time_elapsed         | 5102          |
|    total_timesteps      | 219136        |
| train/                  |               |
|    approx_kl            | 0.00024545917 |
|    clip_fraction        | 0.000537      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00132      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 44.8          |
|    n_updates            | 1060          |
|    policy_gradient_loss | -9.69e-05     |
|    value_loss           | 70.8          |
-------------------------------------------
Num timesteps: 220000
Best mean reward: -338.36 - Last mean reward per episode: -332.88
Saving new best model at 219137 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.98e+03      |
|    ep_rew_mean          | -333          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 108           |
|    time_elapsed         | 5148          |
|    total_timesteps      | 221184        |
| train/                  |               |
|    approx_kl            | 2.2114342e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000992     |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 59.8          |
|    n_updates            | 1070          |
|    policy_gradient_loss | -4.24e-05     |
|    value_loss           | 82.8          |
-------------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 3.98e+03 |
|    ep_rew_mean          | -333     |
| time/                   |          |
|    fps                  | 42       |
|    iterations           | 109      |
|    time_elapsed         | 5196     |
|    total_timesteps      | 223232   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00096 |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.0003   |
|    loss                 | 34.4     |
|    n_updates            | 1080     |
|    policy_gradient_loss | 3.36e-10 |
|    value_loss           | 74.3     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 3.99e+03 |
|    ep_rew_mean          | -336     |
| time/                   |          |
|    fps                  | 42       |
|    iterations           | 110      |
|    time_elapsed         | 5243     |
|    total_timesteps      | 225280   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00096 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 26.6     |
|    n_updates            | 1090     |
|    policy_gradient_loss | 1.67e-09 |
|    value_loss           | 74.1     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99e+03  |
|    ep_rew_mean          | -336      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 111       |
|    time_elapsed         | 5289      |
|    total_timesteps      | 227328    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00096  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 47.2      |
|    n_updates            | 1100      |
|    policy_gradient_loss | -5.82e-10 |
|    value_loss           | 81        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99e+03  |
|    ep_rew_mean          | -334      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 112       |
|    time_elapsed         | 5336      |
|    total_timesteps      | 229376    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00096  |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 43.1      |
|    n_updates            | 1110      |
|    policy_gradient_loss | -4.34e-10 |
|    value_loss           | 81.8      |
---------------------------------------
Num timesteps: 230000
Best mean reward: -332.88 - Last mean reward per episode: -334.11
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 3.99e+03 |
|    ep_rew_mean          | -334     |
| time/                   |          |
|    fps                  | 42       |
|    iterations           | 113      |
|    time_elapsed         | 5384     |
|    total_timesteps      | 231424   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00096 |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.0003   |
|    loss                 | 32.7     |
|    n_updates            | 1120     |
|    policy_gradient_loss | 1.16e-10 |
|    value_loss           | 77       |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99e+03  |
|    ep_rew_mean          | -332      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 114       |
|    time_elapsed         | 5433      |
|    total_timesteps      | 233472    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00096  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 57.6      |
|    n_updates            | 1130      |
|    policy_gradient_loss | -1.67e-09 |
|    value_loss           | 86.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99e+03  |
|    ep_rew_mean          | -332      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 115       |
|    time_elapsed         | 5480      |
|    total_timesteps      | 235520    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00096  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 53.7      |
|    n_updates            | 1140      |
|    policy_gradient_loss | -9.05e-10 |
|    value_loss           | 82.2      |
---------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.99e+03     |
|    ep_rew_mean          | -327         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 116          |
|    time_elapsed         | 5526         |
|    total_timesteps      | 237568       |
| train/                  |              |
|    approx_kl            | 8.474302e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0006      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 29.6         |
|    n_updates            | 1150         |
|    policy_gradient_loss | -5.24e-05    |
|    value_loss           | 85.5         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.99e+03      |
|    ep_rew_mean          | -327          |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 117           |
|    time_elapsed         | 5573          |
|    total_timesteps      | 239616        |
| train/                  |               |
|    approx_kl            | 5.8299454e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000403     |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 37.4          |
|    n_updates            | 1160          |
|    policy_gradient_loss | -6.98e-05     |
|    value_loss           | 77            |
-------------------------------------------
Num timesteps: 240000
Best mean reward: -332.88 - Last mean reward per episode: -327.81
Saving new best model at 239617 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99e+03  |
|    ep_rew_mean          | -328      |
| time/                   |           |
|    fps                  | 43        |
|    iterations           | 118       |
|    time_elapsed         | 5620      |
|    total_timesteps      | 241664    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 22.9      |
|    n_updates            | 1170      |
|    policy_gradient_loss | -1.94e-09 |
|    value_loss           | 77.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99e+03  |
|    ep_rew_mean          | -328      |
| time/                   |           |
|    fps                  | 43        |
|    iterations           | 119       |
|    time_elapsed         | 5667      |
|    total_timesteps      | 243712    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 41.1      |
|    n_updates            | 1180      |
|    policy_gradient_loss | -6.96e-10 |
|    value_loss           | 89.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4e+03     |
|    ep_rew_mean          | -325      |
| time/                   |           |
|    fps                  | 43        |
|    iterations           | 120       |
|    time_elapsed         | 5714      |
|    total_timesteps      | 245760    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 35.1      |
|    n_updates            | 1190      |
|    policy_gradient_loss | 1.19e-09  |
|    value_loss           | 82.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4e+03     |
|    ep_rew_mean          | -325      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 121       |
|    time_elapsed         | 5763      |
|    total_timesteps      | 247808    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 35.7      |
|    n_updates            | 1200      |
|    policy_gradient_loss | -3.13e-10 |
|    value_loss           | 55.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4e+03     |
|    ep_rew_mean          | -327      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 122       |
|    time_elapsed         | 5812      |
|    total_timesteps      | 249856    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 35.9      |
|    n_updates            | 1210      |
|    policy_gradient_loss | 5.82e-10  |
|    value_loss           | 89.7      |
---------------------------------------
Num timesteps: 250000
Best mean reward: -327.81 - Last mean reward per episode: -326.69
Saving new best model at 247809 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4e+03     |
|    ep_rew_mean          | -327      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 123       |
|    time_elapsed         | 5860      |
|    total_timesteps      | 251904    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 36.4      |
|    n_updates            | 1220      |
|    policy_gradient_loss | 2.42e-10  |
|    value_loss           | 81.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4e+03     |
|    ep_rew_mean          | -324      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 124       |
|    time_elapsed         | 5908      |
|    total_timesteps      | 253952    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 48.2      |
|    n_updates            | 1230      |
|    policy_gradient_loss | 5.54e-10  |
|    value_loss           | 81.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4e+03     |
|    ep_rew_mean          | -324      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 125       |
|    time_elapsed         | 5956      |
|    total_timesteps      | 256000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 24        |
|    n_updates            | 1240      |
|    policy_gradient_loss | -2.69e-10 |
|    value_loss           | 76.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4e+03     |
|    ep_rew_mean          | -325      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 126       |
|    time_elapsed         | 6004      |
|    total_timesteps      | 258048    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 28.5      |
|    n_updates            | 1250      |
|    policy_gradient_loss | 2.78e-10  |
|    value_loss           | 76.4      |
---------------------------------------
Num timesteps: 260000
Best mean reward: -326.69 - Last mean reward per episode: -324.70
Saving new best model at 256001 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4e+03     |
|    ep_rew_mean          | -325      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 127       |
|    time_elapsed         | 6052      |
|    total_timesteps      | 260096    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 47        |
|    n_updates            | 1260      |
|    policy_gradient_loss | -1.11e-09 |
|    value_loss           | 75.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4e+03     |
|    ep_rew_mean          | -327      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 128       |
|    time_elapsed         | 6100      |
|    total_timesteps      | 262144    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 30        |
|    n_updates            | 1270      |
|    policy_gradient_loss | -7.17e-10 |
|    value_loss           | 67.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4e+03     |
|    ep_rew_mean          | -327      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 129       |
|    time_elapsed         | 6148      |
|    total_timesteps      | 264192    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 45.5      |
|    n_updates            | 1280      |
|    policy_gradient_loss | 3.13e-10  |
|    value_loss           | 76.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4e+03     |
|    ep_rew_mean          | -324      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 130       |
|    time_elapsed         | 6195      |
|    total_timesteps      | 266240    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 49.6      |
|    n_updates            | 1290      |
|    policy_gradient_loss | -1.02e-11 |
|    value_loss           | 87.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4e+03     |
|    ep_rew_mean          | -324      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 131       |
|    time_elapsed         | 6243      |
|    total_timesteps      | 268288    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 41.9      |
|    n_updates            | 1300      |
|    policy_gradient_loss | 1.2e-09   |
|    value_loss           | 84.1      |
---------------------------------------
Num timesteps: 270000
Best mean reward: -324.70 - Last mean reward per episode: -322.60
Saving new best model at 268289 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4e+03     |
|    ep_rew_mean          | -323      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 132       |
|    time_elapsed         | 6290      |
|    total_timesteps      | 270336    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 47.9      |
|    n_updates            | 1310      |
|    policy_gradient_loss | 9.88e-10  |
|    value_loss           | 85.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4e+03     |
|    ep_rew_mean          | -323      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 133       |
|    time_elapsed         | 6338      |
|    total_timesteps      | 272384    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 56.4      |
|    n_updates            | 1320      |
|    policy_gradient_loss | -1.46e-09 |
|    value_loss           | 85.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01e+03  |
|    ep_rew_mean          | -321      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 134       |
|    time_elapsed         | 6385      |
|    total_timesteps      | 274432    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 35.4      |
|    n_updates            | 1330      |
|    policy_gradient_loss | 1.47e-10  |
|    value_loss           | 77.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01e+03  |
|    ep_rew_mean          | -321      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 135       |
|    time_elapsed         | 6432      |
|    total_timesteps      | 276480    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 30.4      |
|    n_updates            | 1340      |
|    policy_gradient_loss | 8.02e-10  |
|    value_loss           | 78.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01e+03  |
|    ep_rew_mean          | -316      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 136       |
|    time_elapsed         | 6479      |
|    total_timesteps      | 278528    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0003    |
|    loss                 | 33.9      |
|    n_updates            | 1350      |
|    policy_gradient_loss | 7.97e-10  |
|    value_loss           | 85.8      |
---------------------------------------
Num timesteps: 280000
Best mean reward: -322.60 - Last mean reward per episode: -316.09
Saving new best model at 276481 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01e+03  |
|    ep_rew_mean          | -316      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 137       |
|    time_elapsed         | 6528      |
|    total_timesteps      | 280576    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 35.8      |
|    n_updates            | 1360      |
|    policy_gradient_loss | -4.57e-10 |
|    value_loss           | 80.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01e+03  |
|    ep_rew_mean          | -315      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 138       |
|    time_elapsed         | 6577      |
|    total_timesteps      | 282624    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 32.4      |
|    n_updates            | 1370      |
|    policy_gradient_loss | 9.65e-10  |
|    value_loss           | 79.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01e+03  |
|    ep_rew_mean          | -315      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 139       |
|    time_elapsed         | 6626      |
|    total_timesteps      | 284672    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 25.7      |
|    n_updates            | 1380      |
|    policy_gradient_loss | -4.1e-10  |
|    value_loss           | 73        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01e+03  |
|    ep_rew_mean          | -319      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 140       |
|    time_elapsed         | 6674      |
|    total_timesteps      | 286720    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 39.7      |
|    n_updates            | 1390      |
|    policy_gradient_loss | -5.62e-10 |
|    value_loss           | 72        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01e+03  |
|    ep_rew_mean          | -319      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 141       |
|    time_elapsed         | 6722      |
|    total_timesteps      | 288768    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 43.3      |
|    n_updates            | 1400      |
|    policy_gradient_loss | 7.97e-10  |
|    value_loss           | 86.5      |
---------------------------------------
Num timesteps: 290000
Best mean reward: -316.09 - Last mean reward per episode: -314.92
Saving new best model at 288769 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01e+03  |
|    ep_rew_mean          | -315      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 142       |
|    time_elapsed         | 6769      |
|    total_timesteps      | 290816    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 33.7      |
|    n_updates            | 1410      |
|    policy_gradient_loss | -6.64e-10 |
|    value_loss           | 79.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01e+03  |
|    ep_rew_mean          | -315      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 143       |
|    time_elapsed         | 6816      |
|    total_timesteps      | 292864    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 51.9      |
|    n_updates            | 1420      |
|    policy_gradient_loss | -1.09e-11 |
|    value_loss           | 85.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01e+03  |
|    ep_rew_mean          | -312      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 144       |
|    time_elapsed         | 6864      |
|    total_timesteps      | 294912    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 38.2      |
|    n_updates            | 1430      |
|    policy_gradient_loss | -5.32e-10 |
|    value_loss           | 78.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01e+03  |
|    ep_rew_mean          | -312      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 145       |
|    time_elapsed         | 6910      |
|    total_timesteps      | 296960    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 39.5      |
|    n_updates            | 1440      |
|    policy_gradient_loss | -1.99e-10 |
|    value_loss           | 80.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01e+03  |
|    ep_rew_mean          | -309      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 146       |
|    time_elapsed         | 6957      |
|    total_timesteps      | 299008    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 28.9      |
|    n_updates            | 1450      |
|    policy_gradient_loss | -7.2e-10  |
|    value_loss           | 86.6      |
---------------------------------------
Num timesteps: 300000
Best mean reward: -314.92 - Last mean reward per episode: -309.47
Saving new best model at 296961 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01e+03  |
|    ep_rew_mean          | -309      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 147       |
|    time_elapsed         | 7005      |
|    total_timesteps      | 301056    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 38.4      |
|    n_updates            | 1460      |
|    policy_gradient_loss | -5.68e-10 |
|    value_loss           | 68.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01e+03  |
|    ep_rew_mean          | -313      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 148       |
|    time_elapsed         | 7054      |
|    total_timesteps      | 303104    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 47.6      |
|    n_updates            | 1470      |
|    policy_gradient_loss | -8.13e-10 |
|    value_loss           | 80.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01e+03  |
|    ep_rew_mean          | -313      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 149       |
|    time_elapsed         | 7101      |
|    total_timesteps      | 305152    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 39.3      |
|    n_updates            | 1480      |
|    policy_gradient_loss | -7.79e-10 |
|    value_loss           | 86.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02e+03  |
|    ep_rew_mean          | -313      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 150       |
|    time_elapsed         | 7149      |
|    total_timesteps      | 307200    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 29.4      |
|    n_updates            | 1490      |
|    policy_gradient_loss | -4.15e-10 |
|    value_loss           | 67.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02e+03  |
|    ep_rew_mean          | -313      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 151       |
|    time_elapsed         | 7196      |
|    total_timesteps      | 309248    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000382 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 53        |
|    n_updates            | 1500      |
|    policy_gradient_loss | -1.89e-11 |
|    value_loss           | 84.5      |
---------------------------------------
Num timesteps: 310000
Best mean reward: -309.47 - Last mean reward per episode: -312.53
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 4.02e+03     |
|    ep_rew_mean          | -313         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 152          |
|    time_elapsed         | 7244         |
|    total_timesteps      | 311296       |
| train/                  |              |
|    approx_kl            | 0.0003259129 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000176    |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 37.4         |
|    n_updates            | 1510         |
|    policy_gradient_loss | -0.000114    |
|    value_loss           | 79.2         |
------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02e+03  |
|    ep_rew_mean          | -313      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 153       |
|    time_elapsed         | 7293      |
|    total_timesteps      | 313344    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 49.4      |
|    n_updates            | 1520      |
|    policy_gradient_loss | 1.75e-09  |
|    value_loss           | 92.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02e+03  |
|    ep_rew_mean          | -307      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 154       |
|    time_elapsed         | 7342      |
|    total_timesteps      | 315392    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 25.3      |
|    n_updates            | 1530      |
|    policy_gradient_loss | 5.46e-10  |
|    value_loss           | 82.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02e+03  |
|    ep_rew_mean          | -307      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 155       |
|    time_elapsed         | 7390      |
|    total_timesteps      | 317440    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 23.4      |
|    n_updates            | 1540      |
|    policy_gradient_loss | 1.53e-09  |
|    value_loss           | 74.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02e+03  |
|    ep_rew_mean          | -306      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 156       |
|    time_elapsed         | 7438      |
|    total_timesteps      | 319488    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 52.6      |
|    n_updates            | 1550      |
|    policy_gradient_loss | 8.44e-11  |
|    value_loss           | 82.6      |
---------------------------------------
Num timesteps: 320000
Best mean reward: -309.47 - Last mean reward per episode: -305.58
Saving new best model at 317441 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02e+03  |
|    ep_rew_mean          | -306      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 157       |
|    time_elapsed         | 7486      |
|    total_timesteps      | 321536    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 35.5      |
|    n_updates            | 1560      |
|    policy_gradient_loss | 1.63e-09  |
|    value_loss           | 90.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02e+03  |
|    ep_rew_mean          | -306      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 158       |
|    time_elapsed         | 7534      |
|    total_timesteps      | 323584    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 24.3      |
|    n_updates            | 1570      |
|    policy_gradient_loss | -2.31e-10 |
|    value_loss           | 65.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02e+03  |
|    ep_rew_mean          | -306      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 159       |
|    time_elapsed         | 7581      |
|    total_timesteps      | 325632    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 48.6      |
|    n_updates            | 1580      |
|    policy_gradient_loss | -2.62e-10 |
|    value_loss           | 89.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02e+03  |
|    ep_rew_mean          | -303      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 160       |
|    time_elapsed         | 7628      |
|    total_timesteps      | 327680    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 41        |
|    n_updates            | 1590      |
|    policy_gradient_loss | -1.95e-10 |
|    value_loss           | 85        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02e+03  |
|    ep_rew_mean          | -303      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 161       |
|    time_elapsed         | 7675      |
|    total_timesteps      | 329728    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 43.5      |
|    n_updates            | 1600      |
|    policy_gradient_loss | -1.28e-09 |
|    value_loss           | 77.7      |
---------------------------------------
Num timesteps: 330000
Best mean reward: -305.58 - Last mean reward per episode: -303.84
Saving new best model at 329729 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02e+03  |
|    ep_rew_mean          | -304      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 162       |
|    time_elapsed         | 7724      |
|    total_timesteps      | 331776    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 42.4      |
|    n_updates            | 1610      |
|    policy_gradient_loss | 6.66e-10  |
|    value_loss           | 81.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02e+03  |
|    ep_rew_mean          | -304      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 163       |
|    time_elapsed         | 7771      |
|    total_timesteps      | 333824    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 42.7      |
|    n_updates            | 1620      |
|    policy_gradient_loss | -7.31e-10 |
|    value_loss           | 76.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02e+03  |
|    ep_rew_mean          | -304      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 164       |
|    time_elapsed         | 7818      |
|    total_timesteps      | 335872    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 31.3      |
|    n_updates            | 1630      |
|    policy_gradient_loss | 6.11e-10  |
|    value_loss           | 80.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02e+03  |
|    ep_rew_mean          | -304      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 165       |
|    time_elapsed         | 7866      |
|    total_timesteps      | 337920    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 32.7      |
|    n_updates            | 1640      |
|    policy_gradient_loss | -4.02e-10 |
|    value_loss           | 75.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02e+03  |
|    ep_rew_mean          | -302      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 166       |
|    time_elapsed         | 7914      |
|    total_timesteps      | 339968    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 46.6      |
|    n_updates            | 1650      |
|    policy_gradient_loss | -2.23e-10 |
|    value_loss           | 86        |
---------------------------------------
Num timesteps: 340000
Best mean reward: -303.84 - Last mean reward per episode: -302.15
Saving new best model at 337921 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02e+03  |
|    ep_rew_mean          | -302      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 167       |
|    time_elapsed         | 7963      |
|    total_timesteps      | 342016    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 33        |
|    n_updates            | 1660      |
|    policy_gradient_loss | -1.45e-09 |
|    value_loss           | 70.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02e+03  |
|    ep_rew_mean          | -304      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 168       |
|    time_elapsed         | 8010      |
|    total_timesteps      | 344064    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 37.1      |
|    n_updates            | 1670      |
|    policy_gradient_loss | -7.36e-10 |
|    value_loss           | 76.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02e+03  |
|    ep_rew_mean          | -304      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 169       |
|    time_elapsed         | 8058      |
|    total_timesteps      | 346112    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 47.6      |
|    n_updates            | 1680      |
|    policy_gradient_loss | 1.46e-10  |
|    value_loss           | 82.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02e+03  |
|    ep_rew_mean          | -303      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 170       |
|    time_elapsed         | 8107      |
|    total_timesteps      | 348160    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 44.9      |
|    n_updates            | 1690      |
|    policy_gradient_loss | 6.23e-10  |
|    value_loss           | 83.2      |
---------------------------------------
Num timesteps: 350000
Best mean reward: -302.15 - Last mean reward per episode: -302.98
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02e+03  |
|    ep_rew_mean          | -303      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 171       |
|    time_elapsed         | 8154      |
|    total_timesteps      | 350208    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 32.5      |
|    n_updates            | 1700      |
|    policy_gradient_loss | 1.08e-10  |
|    value_loss           | 79.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -304      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 172       |
|    time_elapsed         | 8202      |
|    total_timesteps      | 352256    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 52.6      |
|    n_updates            | 1710      |
|    policy_gradient_loss | -1.33e-09 |
|    value_loss           | 70.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -304      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 173       |
|    time_elapsed         | 8250      |
|    total_timesteps      | 354304    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 1.79e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 42.6      |
|    n_updates            | 1720      |
|    policy_gradient_loss | -4.67e-10 |
|    value_loss           | 74.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -307      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 174       |
|    time_elapsed         | 8297      |
|    total_timesteps      | 356352    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 32        |
|    n_updates            | 1730      |
|    policy_gradient_loss | 1.38e-11  |
|    value_loss           | 72.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -307      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 175       |
|    time_elapsed         | 8344      |
|    total_timesteps      | 358400    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0003    |
|    loss                 | 37.2      |
|    n_updates            | 1740      |
|    policy_gradient_loss | 7.87e-10  |
|    value_loss           | 86        |
---------------------------------------
Num timesteps: 360000
Best mean reward: -302.15 - Last mean reward per episode: -304.50
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -304      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 176       |
|    time_elapsed         | 8393      |
|    total_timesteps      | 360448    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 30.1      |
|    n_updates            | 1750      |
|    policy_gradient_loss | 7.83e-10  |
|    value_loss           | 80.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -304      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 177       |
|    time_elapsed         | 8442      |
|    total_timesteps      | 362496    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 24.1      |
|    n_updates            | 1760      |
|    policy_gradient_loss | 5.92e-10  |
|    value_loss           | 82.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -305      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 178       |
|    time_elapsed         | 8491      |
|    total_timesteps      | 364544    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 38.4      |
|    n_updates            | 1770      |
|    policy_gradient_loss | -1.24e-10 |
|    value_loss           | 73.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -305      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 179       |
|    time_elapsed         | 8539      |
|    total_timesteps      | 366592    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 32.1      |
|    n_updates            | 1780      |
|    policy_gradient_loss | -2.76e-10 |
|    value_loss           | 62.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -306      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 180       |
|    time_elapsed         | 8586      |
|    total_timesteps      | 368640    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 45.8      |
|    n_updates            | 1790      |
|    policy_gradient_loss | 4.71e-10  |
|    value_loss           | 86.6      |
---------------------------------------
Num timesteps: 370000
Best mean reward: -302.15 - Last mean reward per episode: -305.90
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -306      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 181       |
|    time_elapsed         | 8634      |
|    total_timesteps      | 370688    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 45.6      |
|    n_updates            | 1800      |
|    policy_gradient_loss | -2.65e-10 |
|    value_loss           | 85.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -303      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 182       |
|    time_elapsed         | 8682      |
|    total_timesteps      | 372736    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 37        |
|    n_updates            | 1810      |
|    policy_gradient_loss | -5.38e-11 |
|    value_loss           | 85.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -303      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 183       |
|    time_elapsed         | 8730      |
|    total_timesteps      | 374784    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 52.3      |
|    n_updates            | 1820      |
|    policy_gradient_loss | -7.07e-10 |
|    value_loss           | 81.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -303      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 184       |
|    time_elapsed         | 8777      |
|    total_timesteps      | 376832    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 1.79e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 36.8      |
|    n_updates            | 1830      |
|    policy_gradient_loss | 7.81e-10  |
|    value_loss           | 76.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -303      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 185       |
|    time_elapsed         | 8825      |
|    total_timesteps      | 378880    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 34.1      |
|    n_updates            | 1840      |
|    policy_gradient_loss | 4.82e-10  |
|    value_loss           | 82.4      |
---------------------------------------
Num timesteps: 380000
Best mean reward: -302.15 - Last mean reward per episode: -302.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -302      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 186       |
|    time_elapsed         | 8872      |
|    total_timesteps      | 380928    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 50.8      |
|    n_updates            | 1850      |
|    policy_gradient_loss | -1.85e-10 |
|    value_loss           | 85.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -302      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 187       |
|    time_elapsed         | 8920      |
|    total_timesteps      | 382976    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 34.8      |
|    n_updates            | 1860      |
|    policy_gradient_loss | 1.53e-10  |
|    value_loss           | 80        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -305      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 188       |
|    time_elapsed         | 8967      |
|    total_timesteps      | 385024    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 28.4      |
|    n_updates            | 1870      |
|    policy_gradient_loss | -1.28e-09 |
|    value_loss           | 65.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -305      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 189       |
|    time_elapsed         | 9014      |
|    total_timesteps      | 387072    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 28.9      |
|    n_updates            | 1880      |
|    policy_gradient_loss | 8.35e-10  |
|    value_loss           | 79.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -304      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 190       |
|    time_elapsed         | 9062      |
|    total_timesteps      | 389120    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 30.8      |
|    n_updates            | 1890      |
|    policy_gradient_loss | -2.39e-10 |
|    value_loss           | 80.4      |
---------------------------------------
Num timesteps: 390000
Best mean reward: -302.15 - Last mean reward per episode: -304.13
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -304      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 191       |
|    time_elapsed         | 9109      |
|    total_timesteps      | 391168    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 31.2      |
|    n_updates            | 1900      |
|    policy_gradient_loss | -6.55e-11 |
|    value_loss           | 74.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -304      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 192       |
|    time_elapsed         | 9157      |
|    total_timesteps      | 393216    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 47.5      |
|    n_updates            | 1910      |
|    policy_gradient_loss | 9.33e-10  |
|    value_loss           | 85        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -304      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 193       |
|    time_elapsed         | 9205      |
|    total_timesteps      | 395264    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 24.9      |
|    n_updates            | 1920      |
|    policy_gradient_loss | -1.73e-09 |
|    value_loss           | 67.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -306      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 194       |
|    time_elapsed         | 9252      |
|    total_timesteps      | 397312    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 33.5      |
|    n_updates            | 1930      |
|    policy_gradient_loss | 7.97e-10  |
|    value_loss           | 70.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -306      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 195       |
|    time_elapsed         | 9300      |
|    total_timesteps      | 399360    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 31.7      |
|    n_updates            | 1940      |
|    policy_gradient_loss | 5.01e-10  |
|    value_loss           | 86        |
---------------------------------------
Num timesteps: 400000
Best mean reward: -302.15 - Last mean reward per episode: -303.94
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -304      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 196       |
|    time_elapsed         | 9346      |
|    total_timesteps      | 401408    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 39.6      |
|    n_updates            | 1950      |
|    policy_gradient_loss | 2.71e-10  |
|    value_loss           | 83.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -304      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 197       |
|    time_elapsed         | 9393      |
|    total_timesteps      | 403456    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 47        |
|    n_updates            | 1960      |
|    policy_gradient_loss | -4.37e-12 |
|    value_loss           | 77.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -303      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 198       |
|    time_elapsed         | 9440      |
|    total_timesteps      | 405504    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 52.1      |
|    n_updates            | 1970      |
|    policy_gradient_loss | 4.71e-10  |
|    value_loss           | 79.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03e+03  |
|    ep_rew_mean          | -303      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 199       |
|    time_elapsed         | 9488      |
|    total_timesteps      | 407552    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 53.6      |
|    n_updates            | 1980      |
|    policy_gradient_loss | 1.28e-09  |
|    value_loss           | 77.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.04e+03  |
|    ep_rew_mean          | -289      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 200       |
|    time_elapsed         | 9535      |
|    total_timesteps      | 409600    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 40.4      |
|    n_updates            | 1990      |
|    policy_gradient_loss | -6.58e-10 |
|    value_loss           | 90.5      |
---------------------------------------
Num timesteps: 410000
Best mean reward: -302.15 - Last mean reward per episode: -288.65
Saving new best model at 407553 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.04e+03  |
|    ep_rew_mean          | -289      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 201       |
|    time_elapsed         | 9585      |
|    total_timesteps      | 411648    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 45.6      |
|    n_updates            | 2000      |
|    policy_gradient_loss | 1.27e-09  |
|    value_loss           | 82.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.06e+03  |
|    ep_rew_mean          | -277      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 202       |
|    time_elapsed         | 9634      |
|    total_timesteps      | 413696    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 28.5      |
|    n_updates            | 2010      |
|    policy_gradient_loss | 5.62e-10  |
|    value_loss           | 83.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.06e+03  |
|    ep_rew_mean          | -277      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 203       |
|    time_elapsed         | 9682      |
|    total_timesteps      | 415744    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 49        |
|    n_updates            | 2020      |
|    policy_gradient_loss | 7e-10     |
|    value_loss           | 74.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.07e+03  |
|    ep_rew_mean          | -275      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 204       |
|    time_elapsed         | 9730      |
|    total_timesteps      | 417792    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 35.3      |
|    n_updates            | 2030      |
|    policy_gradient_loss | -9.41e-10 |
|    value_loss           | 77.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.07e+03  |
|    ep_rew_mean          | -275      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 205       |
|    time_elapsed         | 9778      |
|    total_timesteps      | 419840    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 23.5      |
|    n_updates            | 2040      |
|    policy_gradient_loss | 2.04e-09  |
|    value_loss           | 74.1      |
---------------------------------------
Num timesteps: 420000
Best mean reward: -288.65 - Last mean reward per episode: -271.36
Saving new best model at 419841 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.08e+03  |
|    ep_rew_mean          | -271      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 206       |
|    time_elapsed         | 9825      |
|    total_timesteps      | 421888    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 27.3      |
|    n_updates            | 2050      |
|    policy_gradient_loss | -1.3e-10  |
|    value_loss           | 66        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.08e+03  |
|    ep_rew_mean          | -271      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 207       |
|    time_elapsed         | 9871      |
|    total_timesteps      | 423936    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 51.2      |
|    n_updates            | 2060      |
|    policy_gradient_loss | 1.14e-10  |
|    value_loss           | 91        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.08e+03  |
|    ep_rew_mean          | -262      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 208       |
|    time_elapsed         | 9917      |
|    total_timesteps      | 425984    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 37.5      |
|    n_updates            | 2070      |
|    policy_gradient_loss | 5.21e-10  |
|    value_loss           | 79.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.08e+03  |
|    ep_rew_mean          | -262      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 209       |
|    time_elapsed         | 9966      |
|    total_timesteps      | 428032    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 35.4      |
|    n_updates            | 2080      |
|    policy_gradient_loss | 1.34e-10  |
|    value_loss           | 72.7      |
---------------------------------------
Num timesteps: 430000
Best mean reward: -271.36 - Last mean reward per episode: -260.00
Saving new best model at 428033 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -260      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 210       |
|    time_elapsed         | 10014     |
|    total_timesteps      | 430080    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 24.7      |
|    n_updates            | 2090      |
|    policy_gradient_loss | -6.51e-10 |
|    value_loss           | 85        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -260      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 211       |
|    time_elapsed         | 10063     |
|    total_timesteps      | 432128    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 28.7      |
|    n_updates            | 2100      |
|    policy_gradient_loss | 1.73e-09  |
|    value_loss           | 81.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -260      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 212       |
|    time_elapsed         | 10111     |
|    total_timesteps      | 434176    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 45.8      |
|    n_updates            | 2110      |
|    policy_gradient_loss | -1.66e-09 |
|    value_loss           | 74.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -260      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 213       |
|    time_elapsed         | 10159     |
|    total_timesteps      | 436224    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 39.1      |
|    n_updates            | 2120      |
|    policy_gradient_loss | -4.35e-10 |
|    value_loss           | 74.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -261      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 214       |
|    time_elapsed         | 10206     |
|    total_timesteps      | 438272    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 27.7      |
|    n_updates            | 2130      |
|    policy_gradient_loss | 1.45e-09  |
|    value_loss           | 80.9      |
---------------------------------------
Num timesteps: 440000
Best mean reward: -260.00 - Last mean reward per episode: -260.60
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -261      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 215       |
|    time_elapsed         | 10252     |
|    total_timesteps      | 440320    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 31.7      |
|    n_updates            | 2140      |
|    policy_gradient_loss | 6.26e-10  |
|    value_loss           | 78.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -261      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 216       |
|    time_elapsed         | 10298     |
|    total_timesteps      | 442368    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 47.3      |
|    n_updates            | 2150      |
|    policy_gradient_loss | 1.75e-11  |
|    value_loss           | 80.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -261      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 217       |
|    time_elapsed         | 10345     |
|    total_timesteps      | 444416    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 31.2      |
|    n_updates            | 2160      |
|    policy_gradient_loss | 1.08e-09  |
|    value_loss           | 68.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -262      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 218       |
|    time_elapsed         | 10392     |
|    total_timesteps      | 446464    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 36.6      |
|    n_updates            | 2170      |
|    policy_gradient_loss | 6.98e-11  |
|    value_loss           | 75.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -262      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 219       |
|    time_elapsed         | 10439     |
|    total_timesteps      | 448512    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 39.8      |
|    n_updates            | 2180      |
|    policy_gradient_loss | -7.16e-10 |
|    value_loss           | 73.7      |
---------------------------------------
Num timesteps: 450000
Best mean reward: -260.00 - Last mean reward per episode: -262.70
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -263      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 220       |
|    time_elapsed         | 10487     |
|    total_timesteps      | 450560    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 29.1      |
|    n_updates            | 2190      |
|    policy_gradient_loss | -4.23e-10 |
|    value_loss           | 84.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -263      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 221       |
|    time_elapsed         | 10536     |
|    total_timesteps      | 452608    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 29.9      |
|    n_updates            | 2200      |
|    policy_gradient_loss | 2.47e-10  |
|    value_loss           | 79.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -265      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 222       |
|    time_elapsed         | 10585     |
|    total_timesteps      | 454656    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 49.5      |
|    n_updates            | 2210      |
|    policy_gradient_loss | 1.11e-09  |
|    value_loss           | 75.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -265      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 223       |
|    time_elapsed         | 10631     |
|    total_timesteps      | 456704    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 49.6      |
|    n_updates            | 2220      |
|    policy_gradient_loss | -1.45e-10 |
|    value_loss           | 87.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -265      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 224       |
|    time_elapsed         | 10678     |
|    total_timesteps      | 458752    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 27.1      |
|    n_updates            | 2230      |
|    policy_gradient_loss | -2.52e-10 |
|    value_loss           | 75.5      |
---------------------------------------
Num timesteps: 460000
Best mean reward: -260.00 - Last mean reward per episode: -265.10
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -265      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 225       |
|    time_elapsed         | 10726     |
|    total_timesteps      | 460800    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 55.8      |
|    n_updates            | 2240      |
|    policy_gradient_loss | 2.17e-10  |
|    value_loss           | 91.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -264      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 226       |
|    time_elapsed         | 10774     |
|    total_timesteps      | 462848    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 44.7      |
|    n_updates            | 2250      |
|    policy_gradient_loss | -7.7e-10  |
|    value_loss           | 80.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -264      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 227       |
|    time_elapsed         | 10821     |
|    total_timesteps      | 464896    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 46.9      |
|    n_updates            | 2260      |
|    policy_gradient_loss | 2.25e-10  |
|    value_loss           | 78.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -264      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 228       |
|    time_elapsed         | 10868     |
|    total_timesteps      | 466944    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 41.7      |
|    n_updates            | 2270      |
|    policy_gradient_loss | -1.53e-10 |
|    value_loss           | 86.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -264      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 229       |
|    time_elapsed         | 10915     |
|    total_timesteps      | 468992    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 37.4      |
|    n_updates            | 2280      |
|    policy_gradient_loss | -4.99e-10 |
|    value_loss           | 79.9      |
---------------------------------------
Num timesteps: 470000
Best mean reward: -260.00 - Last mean reward per episode: -264.80
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -265      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 230       |
|    time_elapsed         | 10963     |
|    total_timesteps      | 471040    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 57.9      |
|    n_updates            | 2290      |
|    policy_gradient_loss | 9.07e-10  |
|    value_loss           | 84.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -265      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 231       |
|    time_elapsed         | 11010     |
|    total_timesteps      | 473088    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 45.9      |
|    n_updates            | 2300      |
|    policy_gradient_loss | 1.82e-10  |
|    value_loss           | 87.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -264      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 232       |
|    time_elapsed         | 11056     |
|    total_timesteps      | 475136    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 47.6      |
|    n_updates            | 2310      |
|    policy_gradient_loss | 1.29e-09  |
|    value_loss           | 77.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -264      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 233       |
|    time_elapsed         | 11101     |
|    total_timesteps      | 477184    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 39.7      |
|    n_updates            | 2320      |
|    policy_gradient_loss | 5.03e-10  |
|    value_loss           | 79.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -264      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 234       |
|    time_elapsed         | 11147     |
|    total_timesteps      | 479232    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 41.1      |
|    n_updates            | 2330      |
|    policy_gradient_loss | 4.51e-10  |
|    value_loss           | 69.9      |
---------------------------------------
Num timesteps: 480000
Best mean reward: -260.00 - Last mean reward per episode: -263.90
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -264      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 235       |
|    time_elapsed         | 11194     |
|    total_timesteps      | 481280    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 46.1      |
|    n_updates            | 2340      |
|    policy_gradient_loss | 4.05e-10  |
|    value_loss           | 81.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -265      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 236       |
|    time_elapsed         | 11241     |
|    total_timesteps      | 483328    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 44        |
|    n_updates            | 2350      |
|    policy_gradient_loss | 4.51e-10  |
|    value_loss           | 73.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -265      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 237       |
|    time_elapsed         | 11289     |
|    total_timesteps      | 485376    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 24.2      |
|    n_updates            | 2360      |
|    policy_gradient_loss | 1.46e-11  |
|    value_loss           | 79.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -265      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 238       |
|    time_elapsed         | 11338     |
|    total_timesteps      | 487424    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 46.9      |
|    n_updates            | 2370      |
|    policy_gradient_loss | 1.32e-09  |
|    value_loss           | 76.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -265      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 239       |
|    time_elapsed         | 11385     |
|    total_timesteps      | 489472    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 39.1      |
|    n_updates            | 2380      |
|    policy_gradient_loss | -7.33e-10 |
|    value_loss           | 81.2      |
---------------------------------------
Num timesteps: 490000
Best mean reward: -260.00 - Last mean reward per episode: -264.80
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -265      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 240       |
|    time_elapsed         | 11433     |
|    total_timesteps      | 491520    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 49.5      |
|    n_updates            | 2390      |
|    policy_gradient_loss | 1.51e-10  |
|    value_loss           | 82.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -265      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 241       |
|    time_elapsed         | 11481     |
|    total_timesteps      | 493568    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 33.3      |
|    n_updates            | 2400      |
|    policy_gradient_loss | -3.52e-10 |
|    value_loss           | 74.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -267      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 242       |
|    time_elapsed         | 11528     |
|    total_timesteps      | 495616    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 50.9      |
|    n_updates            | 2410      |
|    policy_gradient_loss | -1.38e-11 |
|    value_loss           | 85.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -267      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 243       |
|    time_elapsed         | 11575     |
|    total_timesteps      | 497664    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 41.6      |
|    n_updates            | 2420      |
|    policy_gradient_loss | -2.69e-10 |
|    value_loss           | 87.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -264      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 244       |
|    time_elapsed         | 11622     |
|    total_timesteps      | 499712    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 34.3      |
|    n_updates            | 2430      |
|    policy_gradient_loss | -9.18e-10 |
|    value_loss           | 82.2      |
---------------------------------------
Num timesteps: 500000
Best mean reward: -260.00 - Last mean reward per episode: -264.50
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.1e+03   |
|    ep_rew_mean          | -264      |
| time/                   |           |
|    fps                  | 42        |
|    iterations           | 245       |
|    time_elapsed         | 11669     |
|    total_timesteps      | 501760    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000156 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 47.2      |
|    n_updates            | 2440      |
|    policy_gradient_loss | 7.67e-10  |
|    value_loss           | 71.8      |
---------------------------------------