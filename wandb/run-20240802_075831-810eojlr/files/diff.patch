diff --git a/games/encoder/__pycache__/GraphEncoder.cpython-310.pyc b/games/encoder/__pycache__/GraphEncoder.cpython-310.pyc
index 3dc7f23..30515ae 100644
Binary files a/games/encoder/__pycache__/GraphEncoder.cpython-310.pyc and b/games/encoder/__pycache__/GraphEncoder.cpython-310.pyc differ
diff --git a/games/freeway/__pycache__/run_supervised_gnn.cpython-310.pyc b/games/freeway/__pycache__/run_supervised_gnn.cpython-310.pyc
index ef42b16..2bd8735 100644
Binary files a/games/freeway/__pycache__/run_supervised_gnn.cpython-310.pyc and b/games/freeway/__pycache__/run_supervised_gnn.cpython-310.pyc differ
diff --git a/games/freeway/freeway_envs/__pycache__/freeway_env.cpython-310.pyc b/games/freeway/freeway_envs/__pycache__/freeway_env.cpython-310.pyc
index 2093cbd..1b9d49a 100644
Binary files a/games/freeway/freeway_envs/__pycache__/freeway_env.cpython-310.pyc and b/games/freeway/freeway_envs/__pycache__/freeway_env.cpython-310.pyc differ
diff --git a/games/freeway/freeway_envs/freeway_env.py b/games/freeway/freeway_envs/freeway_env.py
index 7e400ca..d5fb643 100644
--- a/games/freeway/freeway_envs/freeway_env.py
+++ b/games/freeway/freeway_envs/freeway_env.py
@@ -31,8 +31,8 @@ class FreewayEnv(gym.Env):
         self.frame_stack = frame_stack
 
         self.lanes = [100, 200, 300, 400, 500, 600, 700]
-        self.lanes = [50,100]
-        self.max_cars = 5
+        self.lanes = [50,80,120]
+        self.max_cars = 10
         # Define action and observation space
         # Actions: 0 - Stay, 1 - Move Up, 2 - Move Down
         self.action_space = spaces.Discrete(3)
@@ -63,6 +63,7 @@ class FreewayEnv(gym.Env):
         super().reset(seed=seed, options=options)
         if seed is not None:
             self.seed(seed)
+        self.steps_since_collision = 0
         self.player_rect = pygame.Rect(self.window_width // 2 - self.player_width // 2,
                                        self.window_height - self.player_height - 10,
                                        self.player_width, self.player_height)
@@ -84,11 +85,15 @@ class FreewayEnv(gym.Env):
         reward = 0
         reward = -0.5
         current_time = pygame.time.get_ticks()
-        if action == 1:  # Up
-            self.player_rect.y = max(0, self.player_rect.y - 5)
-        elif action == 2:  # Down
-            self.player_rect.y = min(self.window_height - self.player_height, self.player_rect.y + 5)
+        if self.steps_since_collision < 30:  # Check the step counter
+            self.steps_since_collision += 1
+        else:
+            if action == 1:  # Up
+                self.player_rect.y = max(0, self.player_rect.y - 5)
+            elif action == 2:  # Down
+                self.player_rect.y = min(self.window_height - self.player_height, self.player_rect.y + 5)
 
+        
         for car in self.cars:
             car['x'] += car['speed']
             if car['x'] > self.window_width:
@@ -99,7 +104,9 @@ class FreewayEnv(gym.Env):
         hit = any(self.player_rect.colliderect(pygame.Rect(car['x'], car['lane'], self.car_width, self.car_height)) for car in self.cars)
         if hit:
             #self.score = -1
+            reward = -2
             self.player_rect.y = self.window_height - self.player_height - 10
+            self.steps_since_collision = 0  # Reset the counter on collisio
         
             self.last_time = current_time
         if current_time - self.episode_start_time >= 60000:  # 60000 milliseconds = 1 minute
@@ -167,9 +174,10 @@ class FreewayEnv(gym.Env):
 
 
 if __name__=="__main__":
-    env = FreewayEnv(render_mode='human', observation_type='pixel')
+    env = FreewayEnv(render_mode='human', observation_type='graph')
 
-    model = PPO.load("ppo_freeway_pixel")
+    #model = PPO.load("ppo_freeway_pixel")
+    model = PPO.load("logs/Freeway-GNN-training/best_model.zip")
     #model = PPO.load("ppo_custom_heterognn")
 
     # # Evaluate the agent
diff --git a/games/freeway/run_supervised_gnn.py b/games/freeway/run_supervised_gnn.py
index 3a19658..3b8c844 100644
--- a/games/freeway/run_supervised_gnn.py
+++ b/games/freeway/run_supervised_gnn.py
@@ -3,9 +3,62 @@ from stable_baselines3 import PPO
 from stable_baselines3.common.env_util import make_vec_env
 from wandb.integration.sb3 import WandbCallback
 #from games.model.policy import CustomActorCriticPolicy
-from games.freeway.freeway_envs.freeway_env import FreewayEnv
 from games.model.policy import CustomCNN, CustomHeteroGNN
+from games.freeway.freeway_envs.freeway_env import FreewayEnv
 import pygame
+from stable_baselines3.common.callbacks import BaseCallback
+import os
+import numpy as np
+from stable_baselines3.common.vec_env import DummyVecEnv
+from stable_baselines3.common.monitor import Monitor
+from stable_baselines3.common.callbacks import EvalCallback
+from stable_baselines3.common.monitor import load_results
+from stable_baselines3.common.results_plotter import ts2xy
+#Initialize wandb
+class SaveOnBestTrainingRewardCallback(BaseCallback):
+    def __init__(self, check_freq, log_dir, verbose=1):
+        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)
+        self.check_freq = check_freq
+        self.log_dir = log_dir
+        self.save_path = os.path.join(log_dir, "best_model")
+        self.best_mean_reward = -np.inf
+
+    def _init_callback(self) -> None:
+        if self.save_path is not None:
+            os.makedirs(self.save_path, exist_ok=True)
+
+    def _on_step(self) -> bool:
+        if self.n_calls % self.check_freq == 0:
+            x, y = ts2xy(load_results(self.log_dir), "timesteps")
+            if len(x) > 0:
+                mean_reward = np.mean(y[-100:])
+                if self.verbose > 0:
+                    print(f"Num timesteps: {self.num_timesteps}")
+                    print(f"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}")
+                if mean_reward > self.best_mean_reward:
+                    self.best_mean_reward = mean_reward
+                    if self.verbose > 0:
+                        print(f"Saving new best model at {x[-1]} timesteps")
+                        print(f"Saving new best model to {self.save_path}.zip")
+                    self.model.save(self.save_path)
+                #wandb.log({"mean_reward": mean_reward, "timesteps": self.num_timesteps})
+            else:
+                device = "cpu"
+                if self.verbose > 0:
+                    print("No data available for logging.")
+                #wandb.log({"timesteps": self.num_timesteps})
+        return True 
+log_dir = "./logs/Freeway-GNN-training/"
+
+def make_env(lanes, max_cars, car_speed, seed=0, rank=None):
+    def _init():
+        env = FreewayEnv( render_mode='human', observation_type='graph')
+        monitor_path = os.path.join(log_dir, f"monitor_{rank}.csv")
+        os.makedirs(log_dir, exist_ok=True)  # Create log directory if it doesn't exist
+        env = Monitor(env, filename=monitor_path, allow_early_resets=True)
+        env.seed(seed)
+        return env
+    return _init 
 # #Initialize wandb
 wandb.init(
     project="gnn_atari_freeway",  # Replace with your project name
@@ -23,18 +76,18 @@ wandb.init(
 
 # Wrap the environment 
 
-env = FreewayEnv(render_mode='human', observation_type='graph')
+#env = FreewayEnv(render_mode='human', observation_type='graph')
 # policy_kwargs = dict(
 #     features_extractor_class=CustomCNN,
 #     features_extractor_kwargs=dict(features_dim=128),
 # )
-
+envs = DummyVecEnv([make_env(2, 2, 1, i) for i in range(1)])
 policy_kwargs = dict(
     features_extractor_class=CustomHeteroGNN,
     features_extractor_kwargs=dict(
         features_dim=64,
         hidden_size=64,
-        num_layer=2,
+        num_layer=10,
         obj_type_id='obj',
         arity_dict={'ChickenOnLane':2, 'CarOnLane':2, 'LaneNextToLane':2},
         game = 'freeway'
@@ -42,9 +95,10 @@ policy_kwargs = dict(
 )
 
 # # Create the PPO model with the custom feature extractor
-model = PPO('MlpPolicy', env, policy_kwargs=policy_kwargs, verbose=2)
+model = PPO('MlpPolicy', envs, policy_kwargs=policy_kwargs, verbose=2)
 # # Train the model with WandbCallback
-model.learn(total_timesteps=1000000, callback=WandbCallback() )
+callback = SaveOnBestTrainingRewardCallback(check_freq=10000, log_dir=log_dir)
+model.learn(total_timesteps=500000, callback=[callback, WandbCallback()] )
 # # Save the model
 model.save("ppo_custom_heterognn")
 
diff --git a/games/model/__pycache__/hetero_gnn.cpython-310.pyc b/games/model/__pycache__/hetero_gnn.cpython-310.pyc
index 7853617..237d220 100644
Binary files a/games/model/__pycache__/hetero_gnn.cpython-310.pyc and b/games/model/__pycache__/hetero_gnn.cpython-310.pyc differ
diff --git a/games/model/__pycache__/hetero_message_passing.cpython-310.pyc b/games/model/__pycache__/hetero_message_passing.cpython-310.pyc
index 243e3b4..069cd92 100644
Binary files a/games/model/__pycache__/hetero_message_passing.cpython-310.pyc and b/games/model/__pycache__/hetero_message_passing.cpython-310.pyc differ
diff --git a/games/model/__pycache__/policy.cpython-310.pyc b/games/model/__pycache__/policy.cpython-310.pyc
index 18a2c62..9e8820c 100644
Binary files a/games/model/__pycache__/policy.cpython-310.pyc and b/games/model/__pycache__/policy.cpython-310.pyc differ
diff --git a/ppo_custom_heterognn.zip b/ppo_custom_heterognn.zip
index 5b88437..270d7a2 100644
Binary files a/ppo_custom_heterognn.zip and b/ppo_custom_heterognn.zip differ
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index 0355c64..105a13a 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20240626_173531-u9sj3nyk/logs/debug-internal.log
\ No newline at end of file
+run-20240802_075831-810eojlr/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index c7c35e9..807fdf7 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20240626_173531-u9sj3nyk/logs/debug.log
\ No newline at end of file
+run-20240802_075831-810eojlr/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index ae14d51..e8620c7 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20240626_173531-u9sj3nyk
\ No newline at end of file
+run-20240802_075831-810eojlr
\ No newline at end of file
