/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.
  logger.warn(
Using cpu device
-----------------------------
| time/              |      |
|    fps             | 114  |
|    iterations      | 1    |
|    time_elapsed    | 17   |
|    total_timesteps | 2048 |
-----------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -1.04e+03   |
| time/                   |             |
|    fps                  | 43          |
|    iterations           | 2           |
|    time_elapsed         | 94          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.012082782 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | -0.00226    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.463       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00698    |
|    value_loss           | 3.12        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -1.04e+03   |
| time/                   |             |
|    fps                  | 36          |
|    iterations           | 3           |
|    time_elapsed         | 169         |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.008991761 |
|    clip_fraction        | 0.0111      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 1.28e-05    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0656      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00164    |
|    value_loss           | 15.4        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -1.04e+03   |
| time/                   |             |
|    fps                  | 33          |
|    iterations           | 4           |
|    time_elapsed         | 246         |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.008389471 |
|    clip_fraction        | 0.0455      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.123       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0043     |
|    value_loss           | 10.8        |
-----------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -1035.75
Saving new best model at 8193 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -1.04e+03   |
| time/                   |             |
|    fps                  | 31          |
|    iterations           | 5           |
|    time_elapsed         | 320         |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.016171776 |
|    clip_fraction        | 0.0775      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 0.041       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00483    |
|    value_loss           | 6.7         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -1.03e+03   |
| time/                   |             |
|    fps                  | 31          |
|    iterations           | 6           |
|    time_elapsed         | 395         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.014355153 |
|    clip_fraction        | 0.0746      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.988      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00669     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00197    |
|    value_loss           | 4.15        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -1.03e+03   |
| time/                   |             |
|    fps                  | 30          |
|    iterations           | 7           |
|    time_elapsed         | 474         |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.012815731 |
|    clip_fraction        | 0.0399      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.904      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.014      |
|    n_updates            | 60          |
|    policy_gradient_loss | -7.46e-05   |
|    value_loss           | 2.52        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -1.03e+03   |
| time/                   |             |
|    fps                  | 29          |
|    iterations           | 8           |
|    time_elapsed         | 551         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.003066627 |
|    clip_fraction        | 0.0589      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.788      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0102      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00187    |
|    value_loss           | 1.9         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -1.03e+03   |
| time/                   |             |
|    fps                  | 29          |
|    iterations           | 9           |
|    time_elapsed         | 629         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.011024769 |
|    clip_fraction        | 0.0769      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.752      |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0473      |
|    n_updates            | 80          |
|    policy_gradient_loss | 0.000234    |
|    value_loss           | 1.37        |
-----------------------------------------
Num timesteps: 20000
Best mean reward: -1035.75 - Last mean reward per episode: -1030.06
Saving new best model at 18433 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -1.03e+03   |
| time/                   |             |
|    fps                  | 28          |
|    iterations           | 10          |
|    time_elapsed         | 709         |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.001465558 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.701      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00316     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.000477   |
|    value_loss           | 1.18        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -1.03e+03   |
| time/                   |             |
|    fps                  | 28          |
|    iterations           | 11          |
|    time_elapsed         | 785         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.002398402 |
|    clip_fraction        | 0.0268      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.696      |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00898    |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00253    |
|    value_loss           | 1.07        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -1.03e+03   |
| time/                   |             |
|    fps                  | 28          |
|    iterations           | 12          |
|    time_elapsed         | 861         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.004247754 |
|    clip_fraction        | 0.039       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.64       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00521    |
|    n_updates            | 110         |
|    policy_gradient_loss | 0.00109     |
|    value_loss           | 1.03        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -1.03e+03    |
| time/                   |              |
|    fps                  | 28           |
|    iterations           | 13           |
|    time_elapsed         | 940          |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.0053870464 |
|    clip_fraction        | 0.0627       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.588       |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0147       |
|    n_updates            | 120          |
|    policy_gradient_loss | 0.000642     |
|    value_loss           | 1.02         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -1.03e+03    |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 14           |
|    time_elapsed         | 1025         |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0046151076 |
|    clip_fraction        | 0.0655       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.504       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00817      |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.000146    |
|    value_loss           | 1.03         |
------------------------------------------
Num timesteps: 30000
Best mean reward: -1030.06 - Last mean reward per episode: -1027.89
Saving new best model at 28673 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -1.03e+03    |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 15           |
|    time_elapsed         | 1112         |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.0063793454 |
|    clip_fraction        | 0.0494       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.441       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0255       |
|    n_updates            | 140          |
|    policy_gradient_loss | 2.19e-05     |
|    value_loss           | 1.04         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -1.03e+03    |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 16           |
|    time_elapsed         | 1190         |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0008093093 |
|    clip_fraction        | 0.00718      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.448       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0031       |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.000978    |
|    value_loss           | 1.06         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -1.03e+03    |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 17           |
|    time_elapsed         | 1267         |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0016681845 |
|    clip_fraction        | 0.0295       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.413       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00153     |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.000288    |
|    value_loss           | 1.08         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -1.03e+03    |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 18           |
|    time_elapsed         | 1345         |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.0018434695 |
|    clip_fraction        | 0.0328       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.363       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00487     |
|    n_updates            | 170          |
|    policy_gradient_loss | 0.0024       |
|    value_loss           | 1.09         |
------------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 101, in <module>
    model.learn(total_timesteps=500000, callback=[callback, WandbCallback()] )
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 313, in learn
    self.train()
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 217, in train
    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 730, in evaluate_actions
    features = self.extract_features(obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/policy.py", line 45, in forward
    obj_emb = self.model(pyg_data.x_dict, pyg_data.edge_index_dict, pyg_data.batch_dict)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_gnn.py", line 104, in forward
    self.layer(x_dict, edge_index_dict)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_gnn.py", line 82, in layer
    out = self.atom_to_obj(x_dict, edge_index_dict)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_message_passing.py", line 74, in forward
    out = self._internal_forward(x, edge_index_dict[edge_type], edge_type)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_message_passing.py", line 149, in _internal_forward
    return self.select(x, edges_index, int(edge_type[1]))
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_message_passing.py", line 171, in forward
    return self.propagate(edge_index, x=x, position=position)
  File "/var/folders/my/7z0rbf091qj03p882sd10h_00000gn/T/games.model.hetero_message_passing_SelectMP_propagate_gvt0we7o.py", line 197, in propagate
    out = self.aggregate(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py", line 625, in aggregate
    return self.aggr_module(inputs, index, ptr=ptr, dim_size=dim_size,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/experimental.py", line 117, in wrapper
    return func(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/aggr/base.py", line 128, in __call__
    return super().__call__(x, index=index, ptr=ptr, dim_size=dim_size,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/aggr/basic.py", line 22, in forward
    return self.reduce(x, index, ptr, dim_size, dim, reduce='sum')
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/aggr/base.py", line 182, in reduce
    return scatter(x, index, dim, dim_size, reduce)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/utils/_scatter.py", line 75, in scatter
    return src.new_zeros(size).scatter_add_(dim, index, src)
KeyboardInterrupt