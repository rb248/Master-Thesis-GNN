diff --git a/games/encoder/__pycache__/GraphEncoder.cpython-310.pyc b/games/encoder/__pycache__/GraphEncoder.cpython-310.pyc
index 3dc7f23..30515ae 100644
Binary files a/games/encoder/__pycache__/GraphEncoder.cpython-310.pyc and b/games/encoder/__pycache__/GraphEncoder.cpython-310.pyc differ
diff --git a/games/freeway/__pycache__/run_supervised_gnn.cpython-310.pyc b/games/freeway/__pycache__/run_supervised_gnn.cpython-310.pyc
index ef42b16..0011acc 100644
Binary files a/games/freeway/__pycache__/run_supervised_gnn.cpython-310.pyc and b/games/freeway/__pycache__/run_supervised_gnn.cpython-310.pyc differ
diff --git a/games/freeway/freeway_envs/__pycache__/freeway_env.cpython-310.pyc b/games/freeway/freeway_envs/__pycache__/freeway_env.cpython-310.pyc
index 2093cbd..c2bbe80 100644
Binary files a/games/freeway/freeway_envs/__pycache__/freeway_env.cpython-310.pyc and b/games/freeway/freeway_envs/__pycache__/freeway_env.cpython-310.pyc differ
diff --git a/games/freeway/freeway_envs/freeway_env.py b/games/freeway/freeway_envs/freeway_env.py
index 7e400ca..576aac2 100644
--- a/games/freeway/freeway_envs/freeway_env.py
+++ b/games/freeway/freeway_envs/freeway_env.py
@@ -11,6 +11,15 @@ from itertools import combinations
 from stable_baselines3 import PPO
 from stable_baselines3.common.evaluation import evaluate_policy
 
+import random
+import pygame
+import numpy as np
+import gymnasium as gym
+from gymnasium import spaces
+import torch
+from stable_baselines3 import PPO
+from stable_baselines3.common.evaluation import evaluate_policy
+
 class FreewayEnv(gym.Env):
     metadata = {'render_modes': ['human', 'rgb_array']}
 
@@ -20,9 +29,7 @@ class FreewayEnv(gym.Env):
         self.last_time = pygame.time.get_ticks()
         self.render_mode = render_mode
         self.observation_type = observation_type
-        #self.window_width = 800
         self.window_width = 210
-        #self.window_height = 600
         self.window_height = 160
         self.player_width = 5
         self.player_height = 5
@@ -30,17 +37,14 @@ class FreewayEnv(gym.Env):
         self.car_height = 20
         self.frame_stack = frame_stack
 
-        self.lanes = [100, 200, 300, 400, 500, 600, 700]
-        self.lanes = [50,100]
-        self.max_cars = 5
-        # Define action and observation space
-        # Actions: 0 - Stay, 1 - Move Up, 2 - Move Down
+        self.lanes = [50, 80, 120]
+        self.max_cars = 10
         self.action_space = spaces.Discrete(3)
 
         if observation_type == "pixel":
             self.observation_space = spaces.Box(low=0, high=255, shape=(self.frame_stack, 84, 84), dtype=np.uint8)
         else:
-            self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.max_cars+ len(self.lanes)+1, 7), dtype=np.float32)
+            self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.max_cars + len(self.lanes) + 1, 7), dtype=np.float32)
 
         self.window = pygame.display.set_mode((self.window_width, self.window_height))
         self.background_image = pygame.image.load("games/images/Atari - background.png")
@@ -53,6 +57,7 @@ class FreewayEnv(gym.Env):
 
         self.clock = pygame.time.Clock()
         self.reset()
+
     def seed(self, seed=None):
         self.np_random, seed = gym.utils.seeding.np_random(seed)
         random.seed(seed)
@@ -63,6 +68,7 @@ class FreewayEnv(gym.Env):
         super().reset(seed=seed, options=options)
         if seed is not None:
             self.seed(seed)
+        self.steps_since_collision = 0
         self.player_rect = pygame.Rect(self.window_width // 2 - self.player_width // 2,
                                        self.window_height - self.player_height - 10,
                                        self.player_width, self.player_height)
@@ -81,35 +87,40 @@ class FreewayEnv(gym.Env):
             return self.get_object_data(), {}
 
     def step(self, action):
-        reward = 0
         reward = -0.5
         current_time = pygame.time.get_ticks()
-        if action == 1:  # Up
-            self.player_rect.y = max(0, self.player_rect.y - 5)
-        elif action == 2:  # Down
-            self.player_rect.y = min(self.window_height - self.player_height, self.player_rect.y + 5)
+
+        if self.steps_since_collision < 30:
+            self.steps_since_collision += 1
+        else:
+            if action == 1:  # Up
+                self.player_rect.y = max(0, self.player_rect.y - 5)
+                reward += 1  # Reward for moving up
+            elif action == 2:  # Down
+                self.player_rect.y = min(self.window_height - self.player_height, self.player_rect.y + 5)
+                reward -= 1  # Penalty for moving down
 
         for car in self.cars:
             car['x'] += car['speed']
             if car['x'] > self.window_width:
                 car['x'] = -random.randint(100, 300)
-                car['speed'] = random.randint(1,2)
+                car['speed'] = random.randint(1, 2)
 
-        # Collision detection
         hit = any(self.player_rect.colliderect(pygame.Rect(car['x'], car['lane'], self.car_width, self.car_height)) for car in self.cars)
         if hit:
-            #self.score = -1
+            reward -= 10  # Increased penalty for collisions
             self.player_rect.y = self.window_height - self.player_height - 10
-        
-            self.last_time = current_time
-        if current_time - self.episode_start_time >= 60000:  # 60000 milliseconds = 1 minute
-            self.done = True
-            
-        if self.player_rect.y <= 0:  # Reached top
-            self.score +=1
-            reward += 10*(len(self.lanes))
+            self.steps_since_collision = 0
 
+        if self.player_rect.y <= 0:
+            self.score += 1
+            reward += 10 * len(self.lanes)
             self.player_rect.y = self.window_height - self.player_height - 10
+            self.done = True
+            #self._update_environment()
+
+        if current_time - self.episode_start_time >= 60000:
+            self.done = True
 
         if self.observation_type == "pixel":
             self.update_frame_buffer()
@@ -117,11 +128,13 @@ class FreewayEnv(gym.Env):
         else:
             observation = self.get_object_data()
 
+        print(f"Step: {self.steps_since_collision}, Action: {action}, Reward: {reward}, Position: {self.player_rect.y}, Done: {self.done}")
+
         return observation, reward, self.done, False, {}
 
     def update_frame_buffer(self):
         frame = self.render_to_array()
-        grayscale = np.dot(frame[..., :3], [0.2989, 0.5870, 0.1140]).astype(np.uint8)  # Convert to grayscale
+        grayscale = np.dot(frame[..., :3], [0.2989, 0.5870, 0.1140]).astype(np.uint8)
         resized_frame = pygame.transform.scale(pygame.surfarray.make_surface(grayscale), (84, 84))
         frame_array = pygame.surfarray.array3d(resized_frame).transpose(1, 0, 2)[:, :, 0]
 
@@ -141,17 +154,15 @@ class FreewayEnv(gym.Env):
     def get_object_data(self):
         objects = [
             [self.player_rect.x, self.player_rect.y, 0, 0, 1, 0, 0],  # Player
-            
-        ] 
-        # add lanes
+        ]
         for lane in self.lanes:
-            objects.append([self.window_width//2, lane, 0, 0, 0, 1, 0])
+            objects.append([self.window_width // 2, lane, 0, 0, 0, 1, 0])
 
         for i, car in enumerate(self.cars):
             objects.append([car['x'], car['lane'], car['speed'], 0, 0, 0, 1])
 
-        # while len(objects) < self.max_cars + 10:  # Ensure the list has a constant length
-        #     objects.append([0, 0, 0, 0, 0, 0, 0])
+        while len(objects) < self.max_cars + len(self.lanes) + 1:
+            objects.append([0, 0, 0, 0, 0, 0, 0])
 
         return torch.tensor(objects, dtype=torch.float32)
 
@@ -166,10 +177,14 @@ class FreewayEnv(gym.Env):
         pygame.quit()
 
 
+
+
+
 if __name__=="__main__":
-    env = FreewayEnv(render_mode='human', observation_type='pixel')
+    env = FreewayEnv(render_mode='human', observation_type='graph')
 
-    model = PPO.load("ppo_freeway_pixel")
+    #model = PPO.load("ppo_freeway_pixel")
+    model = PPO.load("logs/Freeway-GNN-training/best_model.zip")
     #model = PPO.load("ppo_custom_heterognn")
 
     # # Evaluate the agent
diff --git a/games/freeway/run_supervised_gnn.py b/games/freeway/run_supervised_gnn.py
index 3a19658..7ed6895 100644
--- a/games/freeway/run_supervised_gnn.py
+++ b/games/freeway/run_supervised_gnn.py
@@ -3,9 +3,62 @@ from stable_baselines3 import PPO
 from stable_baselines3.common.env_util import make_vec_env
 from wandb.integration.sb3 import WandbCallback
 #from games.model.policy import CustomActorCriticPolicy
-from games.freeway.freeway_envs.freeway_env import FreewayEnv
 from games.model.policy import CustomCNN, CustomHeteroGNN
+from games.freeway.freeway_envs.freeway_env import FreewayEnv
 import pygame
+from stable_baselines3.common.callbacks import BaseCallback
+import os
+import numpy as np
+from stable_baselines3.common.vec_env import DummyVecEnv
+from stable_baselines3.common.monitor import Monitor
+from stable_baselines3.common.callbacks import EvalCallback
+from stable_baselines3.common.monitor import load_results
+from stable_baselines3.common.results_plotter import ts2xy
+#Initialize wandb
+class SaveOnBestTrainingRewardCallback(BaseCallback):
+    def __init__(self, check_freq, log_dir, verbose=1):
+        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)
+        self.check_freq = check_freq
+        self.log_dir = log_dir
+        self.save_path = os.path.join(log_dir, "best_model")
+        self.best_mean_reward = -np.inf
+
+    def _init_callback(self) -> None:
+        if self.save_path is not None:
+            os.makedirs(self.save_path, exist_ok=True)
+
+    def _on_step(self) -> bool:
+        if self.n_calls % self.check_freq == 0:
+            x, y = ts2xy(load_results(self.log_dir), "timesteps")
+            if len(x) > 0:
+                mean_reward = np.mean(y[-100:])
+                if self.verbose > 0:
+                    print(f"Num timesteps: {self.num_timesteps}")
+                    print(f"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}")
+                if mean_reward > self.best_mean_reward:
+                    self.best_mean_reward = mean_reward
+                    if self.verbose > 0:
+                        print(f"Saving new best model at {x[-1]} timesteps")
+                        print(f"Saving new best model to {self.save_path}.zip")
+                    self.model.save(self.save_path)
+                #wandb.log({"mean_reward": mean_reward, "timesteps": self.num_timesteps})
+            else:
+                device = "cpu"
+                if self.verbose > 0:
+                    print("No data available for logging.")
+                #wandb.log({"timesteps": self.num_timesteps})
+        return True 
+log_dir = "./logs/Freeway-GNN-training/"
+
+def make_env(lanes, max_cars, car_speed, seed=0, rank=None):
+    def _init():
+        env = FreewayEnv( render_mode='human', observation_type='graph')
+        monitor_path = os.path.join(log_dir, f"monitor_{rank}.csv")
+        os.makedirs(log_dir, exist_ok=True)  # Create log directory if it doesn't exist
+        env = Monitor(env, filename=monitor_path, allow_early_resets=True)
+        env.seed(seed)
+        return env
+    return _init 
 # #Initialize wandb
 wandb.init(
     project="gnn_atari_freeway",  # Replace with your project name
@@ -23,18 +76,18 @@ wandb.init(
 
 # Wrap the environment 
 
-env = FreewayEnv(render_mode='human', observation_type='graph')
+#env = FreewayEnv(render_mode='human', observation_type='graph')
 # policy_kwargs = dict(
 #     features_extractor_class=CustomCNN,
 #     features_extractor_kwargs=dict(features_dim=128),
 # )
-
+envs = DummyVecEnv([make_env(2, 2, 1, i) for i in range(1)])
 policy_kwargs = dict(
     features_extractor_class=CustomHeteroGNN,
     features_extractor_kwargs=dict(
         features_dim=64,
         hidden_size=64,
-        num_layer=2,
+        num_layer=10,
         obj_type_id='obj',
         arity_dict={'ChickenOnLane':2, 'CarOnLane':2, 'LaneNextToLane':2},
         game = 'freeway'
@@ -42,9 +95,10 @@ policy_kwargs = dict(
 )
 
 # # Create the PPO model with the custom feature extractor
-model = PPO('MlpPolicy', env, policy_kwargs=policy_kwargs, verbose=2)
+model = PPO('MlpPolicy', envs, policy_kwargs=policy_kwargs, verbose=2, n_steps=128)
 # # Train the model with WandbCallback
-model.learn(total_timesteps=1000000, callback=WandbCallback() )
+callback = SaveOnBestTrainingRewardCallback(check_freq=10000, log_dir=log_dir)
+model.learn(total_timesteps=500000, callback=[callback, WandbCallback()] )
 # # Save the model
 model.save("ppo_custom_heterognn")
 
diff --git a/games/model/__pycache__/hetero_gnn.cpython-310.pyc b/games/model/__pycache__/hetero_gnn.cpython-310.pyc
index 7853617..237d220 100644
Binary files a/games/model/__pycache__/hetero_gnn.cpython-310.pyc and b/games/model/__pycache__/hetero_gnn.cpython-310.pyc differ
diff --git a/games/model/__pycache__/hetero_message_passing.cpython-310.pyc b/games/model/__pycache__/hetero_message_passing.cpython-310.pyc
index 243e3b4..069cd92 100644
Binary files a/games/model/__pycache__/hetero_message_passing.cpython-310.pyc and b/games/model/__pycache__/hetero_message_passing.cpython-310.pyc differ
diff --git a/games/model/__pycache__/policy.cpython-310.pyc b/games/model/__pycache__/policy.cpython-310.pyc
index 18a2c62..9e8820c 100644
Binary files a/games/model/__pycache__/policy.cpython-310.pyc and b/games/model/__pycache__/policy.cpython-310.pyc differ
diff --git a/ppo_custom_heterognn.zip b/ppo_custom_heterognn.zip
index 5b88437..270d7a2 100644
Binary files a/ppo_custom_heterognn.zip and b/ppo_custom_heterognn.zip differ
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index 0355c64..0081b81 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20240626_173531-u9sj3nyk/logs/debug-internal.log
\ No newline at end of file
+run-20240802_180138-u2vhpflk/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index c7c35e9..d71a5d0 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20240626_173531-u9sj3nyk/logs/debug.log
\ No newline at end of file
+run-20240802_180138-u2vhpflk/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index ae14d51..22cc4c2 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20240626_173531-u9sj3nyk
\ No newline at end of file
+run-20240802_180138-u2vhpflk
\ No newline at end of file
