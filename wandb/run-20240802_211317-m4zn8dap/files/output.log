
Using cpu device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -500     |
| time/              |          |
|    fps             | 147      |
|    iterations      | 1        |
|    time_elapsed    | 13       |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 57          |
|    iterations           | 2           |
|    time_elapsed         | 71          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.009974111 |
|    clip_fraction        | 0.0369      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.00112     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.739       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00097    |
|    value_loss           | 4.6         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 45           |
|    iterations           | 3            |
|    time_elapsed         | 133          |
|    total_timesteps      | 6144         |
| train/                  |              |
|    approx_kl            | 0.0043701315 |
|    clip_fraction        | 0.00366      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.304        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.517        |
|    n_updates            | 20           |
|    policy_gradient_loss | 7.27e-05     |
|    value_loss           | 11.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 4            |
|    time_elapsed         | 192          |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.0070085046 |
|    clip_fraction        | 0.0139       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | -4.89e-06    |
|    learning_rate        | 0.0003       |
|    loss                 | 3.9          |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.0019      |
|    value_loss           | 12.6         |
------------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -500.00
Saving new best model at 10000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 41          |
|    iterations           | 5           |
|    time_elapsed         | 248         |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.004822188 |
|    clip_fraction        | 0.0286      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0766      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.000945   |
|    value_loss           | 10.6        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 40          |
|    iterations           | 6           |
|    time_elapsed         | 303         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.010568254 |
|    clip_fraction        | 0.0452      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 5.95        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00169    |
|    value_loss           | 9.72        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 40           |
|    iterations           | 7            |
|    time_elapsed         | 356          |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.0047395546 |
|    clip_fraction        | 0.0343       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.05        |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 0.149        |
|    n_updates            | 60           |
|    policy_gradient_loss | 7.87e-05     |
|    value_loss           | 9.98         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 39          |
|    iterations           | 8           |
|    time_elapsed         | 412         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.010863621 |
|    clip_fraction        | 0.0242      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.138       |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.000928   |
|    value_loss           | 10.5        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | -500       |
| time/                   |            |
|    fps                  | 38         |
|    iterations           | 9          |
|    time_elapsed         | 473        |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.01486413 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.06      |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0003     |
|    loss                 | 0.86       |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.00525   |
|    value_loss           | 11.4       |
----------------------------------------
Num timesteps: 20000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 38          |
|    iterations           | 10          |
|    time_elapsed         | 530         |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.008754775 |
|    clip_fraction        | 0.0396      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 8.14        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0025     |
|    value_loss           | 12.2        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -499        |
| time/                   |             |
|    fps                  | 38          |
|    iterations           | 11          |
|    time_elapsed         | 586         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.011032027 |
|    clip_fraction        | 0.0889      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 7.93        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00286    |
|    value_loss           | 13          |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -495        |
| time/                   |             |
|    fps                  | 38          |
|    iterations           | 12          |
|    time_elapsed         | 644         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.013116991 |
|    clip_fraction        | 0.0394      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 12.6        |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.000943   |
|    value_loss           | 18.4        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -484        |
| time/                   |             |
|    fps                  | 37          |
|    iterations           | 13          |
|    time_elapsed         | 702         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.016913028 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.96       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.39        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00293    |
|    value_loss           | 20.9        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -470        |
| time/                   |             |
|    fps                  | 37          |
|    iterations           | 14          |
|    time_elapsed         | 763         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.011992254 |
|    clip_fraction        | 0.0951      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.878      |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 20.7        |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00517    |
|    value_loss           | 49.6        |
-----------------------------------------
Num timesteps: 30000
Best mean reward: -500.00 - Last mean reward per episode: -454.33
Saving new best model at 30000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -454        |
| time/                   |             |
|    fps                  | 37          |
|    iterations           | 15          |
|    time_elapsed         | 822         |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.008760248 |
|    clip_fraction        | 0.0423      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.781      |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 14          |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00314    |
|    value_loss           | 50.1        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -445         |
| time/                   |              |
|    fps                  | 37           |
|    iterations           | 16           |
|    time_elapsed         | 881          |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0024264916 |
|    clip_fraction        | 0.0244       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.709       |
|    explained_variance   | 1.01e-06     |
|    learning_rate        | 0.0003       |
|    loss                 | 20.5         |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.000742    |
|    value_loss           | 44.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -432         |
| time/                   |              |
|    fps                  | 36           |
|    iterations           | 17           |
|    time_elapsed         | 941          |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0041538686 |
|    clip_fraction        | 0.0356       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.601       |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 22.3         |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00302     |
|    value_loss           | 49.2         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -422         |
| time/                   |              |
|    fps                  | 36           |
|    iterations           | 18           |
|    time_elapsed         | 1000         |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.0041342857 |
|    clip_fraction        | 0.0513       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.512       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 25.3         |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00259     |
|    value_loss           | 38.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -411         |
| time/                   |              |
|    fps                  | 36           |
|    iterations           | 19           |
|    time_elapsed         | 1058         |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0017609166 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.457       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 22.7         |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00118     |
|    value_loss           | 45.6         |
------------------------------------------
Num timesteps: 40000
Best mean reward: -454.33 - Last mean reward per episode: -402.75
Saving new best model at 40000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -403         |
| time/                   |              |
|    fps                  | 36           |
|    iterations           | 20           |
|    time_elapsed         | 1116         |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0029377686 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.466       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 20.5         |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.000495    |
|    value_loss           | 49.9         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -389         |
| time/                   |              |
|    fps                  | 36           |
|    iterations           | 21           |
|    time_elapsed         | 1173         |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0017023716 |
|    clip_fraction        | 0.0244       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.394       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 40.3         |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00175     |
|    value_loss           | 57.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -377         |
| time/                   |              |
|    fps                  | 36           |
|    iterations           | 22           |
|    time_elapsed         | 1234         |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0022132897 |
|    clip_fraction        | 0.0293       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.323       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 31.5         |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00211     |
|    value_loss           | 50.6         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -369         |
| time/                   |              |
|    fps                  | 36           |
|    iterations           | 23           |
|    time_elapsed         | 1296         |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0019944883 |
|    clip_fraction        | 0.021        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.273       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 20.8         |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00173     |
|    value_loss           | 57.4         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -360         |
| time/                   |              |
|    fps                  | 36           |
|    iterations           | 24           |
|    time_elapsed         | 1357         |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0004632036 |
|    clip_fraction        | 0.00796      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.249       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 26.1         |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.000108    |
|    value_loss           | 49.9         |
------------------------------------------
Num timesteps: 50000
Best mean reward: -402.75 - Last mean reward per episode: -355.00
Saving new best model at 50000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -353         |
| time/                   |              |
|    fps                  | 35           |
|    iterations           | 25           |
|    time_elapsed         | 1422         |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0005293999 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.227       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 34.1         |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.000971    |
|    value_loss           | 58.9         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -349         |
| time/                   |              |
|    fps                  | 35           |
|    iterations           | 26           |
|    time_elapsed         | 1485         |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0010499142 |
|    clip_fraction        | 0.028        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.247       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 23.6         |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.0019      |
|    value_loss           | 46.5         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -344          |
| time/                   |               |
|    fps                  | 35            |
|    iterations           | 27            |
|    time_elapsed         | 1547          |
|    total_timesteps      | 55296         |
| train/                  |               |
|    approx_kl            | 0.00042763804 |
|    clip_fraction        | 0.0115        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.235        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 19.6          |
|    n_updates            | 260           |
|    policy_gradient_loss | -0.000203     |
|    value_loss           | 32.1          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -342          |
| time/                   |               |
|    fps                  | 35            |
|    iterations           | 28            |
|    time_elapsed         | 1610          |
|    total_timesteps      | 57344         |
| train/                  |               |
|    approx_kl            | 0.00055227586 |
|    clip_fraction        | 0.00596       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.213        |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 16.4          |
|    n_updates            | 270           |
|    policy_gradient_loss | -0.000153     |
|    value_loss           | 46.9          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -337         |
| time/                   |              |
|    fps                  | 35           |
|    iterations           | 29           |
|    time_elapsed         | 1671         |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0010244402 |
|    clip_fraction        | 0.00513      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.19        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 33.5         |
|    n_updates            | 280          |
|    policy_gradient_loss | -6.58e-05    |
|    value_loss           | 40.7         |
------------------------------------------
Num timesteps: 60000
Best mean reward: -355.00 - Last mean reward per episode: -332.67
Saving new best model at 60000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -331          |
| time/                   |               |
|    fps                  | 35            |
|    iterations           | 30            |
|    time_elapsed         | 1738          |
|    total_timesteps      | 61440         |
| train/                  |               |
|    approx_kl            | 0.00097406714 |
|    clip_fraction        | 0.0064        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.161        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 31.4          |
|    n_updates            | 290           |
|    policy_gradient_loss | -0.000258     |
|    value_loss           | 53.3          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -322          |
| time/                   |               |
|    fps                  | 35            |
|    iterations           | 31            |
|    time_elapsed         | 1807          |
|    total_timesteps      | 63488         |
| train/                  |               |
|    approx_kl            | 0.00027170006 |
|    clip_fraction        | 0.00181       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.139        |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 38.9          |
|    n_updates            | 300           |
|    policy_gradient_loss | -0.000368     |
|    value_loss           | 61.2          |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -317        |
| time/                   |             |
|    fps                  | 34          |
|    iterations           | 32          |
|    time_elapsed         | 1878        |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.000316694 |
|    clip_fraction        | 0.00518     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.141      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 18.5        |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.000175   |
|    value_loss           | 63.1        |
-----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -311          |
| time/                   |               |
|    fps                  | 34            |
|    iterations           | 33            |
|    time_elapsed         | 1946          |
|    total_timesteps      | 67584         |
| train/                  |               |
|    approx_kl            | 0.00056712097 |
|    clip_fraction        | 0.00825       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.134        |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 28            |
|    n_updates            | 320           |
|    policy_gradient_loss | -0.000275     |
|    value_loss           | 46.3          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -306          |
| time/                   |               |
|    fps                  | 34            |
|    iterations           | 34            |
|    time_elapsed         | 2019          |
|    total_timesteps      | 69632         |
| train/                  |               |
|    approx_kl            | 0.00017615268 |
|    clip_fraction        | 0.00952       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.112        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 36            |
|    n_updates            | 330           |
|    policy_gradient_loss | -0.00036      |
|    value_loss           | 59.6          |
-------------------------------------------
Num timesteps: 70000
Best mean reward: -332.67 - Last mean reward per episode: -303.43
Saving new best model at 70000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -300         |
| time/                   |              |
|    fps                  | 34           |
|    iterations           | 35           |
|    time_elapsed         | 2089         |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 7.885421e-05 |
|    clip_fraction        | 0.00376      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0983      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 35.7         |
|    n_updates            | 340          |
|    policy_gradient_loss | -7.2e-05     |
|    value_loss           | 54.1         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -298          |
| time/                   |               |
|    fps                  | 34            |
|    iterations           | 36            |
|    time_elapsed         | 2157          |
|    total_timesteps      | 73728         |
| train/                  |               |
|    approx_kl            | 0.00068037136 |
|    clip_fraction        | 0.00688       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0946       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 30.9          |
|    n_updates            | 350           |
|    policy_gradient_loss | -0.000375     |
|    value_loss           | 55.4          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -296         |
| time/                   |              |
|    fps                  | 34           |
|    iterations           | 37           |
|    time_elapsed         | 2224         |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0004696069 |
|    clip_fraction        | 0.00527      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0756      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 37.8         |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.000426    |
|    value_loss           | 50.9         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -290          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 38            |
|    time_elapsed         | 2289          |
|    total_timesteps      | 77824         |
| train/                  |               |
|    approx_kl            | 0.00035893222 |
|    clip_fraction        | 0.00669       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0572       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 41.2          |
|    n_updates            | 370           |
|    policy_gradient_loss | -0.000568     |
|    value_loss           | 50.8          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -287          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 39            |
|    time_elapsed         | 2354          |
|    total_timesteps      | 79872         |
| train/                  |               |
|    approx_kl            | 0.00011958147 |
|    clip_fraction        | 0.0041        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0625       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 32.4          |
|    n_updates            | 380           |
|    policy_gradient_loss | -0.000194     |
|    value_loss           | 58.3          |
-------------------------------------------
Num timesteps: 80000
Best mean reward: -303.43 - Last mean reward per episode: -286.25
Saving new best model at 80000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -284          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 40            |
|    time_elapsed         | 2423          |
|    total_timesteps      | 81920         |
| train/                  |               |
|    approx_kl            | 7.6813856e-05 |
|    clip_fraction        | 0.00239       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0702       |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 20            |
|    n_updates            | 390           |
|    policy_gradient_loss | 1.57e-05      |
|    value_loss           | 44.6          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -281          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 41            |
|    time_elapsed         | 2490          |
|    total_timesteps      | 83968         |
| train/                  |               |
|    approx_kl            | 0.00036083302 |
|    clip_fraction        | 0.00415       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0812       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 37.6          |
|    n_updates            | 400           |
|    policy_gradient_loss | -0.000236     |
|    value_loss           | 50.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -276          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 42            |
|    time_elapsed         | 2558          |
|    total_timesteps      | 86016         |
| train/                  |               |
|    approx_kl            | 2.3622502e-05 |
|    clip_fraction        | 0.00269       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0773       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 32.7          |
|    n_updates            | 410           |
|    policy_gradient_loss | -6.69e-05     |
|    value_loss           | 55            |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 43            |
|    time_elapsed         | 2622          |
|    total_timesteps      | 88064         |
| train/                  |               |
|    approx_kl            | 0.00013465737 |
|    clip_fraction        | 0.00361       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0737       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 23.4          |
|    n_updates            | 420           |
|    policy_gradient_loss | 8.87e-05      |
|    value_loss           | 55            |
-------------------------------------------
Num timesteps: 90000
Best mean reward: -286.25 - Last mean reward per episode: -270.22
Saving new best model at 90000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -270          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 44            |
|    time_elapsed         | 2684          |
|    total_timesteps      | 90112         |
| train/                  |               |
|    approx_kl            | 0.00014313546 |
|    clip_fraction        | 0.00308       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0846       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 27.5          |
|    n_updates            | 430           |
|    policy_gradient_loss | 2.01e-05      |
|    value_loss           | 46.6          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -268          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 45            |
|    time_elapsed         | 2746          |
|    total_timesteps      | 92160         |
| train/                  |               |
|    approx_kl            | 0.00014954864 |
|    clip_fraction        | 0.00288       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0726       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 19.4          |
|    n_updates            | 440           |
|    policy_gradient_loss | -0.000115     |
|    value_loss           | 46            |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -264          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 46            |
|    time_elapsed         | 2810          |
|    total_timesteps      | 94208         |
| train/                  |               |
|    approx_kl            | 0.00016520519 |
|    clip_fraction        | 0.00371       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0639       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 15.2          |
|    n_updates            | 450           |
|    policy_gradient_loss | -0.000214     |
|    value_loss           | 55.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -264          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 47            |
|    time_elapsed         | 2872          |
|    total_timesteps      | 96256         |
| train/                  |               |
|    approx_kl            | 0.00014548883 |
|    clip_fraction        | 0.0021        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0575       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 31.8          |
|    n_updates            | 460           |
|    policy_gradient_loss | -0.00015      |
|    value_loss           | 57.3          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -262          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 48            |
|    time_elapsed         | 2934          |
|    total_timesteps      | 98304         |
| train/                  |               |
|    approx_kl            | 0.00016474648 |
|    clip_fraction        | 0.00571       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.047        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 21.8          |
|    n_updates            | 470           |
|    policy_gradient_loss | -0.000324     |
|    value_loss           | 47.1          |
-------------------------------------------
Num timesteps: 100000
Best mean reward: -270.22 - Last mean reward per episode: -262.60
Saving new best model at 100000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -263         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 49           |
|    time_elapsed         | 2997         |
|    total_timesteps      | 100352       |
| train/                  |              |
|    approx_kl            | 0.0002890319 |
|    clip_fraction        | 0.00405      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0589      |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 17.9         |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.000331    |
|    value_loss           | 40.3         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -256          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 50            |
|    time_elapsed         | 3060          |
|    total_timesteps      | 102400        |
| train/                  |               |
|    approx_kl            | 0.00016069572 |
|    clip_fraction        | 0.00625       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0747       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 17.2          |
|    n_updates            | 490           |
|    policy_gradient_loss | -0.000397     |
|    value_loss           | 38.8          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -250         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 51           |
|    time_elapsed         | 3126         |
|    total_timesteps      | 104448       |
| train/                  |              |
|    approx_kl            | 9.400447e-05 |
|    clip_fraction        | 0.00537      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0602      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 23.7         |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.000207    |
|    value_loss           | 49.2         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -244          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 52            |
|    time_elapsed         | 3191          |
|    total_timesteps      | 106496        |
| train/                  |               |
|    approx_kl            | 0.00020343953 |
|    clip_fraction        | 0.00342       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0708       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 29.8          |
|    n_updates            | 510           |
|    policy_gradient_loss | -0.000281     |
|    value_loss           | 49.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -237          |
| time/                   |               |
|    fps                  | 33            |
|    iterations           | 53            |
|    time_elapsed         | 3256          |
|    total_timesteps      | 108544        |
| train/                  |               |
|    approx_kl            | 0.00022259422 |
|    clip_fraction        | 0.00425       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0779       |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 21            |
|    n_updates            | 520           |
|    policy_gradient_loss | -0.000312     |
|    value_loss           | 48.3          |
-------------------------------------------
Num timesteps: 110000
Best mean reward: -262.60 - Last mean reward per episode: -231.90
Saving new best model at 110000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -232         |
| time/                   |              |
|    fps                  | 30           |
|    iterations           | 54           |
|    time_elapsed         | 3649         |
|    total_timesteps      | 110592       |
| train/                  |              |
|    approx_kl            | 0.0006465174 |
|    clip_fraction        | 0.00889      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0659      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 24.4         |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.000752    |
|    value_loss           | 51.3         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -226          |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 55            |
|    time_elapsed         | 3723          |
|    total_timesteps      | 112640        |
| train/                  |               |
|    approx_kl            | 0.00043483975 |
|    clip_fraction        | 0.00552       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0552       |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 19.5          |
|    n_updates            | 540           |
|    policy_gradient_loss | -0.000199     |
|    value_loss           | 43.6          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -221          |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 56            |
|    time_elapsed         | 3796          |
|    total_timesteps      | 114688        |
| train/                  |               |
|    approx_kl            | 0.00092978997 |
|    clip_fraction        | 0.00459       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0628       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 23.5          |
|    n_updates            | 550           |
|    policy_gradient_loss | 0.000139      |
|    value_loss           | 46.4          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -216         |
| time/                   |              |
|    fps                  | 30           |
|    iterations           | 57           |
|    time_elapsed         | 3867         |
|    total_timesteps      | 116736       |
| train/                  |              |
|    approx_kl            | 5.643259e-05 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0742      |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 15           |
|    n_updates            | 560          |
|    policy_gradient_loss | 0.000112     |
|    value_loss           | 39.7         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -208          |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 58            |
|    time_elapsed         | 3943          |
|    total_timesteps      | 118784        |
| train/                  |               |
|    approx_kl            | 0.00036716778 |
|    clip_fraction        | 0.00586       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0567       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 28.4          |
|    n_updates            | 570           |
|    policy_gradient_loss | -0.000515     |
|    value_loss           | 53.1          |
-------------------------------------------
Num timesteps: 120000
Best mean reward: -231.90 - Last mean reward per episode: -200.30
Saving new best model at 120000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -200          |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 59            |
|    time_elapsed         | 4020          |
|    total_timesteps      | 120832        |
| train/                  |               |
|    approx_kl            | 0.00013259574 |
|    clip_fraction        | 0.00195       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0475       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 25.5          |
|    n_updates            | 580           |
|    policy_gradient_loss | -0.000176     |
|    value_loss           | 53.8          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -194         |
| time/                   |              |
|    fps                  | 30           |
|    iterations           | 60           |
|    time_elapsed         | 4093         |
|    total_timesteps      | 122880       |
| train/                  |              |
|    approx_kl            | 0.0002675994 |
|    clip_fraction        | 0.00493      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0397      |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 30.3         |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.000528    |
|    value_loss           | 60.4         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -187          |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 61            |
|    time_elapsed         | 4158          |
|    total_timesteps      | 124928        |
| train/                  |               |
|    approx_kl            | 0.00014789586 |
|    clip_fraction        | 0.0022        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0346       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 40.4          |
|    n_updates            | 600           |
|    policy_gradient_loss | 0.000184      |
|    value_loss           | 49.7          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -184          |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 62            |
|    time_elapsed         | 4227          |
|    total_timesteps      | 126976        |
| train/                  |               |
|    approx_kl            | 0.00010431593 |
|    clip_fraction        | 0.00171       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0362       |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 29.1          |
|    n_updates            | 610           |
|    policy_gradient_loss | -4.08e-05     |
|    value_loss           | 46.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -183          |
| time/                   |               |
|    fps                  | 30            |
|    iterations           | 63            |
|    time_elapsed         | 4294          |
|    total_timesteps      | 129024        |
| train/                  |               |
|    approx_kl            | 6.7213405e-05 |
|    clip_fraction        | 0.00171       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0313       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 23.7          |
|    n_updates            | 620           |
|    policy_gradient_loss | -7.16e-05     |
|    value_loss           | 50.4          |
-------------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 290, in <module>
    model.learn(total_timesteps=1000000, callback=[WandbCallback(), save_callback])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 313, in learn
    self.train()
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 217, in train
    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 730, in evaluate_actions
    features = self.extract_features(obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/policy.py", line 40, in forward
    pyg_data = self.encoder.encode(observations)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/encoder/GraphEncoder.py", line 446, in encode
    lane_indices = [i for i in range(num_nodes) if node_features[i, -2] == 1]
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/encoder/GraphEncoder.py", line 446, in <listcomp>
    lane_indices = [i for i in range(num_nodes) if node_features[i, -2] == 1]
KeyboardInterrupt