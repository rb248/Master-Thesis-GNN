
Using cpu device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -500     |
| time/              |          |
|    fps             | 275      |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 2           |
|    time_elapsed         | 46          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.009817352 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.000176    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.861       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00384    |
|    value_loss           | 4.47        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 67          |
|    iterations           | 3           |
|    time_elapsed         | 91          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.008022645 |
|    clip_fraction        | 0.0123      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 3.79e-05    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0933      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.000552   |
|    value_loss           | 15.7        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 58           |
|    iterations           | 4            |
|    time_elapsed         | 140          |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.0129058175 |
|    clip_fraction        | 0.0153       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | -1.67e-06    |
|    learning_rate        | 0.0003       |
|    loss                 | 4.05         |
|    n_updates            | 30           |
|    policy_gradient_loss | 0.000227     |
|    value_loss           | 12.4         |
------------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -500.00
Saving new best model at 10000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 56          |
|    iterations           | 5           |
|    time_elapsed         | 181         |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.004470688 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0928      |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.000335    |
|    value_loss           | 10.5        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 55           |
|    iterations           | 6            |
|    time_elapsed         | 221          |
|    total_timesteps      | 12288        |
| train/                  |              |
|    approx_kl            | 0.0045065796 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 6.09         |
|    n_updates            | 50           |
|    policy_gradient_loss | 0.000131     |
|    value_loss           | 9.72         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 55          |
|    iterations           | 7           |
|    time_elapsed         | 260         |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.011144355 |
|    clip_fraction        | 0.0601      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 2.38e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.153       |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00195    |
|    value_loss           | 10          |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 54          |
|    iterations           | 8           |
|    time_elapsed         | 300         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.003387887 |
|    clip_fraction        | 0.0333      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.137       |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.000215   |
|    value_loss           | 10.6        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 53          |
|    iterations           | 9           |
|    time_elapsed         | 341         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.005837161 |
|    clip_fraction        | 0.0608      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.895       |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00182    |
|    value_loss           | 11.4        |
-----------------------------------------
Num timesteps: 20000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 53           |
|    iterations           | 10           |
|    time_elapsed         | 381          |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.0055666785 |
|    clip_fraction        | 0.029        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.962       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 8.2          |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00059     |
|    value_loss           | 12.2         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 53          |
|    iterations           | 11          |
|    time_elapsed         | 422         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.009269826 |
|    clip_fraction        | 0.0491      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.892      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 7.96        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00074    |
|    value_loss           | 13.1        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 53           |
|    iterations           | 12           |
|    time_elapsed         | 462          |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0017160233 |
|    clip_fraction        | 0.0157       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.843       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 12.5         |
|    n_updates            | 110          |
|    policy_gradient_loss | 0.000103     |
|    value_loss           | 13.7         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 52          |
|    iterations           | 13          |
|    time_elapsed         | 502         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.011221405 |
|    clip_fraction        | 0.0758      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.767      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 2.78        |
|    n_updates            | 120         |
|    policy_gradient_loss | -1.02e-05   |
|    value_loss           | 14.3        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 52          |
|    iterations           | 14          |
|    time_elapsed         | 543         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.006660007 |
|    clip_fraction        | 0.0815      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.828      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.83        |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00269    |
|    value_loss           | 14.9        |
-----------------------------------------
Num timesteps: 30000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 52        |
|    iterations           | 15        |
|    time_elapsed         | 582       |
|    total_timesteps      | 30720     |
| train/                  |           |
|    approx_kl            | 0.0017982 |
|    clip_fraction        | 0.0197    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.853    |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 1.72      |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.00133  |
|    value_loss           | 15.3      |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 52          |
|    iterations           | 16          |
|    time_elapsed         | 622         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.008517181 |
|    clip_fraction        | 0.0766      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.739      |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 12.8        |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.000368   |
|    value_loss           | 15.7        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 52           |
|    iterations           | 17           |
|    time_elapsed         | 664          |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0044551054 |
|    clip_fraction        | 0.0353       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.674       |
|    explained_variance   | 2.38e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.244        |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.000726    |
|    value_loss           | 16           |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 52          |
|    iterations           | 18          |
|    time_elapsed         | 705         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.004653655 |
|    clip_fraction        | 0.0649      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.572      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 7.31        |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.000826   |
|    value_loss           | 16.2        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 52          |
|    iterations           | 19          |
|    time_elapsed         | 746         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.002446217 |
|    clip_fraction        | 0.0722      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.515      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 0.319       |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00158    |
|    value_loss           | 16.4        |
-----------------------------------------
Num timesteps: 40000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 51           |
|    iterations           | 20           |
|    time_elapsed         | 787          |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0032725735 |
|    clip_fraction        | 0.033        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.455       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 5.55         |
|    n_updates            | 190          |
|    policy_gradient_loss | -2.43e-05    |
|    value_loss           | 16.5         |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | -500       |
| time/                   |            |
|    fps                  | 51         |
|    iterations           | 21         |
|    time_elapsed         | 830        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.00331217 |
|    clip_fraction        | 0.0599     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.399     |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 16.1       |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.00155   |
|    value_loss           | 16.7       |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 51           |
|    iterations           | 22           |
|    time_elapsed         | 871          |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0016224722 |
|    clip_fraction        | 0.00854      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.414       |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 4.65         |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.000185    |
|    value_loss           | 24.8         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 51           |
|    iterations           | 23           |
|    time_elapsed         | 909          |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0029160513 |
|    clip_fraction        | 0.0412       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.415       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 13.8         |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00124     |
|    value_loss           | 16.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 51           |
|    iterations           | 24           |
|    time_elapsed         | 948          |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0005818395 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.375       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.93         |
|    n_updates            | 230          |
|    policy_gradient_loss | 0.000484     |
|    value_loss           | 16.8         |
------------------------------------------
Num timesteps: 50000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 51           |
|    iterations           | 25           |
|    time_elapsed         | 988          |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0016733338 |
|    clip_fraction        | 0.0173       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.41        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 9.67         |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00106     |
|    value_loss           | 16.9         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 51           |
|    iterations           | 26           |
|    time_elapsed         | 1031         |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0027202456 |
|    clip_fraction        | 0.0465       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.371       |
|    explained_variance   | 2.38e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 5.46         |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00133     |
|    value_loss           | 17           |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 51          |
|    iterations           | 27          |
|    time_elapsed         | 1073        |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.001184669 |
|    clip_fraction        | 0.00283     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.384      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.85        |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.000335   |
|    value_loss           | 17.1        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 51           |
|    iterations           | 28           |
|    time_elapsed         | 1116         |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0027754116 |
|    clip_fraction        | 0.0429       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.327       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.05         |
|    n_updates            | 270          |
|    policy_gradient_loss | 3.85e-05     |
|    value_loss           | 17.1         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 51           |
|    iterations           | 29           |
|    time_elapsed         | 1160         |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0018036406 |
|    clip_fraction        | 0.0237       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.285       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.263        |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00019     |
|    value_loss           | 17.2         |
------------------------------------------
Num timesteps: 60000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 51            |
|    iterations           | 30            |
|    time_elapsed         | 1201          |
|    total_timesteps      | 61440         |
| train/                  |               |
|    approx_kl            | 0.00015584988 |
|    clip_fraction        | 0.00884       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.302        |
|    explained_variance   | -5.96e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 6.86          |
|    n_updates            | 290           |
|    policy_gradient_loss | -1.93e-05     |
|    value_loss           | 17.2          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 51           |
|    iterations           | 31           |
|    time_elapsed         | 1241         |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.0016323563 |
|    clip_fraction        | 0.00796      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.253       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 7.82         |
|    n_updates            | 300          |
|    policy_gradient_loss | 0.000257     |
|    value_loss           | 17.2         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 51            |
|    iterations           | 32            |
|    time_elapsed         | 1282          |
|    total_timesteps      | 65536         |
| train/                  |               |
|    approx_kl            | 0.00083676487 |
|    clip_fraction        | 0.00825       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.212        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 5.08          |
|    n_updates            | 310           |
|    policy_gradient_loss | -4.58e-05     |
|    value_loss           | 17.3          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 51           |
|    iterations           | 33           |
|    time_elapsed         | 1321         |
|    total_timesteps      | 67584        |
| train/                  |              |
|    approx_kl            | 0.0009903361 |
|    clip_fraction        | 0.0219       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.187       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 10.9         |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.000673    |
|    value_loss           | 17.3         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 51           |
|    iterations           | 34           |
|    time_elapsed         | 1365         |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.0003366214 |
|    clip_fraction        | 0.00151      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.191       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.05         |
|    n_updates            | 330          |
|    policy_gradient_loss | 6.59e-05     |
|    value_loss           | 17.3         |
------------------------------------------
Num timesteps: 70000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 50           |
|    iterations           | 35           |
|    time_elapsed         | 1407         |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.0010136701 |
|    clip_fraction        | 0.0168       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.153       |
|    explained_variance   | -1.07e-06    |
|    learning_rate        | 0.0003       |
|    loss                 | 14.5         |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.000579    |
|    value_loss           | 17.4         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 50           |
|    iterations           | 36           |
|    time_elapsed         | 1447         |
|    total_timesteps      | 73728        |
| train/                  |              |
|    approx_kl            | 0.0005740837 |
|    clip_fraction        | 0.0208       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.122       |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 2.37         |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.00016     |
|    value_loss           | 17.4         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 37            |
|    time_elapsed         | 1487          |
|    total_timesteps      | 75776         |
| train/                  |               |
|    approx_kl            | 0.00034104355 |
|    clip_fraction        | 0.0141        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.109        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 7.13          |
|    n_updates            | 360           |
|    policy_gradient_loss | -0.000158     |
|    value_loss           | 17.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 38            |
|    time_elapsed         | 1527          |
|    total_timesteps      | 77824         |
| train/                  |               |
|    approx_kl            | 0.00049453165 |
|    clip_fraction        | 0.0146        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0877       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 16.9          |
|    n_updates            | 370           |
|    policy_gradient_loss | -0.000506     |
|    value_loss           | 17.4          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 51           |
|    iterations           | 39           |
|    time_elapsed         | 1565         |
|    total_timesteps      | 79872        |
| train/                  |              |
|    approx_kl            | 0.0001809483 |
|    clip_fraction        | 0.00107      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0862      |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 4.29         |
|    n_updates            | 380          |
|    policy_gradient_loss | 2.15e-05     |
|    value_loss           | 17.5         |
------------------------------------------
Num timesteps: 80000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 40            |
|    time_elapsed         | 1606          |
|    total_timesteps      | 81920         |
| train/                  |               |
|    approx_kl            | 0.00044152592 |
|    clip_fraction        | 0.00952       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0709       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 0.416         |
|    n_updates            | 390           |
|    policy_gradient_loss | -0.000257     |
|    value_loss           | 17.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 41            |
|    time_elapsed         | 1649          |
|    total_timesteps      | 83968         |
| train/                  |               |
|    approx_kl            | 0.00022601255 |
|    clip_fraction        | 0.00474       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0818       |
|    explained_variance   | -9.54e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 17.6          |
|    n_updates            | 400           |
|    policy_gradient_loss | -0.000708     |
|    value_loss           | 17.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 42            |
|    time_elapsed         | 1689          |
|    total_timesteps      | 86016         |
| train/                  |               |
|    approx_kl            | 0.00041311714 |
|    clip_fraction        | 0.00757       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0631       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 24.4          |
|    n_updates            | 410           |
|    policy_gradient_loss | -0.000159     |
|    value_loss           | 17.4          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 50           |
|    iterations           | 43           |
|    time_elapsed         | 1730         |
|    total_timesteps      | 88064        |
| train/                  |              |
|    approx_kl            | 0.0009415132 |
|    clip_fraction        | 0.00596      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0507      |
|    explained_variance   | 3.58e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 3.37         |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.000204    |
|    value_loss           | 25.5         |
------------------------------------------
Num timesteps: 90000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 44            |
|    time_elapsed         | 1770          |
|    total_timesteps      | 90112         |
| train/                  |               |
|    approx_kl            | 0.00019170318 |
|    clip_fraction        | 0.00327       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0312       |
|    explained_variance   | 6.56e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 18.9          |
|    n_updates            | 430           |
|    policy_gradient_loss | -0.000161     |
|    value_loss           | 17.2          |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 1e+03          |
|    ep_rew_mean          | -500           |
| time/                   |                |
|    fps                  | 50             |
|    iterations           | 45             |
|    time_elapsed         | 1809           |
|    total_timesteps      | 92160          |
| train/                  |                |
|    approx_kl            | 0.000102133345 |
|    clip_fraction        | 0.0021         |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0249        |
|    explained_variance   | -1.19e-07      |
|    learning_rate        | 0.0003         |
|    loss                 | 2.06           |
|    n_updates            | 440            |
|    policy_gradient_loss | -9.75e-05      |
|    value_loss           | 17.2           |
--------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 50           |
|    iterations           | 46           |
|    time_elapsed         | 1849         |
|    total_timesteps      | 94208        |
| train/                  |              |
|    approx_kl            | 7.570573e-05 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0201      |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 8.77         |
|    n_updates            | 450          |
|    policy_gradient_loss | -6.46e-05    |
|    value_loss           | 17.3         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 47            |
|    time_elapsed         | 1890          |
|    total_timesteps      | 96256         |
| train/                  |               |
|    approx_kl            | 0.00027249663 |
|    clip_fraction        | 0.00347       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.014        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 2.4           |
|    n_updates            | 460           |
|    policy_gradient_loss | -0.000153     |
|    value_loss           | 17.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 48            |
|    time_elapsed         | 1933          |
|    total_timesteps      | 98304         |
| train/                  |               |
|    approx_kl            | 5.2549614e-05 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.011        |
|    explained_variance   | -7.15e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 13.5          |
|    n_updates            | 470           |
|    policy_gradient_loss | -3.33e-05     |
|    value_loss           | 17.4          |
-------------------------------------------
Num timesteps: 100000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 49            |
|    time_elapsed         | 1973          |
|    total_timesteps      | 100352        |
| train/                  |               |
|    approx_kl            | 1.4210469e-05 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00884      |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 11.2          |
|    n_updates            | 480           |
|    policy_gradient_loss | -1.52e-05     |
|    value_loss           | 17.3          |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 50          |
|    iterations           | 50          |
|    time_elapsed         | 2012        |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 2.86203e-05 |
|    clip_fraction        | 0.000879    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00727    |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 2.34        |
|    n_updates            | 490         |
|    policy_gradient_loss | -3.41e-05   |
|    value_loss           | 17.3        |
-----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 51            |
|    time_elapsed         | 2054          |
|    total_timesteps      | 104448        |
| train/                  |               |
|    approx_kl            | 1.2361037e-05 |
|    clip_fraction        | 0.00103       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00869      |
|    explained_variance   | -1.07e-06     |
|    learning_rate        | 0.0003        |
|    loss                 | 1.53          |
|    n_updates            | 500           |
|    policy_gradient_loss | 5.87e-05      |
|    value_loss           | 17.3          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 50           |
|    iterations           | 52           |
|    time_elapsed         | 2097         |
|    total_timesteps      | 106496       |
| train/                  |              |
|    approx_kl            | 8.793635e-06 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00773     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 7.37         |
|    n_updates            | 510          |
|    policy_gradient_loss | 2.7e-05      |
|    value_loss           | 17.4         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 50           |
|    iterations           | 53           |
|    time_elapsed         | 2137         |
|    total_timesteps      | 108544       |
| train/                  |              |
|    approx_kl            | 4.110046e-05 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00535     |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 0.797        |
|    n_updates            | 520          |
|    policy_gradient_loss | -5.02e-05    |
|    value_loss           | 17.4         |
------------------------------------------
Num timesteps: 110000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 54            |
|    time_elapsed         | 2176          |
|    total_timesteps      | 110592        |
| train/                  |               |
|    approx_kl            | 1.1743541e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00469      |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 0.826         |
|    n_updates            | 530           |
|    policy_gradient_loss | -1.18e-05     |
|    value_loss           | 17.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 55            |
|    time_elapsed         | 2215          |
|    total_timesteps      | 112640        |
| train/                  |               |
|    approx_kl            | 1.4505116e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00399      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 32.3          |
|    n_updates            | 540           |
|    policy_gradient_loss | -1.7e-05      |
|    value_loss           | 17.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 56            |
|    time_elapsed         | 2256          |
|    total_timesteps      | 114688        |
| train/                  |               |
|    approx_kl            | 2.1184474e-05 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00345      |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 9.99          |
|    n_updates            | 550           |
|    policy_gradient_loss | -1.39e-05     |
|    value_loss           | 17.4          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 50           |
|    iterations           | 57           |
|    time_elapsed         | 2296         |
|    total_timesteps      | 116736       |
| train/                  |              |
|    approx_kl            | 3.855652e-05 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0028      |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 0.325        |
|    n_updates            | 560          |
|    policy_gradient_loss | -1.49e-05    |
|    value_loss           | 17.4         |
------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 58        |
|    time_elapsed         | 2337      |
|    total_timesteps      | 118784    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00271  |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.871     |
|    n_updates            | 570       |
|    policy_gradient_loss | -6.14e-08 |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 120000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 59            |
|    time_elapsed         | 2377          |
|    total_timesteps      | 120832        |
| train/                  |               |
|    approx_kl            | 3.7856895e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00244      |
|    explained_variance   | 2.38e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 27            |
|    n_updates            | 580           |
|    policy_gradient_loss | -1.78e-05     |
|    value_loss           | 17.4          |
-------------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 50       |
|    iterations           | 60       |
|    time_elapsed         | 2416     |
|    total_timesteps      | 122880   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00236 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 25       |
|    n_updates            | 590      |
|    policy_gradient_loss | 1.14e-08 |
|    value_loss           | 17.4     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 61        |
|    time_elapsed         | 2456      |
|    total_timesteps      | 124928    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00236  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 4.02      |
|    n_updates            | 600       |
|    policy_gradient_loss | -5.65e-10 |
|    value_loss           | 17.4      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 50       |
|    iterations           | 62       |
|    time_elapsed         | 2498     |
|    total_timesteps      | 126976   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00236 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 4.44     |
|    n_updates            | 610      |
|    policy_gradient_loss | 3.19e-08 |
|    value_loss           | 17.4     |
--------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 63            |
|    time_elapsed         | 2540          |
|    total_timesteps      | 129024        |
| train/                  |               |
|    approx_kl            | 1.4694786e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00196      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 19.7          |
|    n_updates            | 620           |
|    policy_gradient_loss | -1.77e-05     |
|    value_loss           | 17.4          |
-------------------------------------------
Num timesteps: 130000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 50           |
|    iterations           | 64           |
|    time_elapsed         | 2580         |
|    total_timesteps      | 131072       |
| train/                  |              |
|    approx_kl            | 8.912175e-07 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00204     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 28.3         |
|    n_updates            | 630          |
|    policy_gradient_loss | 1.46e-06     |
|    value_loss           | 25.4         |
------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 65        |
|    time_elapsed         | 2621      |
|    total_timesteps      | 133120    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00193  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 37        |
|    n_updates            | 640       |
|    policy_gradient_loss | -9.97e-09 |
|    value_loss           | 17.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 66        |
|    time_elapsed         | 2663      |
|    total_timesteps      | 135168    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00192  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 1.1       |
|    n_updates            | 650       |
|    policy_gradient_loss | 7.9e-09   |
|    value_loss           | 17.2      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 50       |
|    iterations           | 67       |
|    time_elapsed         | 2702     |
|    total_timesteps      | 137216   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00193 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 16.2     |
|    n_updates            | 660      |
|    policy_gradient_loss | 1.04e-08 |
|    value_loss           | 17.3     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 68        |
|    time_elapsed         | 2744      |
|    total_timesteps      | 139264    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00192  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 16.7      |
|    n_updates            | 670       |
|    policy_gradient_loss | -4.31e-09 |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 140000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 69        |
|    time_elapsed         | 2786      |
|    total_timesteps      | 141312    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00193  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 35.2      |
|    n_updates            | 680       |
|    policy_gradient_loss | -7.95e-09 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 70        |
|    time_elapsed         | 2829      |
|    total_timesteps      | 143360    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00192  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 5.91      |
|    n_updates            | 690       |
|    policy_gradient_loss | -1.47e-08 |
|    value_loss           | 17.3      |
---------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 1e+03          |
|    ep_rew_mean          | -500           |
| time/                   |                |
|    fps                  | 50             |
|    iterations           | 71             |
|    time_elapsed         | 2872           |
|    total_timesteps      | 145408         |
| train/                  |                |
|    approx_kl            | 0.000110572466 |
|    clip_fraction        | 0.000391       |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00118       |
|    explained_variance   | -1.19e-07      |
|    learning_rate        | 0.0003         |
|    loss                 | 33.3           |
|    n_updates            | 700            |
|    policy_gradient_loss | -1.66e-05      |
|    value_loss           | 17.3           |
--------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 72        |
|    time_elapsed         | 2914      |
|    total_timesteps      | 147456    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00106  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 17.4      |
|    n_updates            | 710       |
|    policy_gradient_loss | -1.12e-08 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 73        |
|    time_elapsed         | 2955      |
|    total_timesteps      | 149504    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00106  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 3.36      |
|    n_updates            | 720       |
|    policy_gradient_loss | -3.41e-08 |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 150000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 74        |
|    time_elapsed         | 2999      |
|    total_timesteps      | 151552    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00106  |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 9.01      |
|    n_updates            | 730       |
|    policy_gradient_loss | -1.08e-08 |
|    value_loss           | 17.3      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 75            |
|    time_elapsed         | 3041          |
|    total_timesteps      | 153600        |
| train/                  |               |
|    approx_kl            | 0.00023689185 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000507     |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 1.72          |
|    n_updates            | 740           |
|    policy_gradient_loss | -1.93e-05     |
|    value_loss           | 17.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 76            |
|    time_elapsed         | 3083          |
|    total_timesteps      | 155648        |
| train/                  |               |
|    approx_kl            | 3.3701217e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00037      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 13.8          |
|    n_updates            | 750           |
|    policy_gradient_loss | -1.77e-05     |
|    value_loss           | 17.4          |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 77        |
|    time_elapsed         | 3123      |
|    total_timesteps      | 157696    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000363 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 7.95      |
|    n_updates            | 760       |
|    policy_gradient_loss | -8.38e-10 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 78        |
|    time_elapsed         | 3167      |
|    total_timesteps      | 159744    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000365 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 30.4      |
|    n_updates            | 770       |
|    policy_gradient_loss | 1.11e-08  |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 160000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 79        |
|    time_elapsed         | 3210      |
|    total_timesteps      | 161792    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000364 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 8.88      |
|    n_updates            | 780       |
|    policy_gradient_loss | -1.78e-08 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 80        |
|    time_elapsed         | 3253      |
|    total_timesteps      | 163840    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000364 |
|    explained_variance   | 2.38e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 4.56      |
|    n_updates            | 790       |
|    policy_gradient_loss | 3.58e-10  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 81        |
|    time_elapsed         | 3294      |
|    total_timesteps      | 165888    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000365 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.924     |
|    n_updates            | 800       |
|    policy_gradient_loss | -3.13e-08 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 82        |
|    time_elapsed         | 3337      |
|    total_timesteps      | 167936    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000364 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.66      |
|    n_updates            | 810       |
|    policy_gradient_loss | -9.12e-09 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 83        |
|    time_elapsed         | 3378      |
|    total_timesteps      | 169984    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000365 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 22.5      |
|    n_updates            | 820       |
|    policy_gradient_loss | -2.34e-08 |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 170000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 84        |
|    time_elapsed         | 3419      |
|    total_timesteps      | 172032    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000366 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 9.03      |
|    n_updates            | 830       |
|    policy_gradient_loss | -1.87e-08 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 85        |
|    time_elapsed         | 3459      |
|    total_timesteps      | 174080    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000365 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0003    |
|    loss                 | 16.3      |
|    n_updates            | 840       |
|    policy_gradient_loss | -4.26e-09 |
|    value_loss           | 24.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 86        |
|    time_elapsed         | 3499      |
|    total_timesteps      | 176128    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000364 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 6.54      |
|    n_updates            | 850       |
|    policy_gradient_loss | 8.6e-10   |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 87        |
|    time_elapsed         | 3540      |
|    total_timesteps      | 178176    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000364 |
|    explained_variance   | 2.38e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 29.2      |
|    n_updates            | 860       |
|    policy_gradient_loss | 4.98e-09  |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 180000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 88        |
|    time_elapsed         | 3582      |
|    total_timesteps      | 180224    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000366 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 19.6      |
|    n_updates            | 870       |
|    policy_gradient_loss | -2.7e-09  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 89        |
|    time_elapsed         | 3623      |
|    total_timesteps      | 182272    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000365 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.856     |
|    n_updates            | 880       |
|    policy_gradient_loss | -6.16e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 90        |
|    time_elapsed         | 3662      |
|    total_timesteps      | 184320    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000365 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 3.11      |
|    n_updates            | 890       |
|    policy_gradient_loss | -9.24e-09 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 91        |
|    time_elapsed         | 3702      |
|    total_timesteps      | 186368    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000364 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.1      |
|    n_updates            | 900       |
|    policy_gradient_loss | 6.87e-09  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 92        |
|    time_elapsed         | 3742      |
|    total_timesteps      | 188416    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000364 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 12.4      |
|    n_updates            | 910       |
|    policy_gradient_loss | -8.37e-09 |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 190000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 93        |
|    time_elapsed         | 3783      |
|    total_timesteps      | 190464    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000364 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 4.27      |
|    n_updates            | 920       |
|    policy_gradient_loss | -1.48e-08 |
|    value_loss           | 17.4      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 50            |
|    iterations           | 94            |
|    time_elapsed         | 3828          |
|    total_timesteps      | 192512        |
| train/                  |               |
|    approx_kl            | 0.00021315913 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000183     |
|    explained_variance   | 2.38e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 0.62          |
|    n_updates            | 930           |
|    policy_gradient_loss | -1.61e-05     |
|    value_loss           | 17.4          |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 95        |
|    time_elapsed         | 3874      |
|    total_timesteps      | 194560    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 27.6      |
|    n_updates            | 940       |
|    policy_gradient_loss | -2.46e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 96        |
|    time_elapsed         | 3914      |
|    total_timesteps      | 196608    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 17.3      |
|    n_updates            | 950       |
|    policy_gradient_loss | 1.2e-09   |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 97        |
|    time_elapsed         | 3957      |
|    total_timesteps      | 198656    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 21.2      |
|    n_updates            | 960       |
|    policy_gradient_loss | 8.5e-09   |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 200000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 98        |
|    time_elapsed         | 4000      |
|    total_timesteps      | 200704    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 16.4      |
|    n_updates            | 970       |
|    policy_gradient_loss | 2.81e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 99        |
|    time_elapsed         | 4041      |
|    total_timesteps      | 202752    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 10.9      |
|    n_updates            | 980       |
|    policy_gradient_loss | 4.58e-09  |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 100       |
|    time_elapsed         | 4083      |
|    total_timesteps      | 204800    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.284     |
|    n_updates            | 990       |
|    policy_gradient_loss | 8.21e-10  |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 101       |
|    time_elapsed         | 4126      |
|    total_timesteps      | 206848    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 8.95      |
|    n_updates            | 1000      |
|    policy_gradient_loss | -1.06e-10 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 102       |
|    time_elapsed         | 4166      |
|    total_timesteps      | 208896    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.412     |
|    n_updates            | 1010      |
|    policy_gradient_loss | 5.11e-08  |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 210000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 103       |
|    time_elapsed         | 4210      |
|    total_timesteps      | 210944    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 10.4      |
|    n_updates            | 1020      |
|    policy_gradient_loss | -1.35e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 104       |
|    time_elapsed         | 4250      |
|    total_timesteps      | 212992    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000163 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 16.3      |
|    n_updates            | 1030      |
|    policy_gradient_loss | -5.5e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 105       |
|    time_elapsed         | 4291      |
|    total_timesteps      | 215040    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.433     |
|    n_updates            | 1040      |
|    policy_gradient_loss | -2.74e-08 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 106       |
|    time_elapsed         | 4331      |
|    total_timesteps      | 217088    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 1.79e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 14.4      |
|    n_updates            | 1050      |
|    policy_gradient_loss | -8.21e-10 |
|    value_loss           | 22.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 107       |
|    time_elapsed         | 4372      |
|    total_timesteps      | 219136    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 10.1      |
|    n_updates            | 1060      |
|    policy_gradient_loss | -1e-09    |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 220000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 108       |
|    time_elapsed         | 4415      |
|    total_timesteps      | 221184    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 1.2       |
|    n_updates            | 1070      |
|    policy_gradient_loss | -1.77e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 109       |
|    time_elapsed         | 4455      |
|    total_timesteps      | 223232    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.998     |
|    n_updates            | 1080      |
|    policy_gradient_loss | -6.7e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 110       |
|    time_elapsed         | 4497      |
|    total_timesteps      | 225280    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.51      |
|    n_updates            | 1090      |
|    policy_gradient_loss | -4.61e-10 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 111       |
|    time_elapsed         | 4538      |
|    total_timesteps      | 227328    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 2.31      |
|    n_updates            | 1100      |
|    policy_gradient_loss | 1.45e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 112       |
|    time_elapsed         | 4579      |
|    total_timesteps      | 229376    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 25        |
|    n_updates            | 1110      |
|    policy_gradient_loss | 2.46e-10  |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 230000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 113       |
|    time_elapsed         | 4622      |
|    total_timesteps      | 231424    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.952     |
|    n_updates            | 1120      |
|    policy_gradient_loss | 1.8e-09   |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 114       |
|    time_elapsed         | 4666      |
|    total_timesteps      | 233472    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 1.45      |
|    n_updates            | 1130      |
|    policy_gradient_loss | 5.24e-11  |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 115       |
|    time_elapsed         | 4710      |
|    total_timesteps      | 235520    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.471     |
|    n_updates            | 1140      |
|    policy_gradient_loss | -1.98e-08 |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 116       |
|    time_elapsed         | 4752      |
|    total_timesteps      | 237568    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 2.02      |
|    n_updates            | 1150      |
|    policy_gradient_loss | 3.57e-09  |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 117       |
|    time_elapsed         | 4793      |
|    total_timesteps      | 239616    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 24.1      |
|    n_updates            | 1160      |
|    policy_gradient_loss | 3.25e-10  |
|    value_loss           | 17.5      |
---------------------------------------
Num timesteps: 240000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 118       |
|    time_elapsed         | 4833      |
|    total_timesteps      | 241664    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 1.73      |
|    n_updates            | 1170      |
|    policy_gradient_loss | 6.58e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 119       |
|    time_elapsed         | 4874      |
|    total_timesteps      | 243712    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 1.66      |
|    n_updates            | 1180      |
|    policy_gradient_loss | 6.99e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 120       |
|    time_elapsed         | 4914      |
|    total_timesteps      | 245760    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.6       |
|    n_updates            | 1190      |
|    policy_gradient_loss | -3.07e-08 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 121       |
|    time_elapsed         | 4953      |
|    total_timesteps      | 247808    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.257     |
|    n_updates            | 1200      |
|    policy_gradient_loss | -5.45e-09 |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 122       |
|    time_elapsed         | 4995      |
|    total_timesteps      | 249856    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.482     |
|    n_updates            | 1210      |
|    policy_gradient_loss | -8.27e-10 |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 250000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 123       |
|    time_elapsed         | 5035      |
|    total_timesteps      | 251904    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000163 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 2.92      |
|    n_updates            | 1220      |
|    policy_gradient_loss | 2.17e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 124       |
|    time_elapsed         | 5076      |
|    total_timesteps      | 253952    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 4.01      |
|    n_updates            | 1230      |
|    policy_gradient_loss | 2.04e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 50        |
|    iterations           | 125       |
|    time_elapsed         | 5116      |
|    total_timesteps      | 256000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 3.85      |
|    n_updates            | 1240      |
|    policy_gradient_loss | -9.55e-10 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 126       |
|    time_elapsed         | 5161      |
|    total_timesteps      | 258048    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0003    |
|    loss                 | 9.44      |
|    n_updates            | 1250      |
|    policy_gradient_loss | -1.62e-09 |
|    value_loss           | 25.7      |
---------------------------------------
Num timesteps: 260000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 127       |
|    time_elapsed         | 5203      |
|    total_timesteps      | 260096    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 20        |
|    n_updates            | 1260      |
|    policy_gradient_loss | 2.15e-09  |
|    value_loss           | 17.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 128       |
|    time_elapsed         | 5243      |
|    total_timesteps      | 262144    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 6.51      |
|    n_updates            | 1270      |
|    policy_gradient_loss | 1.18e-09  |
|    value_loss           | 17.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 129       |
|    time_elapsed         | 5284      |
|    total_timesteps      | 264192    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 12.2      |
|    n_updates            | 1280      |
|    policy_gradient_loss | -1.49e-09 |
|    value_loss           | 17.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 130       |
|    time_elapsed         | 5326      |
|    total_timesteps      | 266240    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 11.3      |
|    n_updates            | 1290      |
|    policy_gradient_loss | -2.09e-09 |
|    value_loss           | 17.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 131       |
|    time_elapsed         | 5366      |
|    total_timesteps      | 268288    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 8.05      |
|    n_updates            | 1300      |
|    policy_gradient_loss | 2.69e-08  |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 270000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 132       |
|    time_elapsed         | 5407      |
|    total_timesteps      | 270336    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 3.53      |
|    n_updates            | 1310      |
|    policy_gradient_loss | 2.72e-09  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 133       |
|    time_elapsed         | 5448      |
|    total_timesteps      | 272384    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 9.21      |
|    n_updates            | 1320      |
|    policy_gradient_loss | -1.73e-09 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 134       |
|    time_elapsed         | 5492      |
|    total_timesteps      | 274432    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 2.7       |
|    n_updates            | 1330      |
|    policy_gradient_loss | -1.23e-08 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 135       |
|    time_elapsed         | 5536      |
|    total_timesteps      | 276480    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 2.92      |
|    n_updates            | 1340      |
|    policy_gradient_loss | 7.59e-09  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 136       |
|    time_elapsed         | 5578      |
|    total_timesteps      | 278528    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 8.91      |
|    n_updates            | 1350      |
|    policy_gradient_loss | 2.5e-09   |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 280000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 137       |
|    time_elapsed         | 5620      |
|    total_timesteps      | 280576    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.217     |
|    n_updates            | 1360      |
|    policy_gradient_loss | -5.97e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 138       |
|    time_elapsed         | 5662      |
|    total_timesteps      | 282624    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 3.7       |
|    n_updates            | 1370      |
|    policy_gradient_loss | -1.98e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 139       |
|    time_elapsed         | 5704      |
|    total_timesteps      | 284672    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 2.38e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.836     |
|    n_updates            | 1380      |
|    policy_gradient_loss | 8.38e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 140       |
|    time_elapsed         | 5744      |
|    total_timesteps      | 286720    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 9.37      |
|    n_updates            | 1390      |
|    policy_gradient_loss | 5.31e-09  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 141       |
|    time_elapsed         | 5784      |
|    total_timesteps      | 288768    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 4.87      |
|    n_updates            | 1400      |
|    policy_gradient_loss | 4.08e-09  |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 290000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 142       |
|    time_elapsed         | 5824      |
|    total_timesteps      | 290816    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 5.5       |
|    n_updates            | 1410      |
|    policy_gradient_loss | -1.46e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 143       |
|    time_elapsed         | 5868      |
|    total_timesteps      | 292864    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 10.4      |
|    n_updates            | 1420      |
|    policy_gradient_loss | -9.09e-11 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 144       |
|    time_elapsed         | 5908      |
|    total_timesteps      | 294912    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 4.81      |
|    n_updates            | 1430      |
|    policy_gradient_loss | -1.19e-08 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 145       |
|    time_elapsed         | 5952      |
|    total_timesteps      | 296960    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 2.25      |
|    n_updates            | 1440      |
|    policy_gradient_loss | -3.19e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 146       |
|    time_elapsed         | 5994      |
|    total_timesteps      | 299008    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 2.79      |
|    n_updates            | 1450      |
|    policy_gradient_loss | 2.18e-09  |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 300000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 147       |
|    time_elapsed         | 6035      |
|    total_timesteps      | 301056    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 3.75      |
|    n_updates            | 1460      |
|    policy_gradient_loss | -3.01e-09 |
|    value_loss           | 25.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 148       |
|    time_elapsed         | 6074      |
|    total_timesteps      | 303104    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000163 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 9.03      |
|    n_updates            | 1470      |
|    policy_gradient_loss | 8.01e-09  |
|    value_loss           | 17.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 149       |
|    time_elapsed         | 6116      |
|    total_timesteps      | 305152    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 24.9      |
|    n_updates            | 1480      |
|    policy_gradient_loss | -1.55e-09 |
|    value_loss           | 17.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 150       |
|    time_elapsed         | 6159      |
|    total_timesteps      | 307200    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.794     |
|    n_updates            | 1490      |
|    policy_gradient_loss | -1.64e-09 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 151       |
|    time_elapsed         | 6203      |
|    total_timesteps      | 309248    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 4.5       |
|    n_updates            | 1500      |
|    policy_gradient_loss | -3.4e-09  |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 310000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 152       |
|    time_elapsed         | 6243      |
|    total_timesteps      | 311296    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.6      |
|    n_updates            | 1510      |
|    policy_gradient_loss | -3.2e-09  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 153       |
|    time_elapsed         | 6284      |
|    total_timesteps      | 313344    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.484     |
|    n_updates            | 1520      |
|    policy_gradient_loss | 7.01e-10  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 154       |
|    time_elapsed         | 6326      |
|    total_timesteps      | 315392    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000162 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.25      |
|    n_updates            | 1530      |
|    policy_gradient_loss | -3.58e-10 |
|    value_loss           | 17.3      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 49            |
|    iterations           | 155           |
|    time_elapsed         | 6369          |
|    total_timesteps      | 317440        |
| train/                  |               |
|    approx_kl            | 0.00013402913 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.42e-05     |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 17            |
|    n_updates            | 1540          |
|    policy_gradient_loss | -1.96e-05     |
|    value_loss           | 17.3          |
-------------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 156      |
|    time_elapsed         | 6410     |
|    total_timesteps      | 319488   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -9e-05   |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 16.7     |
|    n_updates            | 1550     |
|    policy_gradient_loss | 4.13e-09 |
|    value_loss           | 17.3     |
--------------------------------------
Num timesteps: 320000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 157       |
|    time_elapsed         | 6450      |
|    total_timesteps      | 321536    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.02e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 7.78      |
|    n_updates            | 1560      |
|    policy_gradient_loss | -9.75e-09 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 158       |
|    time_elapsed         | 6491      |
|    total_timesteps      | 323584    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.05e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 11.2      |
|    n_updates            | 1570      |
|    policy_gradient_loss | -1.03e-08 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 159       |
|    time_elapsed         | 6531      |
|    total_timesteps      | 325632    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.04e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 4.99      |
|    n_updates            | 1580      |
|    policy_gradient_loss | -3.23e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 160       |
|    time_elapsed         | 6572      |
|    total_timesteps      | 327680    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.04e-05 |
|    explained_variance   | 3.58e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 1.53      |
|    n_updates            | 1590      |
|    policy_gradient_loss | 5.32e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 161       |
|    time_elapsed         | 6614      |
|    total_timesteps      | 329728    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.02e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 33        |
|    n_updates            | 1600      |
|    policy_gradient_loss | 4.48e-09  |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 330000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 162      |
|    time_elapsed         | 6656     |
|    total_timesteps      | 331776   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -9e-05   |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 0.256    |
|    n_updates            | 1610     |
|    policy_gradient_loss | 4.39e-10 |
|    value_loss           | 17.5     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 163       |
|    time_elapsed         | 6699      |
|    total_timesteps      | 333824    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9e-05    |
|    explained_variance   | 2.38e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 6.33      |
|    n_updates            | 1620      |
|    policy_gradient_loss | -3.68e-09 |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 164       |
|    time_elapsed         | 6744      |
|    total_timesteps      | 335872    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.01e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 4.52      |
|    n_updates            | 1630      |
|    policy_gradient_loss | -7.28e-11 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 165       |
|    time_elapsed         | 6786      |
|    total_timesteps      | 337920    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.01e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.718     |
|    n_updates            | 1640      |
|    policy_gradient_loss | -3.35e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 166       |
|    time_elapsed         | 6832      |
|    total_timesteps      | 339968    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.02e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 5.4       |
|    n_updates            | 1650      |
|    policy_gradient_loss | 6.13e-09  |
|    value_loss           | 17.5      |
---------------------------------------
Num timesteps: 340000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 167       |
|    time_elapsed         | 6876      |
|    total_timesteps      | 342016    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.01e-05 |
|    explained_variance   | 2.38e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 5.34      |
|    n_updates            | 1660      |
|    policy_gradient_loss | -9.63e-10 |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 168       |
|    time_elapsed         | 6915      |
|    total_timesteps      | 344064    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.05e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 10.1      |
|    n_updates            | 1670      |
|    policy_gradient_loss | 6.84e-10  |
|    value_loss           | 25.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 169       |
|    time_elapsed         | 6956      |
|    total_timesteps      | 346112    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.03e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.533     |
|    n_updates            | 1680      |
|    policy_gradient_loss | -1.46e-09 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 170       |
|    time_elapsed         | 6998      |
|    total_timesteps      | 348160    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.04e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.83      |
|    n_updates            | 1690      |
|    policy_gradient_loss | 2.05e-09  |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 350000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 171       |
|    time_elapsed         | 7037      |
|    total_timesteps      | 350208    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.03e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.195     |
|    n_updates            | 1700      |
|    policy_gradient_loss | 9.18e-10  |
|    value_loss           | 17.3      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 172      |
|    time_elapsed         | 7081     |
|    total_timesteps      | 352256   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -9e-05   |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 4.29     |
|    n_updates            | 1710     |
|    policy_gradient_loss | 1.66e-09 |
|    value_loss           | 17.3     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 173       |
|    time_elapsed         | 7121      |
|    total_timesteps      | 354304    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.03e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 12.1      |
|    n_updates            | 1720      |
|    policy_gradient_loss | -2.33e-08 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 174       |
|    time_elapsed         | 7162      |
|    total_timesteps      | 356352    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.05e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.69      |
|    n_updates            | 1730      |
|    policy_gradient_loss | -8.83e-10 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 175       |
|    time_elapsed         | 7203      |
|    total_timesteps      | 358400    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.03e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 4.81      |
|    n_updates            | 1740      |
|    policy_gradient_loss | 2.92e-09  |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 360000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 176       |
|    time_elapsed         | 7245      |
|    total_timesteps      | 360448    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.01e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 8.93      |
|    n_updates            | 1750      |
|    policy_gradient_loss | 5.79e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 177       |
|    time_elapsed         | 7285      |
|    total_timesteps      | 362496    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.05e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 14.9      |
|    n_updates            | 1760      |
|    policy_gradient_loss | -2.62e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 178       |
|    time_elapsed         | 7327      |
|    total_timesteps      | 364544    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.03e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 11.9      |
|    n_updates            | 1770      |
|    policy_gradient_loss | -3.78e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 179       |
|    time_elapsed         | 7370      |
|    total_timesteps      | 366592    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.03e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 19.8      |
|    n_updates            | 1780      |
|    policy_gradient_loss | 4.76e-10  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 180       |
|    time_elapsed         | 7410      |
|    total_timesteps      | 368640    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.05e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 10.5      |
|    n_updates            | 1790      |
|    policy_gradient_loss | 7.39e-09  |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 370000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 181       |
|    time_elapsed         | 7449      |
|    total_timesteps      | 370688    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.04e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 13.7      |
|    n_updates            | 1800      |
|    policy_gradient_loss | -4.04e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 182       |
|    time_elapsed         | 7489      |
|    total_timesteps      | 372736    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.02e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 12.9      |
|    n_updates            | 1810      |
|    policy_gradient_loss | 1.09e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 183       |
|    time_elapsed         | 7530      |
|    total_timesteps      | 374784    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.03e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 21.7      |
|    n_updates            | 1820      |
|    policy_gradient_loss | -8.15e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 184       |
|    time_elapsed         | 7570      |
|    total_timesteps      | 376832    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.02e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 32        |
|    n_updates            | 1830      |
|    policy_gradient_loss | -2.66e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 185       |
|    time_elapsed         | 7609      |
|    total_timesteps      | 378880    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.07e-05 |
|    explained_variance   | 3.58e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 7.7       |
|    n_updates            | 1840      |
|    policy_gradient_loss | 4.21e-09  |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 380000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 186       |
|    time_elapsed         | 7650      |
|    total_timesteps      | 380928    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.05e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.413     |
|    n_updates            | 1850      |
|    policy_gradient_loss | -9.55e-10 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 187       |
|    time_elapsed         | 7690      |
|    total_timesteps      | 382976    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.04e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.55      |
|    n_updates            | 1860      |
|    policy_gradient_loss | 2.03e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 188       |
|    time_elapsed         | 7729      |
|    total_timesteps      | 385024    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.06e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.395     |
|    n_updates            | 1870      |
|    policy_gradient_loss | 7.37e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 189       |
|    time_elapsed         | 7767      |
|    total_timesteps      | 387072    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.06e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 15.6      |
|    n_updates            | 1880      |
|    policy_gradient_loss | -2.49e-09 |
|    value_loss           | 25.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 190       |
|    time_elapsed         | 7809      |
|    total_timesteps      | 389120    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.99e-05 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.958     |
|    n_updates            | 1890      |
|    policy_gradient_loss | 3.73e-10  |
|    value_loss           | 17.2      |
---------------------------------------
Num timesteps: 390000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 191       |
|    time_elapsed         | 7852      |
|    total_timesteps      | 391168    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.99e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 2.74      |
|    n_updates            | 1900      |
|    policy_gradient_loss | 8.23e-09  |
|    value_loss           | 17.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 192       |
|    time_elapsed         | 7894      |
|    total_timesteps      | 393216    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.03e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.953     |
|    n_updates            | 1910      |
|    policy_gradient_loss | -3.19e-09 |
|    value_loss           | 17.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 193       |
|    time_elapsed         | 7933      |
|    total_timesteps      | 395264    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.06e-05 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 11        |
|    n_updates            | 1920      |
|    policy_gradient_loss | -6.68e-09 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 194       |
|    time_elapsed         | 7974      |
|    total_timesteps      | 397312    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.02e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 9.5       |
|    n_updates            | 1930      |
|    policy_gradient_loss | 5.07e-08  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 195       |
|    time_elapsed         | 8015      |
|    total_timesteps      | 399360    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.04e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.169     |
|    n_updates            | 1940      |
|    policy_gradient_loss | -2.03e-09 |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 400000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 196       |
|    time_elapsed         | 8059      |
|    total_timesteps      | 401408    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.01e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 12.9      |
|    n_updates            | 1950      |
|    policy_gradient_loss | -1.54e-10 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 197       |
|    time_elapsed         | 8102      |
|    total_timesteps      | 403456    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.01e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 18.7      |
|    n_updates            | 1960      |
|    policy_gradient_loss | -3.61e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 198       |
|    time_elapsed         | 8141      |
|    total_timesteps      | 405504    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.04e-05 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 7.45      |
|    n_updates            | 1970      |
|    policy_gradient_loss | -1.28e-09 |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 199       |
|    time_elapsed         | 8181      |
|    total_timesteps      | 407552    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.02e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 13.6      |
|    n_updates            | 1980      |
|    policy_gradient_loss | -4.66e-11 |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 200       |
|    time_elapsed         | 8224      |
|    total_timesteps      | 409600    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.01e-05 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 19.1      |
|    n_updates            | 1990      |
|    policy_gradient_loss | -1.55e-09 |
|    value_loss           | 17.5      |
---------------------------------------
Num timesteps: 410000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 201       |
|    time_elapsed         | 8264      |
|    total_timesteps      | 411648    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.02e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.2       |
|    n_updates            | 2000      |
|    policy_gradient_loss | 5.1e-09   |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 202       |
|    time_elapsed         | 8306      |
|    total_timesteps      | 413696    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.02e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.297     |
|    n_updates            | 2010      |
|    policy_gradient_loss | -5.24e-09 |
|    value_loss           | 17.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 203       |
|    time_elapsed         | 8347      |
|    total_timesteps      | 415744    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.03e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 2.97      |
|    n_updates            | 2020      |
|    policy_gradient_loss | 1.1e-09   |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 204       |
|    time_elapsed         | 8391      |
|    total_timesteps      | 417792    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.02e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 8.25      |
|    n_updates            | 2030      |
|    policy_gradient_loss | -4.54e-10 |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 205       |
|    time_elapsed         | 8433      |
|    total_timesteps      | 419840    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.02e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.277     |
|    n_updates            | 2040      |
|    policy_gradient_loss | -2.72e-09 |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 420000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 206       |
|    time_elapsed         | 8472      |
|    total_timesteps      | 421888    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.06e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 5.6       |
|    n_updates            | 2050      |
|    policy_gradient_loss | -7.84e-10 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 207       |
|    time_elapsed         | 8515      |
|    total_timesteps      | 423936    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.04e-05 |
|    explained_variance   | 2.38e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 16.5      |
|    n_updates            | 2060      |
|    policy_gradient_loss | -3.02e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 208       |
|    time_elapsed         | 8556      |
|    total_timesteps      | 425984    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.04e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 2.47      |
|    n_updates            | 2070      |
|    policy_gradient_loss | -1.38e-08 |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 209       |
|    time_elapsed         | 8598      |
|    total_timesteps      | 428032    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.02e-05 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 1.04      |
|    n_updates            | 2080      |
|    policy_gradient_loss | -6.94e-08 |
|    value_loss           | 17.5      |
---------------------------------------
Num timesteps: 430000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 210       |
|    time_elapsed         | 8641      |
|    total_timesteps      | 430080    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.03e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 21.8      |
|    n_updates            | 2090      |
|    policy_gradient_loss | 1.47e-09  |
|    value_loss           | 24.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 211       |
|    time_elapsed         | 8681      |
|    total_timesteps      | 432128    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.02e-05 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 6.99      |
|    n_updates            | 2100      |
|    policy_gradient_loss | -5.94e-09 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 212       |
|    time_elapsed         | 8724      |
|    total_timesteps      | 434176    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.04e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 2.77      |
|    n_updates            | 2110      |
|    policy_gradient_loss | 6.6e-09   |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 213       |
|    time_elapsed         | 8769      |
|    total_timesteps      | 436224    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.01e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 11.5      |
|    n_updates            | 2120      |
|    policy_gradient_loss | -4.27e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 214       |
|    time_elapsed         | 8812      |
|    total_timesteps      | 438272    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.02e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.751     |
|    n_updates            | 2130      |
|    policy_gradient_loss | -6.23e-09 |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 440000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 215       |
|    time_elapsed         | 8853      |
|    total_timesteps      | 440320    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.06e-05 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 11.6      |
|    n_updates            | 2140      |
|    policy_gradient_loss | -3.52e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 216       |
|    time_elapsed         | 8899      |
|    total_timesteps      | 442368    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.02e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 16        |
|    n_updates            | 2150      |
|    policy_gradient_loss | -3.19e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 217       |
|    time_elapsed         | 8943      |
|    total_timesteps      | 444416    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.01e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.87      |
|    n_updates            | 2160      |
|    policy_gradient_loss | -1.31e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 218       |
|    time_elapsed         | 8985      |
|    total_timesteps      | 446464    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.01e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.762     |
|    n_updates            | 2170      |
|    policy_gradient_loss | -3.73e-09 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 219       |
|    time_elapsed         | 9025      |
|    total_timesteps      | 448512    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.05e-05 |
|    explained_variance   | 2.38e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 2.83      |
|    n_updates            | 2180      |
|    policy_gradient_loss | -4.86e-09 |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 450000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 220       |
|    time_elapsed         | 9067      |
|    total_timesteps      | 450560    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.03e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 9.4       |
|    n_updates            | 2190      |
|    policy_gradient_loss | 2.35e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 221       |
|    time_elapsed         | 9111      |
|    total_timesteps      | 452608    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.01e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.438     |
|    n_updates            | 2200      |
|    policy_gradient_loss | 5.51e-09  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 222       |
|    time_elapsed         | 9154      |
|    total_timesteps      | 454656    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.01e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 3.1       |
|    n_updates            | 2210      |
|    policy_gradient_loss | -1.89e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 223       |
|    time_elapsed         | 9196      |
|    total_timesteps      | 456704    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.02e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.371     |
|    n_updates            | 2220      |
|    policy_gradient_loss | -1.99e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 224       |
|    time_elapsed         | 9239      |
|    total_timesteps      | 458752    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.02e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 11.3      |
|    n_updates            | 2230      |
|    policy_gradient_loss | 6.24e-09  |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 460000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 225       |
|    time_elapsed         | 9279      |
|    total_timesteps      | 460800    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.05e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 13        |
|    n_updates            | 2240      |
|    policy_gradient_loss | 5.97e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 226       |
|    time_elapsed         | 9322      |
|    total_timesteps      | 462848    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.03e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.293     |
|    n_updates            | 2250      |
|    policy_gradient_loss | 3.55e-09  |
|    value_loss           | 17.4      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 227      |
|    time_elapsed         | 9363     |
|    total_timesteps      | 464896   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -9e-05   |
|    explained_variance   | 2.38e-07 |
|    learning_rate        | 0.0003   |
|    loss                 | 6.3      |
|    n_updates            | 2260     |
|    policy_gradient_loss | 5.83e-09 |
|    value_loss           | 17.4     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 228       |
|    time_elapsed         | 9405      |
|    total_timesteps      | 466944    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.01e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 3.42      |
|    n_updates            | 2270      |
|    policy_gradient_loss | 3.88e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 229       |
|    time_elapsed         | 9446      |
|    total_timesteps      | 468992    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.02e-05 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 3.4       |
|    n_updates            | 2280      |
|    policy_gradient_loss | -8.37e-10 |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 470000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 230       |
|    time_elapsed         | 9488      |
|    total_timesteps      | 471040    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.03e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.287     |
|    n_updates            | 2290      |
|    policy_gradient_loss | -5.65e-09 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 231       |
|    time_elapsed         | 9531      |
|    total_timesteps      | 473088    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.03e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 10.9      |
|    n_updates            | 2300      |
|    policy_gradient_loss | -1.36e-09 |
|    value_loss           | 22.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 232       |
|    time_elapsed         | 9574      |
|    total_timesteps      | 475136    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.03e-05 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 1.13      |
|    n_updates            | 2310      |
|    policy_gradient_loss | -6.59e-09 |
|    value_loss           | 17.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 233       |
|    time_elapsed         | 9617      |
|    total_timesteps      | 477184    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.01e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 8.37      |
|    n_updates            | 2320      |
|    policy_gradient_loss | -1.3e-08  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 234       |
|    time_elapsed         | 9656      |
|    total_timesteps      | 479232    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.03e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 3.97      |
|    n_updates            | 2330      |
|    policy_gradient_loss | 8.08e-09  |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 480000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 235       |
|    time_elapsed         | 9697      |
|    total_timesteps      | 481280    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.07e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 10.4      |
|    n_updates            | 2340      |
|    policy_gradient_loss | -3.41e-09 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 236       |
|    time_elapsed         | 9738      |
|    total_timesteps      | 483328    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.04e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 6.14      |
|    n_updates            | 2350      |
|    policy_gradient_loss | 3.63e-09  |
|    value_loss           | 17.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 237       |
|    time_elapsed         | 9781      |
|    total_timesteps      | 485376    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.03e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 8.19      |
|    n_updates            | 2360      |
|    policy_gradient_loss | 2.95e-09  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 238       |
|    time_elapsed         | 9823      |
|    total_timesteps      | 487424    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9e-05    |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 27.6      |
|    n_updates            | 2370      |
|    policy_gradient_loss | -2.49e-09 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 239       |
|    time_elapsed         | 9864      |
|    total_timesteps      | 489472    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.03e-05 |
|    explained_variance   | 2.38e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 5.78      |
|    n_updates            | 2380      |
|    policy_gradient_loss | 7.11e-10  |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 490000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 240       |
|    time_elapsed         | 9904      |
|    total_timesteps      | 491520    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.05e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 8.87      |
|    n_updates            | 2390      |
|    policy_gradient_loss | -9.4e-09  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 241       |
|    time_elapsed         | 9948      |
|    total_timesteps      | 493568    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.01e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 9.95      |
|    n_updates            | 2400      |
|    policy_gradient_loss | 9.16e-09  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 242       |
|    time_elapsed         | 9989      |
|    total_timesteps      | 495616    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.02e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 25.2      |
|    n_updates            | 2410      |
|    policy_gradient_loss | 7.74e-09  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 243       |
|    time_elapsed         | 10030     |
|    total_timesteps      | 497664    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.04e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 6.23      |
|    n_updates            | 2420      |
|    policy_gradient_loss | 1.91e-09  |
|    value_loss           | 17.3      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 49            |
|    iterations           | 244           |
|    time_elapsed         | 10071         |
|    total_timesteps      | 499712        |
| train/                  |               |
|    approx_kl            | 5.6177058e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.69e-05     |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 12.5          |
|    n_updates            | 2430          |
|    policy_gradient_loss | -1.95e-05     |
|    value_loss           | 17.3          |
-------------------------------------------
Num timesteps: 500000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 245       |
|    time_elapsed         | 10111     |
|    total_timesteps      | 501760    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.41e-05 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 12.4      |
|    n_updates            | 2440      |
|    policy_gradient_loss | -1.81e-08 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 246       |
|    time_elapsed         | 10152     |
|    total_timesteps      | 503808    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.37e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 22.3      |
|    n_updates            | 2450      |
|    policy_gradient_loss | 2.27e-10  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 247       |
|    time_elapsed         | 10196     |
|    total_timesteps      | 505856    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.34e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 7.87      |
|    n_updates            | 2460      |
|    policy_gradient_loss | -1.2e-08  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 248       |
|    time_elapsed         | 10241     |
|    total_timesteps      | 507904    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.34e-05 |
|    explained_variance   | 2.38e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.289     |
|    n_updates            | 2470      |
|    policy_gradient_loss | 1.49e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 249       |
|    time_elapsed         | 10284     |
|    total_timesteps      | 509952    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.38e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 5.09      |
|    n_updates            | 2480      |
|    policy_gradient_loss | -2.48e-08 |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 510000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 250       |
|    time_elapsed         | 10325     |
|    total_timesteps      | 512000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.4e-05  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 14.2      |
|    n_updates            | 2490      |
|    policy_gradient_loss | -1.18e-08 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 251       |
|    time_elapsed         | 10368     |
|    total_timesteps      | 514048    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.36e-05 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0003    |
|    loss                 | 19        |
|    n_updates            | 2500      |
|    policy_gradient_loss | -7.1e-10  |
|    value_loss           | 25.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 252       |
|    time_elapsed         | 10407     |
|    total_timesteps      | 516096    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.44e-05 |
|    explained_variance   | 2.38e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 3.73      |
|    n_updates            | 2510      |
|    policy_gradient_loss | 5.11e-09  |
|    value_loss           | 17.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 253       |
|    time_elapsed         | 10448     |
|    total_timesteps      | 518144    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.37e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 3.9       |
|    n_updates            | 2520      |
|    policy_gradient_loss | 8.45e-09  |
|    value_loss           | 17.2      |
---------------------------------------
Num timesteps: 520000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 254       |
|    time_elapsed         | 10490     |
|    total_timesteps      | 520192    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.38e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 6.88      |
|    n_updates            | 2530      |
|    policy_gradient_loss | -2.09e-09 |
|    value_loss           | 17.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 255       |
|    time_elapsed         | 10531     |
|    total_timesteps      | 522240    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.39e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 14.8      |
|    n_updates            | 2540      |
|    policy_gradient_loss | 4.15e-09  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 256       |
|    time_elapsed         | 10572     |
|    total_timesteps      | 524288    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.35e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 19.2      |
|    n_updates            | 2550      |
|    policy_gradient_loss | -4.44e-09 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 257       |
|    time_elapsed         | 10616     |
|    total_timesteps      | 526336    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.34e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 2.49      |
|    n_updates            | 2560      |
|    policy_gradient_loss | -3e-09    |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 258       |
|    time_elapsed         | 10660     |
|    total_timesteps      | 528384    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.34e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.34      |
|    n_updates            | 2570      |
|    policy_gradient_loss | 5.54e-09  |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 530000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 259       |
|    time_elapsed         | 10701     |
|    total_timesteps      | 530432    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.37e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 3.43      |
|    n_updates            | 2580      |
|    policy_gradient_loss | 1.06e-08  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 260       |
|    time_elapsed         | 10741     |
|    total_timesteps      | 532480    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.42e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.214     |
|    n_updates            | 2590      |
|    policy_gradient_loss | 2.62e-08  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 261       |
|    time_elapsed         | 10783     |
|    total_timesteps      | 534528    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.34e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 8.2       |
|    n_updates            | 2600      |
|    policy_gradient_loss | 8.7e-10   |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 262       |
|    time_elapsed         | 10824     |
|    total_timesteps      | 536576    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.39e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 8.91      |
|    n_updates            | 2610      |
|    policy_gradient_loss | 9.87e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 263       |
|    time_elapsed         | 10867     |
|    total_timesteps      | 538624    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.39e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.94      |
|    n_updates            | 2620      |
|    policy_gradient_loss | 6.19e-09  |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 540000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 264       |
|    time_elapsed         | 10910     |
|    total_timesteps      | 540672    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.35e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.95      |
|    n_updates            | 2630      |
|    policy_gradient_loss | -1.33e-08 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 265       |
|    time_elapsed         | 10950     |
|    total_timesteps      | 542720    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.34e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 4.32      |
|    n_updates            | 2640      |
|    policy_gradient_loss | -2.04e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 266       |
|    time_elapsed         | 10990     |
|    total_timesteps      | 544768    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.34e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.77      |
|    n_updates            | 2650      |
|    policy_gradient_loss | -1.84e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 267       |
|    time_elapsed         | 11033     |
|    total_timesteps      | 546816    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.33e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 7.1       |
|    n_updates            | 2660      |
|    policy_gradient_loss | -3.2e-08  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 268       |
|    time_elapsed         | 11073     |
|    total_timesteps      | 548864    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.38e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 6.52      |
|    n_updates            | 2670      |
|    policy_gradient_loss | 4.48e-09  |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 550000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 269       |
|    time_elapsed         | 11115     |
|    total_timesteps      | 550912    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.33e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.5       |
|    n_updates            | 2680      |
|    policy_gradient_loss | -2.45e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 270       |
|    time_elapsed         | 11157     |
|    total_timesteps      | 552960    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.35e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 2.85      |
|    n_updates            | 2690      |
|    policy_gradient_loss | 4.64e-10  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 271       |
|    time_elapsed         | 11198     |
|    total_timesteps      | 555008    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.38e-05 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 6.25      |
|    n_updates            | 2700      |
|    policy_gradient_loss | -9.2e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 272       |
|    time_elapsed         | 11242     |
|    total_timesteps      | 557056    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.35e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.6      |
|    n_updates            | 2710      |
|    policy_gradient_loss | -9e-09    |
|    value_loss           | 25.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 273       |
|    time_elapsed         | 11284     |
|    total_timesteps      | 559104    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.36e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 18.5      |
|    n_updates            | 2720      |
|    policy_gradient_loss | -5.36e-10 |
|    value_loss           | 17.2      |
---------------------------------------
Num timesteps: 560000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 274       |
|    time_elapsed         | 11326     |
|    total_timesteps      | 561152    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.34e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 27.1      |
|    n_updates            | 2730      |
|    policy_gradient_loss | -4.23e-09 |
|    value_loss           | 17.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 275       |
|    time_elapsed         | 11366     |
|    total_timesteps      | 563200    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.38e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.888     |
|    n_updates            | 2740      |
|    policy_gradient_loss | 8.67e-09  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 276       |
|    time_elapsed         | 11406     |
|    total_timesteps      | 565248    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.36e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 10        |
|    n_updates            | 2750      |
|    policy_gradient_loss | -5.01e-10 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 277       |
|    time_elapsed         | 11446     |
|    total_timesteps      | 567296    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.33e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 17.6      |
|    n_updates            | 2760      |
|    policy_gradient_loss | -1.57e-09 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 278       |
|    time_elapsed         | 11488     |
|    total_timesteps      | 569344    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.37e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 3.46      |
|    n_updates            | 2770      |
|    policy_gradient_loss | 1.69e-08  |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 570000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 279       |
|    time_elapsed         | 11527     |
|    total_timesteps      | 571392    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.44e-05 |
|    explained_variance   | 3.58e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 16.3      |
|    n_updates            | 2780      |
|    policy_gradient_loss | -1.33e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 280       |
|    time_elapsed         | 11567     |
|    total_timesteps      | 573440    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.4e-05  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 1.21      |
|    n_updates            | 2790      |
|    policy_gradient_loss | -7.3e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 281       |
|    time_elapsed         | 11607     |
|    total_timesteps      | 575488    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.38e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 6.75      |
|    n_updates            | 2800      |
|    policy_gradient_loss | 7.74e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 282       |
|    time_elapsed         | 11647     |
|    total_timesteps      | 577536    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.35e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 13        |
|    n_updates            | 2810      |
|    policy_gradient_loss | -1.05e-08 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 283       |
|    time_elapsed         | 11688     |
|    total_timesteps      | 579584    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.39e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 16.5      |
|    n_updates            | 2820      |
|    policy_gradient_loss | -1.06e-08 |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 580000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 284       |
|    time_elapsed         | 11730     |
|    total_timesteps      | 581632    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.4e-05  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 19.6      |
|    n_updates            | 2830      |
|    policy_gradient_loss | -6.65e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 285       |
|    time_elapsed         | 11771     |
|    total_timesteps      | 583680    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.38e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.218     |
|    n_updates            | 2840      |
|    policy_gradient_loss | -1.77e-08 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 286       |
|    time_elapsed         | 11815     |
|    total_timesteps      | 585728    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.34e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 2.2       |
|    n_updates            | 2850      |
|    policy_gradient_loss | -2.2e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 287       |
|    time_elapsed         | 11858     |
|    total_timesteps      | 587776    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.36e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.392     |
|    n_updates            | 2860      |
|    policy_gradient_loss | 6.98e-09  |
|    value_loss           | 17.4      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 288      |
|    time_elapsed         | 11899    |
|    total_timesteps      | 589824   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -6.4e-05 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 6.94     |
|    n_updates            | 2870     |
|    policy_gradient_loss | 2.8e-08  |
|    value_loss           | 17.4     |
--------------------------------------
Num timesteps: 590000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 289       |
|    time_elapsed         | 11942     |
|    total_timesteps      | 591872    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.41e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.11      |
|    n_updates            | 2880      |
|    policy_gradient_loss | 8.79e-10  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 290       |
|    time_elapsed         | 11984     |
|    total_timesteps      | 593920    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.37e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 13.2      |
|    n_updates            | 2890      |
|    policy_gradient_loss | -1.03e-08 |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 291       |
|    time_elapsed         | 12027     |
|    total_timesteps      | 595968    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.35e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 12.5      |
|    n_updates            | 2900      |
|    policy_gradient_loss | 1.67e-08  |
|    value_loss           | 17.5      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 49            |
|    iterations           | 292           |
|    time_elapsed         | 12068         |
|    total_timesteps      | 598016        |
| train/                  |               |
|    approx_kl            | 0.00020962622 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.32e-05     |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 41            |
|    n_updates            | 2910          |
|    policy_gradient_loss | -1.78e-05     |
|    value_loss           | 17.5          |
-------------------------------------------
Num timesteps: 600000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 293       |
|    time_elapsed         | 12108     |
|    total_timesteps      | 600064    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 11.6      |
|    n_updates            | 2920      |
|    policy_gradient_loss | 2.64e-08  |
|    value_loss           | 25.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 294       |
|    time_elapsed         | 12147     |
|    total_timesteps      | 602112    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.19e-05 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 3.06      |
|    n_updates            | 2930      |
|    policy_gradient_loss | 5.49e-09  |
|    value_loss           | 17.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 295       |
|    time_elapsed         | 12187     |
|    total_timesteps      | 604160    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 6.13      |
|    n_updates            | 2940      |
|    policy_gradient_loss | -1.95e-08 |
|    value_loss           | 17.3      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 296      |
|    time_elapsed         | 12228    |
|    total_timesteps      | 606208   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.2e-05 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 10.1     |
|    n_updates            | 2950     |
|    policy_gradient_loss | -1.4e-09 |
|    value_loss           | 17.4     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 297       |
|    time_elapsed         | 12271     |
|    total_timesteps      | 608256    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.19e-05 |
|    explained_variance   | 3.58e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 15.5      |
|    n_updates            | 2960      |
|    policy_gradient_loss | -6.5e-09  |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 610000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 298       |
|    time_elapsed         | 12314     |
|    total_timesteps      | 610304    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.19e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 25.8      |
|    n_updates            | 2970      |
|    policy_gradient_loss | -1.22e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 299       |
|    time_elapsed         | 12355     |
|    total_timesteps      | 612352    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 2.87      |
|    n_updates            | 2980      |
|    policy_gradient_loss | -6.54e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 300       |
|    time_elapsed         | 12395     |
|    total_timesteps      | 614400    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 11        |
|    n_updates            | 2990      |
|    policy_gradient_loss | 6.09e-09  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 301       |
|    time_elapsed         | 12435     |
|    total_timesteps      | 616448    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 33.4      |
|    n_updates            | 3000      |
|    policy_gradient_loss | -1.03e-08 |
|    value_loss           | 17.4      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 302      |
|    time_elapsed         | 12475    |
|    total_timesteps      | 618496   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.2e-05 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 14.5     |
|    n_updates            | 3010     |
|    policy_gradient_loss | 4.39e-09 |
|    value_loss           | 17.4     |
--------------------------------------
Num timesteps: 620000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 303       |
|    time_elapsed         | 12516     |
|    total_timesteps      | 620544    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.19e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 2.82      |
|    n_updates            | 3020      |
|    policy_gradient_loss | 8.16e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 304       |
|    time_elapsed         | 12556     |
|    total_timesteps      | 622592    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.19e-05 |
|    explained_variance   | 2.38e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 10.2      |
|    n_updates            | 3030      |
|    policy_gradient_loss | -2.9e-09  |
|    value_loss           | 17.4      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 305      |
|    time_elapsed         | 12598    |
|    total_timesteps      | 624640   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.2e-05 |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.0003   |
|    loss                 | 3.38     |
|    n_updates            | 3040     |
|    policy_gradient_loss | 1.97e-09 |
|    value_loss           | 17.4     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 306       |
|    time_elapsed         | 12640     |
|    total_timesteps      | 626688    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.19e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 3.67      |
|    n_updates            | 3050      |
|    policy_gradient_loss | 1.03e-08  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 307       |
|    time_elapsed         | 12680     |
|    total_timesteps      | 628736    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 2.54      |
|    n_updates            | 3060      |
|    policy_gradient_loss | -4.49e-09 |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 630000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 308      |
|    time_elapsed         | 12721    |
|    total_timesteps      | 630784   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.2e-05 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 3.73     |
|    n_updates            | 3070     |
|    policy_gradient_loss | 4.87e-09 |
|    value_loss           | 17.4     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 309       |
|    time_elapsed         | 12763     |
|    total_timesteps      | 632832    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.19e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.889     |
|    n_updates            | 3080      |
|    policy_gradient_loss | -2.82e-10 |
|    value_loss           | 17.4      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 310      |
|    time_elapsed         | 12805    |
|    total_timesteps      | 634880   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.2e-05 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 0.334    |
|    n_updates            | 3090     |
|    policy_gradient_loss | 1.55e-09 |
|    value_loss           | 17.4     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 311       |
|    time_elapsed         | 12847     |
|    total_timesteps      | 636928    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.945     |
|    n_updates            | 3100      |
|    policy_gradient_loss | 6.83e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 312       |
|    time_elapsed         | 12887     |
|    total_timesteps      | 638976    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.19e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 34.1      |
|    n_updates            | 3110      |
|    policy_gradient_loss | -2.62e-09 |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 640000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 313       |
|    time_elapsed         | 12927     |
|    total_timesteps      | 641024    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 1.94      |
|    n_updates            | 3120      |
|    policy_gradient_loss | -2.17e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 314       |
|    time_elapsed         | 12967     |
|    total_timesteps      | 643072    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 23.2      |
|    n_updates            | 3130      |
|    policy_gradient_loss | -2.69e-09 |
|    value_loss           | 25.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 315       |
|    time_elapsed         | 13007     |
|    total_timesteps      | 645120    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.19e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.214     |
|    n_updates            | 3140      |
|    policy_gradient_loss | 5.03e-09  |
|    value_loss           | 17.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 316       |
|    time_elapsed         | 13051     |
|    total_timesteps      | 647168    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.19e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 20        |
|    n_updates            | 3150      |
|    policy_gradient_loss | -6.02e-10 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 317       |
|    time_elapsed         | 13095     |
|    total_timesteps      | 649216    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.19e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 8.01      |
|    n_updates            | 3160      |
|    policy_gradient_loss | 3.82e-09  |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 650000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 318       |
|    time_elapsed         | 13135     |
|    total_timesteps      | 651264    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 15.8      |
|    n_updates            | 3170      |
|    policy_gradient_loss | 4.01e-09  |
|    value_loss           | 17.3      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 319      |
|    time_elapsed         | 13178    |
|    total_timesteps      | 653312   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.2e-05 |
|    explained_variance   | 2.38e-07 |
|    learning_rate        | 0.0003   |
|    loss                 | 14.4     |
|    n_updates            | 3180     |
|    policy_gradient_loss | 3.83e-09 |
|    value_loss           | 17.3     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 320       |
|    time_elapsed         | 13221     |
|    total_timesteps      | 655360    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.304     |
|    n_updates            | 3190      |
|    policy_gradient_loss | -3.68e-08 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 321       |
|    time_elapsed         | 13262     |
|    total_timesteps      | 657408    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.19e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 42.4      |
|    n_updates            | 3200      |
|    policy_gradient_loss | 1.06e-08  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 322       |
|    time_elapsed         | 13304     |
|    total_timesteps      | 659456    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 26.1      |
|    n_updates            | 3210      |
|    policy_gradient_loss | -1.37e-09 |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 660000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 323       |
|    time_elapsed         | 13345     |
|    total_timesteps      | 661504    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 2.78      |
|    n_updates            | 3220      |
|    policy_gradient_loss | -1.14e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 324       |
|    time_elapsed         | 13384     |
|    total_timesteps      | 663552    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 3.7       |
|    n_updates            | 3230      |
|    policy_gradient_loss | -5.26e-10 |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 325       |
|    time_elapsed         | 13425     |
|    total_timesteps      | 665600    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 4.68      |
|    n_updates            | 3240      |
|    policy_gradient_loss | 8.03e-09  |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 326       |
|    time_elapsed         | 13468     |
|    total_timesteps      | 667648    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.19e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.27      |
|    n_updates            | 3250      |
|    policy_gradient_loss | 2.43e-10  |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 327       |
|    time_elapsed         | 13510     |
|    total_timesteps      | 669696    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 6.16      |
|    n_updates            | 3260      |
|    policy_gradient_loss | -8.49e-09 |
|    value_loss           | 17.5      |
---------------------------------------
Num timesteps: 670000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 328      |
|    time_elapsed         | 13550    |
|    total_timesteps      | 671744   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.2e-05 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 13.6     |
|    n_updates            | 3270     |
|    policy_gradient_loss | 4.17e-09 |
|    value_loss           | 17.5     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 329       |
|    time_elapsed         | 13589     |
|    total_timesteps      | 673792    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.526     |
|    n_updates            | 3280      |
|    policy_gradient_loss | -2.44e-09 |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 330       |
|    time_elapsed         | 13630     |
|    total_timesteps      | 675840    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.289     |
|    n_updates            | 3290      |
|    policy_gradient_loss | 3.47e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 331       |
|    time_elapsed         | 13673     |
|    total_timesteps      | 677888    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 7.82      |
|    n_updates            | 3300      |
|    policy_gradient_loss | 2.12e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 332       |
|    time_elapsed         | 13712     |
|    total_timesteps      | 679936    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 2.38e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 5.55      |
|    n_updates            | 3310      |
|    policy_gradient_loss | -1.05e-09 |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 680000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 333       |
|    time_elapsed         | 13754     |
|    total_timesteps      | 681984    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.98      |
|    n_updates            | 3320      |
|    policy_gradient_loss | -7.58e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 334       |
|    time_elapsed         | 13798     |
|    total_timesteps      | 684032    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 3.32      |
|    n_updates            | 3330      |
|    policy_gradient_loss | -9.74e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 335       |
|    time_elapsed         | 13838     |
|    total_timesteps      | 686080    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 35.4      |
|    n_updates            | 3340      |
|    policy_gradient_loss | -1.08e-08 |
|    value_loss           | 24.6      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 336      |
|    time_elapsed         | 13878    |
|    total_timesteps      | 688128   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.2e-05 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 8.02     |
|    n_updates            | 3350     |
|    policy_gradient_loss | 3.82e-09 |
|    value_loss           | 17.2     |
--------------------------------------
Num timesteps: 690000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 337       |
|    time_elapsed         | 13919     |
|    total_timesteps      | 690176    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 29.2      |
|    n_updates            | 3360      |
|    policy_gradient_loss | 4.9e-09   |
|    value_loss           | 17.2      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 338      |
|    time_elapsed         | 13961    |
|    total_timesteps      | 692224   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.2e-05 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 1.06     |
|    n_updates            | 3370     |
|    policy_gradient_loss | -1.8e-09 |
|    value_loss           | 17.3     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 339       |
|    time_elapsed         | 14002     |
|    total_timesteps      | 694272    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 16.9      |
|    n_updates            | 3380      |
|    policy_gradient_loss | -5.25e-09 |
|    value_loss           | 17.3      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 340      |
|    time_elapsed         | 14044    |
|    total_timesteps      | 696320   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.2e-05 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 12.7     |
|    n_updates            | 3390     |
|    policy_gradient_loss | 8.74e-09 |
|    value_loss           | 17.3     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 341       |
|    time_elapsed         | 14088     |
|    total_timesteps      | 698368    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.19e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 7.04      |
|    n_updates            | 3400      |
|    policy_gradient_loss | 2.7e-09   |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 700000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 342      |
|    time_elapsed         | 14134    |
|    total_timesteps      | 700416   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.2e-05 |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.0003   |
|    loss                 | 9.39     |
|    n_updates            | 3410     |
|    policy_gradient_loss | 1.97e-09 |
|    value_loss           | 17.3     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 343       |
|    time_elapsed         | 14176     |
|    total_timesteps      | 702464    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 11.3      |
|    n_updates            | 3420      |
|    policy_gradient_loss | 1.7e-08   |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 344       |
|    time_elapsed         | 14221     |
|    total_timesteps      | 704512    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.19e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 2.78      |
|    n_updates            | 3430      |
|    policy_gradient_loss | -1.05e-09 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 345       |
|    time_elapsed         | 14265     |
|    total_timesteps      | 706560    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 17.9      |
|    n_updates            | 3440      |
|    policy_gradient_loss | -1.87e-09 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 346       |
|    time_elapsed         | 14305     |
|    total_timesteps      | 708608    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 16.6      |
|    n_updates            | 3450      |
|    policy_gradient_loss | -9.62e-09 |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 710000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 347       |
|    time_elapsed         | 14345     |
|    total_timesteps      | 710656    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 2.5       |
|    n_updates            | 3460      |
|    policy_gradient_loss | 4.58e-09  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 348       |
|    time_elapsed         | 14387     |
|    total_timesteps      | 712704    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 4.94      |
|    n_updates            | 3470      |
|    policy_gradient_loss | -3.37e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 349       |
|    time_elapsed         | 14427     |
|    total_timesteps      | 714752    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.407     |
|    n_updates            | 3480      |
|    policy_gradient_loss | -3.7e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 350       |
|    time_elapsed         | 14468     |
|    total_timesteps      | 716800    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.375     |
|    n_updates            | 3490      |
|    policy_gradient_loss | -1.19e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 351       |
|    time_elapsed         | 14509     |
|    total_timesteps      | 718848    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 2.38e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.423     |
|    n_updates            | 3500      |
|    policy_gradient_loss | -1.32e-08 |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 720000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 352      |
|    time_elapsed         | 14553    |
|    total_timesteps      | 720896   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.2e-05 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 1.7      |
|    n_updates            | 3510     |
|    policy_gradient_loss | 2.96e-09 |
|    value_loss           | 17.4     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 353       |
|    time_elapsed         | 14592     |
|    total_timesteps      | 722944    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 2.99      |
|    n_updates            | 3520      |
|    policy_gradient_loss | -8.24e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 354       |
|    time_elapsed         | 14635     |
|    total_timesteps      | 724992    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.72      |
|    n_updates            | 3530      |
|    policy_gradient_loss | -1.28e-08 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 355       |
|    time_elapsed         | 14678     |
|    total_timesteps      | 727040    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.19e-05 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.567     |
|    n_updates            | 3540      |
|    policy_gradient_loss | -3.25e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 356       |
|    time_elapsed         | 14718     |
|    total_timesteps      | 729088    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 16.9      |
|    n_updates            | 3550      |
|    policy_gradient_loss | -2.62e-09 |
|    value_loss           | 22.8      |
---------------------------------------
Num timesteps: 730000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 357       |
|    time_elapsed         | 14758     |
|    total_timesteps      | 731136    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 7         |
|    n_updates            | 3560      |
|    policy_gradient_loss | -7.32e-09 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 358       |
|    time_elapsed         | 14800     |
|    total_timesteps      | 733184    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 3.79      |
|    n_updates            | 3570      |
|    policy_gradient_loss | 2.89e-09  |
|    value_loss           | 17.3      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 359      |
|    time_elapsed         | 14841    |
|    total_timesteps      | 735232   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.2e-05 |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.0003   |
|    loss                 | 3.36     |
|    n_updates            | 3580     |
|    policy_gradient_loss | 1.99e-09 |
|    value_loss           | 17.4     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 360       |
|    time_elapsed         | 14882     |
|    total_timesteps      | 737280    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 20.2      |
|    n_updates            | 3590      |
|    policy_gradient_loss | -2.78e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 361       |
|    time_elapsed         | 14921     |
|    total_timesteps      | 739328    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.08      |
|    n_updates            | 3600      |
|    policy_gradient_loss | 6.96e-09  |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 740000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 362       |
|    time_elapsed         | 14961     |
|    total_timesteps      | 741376    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.62      |
|    n_updates            | 3610      |
|    policy_gradient_loss | -8.38e-10 |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 363       |
|    time_elapsed         | 15002     |
|    total_timesteps      | 743424    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 4.37      |
|    n_updates            | 3620      |
|    policy_gradient_loss | 7.85e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 364       |
|    time_elapsed         | 15044     |
|    total_timesteps      | 745472    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 8.86      |
|    n_updates            | 3630      |
|    policy_gradient_loss | -7.79e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 365       |
|    time_elapsed         | 15088     |
|    total_timesteps      | 747520    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.592     |
|    n_updates            | 3640      |
|    policy_gradient_loss | 1.21e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 366       |
|    time_elapsed         | 15130     |
|    total_timesteps      | 749568    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.7      |
|    n_updates            | 3650      |
|    policy_gradient_loss | -4.6e-09  |
|    value_loss           | 17.5      |
---------------------------------------
Num timesteps: 750000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 367       |
|    time_elapsed         | 15173     |
|    total_timesteps      | 751616    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 10.2      |
|    n_updates            | 3660      |
|    policy_gradient_loss | -4.66e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 368       |
|    time_elapsed         | 15214     |
|    total_timesteps      | 753664    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 3.4       |
|    n_updates            | 3670      |
|    policy_gradient_loss | -8.66e-10 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 369       |
|    time_elapsed         | 15254     |
|    total_timesteps      | 755712    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.52      |
|    n_updates            | 3680      |
|    policy_gradient_loss | 1.08e-08  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 370       |
|    time_elapsed         | 15296     |
|    total_timesteps      | 757760    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 6.38      |
|    n_updates            | 3690      |
|    policy_gradient_loss | -6.65e-09 |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 371       |
|    time_elapsed         | 15335     |
|    total_timesteps      | 759808    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 3.58e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.337     |
|    n_updates            | 3700      |
|    policy_gradient_loss | 5.08e-09  |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 760000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 372      |
|    time_elapsed         | 15376    |
|    total_timesteps      | 761856   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.2e-05 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 6.67     |
|    n_updates            | 3710     |
|    policy_gradient_loss | 1.34e-09 |
|    value_loss           | 17.4     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 373      |
|    time_elapsed         | 15417    |
|    total_timesteps      | 763904   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.2e-05 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 14.5     |
|    n_updates            | 3720     |
|    policy_gradient_loss | 8.65e-09 |
|    value_loss           | 17.3     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 374       |
|    time_elapsed         | 15458     |
|    total_timesteps      | 765952    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 9.76      |
|    n_updates            | 3730      |
|    policy_gradient_loss | 1.54e-08  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 375       |
|    time_elapsed         | 15502     |
|    total_timesteps      | 768000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 12.8      |
|    n_updates            | 3740      |
|    policy_gradient_loss | -5.36e-09 |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 770000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 376       |
|    time_elapsed         | 15544     |
|    total_timesteps      | 770048    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 17.3      |
|    n_updates            | 3750      |
|    policy_gradient_loss | -3.63e-09 |
|    value_loss           | 25.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 377       |
|    time_elapsed         | 15586     |
|    total_timesteps      | 772096    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.745     |
|    n_updates            | 3760      |
|    policy_gradient_loss | 1.22e-09  |
|    value_loss           | 17.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 378       |
|    time_elapsed         | 15628     |
|    total_timesteps      | 774144    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 2.97      |
|    n_updates            | 3770      |
|    policy_gradient_loss | 9.19e-09  |
|    value_loss           | 17.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 379       |
|    time_elapsed         | 15667     |
|    total_timesteps      | 776192    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.9      |
|    n_updates            | 3780      |
|    policy_gradient_loss | 2.38e-09  |
|    value_loss           | 17.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 380       |
|    time_elapsed         | 15707     |
|    total_timesteps      | 778240    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.889     |
|    n_updates            | 3790      |
|    policy_gradient_loss | -8.05e-09 |
|    value_loss           | 17.2      |
---------------------------------------
Num timesteps: 780000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 381       |
|    time_elapsed         | 15748     |
|    total_timesteps      | 780288    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.222     |
|    n_updates            | 3800      |
|    policy_gradient_loss | 2.13e-09  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 382       |
|    time_elapsed         | 15791     |
|    total_timesteps      | 782336    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 1.51      |
|    n_updates            | 3810      |
|    policy_gradient_loss | 1.64e-09  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 383       |
|    time_elapsed         | 15835     |
|    total_timesteps      | 784384    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | 2.38e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.926     |
|    n_updates            | 3820      |
|    policy_gradient_loss | -3.18e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 384       |
|    time_elapsed         | 15876     |
|    total_timesteps      | 786432    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 2.65      |
|    n_updates            | 3830      |
|    policy_gradient_loss | 4.69e-09  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 385       |
|    time_elapsed         | 15919     |
|    total_timesteps      | 788480    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.967     |
|    n_updates            | 3840      |
|    policy_gradient_loss | -7.36e-09 |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 790000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 386       |
|    time_elapsed         | 15960     |
|    total_timesteps      | 790528    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18        |
|    n_updates            | 3850      |
|    policy_gradient_loss | -2.19e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 387       |
|    time_elapsed         | 15998     |
|    total_timesteps      | 792576    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.791     |
|    n_updates            | 3860      |
|    policy_gradient_loss | 8.82e-10  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 388       |
|    time_elapsed         | 16039     |
|    total_timesteps      | 794624    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 14.6      |
|    n_updates            | 3870      |
|    policy_gradient_loss | -4.72e-09 |
|    value_loss           | 17.4      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 389      |
|    time_elapsed         | 16080    |
|    total_timesteps      | 796672   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.2e-05 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 21.6     |
|    n_updates            | 3880     |
|    policy_gradient_loss | -1.4e-09 |
|    value_loss           | 17.3     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1e+03    |
|    ep_rew_mean          | -500     |
| time/                   |          |
|    fps                  | 49       |
|    iterations           | 390      |
|    time_elapsed         | 16121    |
|    total_timesteps      | 798720   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.2e-05 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 1.95     |
|    n_updates            | 3890     |
|    policy_gradient_loss | 4.14e-09 |
|    value_loss           | 17.3     |
--------------------------------------
Num timesteps: 800000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 391       |
|    time_elapsed         | 16161     |
|    total_timesteps      | 800768    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 1.77      |
|    n_updates            | 3900      |
|    policy_gradient_loss | -1.01e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 392       |
|    time_elapsed         | 16202     |
|    total_timesteps      | 802816    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 29.4      |
|    n_updates            | 3910      |
|    policy_gradient_loss | -7.33e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 393       |
|    time_elapsed         | 16245     |
|    total_timesteps      | 804864    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 5.45      |
|    n_updates            | 3920      |
|    policy_gradient_loss | 4.21e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 394       |
|    time_elapsed         | 16286     |
|    total_timesteps      | 806912    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 9.14      |
|    n_updates            | 3930      |
|    policy_gradient_loss | 8.03e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 395       |
|    time_elapsed         | 16325     |
|    total_timesteps      | 808960    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.526     |
|    n_updates            | 3940      |
|    policy_gradient_loss | -9.13e-09 |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 810000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 396       |
|    time_elapsed         | 16368     |
|    total_timesteps      | 811008    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 2.42      |
|    n_updates            | 3950      |
|    policy_gradient_loss | 2.76e-09  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 397       |
|    time_elapsed         | 16410     |
|    total_timesteps      | 813056    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 1.79e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 15.9      |
|    n_updates            | 3960      |
|    policy_gradient_loss | 5.64e-09  |
|    value_loss           | 25.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 398       |
|    time_elapsed         | 16452     |
|    total_timesteps      | 815104    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 19.8      |
|    n_updates            | 3970      |
|    policy_gradient_loss | 1.1e-09   |
|    value_loss           | 17.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 399       |
|    time_elapsed         | 16494     |
|    total_timesteps      | 817152    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.39      |
|    n_updates            | 3980      |
|    policy_gradient_loss | -2.57e-09 |
|    value_loss           | 17.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 400       |
|    time_elapsed         | 16537     |
|    total_timesteps      | 819200    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 4.8       |
|    n_updates            | 3990      |
|    policy_gradient_loss | 2.57e-09  |
|    value_loss           | 17.2      |
---------------------------------------
Num timesteps: 820000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 401       |
|    time_elapsed         | 16579     |
|    total_timesteps      | 821248    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1         |
|    n_updates            | 4000      |
|    policy_gradient_loss | 1.15e-08  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 402       |
|    time_elapsed         | 16621     |
|    total_timesteps      | 823296    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 16.3      |
|    n_updates            | 4010      |
|    policy_gradient_loss | 3.27e-09  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 403       |
|    time_elapsed         | 16664     |
|    total_timesteps      | 825344    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 19.2      |
|    n_updates            | 4020      |
|    policy_gradient_loss | 9.59e-10  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 404       |
|    time_elapsed         | 16704     |
|    total_timesteps      | 827392    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.804     |
|    n_updates            | 4030      |
|    policy_gradient_loss | -1.35e-08 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 405       |
|    time_elapsed         | 16746     |
|    total_timesteps      | 829440    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 2.13      |
|    n_updates            | 4040      |
|    policy_gradient_loss | 3.33e-10  |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 830000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 406       |
|    time_elapsed         | 16789     |
|    total_timesteps      | 831488    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 3.39      |
|    n_updates            | 4050      |
|    policy_gradient_loss | 1.54e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 407       |
|    time_elapsed         | 16831     |
|    total_timesteps      | 833536    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.247     |
|    n_updates            | 4060      |
|    policy_gradient_loss | 8.53e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 408       |
|    time_elapsed         | 16873     |
|    total_timesteps      | 835584    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 12.5      |
|    n_updates            | 4070      |
|    policy_gradient_loss | 1.17e-08  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 409       |
|    time_elapsed         | 16915     |
|    total_timesteps      | 837632    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 21.9      |
|    n_updates            | 4080      |
|    policy_gradient_loss | 5.53e-10  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 410       |
|    time_elapsed         | 16956     |
|    total_timesteps      | 839680    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 27.2      |
|    n_updates            | 4090      |
|    policy_gradient_loss | -3.03e-10 |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 840000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 411       |
|    time_elapsed         | 16996     |
|    total_timesteps      | 841728    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.23e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 24.9      |
|    n_updates            | 4100      |
|    policy_gradient_loss | -2.38e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 412       |
|    time_elapsed         | 17038     |
|    total_timesteps      | 843776    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 13.3      |
|    n_updates            | 4110      |
|    policy_gradient_loss | 2.18e-10  |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 413       |
|    time_elapsed         | 17080     |
|    total_timesteps      | 845824    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 11.4      |
|    n_updates            | 4120      |
|    policy_gradient_loss | 3.98e-09  |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 414       |
|    time_elapsed         | 17123     |
|    total_timesteps      | 847872    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 8.96      |
|    n_updates            | 4130      |
|    policy_gradient_loss | 9.43e-10  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 415       |
|    time_elapsed         | 17167     |
|    total_timesteps      | 849920    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 33.3      |
|    n_updates            | 4140      |
|    policy_gradient_loss | -1.53e-08 |
|    value_loss           | 17.5      |
---------------------------------------
Num timesteps: 850000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 416       |
|    time_elapsed         | 17211     |
|    total_timesteps      | 851968    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.2e-05  |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 9.56      |
|    n_updates            | 4150      |
|    policy_gradient_loss | -2.44e-09 |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 417       |
|    time_elapsed         | 17254     |
|    total_timesteps      | 854016    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.965     |
|    n_updates            | 4160      |
|    policy_gradient_loss | 5.8e-09   |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 418       |
|    time_elapsed         | 17293     |
|    total_timesteps      | 856064    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 11        |
|    n_updates            | 4170      |
|    policy_gradient_loss | -2.87e-10 |
|    value_loss           | 25.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 419       |
|    time_elapsed         | 17334     |
|    total_timesteps      | 858112    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 5.41      |
|    n_updates            | 4180      |
|    policy_gradient_loss | -2.98e-09 |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 860000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 420       |
|    time_elapsed         | 17376     |
|    total_timesteps      | 860160    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 26        |
|    n_updates            | 4190      |
|    policy_gradient_loss | -8.88e-10 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 421       |
|    time_elapsed         | 17416     |
|    total_timesteps      | 862208    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 11.5      |
|    n_updates            | 4200      |
|    policy_gradient_loss | 4.22e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 422       |
|    time_elapsed         | 17457     |
|    total_timesteps      | 864256    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 18        |
|    n_updates            | 4210      |
|    policy_gradient_loss | 5.17e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 423       |
|    time_elapsed         | 17499     |
|    total_timesteps      | 866304    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 3.11      |
|    n_updates            | 4220      |
|    policy_gradient_loss | -9.61e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 424       |
|    time_elapsed         | 17541     |
|    total_timesteps      | 868352    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.327     |
|    n_updates            | 4230      |
|    policy_gradient_loss | -1.18e-08 |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 870000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 425       |
|    time_elapsed         | 17586     |
|    total_timesteps      | 870400    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 1.26      |
|    n_updates            | 4240      |
|    policy_gradient_loss | 3.92e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 426       |
|    time_elapsed         | 17626     |
|    total_timesteps      | 872448    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 29.3      |
|    n_updates            | 4250      |
|    policy_gradient_loss | -1.45e-08 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 427       |
|    time_elapsed         | 17667     |
|    total_timesteps      | 874496    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 11.9      |
|    n_updates            | 4260      |
|    policy_gradient_loss | -6.28e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 428       |
|    time_elapsed         | 17710     |
|    total_timesteps      | 876544    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 14.1      |
|    n_updates            | 4270      |
|    policy_gradient_loss | -4.74e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 429       |
|    time_elapsed         | 17754     |
|    total_timesteps      | 878592    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 2.38e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.38      |
|    n_updates            | 4280      |
|    policy_gradient_loss | -5.33e-09 |
|    value_loss           | 17.5      |
---------------------------------------
Num timesteps: 880000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 430       |
|    time_elapsed         | 17795     |
|    total_timesteps      | 880640    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 10.3      |
|    n_updates            | 4290      |
|    policy_gradient_loss | 3.57e-09  |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 431       |
|    time_elapsed         | 17836     |
|    total_timesteps      | 882688    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 8.99      |
|    n_updates            | 4300      |
|    policy_gradient_loss | 3.99e-09  |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 432       |
|    time_elapsed         | 17879     |
|    total_timesteps      | 884736    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.706     |
|    n_updates            | 4310      |
|    policy_gradient_loss | 6.66e-10  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 433       |
|    time_elapsed         | 17921     |
|    total_timesteps      | 886784    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 20.5      |
|    n_updates            | 4320      |
|    policy_gradient_loss | 3.57e-10  |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 434       |
|    time_elapsed         | 17965     |
|    total_timesteps      | 888832    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 14.9      |
|    n_updates            | 4330      |
|    policy_gradient_loss | -3.16e-10 |
|    value_loss           | 17.5      |
---------------------------------------
Num timesteps: 890000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 435       |
|    time_elapsed         | 18008     |
|    total_timesteps      | 890880    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.54      |
|    n_updates            | 4340      |
|    policy_gradient_loss | -2.96e-09 |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 436       |
|    time_elapsed         | 18047     |
|    total_timesteps      | 892928    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.23e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 3.82      |
|    n_updates            | 4350      |
|    policy_gradient_loss | 8.85e-09  |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 437       |
|    time_elapsed         | 18088     |
|    total_timesteps      | 894976    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 2.38e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 17.9      |
|    n_updates            | 4360      |
|    policy_gradient_loss | 1.58e-09  |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 438       |
|    time_elapsed         | 18127     |
|    total_timesteps      | 897024    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.24e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 2.98      |
|    n_updates            | 4370      |
|    policy_gradient_loss | -2.53e-09 |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 439       |
|    time_elapsed         | 18171     |
|    total_timesteps      | 899072    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0003    |
|    loss                 | 8.34      |
|    n_updates            | 4380      |
|    policy_gradient_loss | 5.53e-09  |
|    value_loss           | 25.5      |
---------------------------------------
Num timesteps: 900000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 440       |
|    time_elapsed         | 18215     |
|    total_timesteps      | 901120    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.853     |
|    n_updates            | 4390      |
|    policy_gradient_loss | -6.66e-09 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 441       |
|    time_elapsed         | 18258     |
|    total_timesteps      | 903168    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 13.4      |
|    n_updates            | 4400      |
|    policy_gradient_loss | -4.81e-10 |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 442       |
|    time_elapsed         | 18297     |
|    total_timesteps      | 905216    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.23e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.26      |
|    n_updates            | 4410      |
|    policy_gradient_loss | 3.09e-10  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 443       |
|    time_elapsed         | 18336     |
|    total_timesteps      | 907264    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.219     |
|    n_updates            | 4420      |
|    policy_gradient_loss | -1.34e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 444       |
|    time_elapsed         | 18379     |
|    total_timesteps      | 909312    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.261     |
|    n_updates            | 4430      |
|    policy_gradient_loss | 4.34e-09  |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 910000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 445       |
|    time_elapsed         | 18421     |
|    total_timesteps      | 911360    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 13.2      |
|    n_updates            | 4440      |
|    policy_gradient_loss | -7.57e-10 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 446       |
|    time_elapsed         | 18461     |
|    total_timesteps      | 913408    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.23e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 34.1      |
|    n_updates            | 4450      |
|    policy_gradient_loss | -5.21e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 447       |
|    time_elapsed         | 18501     |
|    total_timesteps      | 915456    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 24.8      |
|    n_updates            | 4460      |
|    policy_gradient_loss | -5.71e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 448       |
|    time_elapsed         | 18543     |
|    total_timesteps      | 917504    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | -4.77e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 14.6      |
|    n_updates            | 4470      |
|    policy_gradient_loss | 4.33e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 449       |
|    time_elapsed         | 18584     |
|    total_timesteps      | 919552    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 4.17e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 26.1      |
|    n_updates            | 4480      |
|    policy_gradient_loss | 1.02e-09  |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 920000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 450       |
|    time_elapsed         | 18627     |
|    total_timesteps      | 921600    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 10        |
|    n_updates            | 4490      |
|    policy_gradient_loss | -8.37e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 451       |
|    time_elapsed         | 18670     |
|    total_timesteps      | 923648    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.801     |
|    n_updates            | 4500      |
|    policy_gradient_loss | -1.15e-08 |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 452       |
|    time_elapsed         | 18712     |
|    total_timesteps      | 925696    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.838     |
|    n_updates            | 4510      |
|    policy_gradient_loss | 9.34e-09  |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 453       |
|    time_elapsed         | 18755     |
|    total_timesteps      | 927744    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 15.2      |
|    n_updates            | 4520      |
|    policy_gradient_loss | -1.51e-10 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 454       |
|    time_elapsed         | 18795     |
|    total_timesteps      | 929792    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.23e-05 |
|    explained_variance   | 2.38e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 3.55      |
|    n_updates            | 4530      |
|    policy_gradient_loss | 2.75e-09  |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 930000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 455       |
|    time_elapsed         | 18839     |
|    total_timesteps      | 931840    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 9.01      |
|    n_updates            | 4540      |
|    policy_gradient_loss | -2.97e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 456       |
|    time_elapsed         | 18881     |
|    total_timesteps      | 933888    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 9.44      |
|    n_updates            | 4550      |
|    policy_gradient_loss | -4.23e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 457       |
|    time_elapsed         | 18926     |
|    total_timesteps      | 935936    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 5.36e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 15.3      |
|    n_updates            | 4560      |
|    policy_gradient_loss | 7.63e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 458       |
|    time_elapsed         | 18967     |
|    total_timesteps      | 937984    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.728     |
|    n_updates            | 4570      |
|    policy_gradient_loss | -7.4e-09  |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 940000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 459       |
|    time_elapsed         | 19011     |
|    total_timesteps      | 940032    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 2.23      |
|    n_updates            | 4580      |
|    policy_gradient_loss | 1.5e-08   |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 460       |
|    time_elapsed         | 19052     |
|    total_timesteps      | 942080    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.23e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 15.5      |
|    n_updates            | 4590      |
|    policy_gradient_loss | 1.25e-09  |
|    value_loss           | 24.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 461       |
|    time_elapsed         | 19094     |
|    total_timesteps      | 944128    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 19.8      |
|    n_updates            | 4600      |
|    policy_gradient_loss | 1.42e-08  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 462       |
|    time_elapsed         | 19134     |
|    total_timesteps      | 946176    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.57      |
|    n_updates            | 4610      |
|    policy_gradient_loss | 1.12e-08  |
|    value_loss           | 17.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 463       |
|    time_elapsed         | 19175     |
|    total_timesteps      | 948224    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.21e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 4.32      |
|    n_updates            | 4620      |
|    policy_gradient_loss | 2.07e-08  |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 950000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 464       |
|    time_elapsed         | 19219     |
|    total_timesteps      | 950272    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 20.6      |
|    n_updates            | 4630      |
|    policy_gradient_loss | -9.29e-08 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 465       |
|    time_elapsed         | 19262     |
|    total_timesteps      | 952320    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 3.59      |
|    n_updates            | 4640      |
|    policy_gradient_loss | 3.79e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 466       |
|    time_elapsed         | 19300     |
|    total_timesteps      | 954368    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.23e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.2       |
|    n_updates            | 4650      |
|    policy_gradient_loss | -6.91e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 467       |
|    time_elapsed         | 19340     |
|    total_timesteps      | 956416    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.23e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 26.1      |
|    n_updates            | 4660      |
|    policy_gradient_loss | -3.35e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 468       |
|    time_elapsed         | 19384     |
|    total_timesteps      | 958464    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 6.71      |
|    n_updates            | 4670      |
|    policy_gradient_loss | 7.75e-09  |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 960000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 469       |
|    time_elapsed         | 19425     |
|    total_timesteps      | 960512    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.23e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 1.14      |
|    n_updates            | 4680      |
|    policy_gradient_loss | 1.75e-08  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 470       |
|    time_elapsed         | 19467     |
|    total_timesteps      | 962560    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 20.6      |
|    n_updates            | 4690      |
|    policy_gradient_loss | -9.15e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 471       |
|    time_elapsed         | 19507     |
|    total_timesteps      | 964608    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.23e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 9.07      |
|    n_updates            | 4700      |
|    policy_gradient_loss | -4.5e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 472       |
|    time_elapsed         | 19551     |
|    total_timesteps      | 966656    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 4.1       |
|    n_updates            | 4710      |
|    policy_gradient_loss | 6.13e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 473       |
|    time_elapsed         | 19591     |
|    total_timesteps      | 968704    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 6.02      |
|    n_updates            | 4720      |
|    policy_gradient_loss | -5.09e-09 |
|    value_loss           | 17.5      |
---------------------------------------
Num timesteps: 970000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 474       |
|    time_elapsed         | 19633     |
|    total_timesteps      | 970752    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 6.06      |
|    n_updates            | 4730      |
|    policy_gradient_loss | -9.86e-10 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 475       |
|    time_elapsed         | 19674     |
|    total_timesteps      | 972800    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.23e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 1.61      |
|    n_updates            | 4740      |
|    policy_gradient_loss | 5.04e-09  |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 476       |
|    time_elapsed         | 19719     |
|    total_timesteps      | 974848    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 5.55      |
|    n_updates            | 4750      |
|    policy_gradient_loss | 5.53e-10  |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 477       |
|    time_elapsed         | 19758     |
|    total_timesteps      | 976896    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.23e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 8.04      |
|    n_updates            | 4760      |
|    policy_gradient_loss | -5.85e-09 |
|    value_loss           | 17.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 478       |
|    time_elapsed         | 19798     |
|    total_timesteps      | 978944    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.23e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 15.1      |
|    n_updates            | 4770      |
|    policy_gradient_loss | 6.05e-09  |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 980000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 479       |
|    time_elapsed         | 19838     |
|    total_timesteps      | 980992    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.24e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 16.5      |
|    n_updates            | 4780      |
|    policy_gradient_loss | -4.51e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 480       |
|    time_elapsed         | 19882     |
|    total_timesteps      | 983040    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 7.49      |
|    n_updates            | 4790      |
|    policy_gradient_loss | -4.41e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 481       |
|    time_elapsed         | 19922     |
|    total_timesteps      | 985088    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.23e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 2.16      |
|    n_updates            | 4800      |
|    policy_gradient_loss | -1.09e-09 |
|    value_loss           | 22.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 482       |
|    time_elapsed         | 19961     |
|    total_timesteps      | 987136    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.24e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 2.57      |
|    n_updates            | 4810      |
|    policy_gradient_loss | -2.41e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 483       |
|    time_elapsed         | 20003     |
|    total_timesteps      | 989184    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.23e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 2.35      |
|    n_updates            | 4820      |
|    policy_gradient_loss | 1.01e-09  |
|    value_loss           | 17.4      |
---------------------------------------
Num timesteps: 990000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 484       |
|    time_elapsed         | 20044     |
|    total_timesteps      | 991232    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.23e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 4.09      |
|    n_updates            | 4830      |
|    policy_gradient_loss | 2.78e-09  |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 485       |
|    time_elapsed         | 20086     |
|    total_timesteps      | 993280    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.23e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 0.943     |
|    n_updates            | 4840      |
|    policy_gradient_loss | -4.57e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 486       |
|    time_elapsed         | 20127     |
|    total_timesteps      | 995328    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.24e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 3.04      |
|    n_updates            | 4850      |
|    policy_gradient_loss | -2.36e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 487       |
|    time_elapsed         | 20168     |
|    total_timesteps      | 997376    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.23e-05 |
|    explained_variance   | 2.38e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 1.71      |
|    n_updates            | 4860      |
|    policy_gradient_loss | -5.87e-09 |
|    value_loss           | 17.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 488       |
|    time_elapsed         | 20207     |
|    total_timesteps      | 999424    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.23e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 18.3      |
|    n_updates            | 4870      |
|    policy_gradient_loss | 2.63e-09  |
|    value_loss           | 17.3      |
---------------------------------------
Num timesteps: 1000000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -500      |
| time/                   |           |
|    fps                  | 49        |
|    iterations           | 489       |
|    time_elapsed         | 20250     |
|    total_timesteps      | 1001472   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.23e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 9.94      |
|    n_updates            | 4880      |
|    policy_gradient_loss | 1.25e-08  |
|    value_loss           | 17.4      |
---------------------------------------