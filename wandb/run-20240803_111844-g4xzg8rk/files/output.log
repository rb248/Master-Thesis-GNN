/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead
  warnings.warn(out)
Using cpu device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -490     |
| time/              |          |
|    fps             | 671      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -495        |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 2           |
|    time_elapsed         | 22          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.009530418 |
|    clip_fraction        | 0.0868      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | -0.00468    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.942       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00496    |
|    value_loss           | 10.8        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -488         |
| time/                   |              |
|    fps                  | 142          |
|    iterations           | 3            |
|    time_elapsed         | 43           |
|    total_timesteps      | 6144         |
| train/                  |              |
|    approx_kl            | 0.0015161991 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.00139      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.131        |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.000356    |
|    value_loss           | 14.1         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -485        |
| time/                   |             |
|    fps                  | 129         |
|    iterations           | 4           |
|    time_elapsed         | 63          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.004319489 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.308       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.1         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00112    |
|    value_loss           | 13.2        |
-----------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -476.00
Saving new best model at 10000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -476        |
| time/                   |             |
|    fps                  | 122         |
|    iterations           | 5           |
|    time_elapsed         | 83          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.010680059 |
|    clip_fraction        | 0.0146      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.444       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.12        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00259    |
|    value_loss           | 10.8        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -469        |
| time/                   |             |
|    fps                  | 117         |
|    iterations           | 6           |
|    time_elapsed         | 104         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.016986003 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.987      |
|    explained_variance   | 0.58        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.97        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 9.62        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -466         |
| time/                   |              |
|    fps                  | 115          |
|    iterations           | 7            |
|    time_elapsed         | 124          |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.0047763437 |
|    clip_fraction        | 0.0246       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.926       |
|    explained_variance   | 0.613        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.84         |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00327     |
|    value_loss           | 12.4         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -448         |
| time/                   |              |
|    fps                  | 112          |
|    iterations           | 8            |
|    time_elapsed         | 145          |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 0.0057415497 |
|    clip_fraction        | 0.0123       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.92        |
|    explained_variance   | 0.653        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.19         |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00371     |
|    value_loss           | 12.8         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -442        |
| time/                   |             |
|    fps                  | 110         |
|    iterations           | 9           |
|    time_elapsed         | 166         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.010969086 |
|    clip_fraction        | 0.0919      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.836      |
|    explained_variance   | 0.763       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.72        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00841    |
|    value_loss           | 12          |
-----------------------------------------
Num timesteps: 20000
Best mean reward: -476.00 - Last mean reward per episode: -430.00
Saving new best model at 20000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -430        |
| time/                   |             |
|    fps                  | 109         |
|    iterations           | 10          |
|    time_elapsed         | 187         |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.009649488 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.797      |
|    explained_variance   | 0.679       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.2        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00469    |
|    value_loss           | 18.4        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -424         |
| time/                   |              |
|    fps                  | 108          |
|    iterations           | 11           |
|    time_elapsed         | 206          |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0046064965 |
|    clip_fraction        | 0.0353       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.669       |
|    explained_variance   | 0.638        |
|    learning_rate        | 0.0003       |
|    loss                 | 14.8         |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00318     |
|    value_loss           | 25.4         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -423         |
| time/                   |              |
|    fps                  | 107          |
|    iterations           | 12           |
|    time_elapsed         | 228          |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0065151965 |
|    clip_fraction        | 0.0745       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.604       |
|    explained_variance   | 0.719        |
|    learning_rate        | 0.0003       |
|    loss                 | 14.6         |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00484     |
|    value_loss           | 20.2         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -409         |
| time/                   |              |
|    fps                  | 106          |
|    iterations           | 13           |
|    time_elapsed         | 249          |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.0026439573 |
|    clip_fraction        | 0.0142       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.598       |
|    explained_variance   | 0.701        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.06         |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.0018      |
|    value_loss           | 20.5         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -401         |
| time/                   |              |
|    fps                  | 106          |
|    iterations           | 14           |
|    time_elapsed         | 269          |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0024327221 |
|    clip_fraction        | 0.0292       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.438       |
|    explained_variance   | 0.798        |
|    learning_rate        | 0.0003       |
|    loss                 | 9.6          |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00224     |
|    value_loss           | 18.7         |
------------------------------------------
Num timesteps: 30000
Best mean reward: -430.00 - Last mean reward per episode: -392.33
Saving new best model at 30000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -392         |
| time/                   |              |
|    fps                  | 105          |
|    iterations           | 15           |
|    time_elapsed         | 289          |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.0012160037 |
|    clip_fraction        | 0.0173       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.425       |
|    explained_variance   | 0.72         |
|    learning_rate        | 0.0003       |
|    loss                 | 11.5         |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.000101    |
|    value_loss           | 24.1         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -388         |
| time/                   |              |
|    fps                  | 105          |
|    iterations           | 16           |
|    time_elapsed         | 311          |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0009682605 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.355       |
|    explained_variance   | 0.745        |
|    learning_rate        | 0.0003       |
|    loss                 | 12.4         |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.000294    |
|    value_loss           | 29.2         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -388          |
| time/                   |               |
|    fps                  | 104           |
|    iterations           | 17            |
|    time_elapsed         | 332           |
|    total_timesteps      | 34816         |
| train/                  |               |
|    approx_kl            | 0.00094711955 |
|    clip_fraction        | 0.00518       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.391        |
|    explained_variance   | 0.64          |
|    learning_rate        | 0.0003        |
|    loss                 | 7.72          |
|    n_updates            | 160           |
|    policy_gradient_loss | -0.00106      |
|    value_loss           | 22.6          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -383         |
| time/                   |              |
|    fps                  | 104          |
|    iterations           | 18           |
|    time_elapsed         | 352          |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.0023592296 |
|    clip_fraction        | 0.0277       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.396       |
|    explained_variance   | 0.692        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.1         |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00308     |
|    value_loss           | 23.2         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -380         |
| time/                   |              |
|    fps                  | 104          |
|    iterations           | 19           |
|    time_elapsed         | 373          |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0024309761 |
|    clip_fraction        | 0.0199       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.389       |
|    explained_variance   | 0.737        |
|    learning_rate        | 0.0003       |
|    loss                 | 9.57         |
|    n_updates            | 180          |
|    policy_gradient_loss | 0.000244     |
|    value_loss           | 22.6         |
------------------------------------------
Num timesteps: 40000
Best mean reward: -392.33 - Last mean reward per episode: -376.00
Saving new best model at 40000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -376        |
| time/                   |             |
|    fps                  | 103         |
|    iterations           | 20          |
|    time_elapsed         | 393         |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.005601543 |
|    clip_fraction        | 0.0505      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.516      |
|    explained_variance   | 0.674       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.7         |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.000542   |
|    value_loss           | 21.6        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -367         |
| time/                   |              |
|    fps                  | 103          |
|    iterations           | 21           |
|    time_elapsed         | 416          |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0034333006 |
|    clip_fraction        | 0.0321       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.396       |
|    explained_variance   | 0.712        |
|    learning_rate        | 0.0003       |
|    loss                 | 10.9         |
|    n_updates            | 200          |
|    policy_gradient_loss | -8.52e-05    |
|    value_loss           | 22.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -366         |
| time/                   |              |
|    fps                  | 103          |
|    iterations           | 22           |
|    time_elapsed         | 436          |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0013722413 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.332       |
|    explained_variance   | 0.737        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.8         |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.000247    |
|    value_loss           | 33.9         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -362         |
| time/                   |              |
|    fps                  | 103          |
|    iterations           | 23           |
|    time_elapsed         | 457          |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0021651294 |
|    clip_fraction        | 0.0306       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.408       |
|    explained_variance   | 0.776        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.5         |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.000751    |
|    value_loss           | 18.4         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -359         |
| time/                   |              |
|    fps                  | 103          |
|    iterations           | 24           |
|    time_elapsed         | 477          |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0010972135 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.359       |
|    explained_variance   | 0.757        |
|    learning_rate        | 0.0003       |
|    loss                 | 10.8         |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.000455    |
|    value_loss           | 25.9         |
------------------------------------------
Num timesteps: 50000
Best mean reward: -376.00 - Last mean reward per episode: -356.60
Saving new best model at 50000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -355         |
| time/                   |              |
|    fps                  | 102          |
|    iterations           | 25           |
|    time_elapsed         | 498          |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0023638294 |
|    clip_fraction        | 0.0274       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.419       |
|    explained_variance   | 0.786        |
|    learning_rate        | 0.0003       |
|    loss                 | 9.43         |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.000386    |
|    value_loss           | 20.5         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -356         |
| time/                   |              |
|    fps                  | 102          |
|    iterations           | 26           |
|    time_elapsed         | 519          |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0026241662 |
|    clip_fraction        | 0.0206       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.412       |
|    explained_variance   | 0.77         |
|    learning_rate        | 0.0003       |
|    loss                 | 6.28         |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.000295    |
|    value_loss           | 18           |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -351         |
| time/                   |              |
|    fps                  | 102          |
|    iterations           | 27           |
|    time_elapsed         | 539          |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0021126075 |
|    clip_fraction        | 0.0275       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.324       |
|    explained_variance   | 0.672        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.97         |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.000602    |
|    value_loss           | 18.6         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -349         |
| time/                   |              |
|    fps                  | 102          |
|    iterations           | 28           |
|    time_elapsed         | 559          |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0031138062 |
|    clip_fraction        | 0.0309       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.326       |
|    explained_variance   | 0.755        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.83         |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00218     |
|    value_loss           | 21.1         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -346         |
| time/                   |              |
|    fps                  | 102          |
|    iterations           | 29           |
|    time_elapsed         | 580          |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0009107098 |
|    clip_fraction        | 0.0149       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.306       |
|    explained_variance   | 0.734        |
|    learning_rate        | 0.0003       |
|    loss                 | 9.59         |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.000623    |
|    value_loss           | 23.9         |
------------------------------------------
Num timesteps: 60000
Best mean reward: -356.60 - Last mean reward per episode: -344.67
Saving new best model at 60000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -345         |
| time/                   |              |
|    fps                  | 102          |
|    iterations           | 30           |
|    time_elapsed         | 602          |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0023821788 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.295       |
|    explained_variance   | 0.701        |
|    learning_rate        | 0.0003       |
|    loss                 | 13           |
|    n_updates            | 290          |
|    policy_gradient_loss | 0.000234     |
|    value_loss           | 24.8         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -346         |
| time/                   |              |
|    fps                  | 101          |
|    iterations           | 31           |
|    time_elapsed         | 625          |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.0015598056 |
|    clip_fraction        | 0.034        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.304       |
|    explained_variance   | 0.762        |
|    learning_rate        | 0.0003       |
|    loss                 | 10.3         |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00134     |
|    value_loss           | 16.8         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -343        |
| time/                   |             |
|    fps                  | 101         |
|    iterations           | 32          |
|    time_elapsed         | 646         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.000973889 |
|    clip_fraction        | 0.0235      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.321      |
|    explained_variance   | 0.747       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.1        |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.000298   |
|    value_loss           | 22.4        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -339         |
| time/                   |              |
|    fps                  | 100          |
|    iterations           | 33           |
|    time_elapsed         | 670          |
|    total_timesteps      | 67584        |
| train/                  |              |
|    approx_kl            | 0.0012306792 |
|    clip_fraction        | 0.00806      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.281       |
|    explained_variance   | 0.812        |
|    learning_rate        | 0.0003       |
|    loss                 | 8.12         |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.000137    |
|    value_loss           | 18.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -339         |
| time/                   |              |
|    fps                  | 100          |
|    iterations           | 34           |
|    time_elapsed         | 692          |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.0009041355 |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.286       |
|    explained_variance   | 0.794        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.86         |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.00014     |
|    value_loss           | 22.6         |
------------------------------------------
Num timesteps: 70000
Best mean reward: -344.67 - Last mean reward per episode: -339.71
Saving new best model at 70000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -338         |
| time/                   |              |
|    fps                  | 100          |
|    iterations           | 35           |
|    time_elapsed         | 713          |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.0013865025 |
|    clip_fraction        | 0.0157       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.305       |
|    explained_variance   | 0.739        |
|    learning_rate        | 0.0003       |
|    loss                 | 12.5         |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.000561    |
|    value_loss           | 20.8         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -335         |
| time/                   |              |
|    fps                  | 100          |
|    iterations           | 36           |
|    time_elapsed         | 734          |
|    total_timesteps      | 73728        |
| train/                  |              |
|    approx_kl            | 0.0018689587 |
|    clip_fraction        | 0.0168       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.244       |
|    explained_variance   | 0.775        |
|    learning_rate        | 0.0003       |
|    loss                 | 10.2         |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.000253    |
|    value_loss           | 23.1         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -332         |
| time/                   |              |
|    fps                  | 100          |
|    iterations           | 37           |
|    time_elapsed         | 754          |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0017005808 |
|    clip_fraction        | 0.0171       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.239       |
|    explained_variance   | 0.809        |
|    learning_rate        | 0.0003       |
|    loss                 | 9.27         |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.000241    |
|    value_loss           | 22           |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -331         |
| time/                   |              |
|    fps                  | 100          |
|    iterations           | 38           |
|    time_elapsed         | 776          |
|    total_timesteps      | 77824        |
| train/                  |              |
|    approx_kl            | 0.0013917122 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.22        |
|    explained_variance   | 0.733        |
|    learning_rate        | 0.0003       |
|    loss                 | 9.63         |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.000558    |
|    value_loss           | 25.6         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -327         |
| time/                   |              |
|    fps                  | 99           |
|    iterations           | 39           |
|    time_elapsed         | 799          |
|    total_timesteps      | 79872        |
| train/                  |              |
|    approx_kl            | 0.0011692301 |
|    clip_fraction        | 0.0232       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.17        |
|    explained_variance   | 0.798        |
|    learning_rate        | 0.0003       |
|    loss                 | 10.1         |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.00162     |
|    value_loss           | 23.7         |
------------------------------------------
Num timesteps: 80000
Best mean reward: -339.71 - Last mean reward per episode: -327.00
Saving new best model at 80000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -327          |
| time/                   |               |
|    fps                  | 99            |
|    iterations           | 40            |
|    time_elapsed         | 821           |
|    total_timesteps      | 81920         |
| train/                  |               |
|    approx_kl            | 0.00090119155 |
|    clip_fraction        | 0.0112        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.176        |
|    explained_variance   | 0.816         |
|    learning_rate        | 0.0003        |
|    loss                 | 9.74          |
|    n_updates            | 390           |
|    policy_gradient_loss | -0.00089      |
|    value_loss           | 23.3          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -326         |
| time/                   |              |
|    fps                  | 99           |
|    iterations           | 41           |
|    time_elapsed         | 842          |
|    total_timesteps      | 83968        |
| train/                  |              |
|    approx_kl            | 0.0007245195 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.199       |
|    explained_variance   | 0.77         |
|    learning_rate        | 0.0003       |
|    loss                 | 11.7         |
|    n_updates            | 400          |
|    policy_gradient_loss | 8.97e-05     |
|    value_loss           | 20.1         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -324          |
| time/                   |               |
|    fps                  | 99            |
|    iterations           | 42            |
|    time_elapsed         | 865           |
|    total_timesteps      | 86016         |
| train/                  |               |
|    approx_kl            | 0.00084936415 |
|    clip_fraction        | 0.0129        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.153        |
|    explained_variance   | 0.773         |
|    learning_rate        | 0.0003        |
|    loss                 | 12.4          |
|    n_updates            | 410           |
|    policy_gradient_loss | -0.000188     |
|    value_loss           | 24.2          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -324         |
| time/                   |              |
|    fps                  | 99           |
|    iterations           | 43           |
|    time_elapsed         | 887          |
|    total_timesteps      | 88064        |
| train/                  |              |
|    approx_kl            | 0.0021164352 |
|    clip_fraction        | 0.0223       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.161       |
|    explained_variance   | 0.74         |
|    learning_rate        | 0.0003       |
|    loss                 | 10.6         |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.000333    |
|    value_loss           | 26           |
------------------------------------------
Num timesteps: 90000
Best mean reward: -327.00 - Last mean reward per episode: -322.22
Saving new best model at 90000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -322         |
| time/                   |              |
|    fps                  | 98           |
|    iterations           | 44           |
|    time_elapsed         | 912          |
|    total_timesteps      | 90112        |
| train/                  |              |
|    approx_kl            | 0.0017752654 |
|    clip_fraction        | 0.0161       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.184       |
|    explained_variance   | 0.768        |
|    learning_rate        | 0.0003       |
|    loss                 | 15.5         |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.000429    |
|    value_loss           | 24.2         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -321        |
| time/                   |             |
|    fps                  | 98          |
|    iterations           | 45          |
|    time_elapsed         | 935         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.006274647 |
|    clip_fraction        | 0.0209      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.207      |
|    explained_variance   | 0.818       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.6        |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.000971   |
|    value_loss           | 22.5        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -319         |
| time/                   |              |
|    fps                  | 98           |
|    iterations           | 46           |
|    time_elapsed         | 957          |
|    total_timesteps      | 94208        |
| train/                  |              |
|    approx_kl            | 0.0034669891 |
|    clip_fraction        | 0.0311       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.22        |
|    explained_variance   | 0.746        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.9         |
|    n_updates            | 450          |
|    policy_gradient_loss | 0.000963     |
|    value_loss           | 30.2         |
------------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 291, in <module>
    model.learn(total_timesteps=1000000, callback=[WandbCallback(), save_callback])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 300, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 179, in collect_rollouts
    actions, values, log_probs = self.policy(obs_tensor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 645, in forward
    features = self.extract_features(obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/policy.py", line 49, in forward
    obj_emb = self.model(pyg_data, pyg_data.batch_dict)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_gnn.py", line 184, in forward
    x = conv(x, edge_index)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/conv/gat_conv.py", line 324, in forward
    edge_index, edge_attr = add_self_loops(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/utils/loop.py", line 467, in add_self_loops
    torch.arange(0, N, device=device).view(1, -1).repeat(2, 1),
KeyboardInterrupt