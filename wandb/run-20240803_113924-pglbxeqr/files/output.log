
Using cpu device
/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead
  warnings.warn(out)
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -502     |
| time/              |          |
|    fps             | 612      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 2048     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -513         |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 2            |
|    time_elapsed         | 22           |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0091631785 |
|    clip_fraction        | 0.0292       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | -0.00132     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.822        |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.000541    |
|    value_loss           | 10.2         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -512         |
| time/                   |              |
|    fps                  | 144          |
|    iterations           | 3            |
|    time_elapsed         | 42           |
|    total_timesteps      | 6144         |
| train/                  |              |
|    approx_kl            | 0.0026763752 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.00718      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.178        |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.000374    |
|    value_loss           | 16.8         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -502        |
| time/                   |             |
|    fps                  | 132         |
|    iterations           | 4           |
|    time_elapsed         | 61          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.008905446 |
|    clip_fraction        | 0.0275      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.000827    |
|    learning_rate        | 0.0003      |
|    loss                 | 4.6         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00079    |
|    value_loss           | 14.5        |
-----------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -500.80
Saving new best model at 10000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -501        |
| time/                   |             |
|    fps                  | 124         |
|    iterations           | 5           |
|    time_elapsed         | 82          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.011751056 |
|    clip_fraction        | 0.0426      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.000388    |
|    learning_rate        | 0.0003      |
|    loss                 | 3.79        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00141    |
|    value_loss           | 17.4        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -501        |
| time/                   |             |
|    fps                  | 120         |
|    iterations           | 6           |
|    time_elapsed         | 102         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.008764608 |
|    clip_fraction        | 0.0407      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.000641    |
|    learning_rate        | 0.0003      |
|    loss                 | 8.86        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00248    |
|    value_loss           | 12.7        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -501        |
| time/                   |             |
|    fps                  | 117         |
|    iterations           | 7           |
|    time_elapsed         | 121         |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.010882527 |
|    clip_fraction        | 0.0406      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | -0.00197    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.184       |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.000405   |
|    value_loss           | 11.3        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -498        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 8           |
|    time_elapsed         | 143         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.016531898 |
|    clip_fraction        | 0.0981      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.00398     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.275       |
|    n_updates            | 70          |
|    policy_gradient_loss | 0.000928    |
|    value_loss           | 14.9        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -499        |
| time/                   |             |
|    fps                  | 112         |
|    iterations           | 9           |
|    time_elapsed         | 163         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.008668938 |
|    clip_fraction        | 0.0706      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.23        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.08        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00377    |
|    value_loss           | 13.8        |
-----------------------------------------
Num timesteps: 20000
Best mean reward: -500.80 - Last mean reward per episode: -499.95
Saving new best model at 20000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 111          |
|    iterations           | 10           |
|    time_elapsed         | 183          |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.0022099034 |
|    clip_fraction        | 0.00986      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0.471        |
|    learning_rate        | 0.0003       |
|    loss                 | 8.89         |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00134     |
|    value_loss           | 13.9         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -493        |
| time/                   |             |
|    fps                  | 109         |
|    iterations           | 11          |
|    time_elapsed         | 205         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.005781197 |
|    clip_fraction        | 0.0597      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.547       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.29        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00108    |
|    value_loss           | 8.07        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -491        |
| time/                   |             |
|    fps                  | 109         |
|    iterations           | 12          |
|    time_elapsed         | 225         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.008823969 |
|    clip_fraction        | 0.0621      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.577       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.6        |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00386    |
|    value_loss           | 23.5        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -487        |
| time/                   |             |
|    fps                  | 108         |
|    iterations           | 13          |
|    time_elapsed         | 245         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.008362424 |
|    clip_fraction        | 0.0598      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.665       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.46        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0055     |
|    value_loss           | 24.2        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | -479       |
| time/                   |            |
|    fps                  | 107        |
|    iterations           | 14         |
|    time_elapsed         | 266        |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.01021914 |
|    clip_fraction        | 0.0696     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.959     |
|    explained_variance   | 0.641      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.61       |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.00495   |
|    value_loss           | 24.5       |
----------------------------------------
Num timesteps: 30000
Best mean reward: -499.95 - Last mean reward per episode: -465.23
Saving new best model at 30000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -465        |
| time/                   |             |
|    fps                  | 106         |
|    iterations           | 15          |
|    time_elapsed         | 287         |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.014602495 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.825      |
|    explained_variance   | 0.694       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.28        |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00936    |
|    value_loss           | 24          |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -451        |
| time/                   |             |
|    fps                  | 106         |
|    iterations           | 16          |
|    time_elapsed         | 307         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.003919839 |
|    clip_fraction        | 0.00947     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.824      |
|    explained_variance   | 0.705       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.5         |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.000583   |
|    value_loss           | 21.1        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -439         |
| time/                   |              |
|    fps                  | 105          |
|    iterations           | 17           |
|    time_elapsed         | 328          |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0114013925 |
|    clip_fraction        | 0.149        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.747       |
|    explained_variance   | 0.706        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.23         |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00871     |
|    value_loss           | 21.8         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -426        |
| time/                   |             |
|    fps                  | 105         |
|    iterations           | 18          |
|    time_elapsed         | 348         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.009432357 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.662      |
|    explained_variance   | 0.712       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.1        |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0054     |
|    value_loss           | 28.1        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -419         |
| time/                   |              |
|    fps                  | 105          |
|    iterations           | 19           |
|    time_elapsed         | 368          |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0033920412 |
|    clip_fraction        | 0.0315       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.616       |
|    explained_variance   | 0.778        |
|    learning_rate        | 0.0003       |
|    loss                 | 9.73         |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00363     |
|    value_loss           | 19.1         |
------------------------------------------
Num timesteps: 40000
Best mean reward: -465.23 - Last mean reward per episode: -411.45
Saving new best model at 40000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -411         |
| time/                   |              |
|    fps                  | 105          |
|    iterations           | 20           |
|    time_elapsed         | 389          |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0011410895 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.586       |
|    explained_variance   | 0.723        |
|    learning_rate        | 0.0003       |
|    loss                 | 10.7         |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00127     |
|    value_loss           | 25.4         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -397        |
| time/                   |             |
|    fps                  | 105         |
|    iterations           | 21          |
|    time_elapsed         | 409         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.005534101 |
|    clip_fraction        | 0.093       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.531      |
|    explained_variance   | 0.697       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.5        |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00743    |
|    value_loss           | 26          |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -391        |
| time/                   |             |
|    fps                  | 104         |
|    iterations           | 22          |
|    time_elapsed         | 430         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.004907846 |
|    clip_fraction        | 0.0527      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.472      |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.2        |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.003      |
|    value_loss           | 29.5        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -379        |
| time/                   |             |
|    fps                  | 104         |
|    iterations           | 23          |
|    time_elapsed         | 451         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.002335939 |
|    clip_fraction        | 0.0146      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.434      |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.58        |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.000211   |
|    value_loss           | 21.4        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -369         |
| time/                   |              |
|    fps                  | 104          |
|    iterations           | 24           |
|    time_elapsed         | 472          |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0027493737 |
|    clip_fraction        | 0.034        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.38        |
|    explained_variance   | 0.774        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.34         |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00331     |
|    value_loss           | 23.2         |
------------------------------------------
Num timesteps: 50000
Best mean reward: -411.45 - Last mean reward per episode: -366.04
Saving new best model at 50000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -362         |
| time/                   |              |
|    fps                  | 103          |
|    iterations           | 25           |
|    time_elapsed         | 492          |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0032215323 |
|    clip_fraction        | 0.0452       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.313       |
|    explained_variance   | 0.815        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.66         |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00285     |
|    value_loss           | 17.1         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -355         |
| time/                   |              |
|    fps                  | 103          |
|    iterations           | 26           |
|    time_elapsed         | 512          |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0026597497 |
|    clip_fraction        | 0.0416       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.282       |
|    explained_variance   | 0.754        |
|    learning_rate        | 0.0003       |
|    loss                 | 12.2         |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.003       |
|    value_loss           | 30.3         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -348         |
| time/                   |              |
|    fps                  | 103          |
|    iterations           | 27           |
|    time_elapsed         | 535          |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0011636023 |
|    clip_fraction        | 0.00928      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.256       |
|    explained_variance   | 0.797        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.3         |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.000766    |
|    value_loss           | 20.9         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -343          |
| time/                   |               |
|    fps                  | 102           |
|    iterations           | 28            |
|    time_elapsed         | 556           |
|    total_timesteps      | 57344         |
| train/                  |               |
|    approx_kl            | 0.00049131236 |
|    clip_fraction        | 0.00186       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.252        |
|    explained_variance   | 0.795         |
|    learning_rate        | 0.0003        |
|    loss                 | 10.6          |
|    n_updates            | 270           |
|    policy_gradient_loss | -0.00051      |
|    value_loss           | 21.6          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -339          |
| time/                   |               |
|    fps                  | 102           |
|    iterations           | 29            |
|    time_elapsed         | 580           |
|    total_timesteps      | 59392         |
| train/                  |               |
|    approx_kl            | 0.00038050947 |
|    clip_fraction        | 0.00679       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.281        |
|    explained_variance   | 0.74          |
|    learning_rate        | 0.0003        |
|    loss                 | 11.8          |
|    n_updates            | 280           |
|    policy_gradient_loss | -0.000737     |
|    value_loss           | 35            |
-------------------------------------------
Num timesteps: 60000
Best mean reward: -366.04 - Last mean reward per episode: -334.37
Saving new best model at 60000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -331         |
| time/                   |              |
|    fps                  | 101          |
|    iterations           | 30           |
|    time_elapsed         | 603          |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0012727033 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.241       |
|    explained_variance   | 0.768        |
|    learning_rate        | 0.0003       |
|    loss                 | 10.3         |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00164     |
|    value_loss           | 30.1         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -326         |
| time/                   |              |
|    fps                  | 101          |
|    iterations           | 31           |
|    time_elapsed         | 624          |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.0015085915 |
|    clip_fraction        | 0.0186       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.204       |
|    explained_variance   | 0.789        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.3         |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00176     |
|    value_loss           | 22.2         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -319         |
| time/                   |              |
|    fps                  | 101          |
|    iterations           | 32           |
|    time_elapsed         | 645          |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0010094391 |
|    clip_fraction        | 0.0177       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.181       |
|    explained_variance   | 0.766        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.9         |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.000564    |
|    value_loss           | 24.2         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -315          |
| time/                   |               |
|    fps                  | 101           |
|    iterations           | 33            |
|    time_elapsed         | 665           |
|    total_timesteps      | 67584         |
| train/                  |               |
|    approx_kl            | 0.00035936618 |
|    clip_fraction        | 0.00205       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.187        |
|    explained_variance   | 0.809         |
|    learning_rate        | 0.0003        |
|    loss                 | 9.46          |
|    n_updates            | 320           |
|    policy_gradient_loss | -0.000245     |
|    value_loss           | 21.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -311          |
| time/                   |               |
|    fps                  | 101           |
|    iterations           | 34            |
|    time_elapsed         | 685           |
|    total_timesteps      | 69632         |
| train/                  |               |
|    approx_kl            | 0.00081118336 |
|    clip_fraction        | 0.00591       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.177        |
|    explained_variance   | 0.777         |
|    learning_rate        | 0.0003        |
|    loss                 | 12.4          |
|    n_updates            | 330           |
|    policy_gradient_loss | -0.000945     |
|    value_loss           | 30            |
-------------------------------------------
Num timesteps: 70000
Best mean reward: -334.37 - Last mean reward per episode: -311.31
Saving new best model at 70000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -308          |
| time/                   |               |
|    fps                  | 101           |
|    iterations           | 35            |
|    time_elapsed         | 706           |
|    total_timesteps      | 71680         |
| train/                  |               |
|    approx_kl            | 0.00040367583 |
|    clip_fraction        | 0.00898       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.189        |
|    explained_variance   | 0.795         |
|    learning_rate        | 0.0003        |
|    loss                 | 13.7          |
|    n_updates            | 340           |
|    policy_gradient_loss | -0.000489     |
|    value_loss           | 28.9          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -304         |
| time/                   |              |
|    fps                  | 101          |
|    iterations           | 36           |
|    time_elapsed         | 728          |
|    total_timesteps      | 73728        |
| train/                  |              |
|    approx_kl            | 0.0008721497 |
|    clip_fraction        | 0.00981      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.19        |
|    explained_variance   | 0.795        |
|    learning_rate        | 0.0003       |
|    loss                 | 15.8         |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.00151     |
|    value_loss           | 30.4         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -299          |
| time/                   |               |
|    fps                  | 101           |
|    iterations           | 37            |
|    time_elapsed         | 749           |
|    total_timesteps      | 75776         |
| train/                  |               |
|    approx_kl            | 0.00068251067 |
|    clip_fraction        | 0.0132        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.162        |
|    explained_variance   | 0.786         |
|    learning_rate        | 0.0003        |
|    loss                 | 9.06          |
|    n_updates            | 360           |
|    policy_gradient_loss | -0.00102      |
|    value_loss           | 27.5          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -294          |
| time/                   |               |
|    fps                  | 100           |
|    iterations           | 38            |
|    time_elapsed         | 771           |
|    total_timesteps      | 77824         |
| train/                  |               |
|    approx_kl            | 0.00030097706 |
|    clip_fraction        | 0.0064        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.168        |
|    explained_variance   | 0.771         |
|    learning_rate        | 0.0003        |
|    loss                 | 10.5          |
|    n_updates            | 370           |
|    policy_gradient_loss | -0.000222     |
|    value_loss           | 24.8          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -293          |
| time/                   |               |
|    fps                  | 100           |
|    iterations           | 39            |
|    time_elapsed         | 793           |
|    total_timesteps      | 79872         |
| train/                  |               |
|    approx_kl            | 0.00022154296 |
|    clip_fraction        | 0.00898       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.189        |
|    explained_variance   | 0.818         |
|    learning_rate        | 0.0003        |
|    loss                 | 7.56          |
|    n_updates            | 380           |
|    policy_gradient_loss | -0.000663     |
|    value_loss           | 19.9          |
-------------------------------------------
Num timesteps: 80000
Best mean reward: -311.31 - Last mean reward per episode: -292.26
Saving new best model at 80000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -291         |
| time/                   |              |
|    fps                  | 100          |
|    iterations           | 40           |
|    time_elapsed         | 813          |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.0012111892 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.167       |
|    explained_variance   | 0.739        |
|    learning_rate        | 0.0003       |
|    loss                 | 12.6         |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.000325    |
|    value_loss           | 31.1         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -288          |
| time/                   |               |
|    fps                  | 100           |
|    iterations           | 41            |
|    time_elapsed         | 833           |
|    total_timesteps      | 83968         |
| train/                  |               |
|    approx_kl            | 0.00039404142 |
|    clip_fraction        | 0.00195       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.153        |
|    explained_variance   | 0.777         |
|    learning_rate        | 0.0003        |
|    loss                 | 9.95          |
|    n_updates            | 400           |
|    policy_gradient_loss | -0.000898     |
|    value_loss           | 24.8          |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -283        |
| time/                   |             |
|    fps                  | 100         |
|    iterations           | 42          |
|    time_elapsed         | 855         |
|    total_timesteps      | 86016       |
| train/                  |             |
|    approx_kl            | 0.001887728 |
|    clip_fraction        | 0.0131      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.131      |
|    explained_variance   | 0.795       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.9        |
|    n_updates            | 410         |
|    policy_gradient_loss | -6.65e-05   |
|    value_loss           | 22.8        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -278         |
| time/                   |              |
|    fps                  | 100          |
|    iterations           | 43           |
|    time_elapsed         | 877          |
|    total_timesteps      | 88064        |
| train/                  |              |
|    approx_kl            | 0.0011661048 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.102       |
|    explained_variance   | 0.799        |
|    learning_rate        | 0.0003       |
|    loss                 | 12           |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.000551    |
|    value_loss           | 29           |
------------------------------------------
Num timesteps: 90000
Best mean reward: -292.26 - Last mean reward per episode: -275.60
Saving new best model at 90000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -276         |
| time/                   |              |
|    fps                  | 100          |
|    iterations           | 44           |
|    time_elapsed         | 897          |
|    total_timesteps      | 90112        |
| train/                  |              |
|    approx_kl            | 0.0014044037 |
|    clip_fraction        | 0.00801      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0806      |
|    explained_variance   | 0.69         |
|    learning_rate        | 0.0003       |
|    loss                 | 10.9         |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00137     |
|    value_loss           | 22.9         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 100           |
|    iterations           | 45            |
|    time_elapsed         | 919           |
|    total_timesteps      | 92160         |
| train/                  |               |
|    approx_kl            | 0.00062675786 |
|    clip_fraction        | 0.00762       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0841       |
|    explained_variance   | 0.792         |
|    learning_rate        | 0.0003        |
|    loss                 | 13.6          |
|    n_updates            | 440           |
|    policy_gradient_loss | -0.000709     |
|    value_loss           | 25.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -270          |
| time/                   |               |
|    fps                  | 100           |
|    iterations           | 46            |
|    time_elapsed         | 939           |
|    total_timesteps      | 94208         |
| train/                  |               |
|    approx_kl            | 0.00042777485 |
|    clip_fraction        | 0.00435       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0829       |
|    explained_variance   | 0.776         |
|    learning_rate        | 0.0003        |
|    loss                 | 12.1          |
|    n_updates            | 450           |
|    policy_gradient_loss | 3.2e-05       |
|    value_loss           | 22.1          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -268          |
| time/                   |               |
|    fps                  | 99            |
|    iterations           | 47            |
|    time_elapsed         | 962           |
|    total_timesteps      | 96256         |
| train/                  |               |
|    approx_kl            | 0.00017587075 |
|    clip_fraction        | 0.00244       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0681       |
|    explained_variance   | 0.794         |
|    learning_rate        | 0.0003        |
|    loss                 | 13.5          |
|    n_updates            | 460           |
|    policy_gradient_loss | -0.000479     |
|    value_loss           | 24.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -268          |
| time/                   |               |
|    fps                  | 99            |
|    iterations           | 48            |
|    time_elapsed         | 985           |
|    total_timesteps      | 98304         |
| train/                  |               |
|    approx_kl            | 0.00091152143 |
|    clip_fraction        | 0.00947       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0618       |
|    explained_variance   | 0.718         |
|    learning_rate        | 0.0003        |
|    loss                 | 15.5          |
|    n_updates            | 470           |
|    policy_gradient_loss | -0.00105      |
|    value_loss           | 29.6          |
-------------------------------------------
Num timesteps: 100000
Best mean reward: -275.60 - Last mean reward per episode: -265.20
Saving new best model at 100000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -265          |
| time/                   |               |
|    fps                  | 99            |
|    iterations           | 49            |
|    time_elapsed         | 1007          |
|    total_timesteps      | 100352        |
| train/                  |               |
|    approx_kl            | 0.00013665747 |
|    clip_fraction        | 0.00186       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0579       |
|    explained_variance   | 0.807         |
|    learning_rate        | 0.0003        |
|    loss                 | 10.4          |
|    n_updates            | 480           |
|    policy_gradient_loss | 7.37e-05      |
|    value_loss           | 23.1          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -257          |
| time/                   |               |
|    fps                  | 99            |
|    iterations           | 50            |
|    time_elapsed         | 1027          |
|    total_timesteps      | 102400        |
| train/                  |               |
|    approx_kl            | 0.00023453753 |
|    clip_fraction        | 0.00215       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0565       |
|    explained_variance   | 0.808         |
|    learning_rate        | 0.0003        |
|    loss                 | 8.33          |
|    n_updates            | 490           |
|    policy_gradient_loss | -0.000512     |
|    value_loss           | 21.6          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -249          |
| time/                   |               |
|    fps                  | 99            |
|    iterations           | 51            |
|    time_elapsed         | 1049          |
|    total_timesteps      | 104448        |
| train/                  |               |
|    approx_kl            | 0.00014289655 |
|    clip_fraction        | 0.00181       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0563       |
|    explained_variance   | 0.841         |
|    learning_rate        | 0.0003        |
|    loss                 | 11.7          |
|    n_updates            | 500           |
|    policy_gradient_loss | -0.000156     |
|    value_loss           | 22.3          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -241          |
| time/                   |               |
|    fps                  | 99            |
|    iterations           | 52            |
|    time_elapsed         | 1069          |
|    total_timesteps      | 106496        |
| train/                  |               |
|    approx_kl            | 0.00057606015 |
|    clip_fraction        | 0.00654       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0563       |
|    explained_variance   | 0.763         |
|    learning_rate        | 0.0003        |
|    loss                 | 16.3          |
|    n_updates            | 510           |
|    policy_gradient_loss | -0.000739     |
|    value_loss           | 29.4          |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 1e+03          |
|    ep_rew_mean          | -234           |
| time/                   |                |
|    fps                  | 99             |
|    iterations           | 53             |
|    time_elapsed         | 1088           |
|    total_timesteps      | 108544         |
| train/                  |                |
|    approx_kl            | 0.000116681855 |
|    clip_fraction        | 0.00166        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.055         |
|    explained_variance   | 0.796          |
|    learning_rate        | 0.0003         |
|    loss                 | 16.9           |
|    n_updates            | 520            |
|    policy_gradient_loss | -0.000444      |
|    value_loss           | 32.1           |
--------------------------------------------
Num timesteps: 110000
Best mean reward: -265.20 - Last mean reward per episode: -225.86
Saving new best model at 110000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -226          |
| time/                   |               |
|    fps                  | 99            |
|    iterations           | 54            |
|    time_elapsed         | 1109          |
|    total_timesteps      | 110592        |
| train/                  |               |
|    approx_kl            | 0.00065194117 |
|    clip_fraction        | 0.00454       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0407       |
|    explained_variance   | 0.792         |
|    learning_rate        | 0.0003        |
|    loss                 | 7.71          |
|    n_updates            | 530           |
|    policy_gradient_loss | -0.000924     |
|    value_loss           | 18.6          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -217         |
| time/                   |              |
|    fps                  | 99           |
|    iterations           | 55           |
|    time_elapsed         | 1129         |
|    total_timesteps      | 112640       |
| train/                  |              |
|    approx_kl            | 0.0005308002 |
|    clip_fraction        | 0.00249      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0446      |
|    explained_variance   | 0.824        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.75         |
|    n_updates            | 540          |
|    policy_gradient_loss | -0.000566    |
|    value_loss           | 16.3         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -210         |
| time/                   |              |
|    fps                  | 99           |
|    iterations           | 56           |
|    time_elapsed         | 1148         |
|    total_timesteps      | 114688       |
| train/                  |              |
|    approx_kl            | 0.0013413447 |
|    clip_fraction        | 0.00459      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0493      |
|    explained_variance   | 0.791        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.71         |
|    n_updates            | 550          |
|    policy_gradient_loss | -5.91e-05    |
|    value_loss           | 18.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -205         |
| time/                   |              |
|    fps                  | 99           |
|    iterations           | 57           |
|    time_elapsed         | 1170         |
|    total_timesteps      | 116736       |
| train/                  |              |
|    approx_kl            | 0.0010259125 |
|    clip_fraction        | 0.00513      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0536      |
|    explained_variance   | 0.733        |
|    learning_rate        | 0.0003       |
|    loss                 | 9.88         |
|    n_updates            | 560          |
|    policy_gradient_loss | -0.000536    |
|    value_loss           | 25.1         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -199          |
| time/                   |               |
|    fps                  | 99            |
|    iterations           | 58            |
|    time_elapsed         | 1190          |
|    total_timesteps      | 118784        |
| train/                  |               |
|    approx_kl            | 0.00048296223 |
|    clip_fraction        | 0.00337       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0538       |
|    explained_variance   | 0.763         |
|    learning_rate        | 0.0003        |
|    loss                 | 7.99          |
|    n_updates            | 570           |
|    policy_gradient_loss | 0.000244      |
|    value_loss           | 22.3          |
-------------------------------------------
Num timesteps: 120000
Best mean reward: -225.86 - Last mean reward per episode: -193.22
Saving new best model at 120000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -193          |
| time/                   |               |
|    fps                  | 99            |
|    iterations           | 59            |
|    time_elapsed         | 1210          |
|    total_timesteps      | 120832        |
| train/                  |               |
|    approx_kl            | 0.00012130852 |
|    clip_fraction        | 0.00117       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0486       |
|    explained_variance   | 0.78          |
|    learning_rate        | 0.0003        |
|    loss                 | 8.6           |
|    n_updates            | 580           |
|    policy_gradient_loss | 0.000221      |
|    value_loss           | 20.7          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -188         |
| time/                   |              |
|    fps                  | 99           |
|    iterations           | 60           |
|    time_elapsed         | 1230         |
|    total_timesteps      | 122880       |
| train/                  |              |
|    approx_kl            | 0.0003008324 |
|    clip_fraction        | 0.00273      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0563      |
|    explained_variance   | 0.763        |
|    learning_rate        | 0.0003       |
|    loss                 | 13.3         |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.000514    |
|    value_loss           | 23.4         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -181          |
| time/                   |               |
|    fps                  | 99            |
|    iterations           | 61            |
|    time_elapsed         | 1250          |
|    total_timesteps      | 124928        |
| train/                  |               |
|    approx_kl            | 0.00015672765 |
|    clip_fraction        | 0.00249       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0494       |
|    explained_variance   | 0.808         |
|    learning_rate        | 0.0003        |
|    loss                 | 9.38          |
|    n_updates            | 600           |
|    policy_gradient_loss | -0.000144     |
|    value_loss           | 22.2          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -175          |
| time/                   |               |
|    fps                  | 99            |
|    iterations           | 62            |
|    time_elapsed         | 1270          |
|    total_timesteps      | 126976        |
| train/                  |               |
|    approx_kl            | 0.00019709009 |
|    clip_fraction        | 0.00396       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0433       |
|    explained_variance   | 0.765         |
|    learning_rate        | 0.0003        |
|    loss                 | 8.84          |
|    n_updates            | 610           |
|    policy_gradient_loss | -0.000535     |
|    value_loss           | 23.1          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -171          |
| time/                   |               |
|    fps                  | 99            |
|    iterations           | 63            |
|    time_elapsed         | 1292          |
|    total_timesteps      | 129024        |
| train/                  |               |
|    approx_kl            | 0.00022847607 |
|    clip_fraction        | 0.00498       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0388       |
|    explained_variance   | 0.752         |
|    learning_rate        | 0.0003        |
|    loss                 | 15.9          |
|    n_updates            | 620           |
|    policy_gradient_loss | -0.00103      |
|    value_loss           | 29.6          |
-------------------------------------------
Num timesteps: 130000
Best mean reward: -193.22 - Last mean reward per episode: -168.87
Saving new best model at 130000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -168          |
| time/                   |               |
|    fps                  | 99            |
|    iterations           | 64            |
|    time_elapsed         | 1312          |
|    total_timesteps      | 131072        |
| train/                  |               |
|    approx_kl            | 0.00032022508 |
|    clip_fraction        | 0.00444       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0289       |
|    explained_variance   | 0.763         |
|    learning_rate        | 0.0003        |
|    loss                 | 17.4          |
|    n_updates            | 630           |
|    policy_gradient_loss | -0.000885     |
|    value_loss           | 27.7          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -165          |
| time/                   |               |
|    fps                  | 99            |
|    iterations           | 65            |
|    time_elapsed         | 1333          |
|    total_timesteps      | 133120        |
| train/                  |               |
|    approx_kl            | 6.4843043e-06 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0286       |
|    explained_variance   | 0.784         |
|    learning_rate        | 0.0003        |
|    loss                 | 11.4          |
|    n_updates            | 640           |
|    policy_gradient_loss | -5.95e-05     |
|    value_loss           | 26.3          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -164         |
| time/                   |              |
|    fps                  | 99           |
|    iterations           | 66           |
|    time_elapsed         | 1353         |
|    total_timesteps      | 135168       |
| train/                  |              |
|    approx_kl            | 8.999015e-05 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.024       |
|    explained_variance   | 0.745        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.76         |
|    n_updates            | 650          |
|    policy_gradient_loss | 2.04e-05     |
|    value_loss           | 22.6         |
------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 1e+03          |
|    ep_rew_mean          | -164           |
| time/                   |                |
|    fps                  | 99             |
|    iterations           | 67             |
|    time_elapsed         | 1372           |
|    total_timesteps      | 137216         |
| train/                  |                |
|    approx_kl            | 0.000118717115 |
|    clip_fraction        | 0.000879       |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0312        |
|    explained_variance   | 0.799          |
|    learning_rate        | 0.0003         |
|    loss                 | 19             |
|    n_updates            | 660            |
|    policy_gradient_loss | -0.000216      |
|    value_loss           | 27.9           |
--------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -162         |
| time/                   |              |
|    fps                  | 99           |
|    iterations           | 68           |
|    time_elapsed         | 1392         |
|    total_timesteps      | 139264       |
| train/                  |              |
|    approx_kl            | 0.0001747738 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0358      |
|    explained_variance   | 0.747        |
|    learning_rate        | 0.0003       |
|    loss                 | 15           |
|    n_updates            | 670          |
|    policy_gradient_loss | -8.52e-05    |
|    value_loss           | 35.4         |
------------------------------------------
Num timesteps: 140000
Best mean reward: -168.87 - Last mean reward per episode: -160.48
Saving new best model at 140000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -160          |
| time/                   |               |
|    fps                  | 99            |
|    iterations           | 69            |
|    time_elapsed         | 1414          |
|    total_timesteps      | 141312        |
| train/                  |               |
|    approx_kl            | 9.0861635e-05 |
|    clip_fraction        | 0.00137       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0359       |
|    explained_variance   | 0.778         |
|    learning_rate        | 0.0003        |
|    loss                 | 12.8          |
|    n_updates            | 680           |
|    policy_gradient_loss | -0.000135     |
|    value_loss           | 26.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -160          |
| time/                   |               |
|    fps                  | 99            |
|    iterations           | 70            |
|    time_elapsed         | 1434          |
|    total_timesteps      | 143360        |
| train/                  |               |
|    approx_kl            | 0.00036715795 |
|    clip_fraction        | 0.00381       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0318       |
|    explained_variance   | 0.789         |
|    learning_rate        | 0.0003        |
|    loss                 | 8.1           |
|    n_updates            | 690           |
|    policy_gradient_loss | -0.000197     |
|    value_loss           | 21.1          |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -155        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 71          |
|    time_elapsed         | 1455        |
|    total_timesteps      | 145408      |
| train/                  |             |
|    approx_kl            | 0.000317767 |
|    clip_fraction        | 0.00396     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0221     |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.6        |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.000545   |
|    value_loss           | 23.7        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -156        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 72          |
|    time_elapsed         | 1476        |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 9.51713e-05 |
|    clip_fraction        | 0.00132     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0239     |
|    explained_variance   | 0.815       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.6        |
|    n_updates            | 710         |
|    policy_gradient_loss | -9.65e-05   |
|    value_loss           | 26.6        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -158         |
| time/                   |              |
|    fps                  | 99           |
|    iterations           | 73           |
|    time_elapsed         | 1496         |
|    total_timesteps      | 149504       |
| train/                  |              |
|    approx_kl            | 5.907353e-05 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0255      |
|    explained_variance   | 0.785        |
|    learning_rate        | 0.0003       |
|    loss                 | 14.9         |
|    n_updates            | 720          |
|    policy_gradient_loss | -9.52e-06    |
|    value_loss           | 26.8         |
------------------------------------------
Num timesteps: 150000
Best mean reward: -160.48 - Last mean reward per episode: -156.73
Saving new best model at 150000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 99            |
|    iterations           | 74            |
|    time_elapsed         | 1519          |
|    total_timesteps      | 151552        |
| train/                  |               |
|    approx_kl            | 0.00022684602 |
|    clip_fraction        | 0.00215       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.021        |
|    explained_variance   | 0.76          |
|    learning_rate        | 0.0003        |
|    loss                 | 15.7          |
|    n_updates            | 730           |
|    policy_gradient_loss | -0.000343     |
|    value_loss           | 29.8          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -157          |
| time/                   |               |
|    fps                  | 99            |
|    iterations           | 75            |
|    time_elapsed         | 1540          |
|    total_timesteps      | 153600        |
| train/                  |               |
|    approx_kl            | 0.00013750861 |
|    clip_fraction        | 0.00112       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0236       |
|    explained_variance   | 0.749         |
|    learning_rate        | 0.0003        |
|    loss                 | 11.3          |
|    n_updates            | 740           |
|    policy_gradient_loss | -0.000232     |
|    value_loss           | 26.1          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -158         |
| time/                   |              |
|    fps                  | 99           |
|    iterations           | 76           |
|    time_elapsed         | 1565         |
|    total_timesteps      | 155648       |
| train/                  |              |
|    approx_kl            | 0.0001166547 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0228      |
|    explained_variance   | 0.819        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.8         |
|    n_updates            | 750          |
|    policy_gradient_loss | -0.00018     |
|    value_loss           | 30.2         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 99            |
|    iterations           | 77            |
|    time_elapsed         | 1591          |
|    total_timesteps      | 157696        |
| train/                  |               |
|    approx_kl            | 0.00015263772 |
|    clip_fraction        | 0.00142       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0236       |
|    explained_variance   | 0.818         |
|    learning_rate        | 0.0003        |
|    loss                 | 11.6          |
|    n_updates            | 760           |
|    policy_gradient_loss | -0.000113     |
|    value_loss           | 24.5          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -156          |
| time/                   |               |
|    fps                  | 98            |
|    iterations           | 78            |
|    time_elapsed         | 1616          |
|    total_timesteps      | 159744        |
| train/                  |               |
|    approx_kl            | 0.00016433053 |
|    clip_fraction        | 0.0019        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0277       |
|    explained_variance   | 0.782         |
|    learning_rate        | 0.0003        |
|    loss                 | 12.8          |
|    n_updates            | 770           |
|    policy_gradient_loss | -0.000211     |
|    value_loss           | 23.9          |
-------------------------------------------
Num timesteps: 160000
Best mean reward: -156.73 - Last mean reward per episode: -157.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -158         |
| time/                   |              |
|    fps                  | 98           |
|    iterations           | 79           |
|    time_elapsed         | 1639         |
|    total_timesteps      | 161792       |
| train/                  |              |
|    approx_kl            | 9.035517e-05 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0312      |
|    explained_variance   | 0.817        |
|    learning_rate        | 0.0003       |
|    loss                 | 8.01         |
|    n_updates            | 780          |
|    policy_gradient_loss | -3.43e-05    |
|    value_loss           | 19.9         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -159        |
| time/                   |             |
|    fps                  | 98          |
|    iterations           | 80          |
|    time_elapsed         | 1661        |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.000162563 |
|    clip_fraction        | 0.00225     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0337     |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.4        |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.000309   |
|    value_loss           | 26.6        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -161         |
| time/                   |              |
|    fps                  | 98           |
|    iterations           | 81           |
|    time_elapsed         | 1681         |
|    total_timesteps      | 165888       |
| train/                  |              |
|    approx_kl            | 0.0001414433 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0368      |
|    explained_variance   | 0.833        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.64         |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.000326    |
|    value_loss           | 15.6         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -160         |
| time/                   |              |
|    fps                  | 98           |
|    iterations           | 82           |
|    time_elapsed         | 1701         |
|    total_timesteps      | 167936       |
| train/                  |              |
|    approx_kl            | 6.383596e-05 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0392      |
|    explained_variance   | 0.806        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.79         |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.000278    |
|    value_loss           | 20.4         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -162        |
| time/                   |             |
|    fps                  | 98          |
|    iterations           | 83          |
|    time_elapsed         | 1721        |
|    total_timesteps      | 169984      |
| train/                  |             |
|    approx_kl            | 0.000122083 |
|    clip_fraction        | 0.00186     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0414     |
|    explained_variance   | 0.803       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.87        |
|    n_updates            | 820         |
|    policy_gradient_loss | -2.72e-05   |
|    value_loss           | 18          |
-----------------------------------------
Num timesteps: 170000
Best mean reward: -156.73 - Last mean reward per episode: -160.11
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -161         |
| time/                   |              |
|    fps                  | 98           |
|    iterations           | 84           |
|    time_elapsed         | 1744         |
|    total_timesteps      | 172032       |
| train/                  |              |
|    approx_kl            | 0.0001258401 |
|    clip_fraction        | 0.00264      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0392      |
|    explained_variance   | 0.765        |
|    learning_rate        | 0.0003       |
|    loss                 | 13.1         |
|    n_updates            | 830          |
|    policy_gradient_loss | -8.76e-05    |
|    value_loss           | 22.4         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -167          |
| time/                   |               |
|    fps                  | 98            |
|    iterations           | 85            |
|    time_elapsed         | 1770          |
|    total_timesteps      | 174080        |
| train/                  |               |
|    approx_kl            | 0.00012055598 |
|    clip_fraction        | 0.00269       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0451       |
|    explained_variance   | 0.804         |
|    learning_rate        | 0.0003        |
|    loss                 | 7.03          |
|    n_updates            | 840           |
|    policy_gradient_loss | -0.000186     |
|    value_loss           | 18.7          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -169          |
| time/                   |               |
|    fps                  | 98            |
|    iterations           | 86            |
|    time_elapsed         | 1792          |
|    total_timesteps      | 176128        |
| train/                  |               |
|    approx_kl            | 2.6160822e-05 |
|    clip_fraction        | 0.00083       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0449       |
|    explained_variance   | 0.707         |
|    learning_rate        | 0.0003        |
|    loss                 | 7.29          |
|    n_updates            | 850           |
|    policy_gradient_loss | -0.000224     |
|    value_loss           | 19.5          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -168          |
| time/                   |               |
|    fps                  | 98            |
|    iterations           | 87            |
|    time_elapsed         | 1813          |
|    total_timesteps      | 178176        |
| train/                  |               |
|    approx_kl            | 0.00044315023 |
|    clip_fraction        | 0.00332       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0522       |
|    explained_variance   | 0.763         |
|    learning_rate        | 0.0003        |
|    loss                 | 18.6          |
|    n_updates            | 860           |
|    policy_gradient_loss | -0.000684     |
|    value_loss           | 29.7          |
-------------------------------------------
Num timesteps: 180000
Best mean reward: -156.73 - Last mean reward per episode: -165.60
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -166          |
| time/                   |               |
|    fps                  | 98            |
|    iterations           | 88            |
|    time_elapsed         | 1834          |
|    total_timesteps      | 180224        |
| train/                  |               |
|    approx_kl            | 0.00012210349 |
|    clip_fraction        | 0.0022        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0483       |
|    explained_variance   | 0.739         |
|    learning_rate        | 0.0003        |
|    loss                 | 13            |
|    n_updates            | 870           |
|    policy_gradient_loss | -0.000205     |
|    value_loss           | 30.8          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -166          |
| time/                   |               |
|    fps                  | 98            |
|    iterations           | 89            |
|    time_elapsed         | 1856          |
|    total_timesteps      | 182272        |
| train/                  |               |
|    approx_kl            | 5.1864627e-05 |
|    clip_fraction        | 0.00083       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0421       |
|    explained_variance   | 0.797         |
|    learning_rate        | 0.0003        |
|    loss                 | 7.07          |
|    n_updates            | 880           |
|    policy_gradient_loss | -0.000368     |
|    value_loss           | 27.5          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -166         |
| time/                   |              |
|    fps                  | 98           |
|    iterations           | 90           |
|    time_elapsed         | 1878         |
|    total_timesteps      | 184320       |
| train/                  |              |
|    approx_kl            | 0.0003170275 |
|    clip_fraction        | 0.00293      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0382      |
|    explained_variance   | 0.79         |
|    learning_rate        | 0.0003       |
|    loss                 | 11           |
|    n_updates            | 890          |
|    policy_gradient_loss | -0.000182    |
|    value_loss           | 24.7         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -167          |
| time/                   |               |
|    fps                  | 98            |
|    iterations           | 91            |
|    time_elapsed         | 1899          |
|    total_timesteps      | 186368        |
| train/                  |               |
|    approx_kl            | 0.00015369998 |
|    clip_fraction        | 0.00254       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0314       |
|    explained_variance   | 0.72          |
|    learning_rate        | 0.0003        |
|    loss                 | 13            |
|    n_updates            | 900           |
|    policy_gradient_loss | -0.000387     |
|    value_loss           | 28.6          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -169         |
| time/                   |              |
|    fps                  | 98           |
|    iterations           | 92           |
|    time_elapsed         | 1921         |
|    total_timesteps      | 188416       |
| train/                  |              |
|    approx_kl            | 9.828151e-05 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0342      |
|    explained_variance   | 0.777        |
|    learning_rate        | 0.0003       |
|    loss                 | 14.6         |
|    n_updates            | 910          |
|    policy_gradient_loss | 2.1e-05      |
|    value_loss           | 26.2         |
------------------------------------------
Num timesteps: 190000
Best mean reward: -156.73 - Last mean reward per episode: -168.12
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -168         |
| time/                   |              |
|    fps                  | 98           |
|    iterations           | 93           |
|    time_elapsed         | 1942         |
|    total_timesteps      | 190464       |
| train/                  |              |
|    approx_kl            | 0.0002637914 |
|    clip_fraction        | 0.00273      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0389      |
|    explained_variance   | 0.797        |
|    learning_rate        | 0.0003       |
|    loss                 | 16.8         |
|    n_updates            | 920          |
|    policy_gradient_loss | 5.05e-05     |
|    value_loss           | 30.5         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -167          |
| time/                   |               |
|    fps                  | 98            |
|    iterations           | 94            |
|    time_elapsed         | 1964          |
|    total_timesteps      | 192512        |
| train/                  |               |
|    approx_kl            | 0.00019756798 |
|    clip_fraction        | 0.00244       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0415       |
|    explained_variance   | 0.766         |
|    learning_rate        | 0.0003        |
|    loss                 | 10.1          |
|    n_updates            | 930           |
|    policy_gradient_loss | 0.00012       |
|    value_loss           | 21.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -169          |
| time/                   |               |
|    fps                  | 97            |
|    iterations           | 95            |
|    time_elapsed         | 1986          |
|    total_timesteps      | 194560        |
| train/                  |               |
|    approx_kl            | 0.00021623302 |
|    clip_fraction        | 0.00205       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0371       |
|    explained_variance   | 0.762         |
|    learning_rate        | 0.0003        |
|    loss                 | 12.4          |
|    n_updates            | 940           |
|    policy_gradient_loss | -0.000115     |
|    value_loss           | 23.3          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -168          |
| time/                   |               |
|    fps                  | 97            |
|    iterations           | 96            |
|    time_elapsed         | 2007          |
|    total_timesteps      | 196608        |
| train/                  |               |
|    approx_kl            | 0.00016907312 |
|    clip_fraction        | 0.00137       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0325       |
|    explained_variance   | 0.796         |
|    learning_rate        | 0.0003        |
|    loss                 | 10.3          |
|    n_updates            | 950           |
|    policy_gradient_loss | -0.000335     |
|    value_loss           | 27.5          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -166         |
| time/                   |              |
|    fps                  | 97           |
|    iterations           | 97           |
|    time_elapsed         | 2028         |
|    total_timesteps      | 198656       |
| train/                  |              |
|    approx_kl            | 0.0003041612 |
|    clip_fraction        | 0.00288      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0248      |
|    explained_variance   | 0.797        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.49         |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.000295    |
|    value_loss           | 23.2         |
------------------------------------------
Num timesteps: 200000
Best mean reward: -156.73 - Last mean reward per episode: -165.65
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -166          |
| time/                   |               |
|    fps                  | 97            |
|    iterations           | 98            |
|    time_elapsed         | 2049          |
|    total_timesteps      | 200704        |
| train/                  |               |
|    approx_kl            | 0.00011744877 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0244       |
|    explained_variance   | 0.795         |
|    learning_rate        | 0.0003        |
|    loss                 | 11            |
|    n_updates            | 970           |
|    policy_gradient_loss | -0.000113     |
|    value_loss           | 20.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -166          |
| time/                   |               |
|    fps                  | 97            |
|    iterations           | 99            |
|    time_elapsed         | 2073          |
|    total_timesteps      | 202752        |
| train/                  |               |
|    approx_kl            | 7.6679775e-05 |
|    clip_fraction        | 0.000928      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.025        |
|    explained_variance   | 0.812         |
|    learning_rate        | 0.0003        |
|    loss                 | 11.2          |
|    n_updates            | 980           |
|    policy_gradient_loss | -0.000301     |
|    value_loss           | 22.1          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -168          |
| time/                   |               |
|    fps                  | 97            |
|    iterations           | 100           |
|    time_elapsed         | 2095          |
|    total_timesteps      | 204800        |
| train/                  |               |
|    approx_kl            | 0.00016705177 |
|    clip_fraction        | 0.00239       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0202       |
|    explained_variance   | 0.825         |
|    learning_rate        | 0.0003        |
|    loss                 | 12.9          |
|    n_updates            | 990           |
|    policy_gradient_loss | -0.000553     |
|    value_loss           | 20.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -168          |
| time/                   |               |
|    fps                  | 97            |
|    iterations           | 101           |
|    time_elapsed         | 2116          |
|    total_timesteps      | 206848        |
| train/                  |               |
|    approx_kl            | 0.00013586486 |
|    clip_fraction        | 0.00146       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0218       |
|    explained_variance   | 0.826         |
|    learning_rate        | 0.0003        |
|    loss                 | 15.3          |
|    n_updates            | 1000          |
|    policy_gradient_loss | -0.000376     |
|    value_loss           | 22.7          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -169         |
| time/                   |              |
|    fps                  | 97           |
|    iterations           | 102          |
|    time_elapsed         | 2137         |
|    total_timesteps      | 208896       |
| train/                  |              |
|    approx_kl            | 4.413433e-05 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0243      |
|    explained_variance   | 0.791        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.4         |
|    n_updates            | 1010         |
|    policy_gradient_loss | 0.000168     |
|    value_loss           | 31.9         |
------------------------------------------
Num timesteps: 210000
Best mean reward: -156.73 - Last mean reward per episode: -170.20
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -170          |
| time/                   |               |
|    fps                  | 97            |
|    iterations           | 103           |
|    time_elapsed         | 2159          |
|    total_timesteps      | 210944        |
| train/                  |               |
|    approx_kl            | 6.0334714e-05 |
|    clip_fraction        | 0.00112       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0232       |
|    explained_variance   | 0.823         |
|    learning_rate        | 0.0003        |
|    loss                 | 11.6          |
|    n_updates            | 1020          |
|    policy_gradient_loss | -0.000318     |
|    value_loss           | 26.5          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -172         |
| time/                   |              |
|    fps                  | 97           |
|    iterations           | 104          |
|    time_elapsed         | 2182         |
|    total_timesteps      | 212992       |
| train/                  |              |
|    approx_kl            | 3.787261e-05 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.022       |
|    explained_variance   | 0.73         |
|    learning_rate        | 0.0003       |
|    loss                 | 12.2         |
|    n_updates            | 1030         |
|    policy_gradient_loss | -8.51e-05    |
|    value_loss           | 28.8         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -171          |
| time/                   |               |
|    fps                  | 97            |
|    iterations           | 105           |
|    time_elapsed         | 2203          |
|    total_timesteps      | 215040        |
| train/                  |               |
|    approx_kl            | 2.8610782e-05 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0248       |
|    explained_variance   | 0.751         |
|    learning_rate        | 0.0003        |
|    loss                 | 20.3          |
|    n_updates            | 1040          |
|    policy_gradient_loss | -9.6e-05      |
|    value_loss           | 25.1          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -170         |
| time/                   |              |
|    fps                  | 97           |
|    iterations           | 106          |
|    time_elapsed         | 2222         |
|    total_timesteps      | 217088       |
| train/                  |              |
|    approx_kl            | 8.909454e-05 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0231      |
|    explained_variance   | 0.768        |
|    learning_rate        | 0.0003       |
|    loss                 | 9.68         |
|    n_updates            | 1050         |
|    policy_gradient_loss | -9.77e-05    |
|    value_loss           | 22.5         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -170         |
| time/                   |              |
|    fps                  | 97           |
|    iterations           | 107          |
|    time_elapsed         | 2242         |
|    total_timesteps      | 219136       |
| train/                  |              |
|    approx_kl            | 7.670018e-05 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0231      |
|    explained_variance   | 0.771        |
|    learning_rate        | 0.0003       |
|    loss                 | 9.81         |
|    n_updates            | 1060         |
|    policy_gradient_loss | -0.000415    |
|    value_loss           | 23.8         |
------------------------------------------
Num timesteps: 220000
Best mean reward: -156.73 - Last mean reward per episode: -169.32
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -170         |
| time/                   |              |
|    fps                  | 97           |
|    iterations           | 108          |
|    time_elapsed         | 2263         |
|    total_timesteps      | 221184       |
| train/                  |              |
|    approx_kl            | 8.358099e-05 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0172      |
|    explained_variance   | 0.784        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.22         |
|    n_updates            | 1070         |
|    policy_gradient_loss | -0.000333    |
|    value_loss           | 21.7         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -170          |
| time/                   |               |
|    fps                  | 97            |
|    iterations           | 109           |
|    time_elapsed         | 2283          |
|    total_timesteps      | 223232        |
| train/                  |               |
|    approx_kl            | 8.7631895e-05 |
|    clip_fraction        | 0.000635      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0155       |
|    explained_variance   | 0.795         |
|    learning_rate        | 0.0003        |
|    loss                 | 6.11          |
|    n_updates            | 1080          |
|    policy_gradient_loss | -0.000226     |
|    value_loss           | 19.2          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -170          |
| time/                   |               |
|    fps                  | 97            |
|    iterations           | 110           |
|    time_elapsed         | 2303          |
|    total_timesteps      | 225280        |
| train/                  |               |
|    approx_kl            | 1.8503342e-06 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0168       |
|    explained_variance   | 0.815         |
|    learning_rate        | 0.0003        |
|    loss                 | 9.34          |
|    n_updates            | 1090          |
|    policy_gradient_loss | -6.37e-05     |
|    value_loss           | 20.6          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -170          |
| time/                   |               |
|    fps                  | 97            |
|    iterations           | 111           |
|    time_elapsed         | 2323          |
|    total_timesteps      | 227328        |
| train/                  |               |
|    approx_kl            | 1.7130078e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0195       |
|    explained_variance   | 0.805         |
|    learning_rate        | 0.0003        |
|    loss                 | 13.4          |
|    n_updates            | 1100          |
|    policy_gradient_loss | -9.52e-05     |
|    value_loss           | 26.7          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -168          |
| time/                   |               |
|    fps                  | 97            |
|    iterations           | 112           |
|    time_elapsed         | 2343          |
|    total_timesteps      | 229376        |
| train/                  |               |
|    approx_kl            | 6.7291665e-05 |
|    clip_fraction        | 0.00127       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0232       |
|    explained_variance   | 0.789         |
|    learning_rate        | 0.0003        |
|    loss                 | 10.6          |
|    n_updates            | 1110          |
|    policy_gradient_loss | -0.000162     |
|    value_loss           | 23.6          |
-------------------------------------------
Num timesteps: 230000
Best mean reward: -156.73 - Last mean reward per episode: -166.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -165         |
| time/                   |              |
|    fps                  | 97           |
|    iterations           | 113          |
|    time_elapsed         | 2364         |
|    total_timesteps      | 231424       |
| train/                  |              |
|    approx_kl            | 4.451201e-05 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.022       |
|    explained_variance   | 0.796        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.2         |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.000145    |
|    value_loss           | 26.4         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -165         |
| time/                   |              |
|    fps                  | 97           |
|    iterations           | 114          |
|    time_elapsed         | 2384         |
|    total_timesteps      | 233472       |
| train/                  |              |
|    approx_kl            | 9.529787e-05 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0177      |
|    explained_variance   | 0.768        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.27         |
|    n_updates            | 1130         |
|    policy_gradient_loss | -4.02e-05    |
|    value_loss           | 20.6         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -164          |
| time/                   |               |
|    fps                  | 97            |
|    iterations           | 115           |
|    time_elapsed         | 2404          |
|    total_timesteps      | 235520        |
| train/                  |               |
|    approx_kl            | 3.3854478e-05 |
|    clip_fraction        | 0.00122       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0212       |
|    explained_variance   | 0.77          |
|    learning_rate        | 0.0003        |
|    loss                 | 10.9          |
|    n_updates            | 1140          |
|    policy_gradient_loss | -0.000203     |
|    value_loss           | 24            |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -164          |
| time/                   |               |
|    fps                  | 97            |
|    iterations           | 116           |
|    time_elapsed         | 2425          |
|    total_timesteps      | 237568        |
| train/                  |               |
|    approx_kl            | 9.5300755e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0204       |
|    explained_variance   | 0.8           |
|    learning_rate        | 0.0003        |
|    loss                 | 8.92          |
|    n_updates            | 1150          |
|    policy_gradient_loss | -0.000166     |
|    value_loss           | 19.3          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -162          |
| time/                   |               |
|    fps                  | 98            |
|    iterations           | 117           |
|    time_elapsed         | 2444          |
|    total_timesteps      | 239616        |
| train/                  |               |
|    approx_kl            | 0.00010322084 |
|    clip_fraction        | 0.00127       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0265       |
|    explained_variance   | 0.776         |
|    learning_rate        | 0.0003        |
|    loss                 | 11.8          |
|    n_updates            | 1160          |
|    policy_gradient_loss | -5.58e-05     |
|    value_loss           | 24.8          |
-------------------------------------------
Num timesteps: 240000
Best mean reward: -156.73 - Last mean reward per episode: -161.74
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -161         |
| time/                   |              |
|    fps                  | 98           |
|    iterations           | 118          |
|    time_elapsed         | 2464         |
|    total_timesteps      | 241664       |
| train/                  |              |
|    approx_kl            | 8.497242e-05 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0292      |
|    explained_variance   | 0.756        |
|    learning_rate        | 0.0003       |
|    loss                 | 12.8         |
|    n_updates            | 1170         |
|    policy_gradient_loss | -0.000232    |
|    value_loss           | 30.4         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -160         |
| time/                   |              |
|    fps                  | 98           |
|    iterations           | 119          |
|    time_elapsed         | 2486         |
|    total_timesteps      | 243712       |
| train/                  |              |
|    approx_kl            | 6.636907e-05 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0334      |
|    explained_variance   | 0.762        |
|    learning_rate        | 0.0003       |
|    loss                 | 15.1         |
|    n_updates            | 1180         |
|    policy_gradient_loss | -0.000101    |
|    value_loss           | 21.5         |
------------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 291, in <module>
    model.learn(total_timesteps=1000000, callback=[WandbCallback(), save_callback])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 300, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 195, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 206, in step
    return self.step_wait()
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 72, in step_wait
    return (self._obs_from_buf(), np.copy(self.buf_rews), np.copy(self.buf_dones), deepcopy(self.buf_infos))
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/copy.py", line 203, in _deepcopy_list
    memo[id(x)] = y
KeyboardInterrupt