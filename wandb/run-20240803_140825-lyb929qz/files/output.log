
Using cpu device
/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead
  warnings.warn(out)
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -500     |
| time/              |          |
|    fps             | 828      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 315         |
|    iterations           | 2           |
|    time_elapsed         | 12          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.007599149 |
|    clip_fraction        | 0.0527      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.00945     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.796       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00141    |
|    value_loss           | 7.05        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 238          |
|    iterations           | 3            |
|    time_elapsed         | 25           |
|    total_timesteps      | 6144         |
| train/                  |              |
|    approx_kl            | 0.0041543394 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 0.111        |
|    n_updates            | 20           |
|    policy_gradient_loss | 8.18e-05     |
|    value_loss           | 15.8         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 203          |
|    iterations           | 4            |
|    time_elapsed         | 40           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.0056013456 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | -1.31e-06    |
|    learning_rate        | 0.0003       |
|    loss                 | 3.91         |
|    n_updates            | 30           |
|    policy_gradient_loss | 0.000433     |
|    value_loss           | 12.7         |
------------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -500.00
Saving new best model at 10000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 193         |
|    iterations           | 5           |
|    time_elapsed         | 52          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.009570722 |
|    clip_fraction        | 0.0345      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.108       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00143    |
|    value_loss           | 10.6        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 190         |
|    iterations           | 6           |
|    time_elapsed         | 64          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.010110429 |
|    clip_fraction        | 0.0284      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 5.92        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.000562   |
|    value_loss           | 9.74        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 187         |
|    iterations           | 7           |
|    time_elapsed         | 76          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.010454006 |
|    clip_fraction        | 0.00762     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.123       |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.000102    |
|    value_loss           | 9.98        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 185         |
|    iterations           | 8           |
|    time_elapsed         | 88          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.017064966 |
|    clip_fraction        | 0.0862      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.138       |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00208    |
|    value_loss           | 10.5        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 181          |
|    iterations           | 9            |
|    time_elapsed         | 101          |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0052898717 |
|    clip_fraction        | 0.00552      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.02        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.875        |
|    n_updates            | 80           |
|    policy_gradient_loss | 0.000594     |
|    value_loss           | 11.4         |
------------------------------------------
Num timesteps: 20000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 177         |
|    iterations           | 10          |
|    time_elapsed         | 115         |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.009789392 |
|    clip_fraction        | 0.045       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.963      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 8.16        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00246    |
|    value_loss           | 12.2        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 169         |
|    iterations           | 11          |
|    time_elapsed         | 132         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.005901469 |
|    clip_fraction        | 0.0354      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.956      |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 7.9         |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.000917   |
|    value_loss           | 13          |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 168         |
|    iterations           | 12          |
|    time_elapsed         | 146         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.005361586 |
|    clip_fraction        | 0.0453      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.979      |
|    explained_variance   | -4.05e-06   |
|    learning_rate        | 0.0003      |
|    loss                 | 12.4        |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00242    |
|    value_loss           | 13.7        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 167          |
|    iterations           | 13           |
|    time_elapsed         | 158          |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.0070307236 |
|    clip_fraction        | 0.0282       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.948       |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 2.76         |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00061     |
|    value_loss           | 14.3         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 165         |
|    iterations           | 14          |
|    time_elapsed         | 173         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.008079443 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.856      |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 3.83        |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0012     |
|    value_loss           | 14.8        |
-----------------------------------------
Num timesteps: 30000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 164          |
|    iterations           | 15           |
|    time_elapsed         | 186          |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.0031904452 |
|    clip_fraction        | 0.0343       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.827       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.71         |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.000321    |
|    value_loss           | 15.3         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -500          |
| time/                   |               |
|    fps                  | 164           |
|    iterations           | 16            |
|    time_elapsed         | 199           |
|    total_timesteps      | 32768         |
| train/                  |               |
|    approx_kl            | 0.00023434495 |
|    clip_fraction        | 0.0399        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.827        |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 12.8          |
|    n_updates            | 150           |
|    policy_gradient_loss | -0.00204      |
|    value_loss           | 15.7          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 162          |
|    iterations           | 17           |
|    time_elapsed         | 214          |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0022454378 |
|    clip_fraction        | 0.0237       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.802       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.262        |
|    n_updates            | 160          |
|    policy_gradient_loss | 9.82e-05     |
|    value_loss           | 16           |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 161          |
|    iterations           | 18           |
|    time_elapsed         | 228          |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.0017528783 |
|    clip_fraction        | 0.00732      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.813       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 7.29         |
|    n_updates            | 170          |
|    policy_gradient_loss | 0.000263     |
|    value_loss           | 16.2         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 160         |
|    iterations           | 19          |
|    time_elapsed         | 242         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.007356626 |
|    clip_fraction        | 0.069       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.715      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 0.31        |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.000788   |
|    value_loss           | 16.4        |
-----------------------------------------
Num timesteps: 40000
Best mean reward: -500.00 - Last mean reward per episode: -500.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 159          |
|    iterations           | 20           |
|    time_elapsed         | 256          |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0051470157 |
|    clip_fraction        | 0.057        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.693       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 5.54         |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00197     |
|    value_loss           | 16.5         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 159          |
|    iterations           | 21           |
|    time_elapsed         | 269          |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0039013017 |
|    clip_fraction        | 0.0437       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.706       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 16.1         |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00117     |
|    value_loss           | 16.7         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 159         |
|    iterations           | 22          |
|    time_elapsed         | 282         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.002948566 |
|    clip_fraction        | 0.0369      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 4.63        |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00154    |
|    value_loss           | 24.8        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | -500       |
| time/                   |            |
|    fps                  | 158        |
|    iterations           | 23         |
|    time_elapsed         | 297        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.00295691 |
|    clip_fraction        | 0.0512     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.57      |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0003     |
|    loss                 | 14.7       |
|    n_updates            | 220        |
|    policy_gradient_loss | -7.79e-05  |
|    value_loss           | 18.1       |
----------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 291, in <module>
    model.learn(total_timesteps=1000000, callback=[WandbCallback(), save_callback])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 300, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 179, in collect_rollouts
    actions, values, log_probs = self.policy(obs_tensor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 645, in forward
    features = self.extract_features(obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/policy.py", line 49, in forward
    obj_emb = self.model(pyg_data, pyg_data.batch_dict)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1696, in __getattr__
    def __getattr__(self, name: str) -> Any:
KeyboardInterrupt