
Using cpu device
/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead
  warnings.warn(out)
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -500     |
| time/              |          |
|    fps             | 805      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -495        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 2           |
|    time_elapsed         | 13          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.011873631 |
|    clip_fraction        | 0.0615      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | -0.125      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.722       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00332    |
|    value_loss           | 5.77        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -472        |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 3           |
|    time_elapsed         | 27          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.004710245 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 5.42e-06    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.239       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.000827   |
|    value_loss           | 16.2        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | -469       |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 4          |
|    time_elapsed         | 40         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.01239349 |
|    clip_fraction        | 0.0649     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.04      |
|    explained_variance   | 0.00276    |
|    learning_rate        | 0.0003     |
|    loss                 | 8.33       |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.00647   |
|    value_loss           | 21.6       |
----------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -453.00
Saving new best model at 10000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -453        |
| time/                   |             |
|    fps                  | 175         |
|    iterations           | 5           |
|    time_elapsed         | 58          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.005672585 |
|    clip_fraction        | 0.0162      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.991      |
|    explained_variance   | 0.00144     |
|    learning_rate        | 0.0003      |
|    loss                 | 3.51        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00054    |
|    value_loss           | 15.5        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -436         |
| time/                   |              |
|    fps                  | 166          |
|    iterations           | 6            |
|    time_elapsed         | 73           |
|    total_timesteps      | 12288        |
| train/                  |              |
|    approx_kl            | 0.0111872945 |
|    clip_fraction        | 0.0576       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.956       |
|    explained_variance   | 0.00369      |
|    learning_rate        | 0.0003       |
|    loss                 | 9.3          |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00355     |
|    value_loss           | 22           |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -418        |
| time/                   |             |
|    fps                  | 158         |
|    iterations           | 7           |
|    time_elapsed         | 90          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.007072052 |
|    clip_fraction        | 0.0265      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.923      |
|    explained_variance   | 0.0304      |
|    learning_rate        | 0.0003      |
|    loss                 | 18.7        |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00414    |
|    value_loss           | 35.2        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -399        |
| time/                   |             |
|    fps                  | 157         |
|    iterations           | 8           |
|    time_elapsed         | 104         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.013274786 |
|    clip_fraction        | 0.0887      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.853      |
|    explained_variance   | 0.138       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.29        |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00556    |
|    value_loss           | 34.1        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -383        |
| time/                   |             |
|    fps                  | 80          |
|    iterations           | 9           |
|    time_elapsed         | 228         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.012106332 |
|    clip_fraction        | 0.0515      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.795      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 15.1        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00339    |
|    value_loss           | 42.1        |
-----------------------------------------
Num timesteps: 20000
Best mean reward: -453.00 - Last mean reward per episode: -364.00
Saving new best model at 20000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -364        |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 10          |
|    time_elapsed         | 247         |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.009134334 |
|    clip_fraction        | 0.0548      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.685      |
|    explained_variance   | 0.13        |
|    learning_rate        | 0.0003      |
|    loss                 | 11.3        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00491    |
|    value_loss           | 32.4        |
-----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -350          |
| time/                   |               |
|    fps                  | 86            |
|    iterations           | 11            |
|    time_elapsed         | 261           |
|    total_timesteps      | 22528         |
| train/                  |               |
|    approx_kl            | 0.00094582664 |
|    clip_fraction        | 0.0176        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.638        |
|    explained_variance   | 0.13          |
|    learning_rate        | 0.0003        |
|    loss                 | 27.1          |
|    n_updates            | 100           |
|    policy_gradient_loss | -0.000786     |
|    value_loss           | 39.5          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -339         |
| time/                   |              |
|    fps                  | 89           |
|    iterations           | 12           |
|    time_elapsed         | 275          |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0030689156 |
|    clip_fraction        | 0.0287       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.621       |
|    explained_variance   | 0.00534      |
|    learning_rate        | 0.0003       |
|    loss                 | 20.5         |
|    n_updates            | 110          |
|    policy_gradient_loss | 0.000412     |
|    value_loss           | 37.4         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -325         |
| time/                   |              |
|    fps                  | 91           |
|    iterations           | 13           |
|    time_elapsed         | 289          |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.0055659004 |
|    clip_fraction        | 0.0532       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.555       |
|    explained_variance   | 0.157        |
|    learning_rate        | 0.0003       |
|    loss                 | 16.5         |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00356     |
|    value_loss           | 38.3         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -310         |
| time/                   |              |
|    fps                  | 94           |
|    iterations           | 14           |
|    time_elapsed         | 303          |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0045273146 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.483       |
|    explained_variance   | 0.235        |
|    learning_rate        | 0.0003       |
|    loss                 | 24.6         |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00293     |
|    value_loss           | 38.9         |
------------------------------------------
Num timesteps: 30000
Best mean reward: -364.00 - Last mean reward per episode: -297.00
Saving new best model at 30000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -297         |
| time/                   |              |
|    fps                  | 96           |
|    iterations           | 15           |
|    time_elapsed         | 318          |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.0014640007 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.466       |
|    explained_variance   | 0.413        |
|    learning_rate        | 0.0003       |
|    loss                 | 9.84         |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00157     |
|    value_loss           | 33.2         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -286        |
| time/                   |             |
|    fps                  | 98          |
|    iterations           | 16          |
|    time_elapsed         | 331         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.004804097 |
|    clip_fraction        | 0.0646      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.386      |
|    explained_variance   | 0.479       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.7        |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0043     |
|    value_loss           | 34.8        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -275         |
| time/                   |              |
|    fps                  | 100          |
|    iterations           | 17           |
|    time_elapsed         | 347          |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0007057547 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.381       |
|    explained_variance   | 0.53         |
|    learning_rate        | 0.0003       |
|    loss                 | 17.5         |
|    n_updates            | 160          |
|    policy_gradient_loss | -5.56e-05    |
|    value_loss           | 36.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -262         |
| time/                   |              |
|    fps                  | 101          |
|    iterations           | 18           |
|    time_elapsed         | 361          |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.0005230368 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.394       |
|    explained_variance   | 0.388        |
|    learning_rate        | 0.0003       |
|    loss                 | 19.8         |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.000208    |
|    value_loss           | 34.5         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -257         |
| time/                   |              |
|    fps                  | 103          |
|    iterations           | 19           |
|    time_elapsed         | 376          |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0019706367 |
|    clip_fraction        | 0.0289       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.314       |
|    explained_variance   | 0.438        |
|    learning_rate        | 0.0003       |
|    loss                 | 17           |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00198     |
|    value_loss           | 34.1         |
------------------------------------------
Num timesteps: 40000
Best mean reward: -297.00 - Last mean reward per episode: -250.75
Saving new best model at 40000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -251         |
| time/                   |              |
|    fps                  | 104          |
|    iterations           | 20           |
|    time_elapsed         | 391          |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0023520743 |
|    clip_fraction        | 0.0246       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.34        |
|    explained_variance   | 0.396        |
|    learning_rate        | 0.0003       |
|    loss                 | 16.8         |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.0019      |
|    value_loss           | 36.3         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -240        |
| time/                   |             |
|    fps                  | 105         |
|    iterations           | 21          |
|    time_elapsed         | 405         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.001224342 |
|    clip_fraction        | 0.00234     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.349      |
|    explained_variance   | 0.483       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.7        |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0002     |
|    value_loss           | 34.1        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -236         |
| time/                   |              |
|    fps                  | 107          |
|    iterations           | 22           |
|    time_elapsed         | 419          |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0017234395 |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.407       |
|    explained_variance   | 0.588        |
|    learning_rate        | 0.0003       |
|    loss                 | 10.2         |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.000351    |
|    value_loss           | 28.6         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -228          |
| time/                   |               |
|    fps                  | 108           |
|    iterations           | 23            |
|    time_elapsed         | 434           |
|    total_timesteps      | 47104         |
| train/                  |               |
|    approx_kl            | 0.00068872777 |
|    clip_fraction        | 0.0116        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.388        |
|    explained_variance   | 0.512         |
|    learning_rate        | 0.0003        |
|    loss                 | 21.4          |
|    n_updates            | 220           |
|    policy_gradient_loss | -0.000187     |
|    value_loss           | 36.2          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -222         |
| time/                   |              |
|    fps                  | 109          |
|    iterations           | 24           |
|    time_elapsed         | 447          |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0023493632 |
|    clip_fraction        | 0.0215       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.355       |
|    explained_variance   | 0.495        |
|    learning_rate        | 0.0003       |
|    loss                 | 18.4         |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00121     |
|    value_loss           | 32.1         |
------------------------------------------
Num timesteps: 50000
Best mean reward: -250.75 - Last mean reward per episode: -218.60
Saving new best model at 50000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -216         |
| time/                   |              |
|    fps                  | 110          |
|    iterations           | 25           |
|    time_elapsed         | 465          |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0024705606 |
|    clip_fraction        | 0.0275       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.305       |
|    explained_variance   | 0.562        |
|    learning_rate        | 0.0003       |
|    loss                 | 14.7         |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00231     |
|    value_loss           | 28.6         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -210         |
| time/                   |              |
|    fps                  | 110          |
|    iterations           | 26           |
|    time_elapsed         | 483          |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0011653067 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.264       |
|    explained_variance   | 0.485        |
|    learning_rate        | 0.0003       |
|    loss                 | 19.5         |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00137     |
|    value_loss           | 34.9         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -207        |
| time/                   |             |
|    fps                  | 111         |
|    iterations           | 27          |
|    time_elapsed         | 496         |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.002011374 |
|    clip_fraction        | 0.0257      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.244      |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.1        |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00295    |
|    value_loss           | 26.8        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -204        |
| time/                   |             |
|    fps                  | 112         |
|    iterations           | 28          |
|    time_elapsed         | 510         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.000994782 |
|    clip_fraction        | 0.0161      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.216      |
|    explained_variance   | 0.554       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.9        |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00125    |
|    value_loss           | 28          |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -199         |
| time/                   |              |
|    fps                  | 113          |
|    iterations           | 29           |
|    time_elapsed         | 524          |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0011709365 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.206       |
|    explained_variance   | 0.644        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.4         |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.000752    |
|    value_loss           | 31.6         |
------------------------------------------
Num timesteps: 60000
Best mean reward: -218.60 - Last mean reward per episode: -198.00
Saving new best model at 60000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -196        |
| time/                   |             |
|    fps                  | 114         |
|    iterations           | 30          |
|    time_elapsed         | 538         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.003451853 |
|    clip_fraction        | 0.0211      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.161      |
|    explained_variance   | 0.623       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.6        |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00211    |
|    value_loss           | 22.4        |
-----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -191          |
| time/                   |               |
|    fps                  | 115           |
|    iterations           | 31            |
|    time_elapsed         | 551           |
|    total_timesteps      | 63488         |
| train/                  |               |
|    approx_kl            | 0.00069373136 |
|    clip_fraction        | 0.00376       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.142        |
|    explained_variance   | 0.689         |
|    learning_rate        | 0.0003        |
|    loss                 | 9.66          |
|    n_updates            | 300           |
|    policy_gradient_loss | -0.000338     |
|    value_loss           | 22.3          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -188          |
| time/                   |               |
|    fps                  | 115           |
|    iterations           | 32            |
|    time_elapsed         | 569           |
|    total_timesteps      | 65536         |
| train/                  |               |
|    approx_kl            | 0.00045188397 |
|    clip_fraction        | 0.00444       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.123        |
|    explained_variance   | 0.676         |
|    learning_rate        | 0.0003        |
|    loss                 | 10.6          |
|    n_updates            | 310           |
|    policy_gradient_loss | -0.000681     |
|    value_loss           | 22.7          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -188          |
| time/                   |               |
|    fps                  | 115           |
|    iterations           | 33            |
|    time_elapsed         | 584           |
|    total_timesteps      | 67584         |
| train/                  |               |
|    approx_kl            | 0.00073527166 |
|    clip_fraction        | 0.00366       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.127        |
|    explained_variance   | 0.676         |
|    learning_rate        | 0.0003        |
|    loss                 | 10.5          |
|    n_updates            | 320           |
|    policy_gradient_loss | -0.000418     |
|    value_loss           | 24.2          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -185         |
| time/                   |              |
|    fps                  | 115          |
|    iterations           | 34           |
|    time_elapsed         | 601          |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.0004969527 |
|    clip_fraction        | 0.00654      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.124       |
|    explained_variance   | 0.68         |
|    learning_rate        | 0.0003       |
|    loss                 | 14.8         |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.000604    |
|    value_loss           | 24.2         |
------------------------------------------
Num timesteps: 70000
Best mean reward: -198.00 - Last mean reward per episode: -184.57
Saving new best model at 70000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -182          |
| time/                   |               |
|    fps                  | 116           |
|    iterations           | 35            |
|    time_elapsed         | 617           |
|    total_timesteps      | 71680         |
| train/                  |               |
|    approx_kl            | 0.00061144633 |
|    clip_fraction        | 0.00591       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.115        |
|    explained_variance   | 0.686         |
|    learning_rate        | 0.0003        |
|    loss                 | 12.6          |
|    n_updates            | 340           |
|    policy_gradient_loss | -0.00115      |
|    value_loss           | 27.6          |
-------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | -178       |
| time/                   |            |
|    fps                  | 116        |
|    iterations           | 36         |
|    time_elapsed         | 632        |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.00067982 |
|    clip_fraction        | 0.00752    |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.102     |
|    explained_variance   | 0.668      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.5       |
|    n_updates            | 350        |
|    policy_gradient_loss | -0.000317  |
|    value_loss           | 26         |
----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -173          |
| time/                   |               |
|    fps                  | 116           |
|    iterations           | 37            |
|    time_elapsed         | 650           |
|    total_timesteps      | 75776         |
| train/                  |               |
|    approx_kl            | 0.00026673256 |
|    clip_fraction        | 0.00537       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0878       |
|    explained_variance   | 0.67          |
|    learning_rate        | 0.0003        |
|    loss                 | 10.4          |
|    n_updates            | 360           |
|    policy_gradient_loss | -0.000175     |
|    value_loss           | 26.9          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -172          |
| time/                   |               |
|    fps                  | 117           |
|    iterations           | 38            |
|    time_elapsed         | 664           |
|    total_timesteps      | 77824         |
| train/                  |               |
|    approx_kl            | 0.00035383832 |
|    clip_fraction        | 0.0061        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0864       |
|    explained_variance   | 0.676         |
|    learning_rate        | 0.0003        |
|    loss                 | 13            |
|    n_updates            | 370           |
|    policy_gradient_loss | -0.000569     |
|    value_loss           | 26.3          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -171         |
| time/                   |              |
|    fps                  | 117          |
|    iterations           | 39           |
|    time_elapsed         | 679          |
|    total_timesteps      | 79872        |
| train/                  |              |
|    approx_kl            | 0.0011582173 |
|    clip_fraction        | 0.00562      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0969      |
|    explained_variance   | 0.648        |
|    learning_rate        | 0.0003       |
|    loss                 | 14.5         |
|    n_updates            | 380          |
|    policy_gradient_loss | 0.000543     |
|    value_loss           | 24.5         |
------------------------------------------
Num timesteps: 80000
Best mean reward: -184.57 - Last mean reward per episode: -168.62
Saving new best model at 80000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -168          |
| time/                   |               |
|    fps                  | 117           |
|    iterations           | 40            |
|    time_elapsed         | 695           |
|    total_timesteps      | 81920         |
| train/                  |               |
|    approx_kl            | 0.00026536747 |
|    clip_fraction        | 0.00488       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0851       |
|    explained_variance   | 0.67          |
|    learning_rate        | 0.0003        |
|    loss                 | 11.6          |
|    n_updates            | 390           |
|    policy_gradient_loss | -0.00069      |
|    value_loss           | 25.8          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -167          |
| time/                   |               |
|    fps                  | 117           |
|    iterations           | 41            |
|    time_elapsed         | 712           |
|    total_timesteps      | 83968         |
| train/                  |               |
|    approx_kl            | 0.00033457865 |
|    clip_fraction        | 0.00522       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0886       |
|    explained_variance   | 0.651         |
|    learning_rate        | 0.0003        |
|    loss                 | 12.3          |
|    n_updates            | 400           |
|    policy_gradient_loss | -0.000872     |
|    value_loss           | 25.2          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -164         |
| time/                   |              |
|    fps                  | 118          |
|    iterations           | 42           |
|    time_elapsed         | 727          |
|    total_timesteps      | 86016        |
| train/                  |              |
|    approx_kl            | 0.0021043473 |
|    clip_fraction        | 0.0062       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0995      |
|    explained_variance   | 0.699        |
|    learning_rate        | 0.0003       |
|    loss                 | 15           |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.000843    |
|    value_loss           | 23.4         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -162         |
| time/                   |              |
|    fps                  | 118          |
|    iterations           | 43           |
|    time_elapsed         | 741          |
|    total_timesteps      | 88064        |
| train/                  |              |
|    approx_kl            | 0.0047904765 |
|    clip_fraction        | 0.00723      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0906      |
|    explained_variance   | 0.687        |
|    learning_rate        | 0.0003       |
|    loss                 | 16.7         |
|    n_updates            | 420          |
|    policy_gradient_loss | 0.00167      |
|    value_loss           | 26.3         |
------------------------------------------
Num timesteps: 90000
Best mean reward: -168.62 - Last mean reward per episode: -158.44
Saving new best model at 90000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 119           |
|    iterations           | 44            |
|    time_elapsed         | 754           |
|    total_timesteps      | 90112         |
| train/                  |               |
|    approx_kl            | 0.00077569764 |
|    clip_fraction        | 0.00747       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0768       |
|    explained_variance   | 0.639         |
|    learning_rate        | 0.0003        |
|    loss                 | 12.3          |
|    n_updates            | 430           |
|    policy_gradient_loss | -0.000285     |
|    value_loss           | 25.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -157          |
| time/                   |               |
|    fps                  | 119           |
|    iterations           | 45            |
|    time_elapsed         | 769           |
|    total_timesteps      | 92160         |
| train/                  |               |
|    approx_kl            | 0.00023594432 |
|    clip_fraction        | 0.0041        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0648       |
|    explained_variance   | 0.69          |
|    learning_rate        | 0.0003        |
|    loss                 | 14.4          |
|    n_updates            | 440           |
|    policy_gradient_loss | -0.000467     |
|    value_loss           | 27            |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -154          |
| time/                   |               |
|    fps                  | 120           |
|    iterations           | 46            |
|    time_elapsed         | 784           |
|    total_timesteps      | 94208         |
| train/                  |               |
|    approx_kl            | 0.00032197693 |
|    clip_fraction        | 0.00352       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.061        |
|    explained_variance   | 0.68          |
|    learning_rate        | 0.0003        |
|    loss                 | 15.6          |
|    n_updates            | 450           |
|    policy_gradient_loss | 0.00011       |
|    value_loss           | 27.6          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -151          |
| time/                   |               |
|    fps                  | 120           |
|    iterations           | 47            |
|    time_elapsed         | 796           |
|    total_timesteps      | 96256         |
| train/                  |               |
|    approx_kl            | 0.00040100238 |
|    clip_fraction        | 0.00601       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0699       |
|    explained_variance   | 0.705         |
|    learning_rate        | 0.0003        |
|    loss                 | 10.7          |
|    n_updates            | 460           |
|    policy_gradient_loss | -0.000421     |
|    value_loss           | 22.7          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -148          |
| time/                   |               |
|    fps                  | 121           |
|    iterations           | 48            |
|    time_elapsed         | 809           |
|    total_timesteps      | 98304         |
| train/                  |               |
|    approx_kl            | 0.00033026183 |
|    clip_fraction        | 0.00322       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0618       |
|    explained_variance   | 0.702         |
|    learning_rate        | 0.0003        |
|    loss                 | 7.89          |
|    n_updates            | 470           |
|    policy_gradient_loss | -2.26e-05     |
|    value_loss           | 22.7          |
-------------------------------------------
Num timesteps: 100000
Best mean reward: -158.44 - Last mean reward per episode: -145.70
Saving new best model at 100000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -146          |
| time/                   |               |
|    fps                  | 121           |
|    iterations           | 49            |
|    time_elapsed         | 823           |
|    total_timesteps      | 100352        |
| train/                  |               |
|    approx_kl            | 0.00012956338 |
|    clip_fraction        | 0.00244       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0681       |
|    explained_variance   | 0.698         |
|    learning_rate        | 0.0003        |
|    loss                 | 8.15          |
|    n_updates            | 480           |
|    policy_gradient_loss | -0.000144     |
|    value_loss           | 22.4          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -138          |
| time/                   |               |
|    fps                  | 122           |
|    iterations           | 50            |
|    time_elapsed         | 836           |
|    total_timesteps      | 102400        |
| train/                  |               |
|    approx_kl            | 0.00013828385 |
|    clip_fraction        | 0.00303       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0665       |
|    explained_variance   | 0.709         |
|    learning_rate        | 0.0003        |
|    loss                 | 13.6          |
|    n_updates            | 490           |
|    policy_gradient_loss | -0.000184     |
|    value_loss           | 23.8          |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -130        |
| time/                   |             |
|    fps                  | 122         |
|    iterations           | 51          |
|    time_elapsed         | 849         |
|    total_timesteps      | 104448      |
| train/                  |             |
|    approx_kl            | 0.022102868 |
|    clip_fraction        | 0.0131      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0734     |
|    explained_variance   | 0.693       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.9        |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.000355   |
|    value_loss           | 24.8        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -125         |
| time/                   |              |
|    fps                  | 123          |
|    iterations           | 52           |
|    time_elapsed         | 863          |
|    total_timesteps      | 106496       |
| train/                  |              |
|    approx_kl            | 0.0010991677 |
|    clip_fraction        | 0.0062       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0811      |
|    explained_variance   | 0.516        |
|    learning_rate        | 0.0003       |
|    loss                 | 13.2         |
|    n_updates            | 510          |
|    policy_gradient_loss | 0.00021      |
|    value_loss           | 24.6         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | -118          |
| time/                   |               |
|    fps                  | 123           |
|    iterations           | 53            |
|    time_elapsed         | 877           |
|    total_timesteps      | 108544        |
| train/                  |               |
|    approx_kl            | 0.00043480418 |
|    clip_fraction        | 0.00566       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0943       |
|    explained_variance   | 0.652         |
|    learning_rate        | 0.0003        |
|    loss                 | 13.3          |
|    n_updates            | 520           |
|    policy_gradient_loss | -0.000945     |
|    value_loss           | 23            |
-------------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 291, in <module>
    model.learn(total_timesteps=1000000, callback=[WandbCallback(), save_callback])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 300, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 179, in collect_rollouts
    actions, values, log_probs = self.policy(obs_tensor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 645, in forward
    features = self.extract_features(obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/policy.py", line 43, in forward
    pyg_data = self.encoder.encode(observations)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/encoder/GraphEncoder.py", line 598, in encode
    return Batch.from_data_list(self.to_pyg_data(batch_data))
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/data/batch.py", line 97, in from_data_list
    batch, slice_dict, inc_dict = collate(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/data/collate.py", line 109, in collate
    value, slices, incs = _collate(attr, values, data_list, stores,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/data/collate.py", line 169, in _collate
    incs = get_incs(key, values, data_list, stores)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/data/collate.py", line 329, in get_incs
    return cumsum(repeats[:-1])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/utils/functions.py", line 24, in cumsum
    torch.cumsum(x, dim=dim, out=out.narrow(dim, 1, x.size(dim)))
KeyboardInterrupt