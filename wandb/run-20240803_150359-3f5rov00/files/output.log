
Using cpu device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 394      |
|    iterations      | 1        |
|    time_elapsed    | 5        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -181        |
| time/                   |             |
|    fps                  | 135         |
|    iterations           | 2           |
|    time_elapsed         | 30          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.014827892 |
|    clip_fraction        | 0.357       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.00943     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.619       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0423     |
|    value_loss           | 1.96        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -168        |
| time/                   |             |
|    fps                  | 97          |
|    iterations           | 3           |
|    time_elapsed         | 62          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.012208572 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.0299      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.913       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 3.54        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -114        |
| time/                   |             |
|    fps                  | 89          |
|    iterations           | 4           |
|    time_elapsed         | 91          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.017285448 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.997      |
|    explained_variance   | 0.0985      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.76        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 7.56        |
-----------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -47.70
Saving new best model at 10000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -47.7       |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 5           |
|    time_elapsed         | 121         |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.016701128 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.905      |
|    explained_variance   | 0.384       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.01        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 5.18        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 24.7        |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 6           |
|    time_elapsed         | 149         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.013836961 |
|    clip_fraction        | 0.0997      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.807      |
|    explained_variance   | 0.579       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.17        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 11.5        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 87.5       |
| time/                   |            |
|    fps                  | 80         |
|    iterations           | 7          |
|    time_elapsed         | 177        |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.01116804 |
|    clip_fraction        | 0.101      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.685     |
|    explained_variance   | 0.691      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.34       |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.012     |
|    value_loss           | 17.7       |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 130          |
| time/                   |              |
|    fps                  | 79           |
|    iterations           | 8            |
|    time_elapsed         | 206          |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 0.0071211266 |
|    clip_fraction        | 0.0824       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.571       |
|    explained_variance   | 0.629        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.27         |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00698     |
|    value_loss           | 25.6         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 162         |
| time/                   |             |
|    fps                  | 78          |
|    iterations           | 9           |
|    time_elapsed         | 236         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.007269459 |
|    clip_fraction        | 0.0603      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.478      |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.59        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00246    |
|    value_loss           | 22.7        |
-----------------------------------------
Num timesteps: 20000
Best mean reward: -47.70 - Last mean reward per episode: 200.35
Saving new best model at 20000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 200         |
| time/                   |             |
|    fps                  | 76          |
|    iterations           | 10          |
|    time_elapsed         | 268         |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.001425069 |
|    clip_fraction        | 0.0085      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.459      |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.08        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00176    |
|    value_loss           | 17.2        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 228         |
| time/                   |             |
|    fps                  | 74          |
|    iterations           | 11          |
|    time_elapsed         | 302         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.003840515 |
|    clip_fraction        | 0.0359      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.432      |
|    explained_variance   | 0.581       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.42        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00301    |
|    value_loss           | 34.3        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 252          |
| time/                   |              |
|    fps                  | 72           |
|    iterations           | 12           |
|    time_elapsed         | 336          |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0029883394 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.441       |
|    explained_variance   | 0.593        |
|    learning_rate        | 0.0003       |
|    loss                 | 12.4         |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00196     |
|    value_loss           | 21           |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 282          |
| time/                   |              |
|    fps                  | 72           |
|    iterations           | 13           |
|    time_elapsed         | 367          |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.0042641363 |
|    clip_fraction        | 0.0512       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.385       |
|    explained_variance   | 0.254        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.79         |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00325     |
|    value_loss           | 26.5         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 302         |
| time/                   |             |
|    fps                  | 71          |
|    iterations           | 14          |
|    time_elapsed         | 400         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.004246694 |
|    clip_fraction        | 0.0425      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.32       |
|    explained_variance   | 0.559       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.5         |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00122    |
|    value_loss           | 31.6        |
-----------------------------------------
Num timesteps: 30000
Best mean reward: 200.35 - Last mean reward per episode: 325.33
Saving new best model at 30000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 325         |
| time/                   |             |
|    fps                  | 70          |
|    iterations           | 15          |
|    time_elapsed         | 432         |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.004518733 |
|    clip_fraction        | 0.0419      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.302      |
|    explained_variance   | 0.559       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.19        |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00366    |
|    value_loss           | 21.9        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 348          |
| time/                   |              |
|    fps                  | 69           |
|    iterations           | 16           |
|    time_elapsed         | 468          |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0009841202 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.25        |
|    explained_variance   | 0.531        |
|    learning_rate        | 0.0003       |
|    loss                 | 16           |
|    n_updates            | 150          |
|    policy_gradient_loss | 0.000178     |
|    value_loss           | 36.2         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 367          |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 17           |
|    time_elapsed         | 506          |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0013213358 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.234       |
|    explained_variance   | 0.548        |
|    learning_rate        | 0.0003       |
|    loss                 | 14.8         |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00124     |
|    value_loss           | 32.2         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 380         |
| time/                   |             |
|    fps                  | 67          |
|    iterations           | 18          |
|    time_elapsed         | 542         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.001256665 |
|    clip_fraction        | 0.0148      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.235      |
|    explained_variance   | 0.463       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.62        |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.000757   |
|    value_loss           | 29.5        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 391         |
| time/                   |             |
|    fps                  | 67          |
|    iterations           | 19          |
|    time_elapsed         | 577         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.003876931 |
|    clip_fraction        | 0.0272      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.247      |
|    explained_variance   | 0.469       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.89        |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00189    |
|    value_loss           | 17.9        |
-----------------------------------------
Num timesteps: 40000
Best mean reward: 325.33 - Last mean reward per episode: 400.38
Saving new best model at 40000 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 400          |
| time/                   |              |
|    fps                  | 67           |
|    iterations           | 20           |
|    time_elapsed         | 608          |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0040945215 |
|    clip_fraction        | 0.0208       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.261       |
|    explained_variance   | 0.446        |
|    learning_rate        | 0.0003       |
|    loss                 | 5            |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00305     |
|    value_loss           | 24.3         |
------------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 291, in <module>
    model.learn(total_timesteps=1000000, callback=[WandbCallback(), save_callback])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 300, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 179, in collect_rollouts
    actions, values, log_probs = self.policy(obs_tensor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 645, in forward
    features = self.extract_features(obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/policy.py", line 48, in forward
    obj_emb = self.model(pyg_data.x_dict, pyg_data.edge_index_dict, pyg_data.batch_dict)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_gnn.py", line 104, in forward
    self.layer(x_dict, edge_index_dict)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_gnn.py", line 85, in layer
    obj_emb = self.obj_update(obj_emb)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/models/mlp.py", line 245, in forward
    x = self.lins[-1](x)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/dense/linear.py", line 147, in forward
    return F.linear(x, self.weight, self.bias)
KeyboardInterrupt