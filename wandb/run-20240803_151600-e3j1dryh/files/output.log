
Using cpu device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -360     |
| time/              |          |
|    fps             | 424      |
|    iterations      | 1        |
|    time_elapsed    | 4        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -380        |
| time/                   |             |
|    fps                  | 148         |
|    iterations           | 2           |
|    time_elapsed         | 27          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.012804006 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | -0.00685    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.789       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 2.22        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 844        |
|    ep_rew_mean          | -308       |
| time/                   |            |
|    fps                  | 114        |
|    iterations           | 3          |
|    time_elapsed         | 53         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.01182998 |
|    clip_fraction        | 0.0425     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.07      |
|    explained_variance   | -4.29e-06  |
|    learning_rate        | 0.0003     |
|    loss                 | 0.763      |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.00382   |
|    value_loss           | 9.28       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 576         |
|    ep_rew_mean          | -198        |
| time/                   |             |
|    fps                  | 103         |
|    iterations           | 4           |
|    time_elapsed         | 78          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.016448053 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.989      |
|    explained_variance   | -0.0126     |
|    learning_rate        | 0.0003      |
|    loss                 | 6.27        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00925    |
|    value_loss           | 23.4        |
-----------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -182.44
Saving new best model at 9687 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 538         |
|    ep_rew_mean          | -182        |
| time/                   |             |
|    fps                  | 98          |
|    iterations           | 5           |
|    time_elapsed         | 103         |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.011536179 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.909      |
|    explained_variance   | 0.00343     |
|    learning_rate        | 0.0003      |
|    loss                 | 13          |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 51.4        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 489          |
|    ep_rew_mean          | -163         |
| time/                   |              |
|    fps                  | 90           |
|    iterations           | 6            |
|    time_elapsed         | 135          |
|    total_timesteps      | 12288        |
| train/                  |              |
|    approx_kl            | 0.0011522043 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.928       |
|    explained_variance   | 0.00436      |
|    learning_rate        | 0.0003       |
|    loss                 | 17.6         |
|    n_updates            | 50           |
|    policy_gradient_loss | 0.000841     |
|    value_loss           | 33           |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 404         |
|    ep_rew_mean          | -130        |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 7           |
|    time_elapsed         | 165         |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.007808844 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.833      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 46.4        |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0048     |
|    value_loss           | 59.6        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 324         |
|    ep_rew_mean          | -98.6       |
| time/                   |             |
|    fps                  | 85          |
|    iterations           | 8           |
|    time_elapsed         | 190         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.010185138 |
|    clip_fraction        | 0.0916      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.758      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 52.8        |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00614    |
|    value_loss           | 89.7        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 297          |
|    ep_rew_mean          | -87.5        |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 9            |
|    time_elapsed         | 217          |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0027704635 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.754       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 23.1         |
|    n_updates            | 80           |
|    policy_gradient_loss | 0.00053      |
|    value_loss           | 107          |
------------------------------------------
Num timesteps: 20000
Best mean reward: -182.44 - Last mean reward per episode: -90.40
Saving new best model at 19645 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 296          |
|    ep_rew_mean          | -87.9        |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 10           |
|    time_elapsed         | 241          |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.0057323556 |
|    clip_fraction        | 0.0514       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.756       |
|    explained_variance   | 0.559        |
|    learning_rate        | 0.0003       |
|    loss                 | 8.97         |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00133     |
|    value_loss           | 25.4         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 274         |
|    ep_rew_mean          | -79.1       |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 11          |
|    time_elapsed         | 265         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.007340597 |
|    clip_fraction        | 0.0698      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.817      |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.89        |
|    n_updates            | 100         |
|    policy_gradient_loss | 0.0014      |
|    value_loss           | 14.8        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 257          |
|    ep_rew_mean          | -72.2        |
| time/                   |              |
|    fps                  | 85           |
|    iterations           | 12           |
|    time_elapsed         | 287          |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0052042115 |
|    clip_fraction        | 0.0537       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.783       |
|    explained_variance   | 0.719        |
|    learning_rate        | 0.0003       |
|    loss                 | 8.54         |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00134     |
|    value_loss           | 27.6         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 244         |
|    ep_rew_mean          | -68.6       |
| time/                   |             |
|    fps                  | 85          |
|    iterations           | 13          |
|    time_elapsed         | 310         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.005670648 |
|    clip_fraction        | 0.0577      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.776      |
|    explained_variance   | 0.791       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.24        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00373    |
|    value_loss           | 12.9        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 212          |
|    ep_rew_mean          | -56          |
| time/                   |              |
|    fps                  | 84           |
|    iterations           | 14           |
|    time_elapsed         | 337          |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0056000706 |
|    clip_fraction        | 0.0478       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.763       |
|    explained_variance   | 0.742        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.02         |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00242     |
|    value_loss           | 16.3         |
------------------------------------------
Num timesteps: 30000
Best mean reward: -90.40 - Last mean reward per episode: -49.52
Saving new best model at 29941 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 182         |
|    ep_rew_mean          | -44.7       |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 15          |
|    time_elapsed         | 363         |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.004281951 |
|    clip_fraction        | 0.0319      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.752      |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.47        |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00204    |
|    value_loss           | 19.9        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 189          |
|    ep_rew_mean          | -47.8        |
| time/                   |              |
|    fps                  | 83           |
|    iterations           | 16           |
|    time_elapsed         | 390          |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0045168027 |
|    clip_fraction        | 0.0375       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.682       |
|    explained_variance   | 0.763        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.14         |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.000116    |
|    value_loss           | 18.6         |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 192        |
|    ep_rew_mean          | -49        |
| time/                   |            |
|    fps                  | 83         |
|    iterations           | 17         |
|    time_elapsed         | 415        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.00470992 |
|    clip_fraction        | 0.0415     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.735     |
|    explained_variance   | 0.647      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.85       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.00264   |
|    value_loss           | 18.8       |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 197          |
|    ep_rew_mean          | -51.3        |
| time/                   |              |
|    fps                  | 83           |
|    iterations           | 18           |
|    time_elapsed         | 443          |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.0065732123 |
|    clip_fraction        | 0.0587       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.744       |
|    explained_variance   | 0.674        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.36         |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00278     |
|    value_loss           | 19.9         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 185         |
|    ep_rew_mean          | -46.2       |
| time/                   |             |
|    fps                  | 83          |
|    iterations           | 19          |
|    time_elapsed         | 468         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.003694125 |
|    clip_fraction        | 0.0281      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.66        |
|    learning_rate        | 0.0003      |
|    loss                 | 2.95        |
|    n_updates            | 180         |
|    policy_gradient_loss | 1.18e-05    |
|    value_loss           | 11.6        |
-----------------------------------------
Num timesteps: 40000
Best mean reward: -49.52 - Last mean reward per episode: -47.98
Saving new best model at 39905 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 183          |
|    ep_rew_mean          | -45.6        |
| time/                   |              |
|    fps                  | 83           |
|    iterations           | 20           |
|    time_elapsed         | 493          |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0039004236 |
|    clip_fraction        | 0.0464       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.65        |
|    explained_variance   | 0.769        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.1         |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.0014      |
|    value_loss           | 20           |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 187         |
|    ep_rew_mean          | -47.7       |
| time/                   |             |
|    fps                  | 83          |
|    iterations           | 21          |
|    time_elapsed         | 517         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.003554741 |
|    clip_fraction        | 0.035       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.644      |
|    explained_variance   | 0.754       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.27        |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00389    |
|    value_loss           | 11          |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 184          |
|    ep_rew_mean          | -47.3        |
| time/                   |              |
|    fps                  | 82           |
|    iterations           | 22           |
|    time_elapsed         | 543          |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0027652034 |
|    clip_fraction        | 0.0327       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.66        |
|    explained_variance   | 0.75         |
|    learning_rate        | 0.0003       |
|    loss                 | 1.86         |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00268     |
|    value_loss           | 8.27         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 176         |
|    ep_rew_mean          | -43.1       |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 23          |
|    time_elapsed         | 569         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.003945334 |
|    clip_fraction        | 0.0329      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.651      |
|    explained_variance   | 0.72        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.24        |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0021     |
|    value_loss           | 15.2        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 167         |
|    ep_rew_mean          | -39.7       |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 24          |
|    time_elapsed         | 593         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.005335598 |
|    clip_fraction        | 0.0492      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.617      |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.23        |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00523    |
|    value_loss           | 14.3        |
-----------------------------------------
Num timesteps: 50000
Best mean reward: -47.98 - Last mean reward per episode: -39.70
Saving new best model at 49290 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 176        |
|    ep_rew_mean          | -44        |
| time/                   |            |
|    fps                  | 82         |
|    iterations           | 25         |
|    time_elapsed         | 619        |
|    total_timesteps      | 51200      |
| train/                  |            |
|    approx_kl            | 0.00493076 |
|    clip_fraction        | 0.0535     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.571     |
|    explained_variance   | 0.698      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.43       |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.00466   |
|    value_loss           | 11.4       |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 161          |
|    ep_rew_mean          | -37.9        |
| time/                   |              |
|    fps                  | 82           |
|    iterations           | 26           |
|    time_elapsed         | 648          |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0035187267 |
|    clip_fraction        | 0.0522       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.605       |
|    explained_variance   | 0.733        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.93         |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00125     |
|    value_loss           | 12.8         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 160          |
|    ep_rew_mean          | -38.5        |
| time/                   |              |
|    fps                  | 82           |
|    iterations           | 27           |
|    time_elapsed         | 673          |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0039846357 |
|    clip_fraction        | 0.0376       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.56        |
|    explained_variance   | 0.766        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.1          |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00153     |
|    value_loss           | 12.8         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 155          |
|    ep_rew_mean          | -36.4        |
| time/                   |              |
|    fps                  | 82           |
|    iterations           | 28           |
|    time_elapsed         | 698          |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0063988743 |
|    clip_fraction        | 0.0646       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.542       |
|    explained_variance   | 0.882        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.67         |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.000462    |
|    value_loss           | 12.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 151          |
|    ep_rew_mean          | -34          |
| time/                   |              |
|    fps                  | 82           |
|    iterations           | 29           |
|    time_elapsed         | 724          |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0030216207 |
|    clip_fraction        | 0.0363       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.433       |
|    explained_variance   | 0.699        |
|    learning_rate        | 0.0003       |
|    loss                 | 1            |
|    n_updates            | 280          |
|    policy_gradient_loss | 0.00101      |
|    value_loss           | 12.1         |
------------------------------------------
Num timesteps: 60000
Best mean reward: -39.70 - Last mean reward per episode: -33.43
Saving new best model at 59943 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 157         |
|    ep_rew_mean          | -36.2       |
| time/                   |             |
|    fps                  | 81          |
|    iterations           | 30          |
|    time_elapsed         | 750         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.002928045 |
|    clip_fraction        | 0.0326      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.45       |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | 5.99        |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.000364   |
|    value_loss           | 15.6        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 154         |
|    ep_rew_mean          | -35.2       |
| time/                   |             |
|    fps                  | 81          |
|    iterations           | 31          |
|    time_elapsed         | 776         |
|    total_timesteps      | 63488       |
| train/                  |             |
|    approx_kl            | 0.007252198 |
|    clip_fraction        | 0.0644      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.469      |
|    explained_variance   | 0.552       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.16        |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00383    |
|    value_loss           | 14.9        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 148         |
|    ep_rew_mean          | -32.4       |
| time/                   |             |
|    fps                  | 81          |
|    iterations           | 32          |
|    time_elapsed         | 800         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.002426776 |
|    clip_fraction        | 0.0228      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.404      |
|    explained_variance   | 0.746       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.13        |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00253    |
|    value_loss           | 14.3        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 150         |
|    ep_rew_mean          | -33.5       |
| time/                   |             |
|    fps                  | 81          |
|    iterations           | 33          |
|    time_elapsed         | 825         |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.003947912 |
|    clip_fraction        | 0.035       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.414      |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.97        |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00171    |
|    value_loss           | 19.4        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 139          |
|    ep_rew_mean          | -27.9        |
| time/                   |              |
|    fps                  | 81           |
|    iterations           | 34           |
|    time_elapsed         | 849          |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.0035270902 |
|    clip_fraction        | 0.0321       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.422       |
|    explained_variance   | 0.742        |
|    learning_rate        | 0.0003       |
|    loss                 | 2            |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.000396    |
|    value_loss           | 12.9         |
------------------------------------------
Num timesteps: 70000
Best mean reward: -33.43 - Last mean reward per episode: -28.76
Saving new best model at 69981 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 143          |
|    ep_rew_mean          | -30.3        |
| time/                   |              |
|    fps                  | 81           |
|    iterations           | 35           |
|    time_elapsed         | 874          |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.0046164016 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.402       |
|    explained_variance   | 0.756        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.63         |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.00267     |
|    value_loss           | 10.6         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 136         |
|    ep_rew_mean          | -27.2       |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 36          |
|    time_elapsed         | 896         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.008528853 |
|    clip_fraction        | 0.0612      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.4        |
|    explained_variance   | 0.689       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.17        |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00178    |
|    value_loss           | 12.9        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 122         |
|    ep_rew_mean          | -21.5       |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 37          |
|    time_elapsed         | 920         |
|    total_timesteps      | 75776       |
| train/                  |             |
|    approx_kl            | 0.007265374 |
|    clip_fraction        | 0.0451      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.389      |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | 11          |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00441    |
|    value_loss           | 12.8        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 120         |
|    ep_rew_mean          | -19.7       |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 38          |
|    time_elapsed         | 945         |
|    total_timesteps      | 77824       |
| train/                  |             |
|    approx_kl            | 0.007819614 |
|    clip_fraction        | 0.0726      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.41       |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.88        |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.00174    |
|    value_loss           | 17.7        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 117          |
|    ep_rew_mean          | -18.5        |
| time/                   |              |
|    fps                  | 82           |
|    iterations           | 39           |
|    time_elapsed         | 969          |
|    total_timesteps      | 79872        |
| train/                  |              |
|    approx_kl            | 0.0055753468 |
|    clip_fraction        | 0.0423       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.364       |
|    explained_variance   | 0.714        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.514        |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.00582     |
|    value_loss           | 8.55         |
------------------------------------------
Num timesteps: 80000
Best mean reward: -28.76 - Last mean reward per episode: -18.49
Saving new best model at 79999 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 124         |
|    ep_rew_mean          | -21.7       |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 40          |
|    time_elapsed         | 993         |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.011276238 |
|    clip_fraction        | 0.0876      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.455      |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.65        |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.00439    |
|    value_loss           | 8.37        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 125         |
|    ep_rew_mean          | -21.6       |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 41          |
|    time_elapsed         | 1016        |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.012286918 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.514      |
|    explained_variance   | 0.672       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.64        |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00642    |
|    value_loss           | 8.63        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 120         |
|    ep_rew_mean          | -18.8       |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 42          |
|    time_elapsed         | 1041        |
|    total_timesteps      | 86016       |
| train/                  |             |
|    approx_kl            | 0.009127805 |
|    clip_fraction        | 0.0881      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.458      |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.32        |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.000548   |
|    value_loss           | 11.2        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 119          |
|    ep_rew_mean          | -18.2        |
| time/                   |              |
|    fps                  | 82           |
|    iterations           | 43           |
|    time_elapsed         | 1065         |
|    total_timesteps      | 88064        |
| train/                  |              |
|    approx_kl            | 0.0121264355 |
|    clip_fraction        | 0.0833       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.414       |
|    explained_variance   | 0.808        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.15         |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.00496     |
|    value_loss           | 8.51         |
------------------------------------------
Num timesteps: 90000
Best mean reward: -18.49 - Last mean reward per episode: -21.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 125         |
|    ep_rew_mean          | -21.8       |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 44          |
|    time_elapsed         | 1088        |
|    total_timesteps      | 90112       |
| train/                  |             |
|    approx_kl            | 0.012437955 |
|    clip_fraction        | 0.0803      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.362      |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.38        |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00185    |
|    value_loss           | 8.15        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 131         |
|    ep_rew_mean          | -24.9       |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 45          |
|    time_elapsed         | 1113        |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.014513688 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.495      |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.21        |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00312    |
|    value_loss           | 8.69        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 121        |
|    ep_rew_mean          | -20.3      |
| time/                   |            |
|    fps                  | 82         |
|    iterations           | 46         |
|    time_elapsed         | 1137       |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.01310079 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.372     |
|    explained_variance   | 0.584      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.61       |
|    n_updates            | 450        |
|    policy_gradient_loss | 9.97e-05   |
|    value_loss           | 11.5       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 121       |
|    ep_rew_mean          | -20.2     |
| time/                   |           |
|    fps                  | 82        |
|    iterations           | 47        |
|    time_elapsed         | 1161      |
|    total_timesteps      | 96256     |
| train/                  |           |
|    approx_kl            | 0.0119785 |
|    clip_fraction        | 0.0682    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.356    |
|    explained_variance   | 0.684     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.391     |
|    n_updates            | 460       |
|    policy_gradient_loss | -0.00219  |
|    value_loss           | 7.4       |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 121         |
|    ep_rew_mean          | -20.6       |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 48          |
|    time_elapsed         | 1185        |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.014635438 |
|    clip_fraction        | 0.0957      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.488      |
|    explained_variance   | 0.795       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.488       |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00772    |
|    value_loss           | 6.04        |
-----------------------------------------
Num timesteps: 100000
Best mean reward: -18.49 - Last mean reward per episode: -22.23
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 125        |
|    ep_rew_mean          | -22.1      |
| time/                   |            |
|    fps                  | 82         |
|    iterations           | 49         |
|    time_elapsed         | 1209       |
|    total_timesteps      | 100352     |
| train/                  |            |
|    approx_kl            | 0.01385962 |
|    clip_fraction        | 0.101      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.458     |
|    explained_variance   | 0.697      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.07       |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.00789   |
|    value_loss           | 8.13       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 121         |
|    ep_rew_mean          | -20.1       |
| time/                   |             |
|    fps                  | 83          |
|    iterations           | 50          |
|    time_elapsed         | 1232        |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.015008014 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.49       |
|    explained_variance   | 0.69        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.699       |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00658    |
|    value_loss           | 7.39        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 118         |
|    ep_rew_mean          | -18.4       |
| time/                   |             |
|    fps                  | 83          |
|    iterations           | 51          |
|    time_elapsed         | 1256        |
|    total_timesteps      | 104448      |
| train/                  |             |
|    approx_kl            | 0.020446628 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.575      |
|    explained_variance   | 0.735       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.876       |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.00311    |
|    value_loss           | 11.4        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 116         |
|    ep_rew_mean          | -17.4       |
| time/                   |             |
|    fps                  | 83          |
|    iterations           | 52          |
|    time_elapsed         | 1281        |
|    total_timesteps      | 106496      |
| train/                  |             |
|    approx_kl            | 0.010906575 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.599      |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.63        |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.00572    |
|    value_loss           | 13.5        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 118          |
|    ep_rew_mean          | -18.4        |
| time/                   |              |
|    fps                  | 83           |
|    iterations           | 53           |
|    time_elapsed         | 1306         |
|    total_timesteps      | 108544       |
| train/                  |              |
|    approx_kl            | 0.0153718535 |
|    clip_fraction        | 0.141        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.539       |
|    explained_variance   | 0.697        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.652        |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.00664     |
|    value_loss           | 8.32         |
------------------------------------------
Num timesteps: 110000
Best mean reward: -18.49 - Last mean reward per episode: -19.44
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 119        |
|    ep_rew_mean          | -18.7      |
| time/                   |            |
|    fps                  | 83         |
|    iterations           | 54         |
|    time_elapsed         | 1330       |
|    total_timesteps      | 110592     |
| train/                  |            |
|    approx_kl            | 0.01263532 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.631     |
|    explained_variance   | 0.74       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.752      |
|    n_updates            | 530        |
|    policy_gradient_loss | -0.00138   |
|    value_loss           | 6.6        |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 120          |
|    ep_rew_mean          | -19.1        |
| time/                   |              |
|    fps                  | 83           |
|    iterations           | 55           |
|    time_elapsed         | 1356         |
|    total_timesteps      | 112640       |
| train/                  |              |
|    approx_kl            | 0.0114836525 |
|    clip_fraction        | 0.111        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.628       |
|    explained_variance   | 0.747        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.36         |
|    n_updates            | 540          |
|    policy_gradient_loss | -0.00838     |
|    value_loss           | 7.47         |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 124        |
|    ep_rew_mean          | -20.9      |
| time/                   |            |
|    fps                  | 83         |
|    iterations           | 56         |
|    time_elapsed         | 1380       |
|    total_timesteps      | 114688     |
| train/                  |            |
|    approx_kl            | 0.02075966 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.617     |
|    explained_variance   | 0.674      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.624      |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.0103    |
|    value_loss           | 5.69       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 123         |
|    ep_rew_mean          | -19.8       |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 57          |
|    time_elapsed         | 1407        |
|    total_timesteps      | 116736      |
| train/                  |             |
|    approx_kl            | 0.018903911 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.687      |
|    explained_variance   | 0.802       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.88        |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 6.84        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 133         |
|    ep_rew_mean          | -25.1       |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 58          |
|    time_elapsed         | 1434        |
|    total_timesteps      | 118784      |
| train/                  |             |
|    approx_kl            | 0.014931081 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.638      |
|    explained_variance   | 0.831       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.71        |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00285    |
|    value_loss           | 10          |
-----------------------------------------
Num timesteps: 120000
Best mean reward: -18.49 - Last mean reward per episode: -24.68
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 133         |
|    ep_rew_mean          | -24.9       |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 59          |
|    time_elapsed         | 1462        |
|    total_timesteps      | 120832      |
| train/                  |             |
|    approx_kl            | 0.018052768 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.721      |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.34        |
|    n_updates            | 580         |
|    policy_gradient_loss | 0.00127     |
|    value_loss           | 8.87        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 133         |
|    ep_rew_mean          | -25         |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 60          |
|    time_elapsed         | 1489        |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.013937552 |
|    clip_fraction        | 0.0989      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.425      |
|    explained_variance   | 0.794       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.658       |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0024     |
|    value_loss           | 6.08        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 137         |
|    ep_rew_mean          | -27.3       |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 61          |
|    time_elapsed         | 1515        |
|    total_timesteps      | 124928      |
| train/                  |             |
|    approx_kl            | 0.019546922 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.509      |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.68        |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.00725    |
|    value_loss           | 10.4        |
-----------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 291, in <module>
    model.learn(total_timesteps=1000000, callback=[WandbCallback(), save_callback])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 313, in learn
    self.train()
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 217, in train
    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 730, in evaluate_actions
    features = self.extract_features(obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/policy.py", line 43, in forward
    pyg_data = self.encoder.encode(observations)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/encoder/GraphEncoder.py", line 495, in encode
    return Batch.from_data_list(self.to_pyg_data(batch_data))
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/encoder/GraphEncoder.py", line 547, in to_pyg_data
    edge_tensor = torch.tensor(edges, dtype=torch.long).t().contiguous()
KeyboardInterrupt