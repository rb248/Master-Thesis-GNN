
Using cpu device
/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead
  warnings.warn(out)
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -500     |
| time/              |          |
|    fps             | 662      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 2048     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 223          |
|    iterations           | 2            |
|    time_elapsed         | 18           |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0072809253 |
|    clip_fraction        | 0.0298       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | -0.137       |
|    learning_rate        | 0.0003       |
|    loss                 | 1            |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.000774    |
|    value_loss           | 5.93         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -500         |
| time/                   |              |
|    fps                  | 172          |
|    iterations           | 3            |
|    time_elapsed         | 35           |
|    total_timesteps      | 6144         |
| train/                  |              |
|    approx_kl            | 0.0053398344 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.082        |
|    n_updates            | 20           |
|    policy_gradient_loss | 8.93e-05     |
|    value_loss           | 16.1         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 966         |
|    ep_rew_mean          | -480        |
| time/                   |             |
|    fps                  | 150         |
|    iterations           | 4           |
|    time_elapsed         | 54          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.011027843 |
|    clip_fraction        | 0.0141      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 4.11        |
|    n_updates            | 30          |
|    policy_gradient_loss | -8.32e-05   |
|    value_loss           | 12.3        |
-----------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -483.60
Saving new best model at 9732 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 973         |
|    ep_rew_mean          | -484        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 5           |
|    time_elapsed         | 72          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.011928499 |
|    clip_fraction        | 0.0337      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.671       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.000489   |
|    value_loss           | 19.6        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 848          |
|    ep_rew_mean          | -417         |
| time/                   |              |
|    fps                  | 134          |
|    iterations           | 6            |
|    time_elapsed         | 91           |
|    total_timesteps      | 12288        |
| train/                  |              |
|    approx_kl            | 0.0056138523 |
|    clip_fraction        | 0.031        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.01        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.516        |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.000346    |
|    value_loss           | 9.75         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 815         |
|    ep_rew_mean          | -399        |
| time/                   |             |
|    fps                  | 127         |
|    iterations           | 7           |
|    time_elapsed         | 112         |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.010384776 |
|    clip_fraction        | 0.0859      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.902      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 2.83        |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00198    |
|    value_loss           | 36.3        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 769         |
|    ep_rew_mean          | -374        |
| time/                   |             |
|    fps                  | 123         |
|    iterations           | 8           |
|    time_elapsed         | 132         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.003634043 |
|    clip_fraction        | 0.0717      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.819      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 15.2        |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00173    |
|    value_loss           | 32.8        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 789         |
|    ep_rew_mean          | -385        |
| time/                   |             |
|    fps                  | 122         |
|    iterations           | 9           |
|    time_elapsed         | 150         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.004931488 |
|    clip_fraction        | 0.0129      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.827      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 41.9        |
|    n_updates            | 80          |
|    policy_gradient_loss | 0.000452    |
|    value_loss           | 50.7        |
-----------------------------------------
Num timesteps: 20000
Best mean reward: -483.60 - Last mean reward per episode: -354.04
Saving new best model at 19678 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 729         |
|    ep_rew_mean          | -354        |
| time/                   |             |
|    fps                  | 120         |
|    iterations           | 10          |
|    time_elapsed         | 170         |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.008770032 |
|    clip_fraction        | 0.0549      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.75       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 3.32        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.000177   |
|    value_loss           | 11.3        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 676          |
|    ep_rew_mean          | -326         |
| time/                   |              |
|    fps                  | 118          |
|    iterations           | 11           |
|    time_elapsed         | 190          |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0063310163 |
|    clip_fraction        | 0.0629       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.665       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.29         |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00197     |
|    value_loss           | 45.5         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 663          |
|    ep_rew_mean          | -319         |
| time/                   |              |
|    fps                  | 118          |
|    iterations           | 12           |
|    time_elapsed         | 208          |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0039333184 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.59        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 91.2         |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.000429    |
|    value_loss           | 68.5         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 641          |
|    ep_rew_mean          | -307         |
| time/                   |              |
|    fps                  | 117          |
|    iterations           | 13           |
|    time_elapsed         | 225          |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.0037254973 |
|    clip_fraction        | 0.0468       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.516       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 54.8         |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.000891    |
|    value_loss           | 56.2         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 644         |
|    ep_rew_mean          | -308        |
| time/                   |             |
|    fps                  | 117         |
|    iterations           | 14          |
|    time_elapsed         | 243         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.001154409 |
|    clip_fraction        | 0.0191      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.519      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 11.5        |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.000724   |
|    value_loss           | 62.1        |
-----------------------------------------
Num timesteps: 30000
Best mean reward: -354.04 - Last mean reward per episode: -298.36
Saving new best model at 29406 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 633          |
|    ep_rew_mean          | -303         |
| time/                   |              |
|    fps                  | 117          |
|    iterations           | 15           |
|    time_elapsed         | 260          |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.0022344259 |
|    clip_fraction        | 0.0269       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.51        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 25.8         |
|    n_updates            | 140          |
|    policy_gradient_loss | 5.41e-05     |
|    value_loss           | 41.5         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 617          |
|    ep_rew_mean          | -294         |
| time/                   |              |
|    fps                  | 117          |
|    iterations           | 16           |
|    time_elapsed         | 278          |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0029963395 |
|    clip_fraction        | 0.0284       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.436       |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 5.94         |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.000242    |
|    value_loss           | 58.3         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 613         |
|    ep_rew_mean          | -292        |
| time/                   |             |
|    fps                  | 116         |
|    iterations           | 17          |
|    time_elapsed         | 299         |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.004030836 |
|    clip_fraction        | 0.0682      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.377      |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 1.8         |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00206    |
|    value_loss           | 48.2        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 625          |
|    ep_rew_mean          | -299         |
| time/                   |              |
|    fps                  | 115          |
|    iterations           | 18           |
|    time_elapsed         | 319          |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.0006368959 |
|    clip_fraction        | 0.0115       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.371       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 47           |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.000352    |
|    value_loss           | 41.2         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 587          |
|    ep_rew_mean          | -279         |
| time/                   |              |
|    fps                  | 114          |
|    iterations           | 19           |
|    time_elapsed         | 338          |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0003683452 |
|    clip_fraction        | 0.00596      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.362       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 3.18         |
|    n_updates            | 180          |
|    policy_gradient_loss | 0.000231     |
|    value_loss           | 27.7         |
------------------------------------------
Num timesteps: 40000
Best mean reward: -298.36 - Last mean reward per episode: -279.57
Saving new best model at 39942 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 578         |
|    ep_rew_mean          | -275        |
| time/                   |             |
|    fps                  | 115         |
|    iterations           | 20          |
|    time_elapsed         | 355         |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.001085479 |
|    clip_fraction        | 0.0235      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.425      |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 46.3        |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.000398   |
|    value_loss           | 90.2        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 572          |
|    ep_rew_mean          | -271         |
| time/                   |              |
|    fps                  | 114          |
|    iterations           | 21           |
|    time_elapsed         | 376          |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0015917795 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.372       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 22.4         |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.000588    |
|    value_loss           | 48.4         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 559         |
|    ep_rew_mean          | -265        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 22          |
|    time_elapsed         | 396         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.003106588 |
|    clip_fraction        | 0.0249      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.439      |
|    explained_variance   | 1.49e-06    |
|    learning_rate        | 0.0003      |
|    loss                 | 50.8        |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.000138   |
|    value_loss           | 56.8        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 549          |
|    ep_rew_mean          | -260         |
| time/                   |              |
|    fps                  | 113          |
|    iterations           | 23           |
|    time_elapsed         | 416          |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0013335248 |
|    clip_fraction        | 0.0249       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.345       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 43.2         |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.000438    |
|    value_loss           | 80           |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 536           |
|    ep_rew_mean          | -253          |
| time/                   |               |
|    fps                  | 112           |
|    iterations           | 24            |
|    time_elapsed         | 436           |
|    total_timesteps      | 49152         |
| train/                  |               |
|    approx_kl            | 0.00040420773 |
|    clip_fraction        | 0.00815       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.342        |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 5.45          |
|    n_updates            | 230           |
|    policy_gradient_loss | 8.73e-05      |
|    value_loss           | 47.9          |
-------------------------------------------
Num timesteps: 50000
Best mean reward: -279.57 - Last mean reward per episode: -250.54
Saving new best model at 49380 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 522          |
|    ep_rew_mean          | -246         |
| time/                   |              |
|    fps                  | 112          |
|    iterations           | 25           |
|    time_elapsed         | 456          |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0009017148 |
|    clip_fraction        | 0.0085       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.334       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 24.2         |
|    n_updates            | 240          |
|    policy_gradient_loss | 0.000141     |
|    value_loss           | 74           |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 498           |
|    ep_rew_mean          | -233          |
| time/                   |               |
|    fps                  | 112           |
|    iterations           | 26            |
|    time_elapsed         | 472           |
|    total_timesteps      | 53248         |
| train/                  |               |
|    approx_kl            | 6.1535655e-05 |
|    clip_fraction        | 0.00776       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.332        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 16.8          |
|    n_updates            | 250           |
|    policy_gradient_loss | -0.000436     |
|    value_loss           | 98.5          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 488          |
|    ep_rew_mean          | -228         |
| time/                   |              |
|    fps                  | 112          |
|    iterations           | 27           |
|    time_elapsed         | 493          |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0019307946 |
|    clip_fraction        | 0.0382       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.28        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 18.6         |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.0013      |
|    value_loss           | 62.6         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 460          |
|    ep_rew_mean          | -213         |
| time/                   |              |
|    fps                  | 112          |
|    iterations           | 28           |
|    time_elapsed         | 508          |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0010843427 |
|    clip_fraction        | 0.019        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.236       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.75         |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.000324    |
|    value_loss           | 37.9         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 421          |
|    ep_rew_mean          | -193         |
| time/                   |              |
|    fps                  | 113          |
|    iterations           | 29           |
|    time_elapsed         | 524          |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0011484597 |
|    clip_fraction        | 0.0043       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.239       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 59.8         |
|    n_updates            | 280          |
|    policy_gradient_loss | 0.000233     |
|    value_loss           | 88.4         |
------------------------------------------
Num timesteps: 60000
Best mean reward: -250.54 - Last mean reward per episode: -184.90
Saving new best model at 59834 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 403          |
|    ep_rew_mean          | -184         |
| time/                   |              |
|    fps                  | 113          |
|    iterations           | 30           |
|    time_elapsed         | 542          |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0005799775 |
|    clip_fraction        | 0.00889      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.209       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 110          |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.000236    |
|    value_loss           | 123          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 392          |
|    ep_rew_mean          | -178         |
| time/                   |              |
|    fps                  | 113          |
|    iterations           | 31           |
|    time_elapsed         | 559          |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.0003360508 |
|    clip_fraction        | 0.00605      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.159       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 9.47         |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.000186    |
|    value_loss           | 80.3         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 389          |
|    ep_rew_mean          | -176         |
| time/                   |              |
|    fps                  | 113          |
|    iterations           | 32           |
|    time_elapsed         | 576          |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0003517719 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.134       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 85.7         |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.000605    |
|    value_loss           | 100          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 370           |
|    ep_rew_mean          | -167          |
| time/                   |               |
|    fps                  | 113           |
|    iterations           | 33            |
|    time_elapsed         | 596           |
|    total_timesteps      | 67584         |
| train/                  |               |
|    approx_kl            | 0.00051715673 |
|    clip_fraction        | 0.0105        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.137        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 10.2          |
|    n_updates            | 320           |
|    policy_gradient_loss | -0.000731     |
|    value_loss           | 57.3          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 361           |
|    ep_rew_mean          | -162          |
| time/                   |               |
|    fps                  | 113           |
|    iterations           | 34            |
|    time_elapsed         | 613           |
|    total_timesteps      | 69632         |
| train/                  |               |
|    approx_kl            | 0.00045473097 |
|    clip_fraction        | 0.00488       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.139        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 42.9          |
|    n_updates            | 330           |
|    policy_gradient_loss | -1.44e-06     |
|    value_loss           | 82.5          |
-------------------------------------------
Num timesteps: 70000
Best mean reward: -184.90 - Last mean reward per episode: -161.67
Saving new best model at 69167 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 344          |
|    ep_rew_mean          | -153         |
| time/                   |              |
|    fps                  | 113          |
|    iterations           | 35           |
|    time_elapsed         | 633          |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.0006186196 |
|    clip_fraction        | 0.0153       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0933      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 24.6         |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.000708    |
|    value_loss           | 69.8         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 359          |
|    ep_rew_mean          | -160         |
| time/                   |              |
|    fps                  | 113          |
|    iterations           | 36           |
|    time_elapsed         | 650          |
|    total_timesteps      | 73728        |
| train/                  |              |
|    approx_kl            | 0.0004926793 |
|    clip_fraction        | 0.00264      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0903      |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 80.2         |
|    n_updates            | 350          |
|    policy_gradient_loss | -3.7e-05     |
|    value_loss           | 95.6         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 359           |
|    ep_rew_mean          | -160          |
| time/                   |               |
|    fps                  | 113           |
|    iterations           | 37            |
|    time_elapsed         | 669           |
|    total_timesteps      | 75776         |
| train/                  |               |
|    approx_kl            | 0.00038152622 |
|    clip_fraction        | 0.00649       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0653       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 14.8          |
|    n_updates            | 360           |
|    policy_gradient_loss | -0.000353     |
|    value_loss           | 30.1          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 346           |
|    ep_rew_mean          | -154          |
| time/                   |               |
|    fps                  | 113           |
|    iterations           | 38            |
|    time_elapsed         | 687           |
|    total_timesteps      | 77824         |
| train/                  |               |
|    approx_kl            | 0.00068466854 |
|    clip_fraction        | 0.0101        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0475       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 0.828         |
|    n_updates            | 370           |
|    policy_gradient_loss | -0.000487     |
|    value_loss           | 31.7          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 345           |
|    ep_rew_mean          | -153          |
| time/                   |               |
|    fps                  | 113           |
|    iterations           | 39            |
|    time_elapsed         | 702           |
|    total_timesteps      | 79872         |
| train/                  |               |
|    approx_kl            | 0.00010825912 |
|    clip_fraction        | 0.00229       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0391       |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 92.4          |
|    n_updates            | 380           |
|    policy_gradient_loss | -0.000178     |
|    value_loss           | 97.2          |
-------------------------------------------
Num timesteps: 80000
Best mean reward: -161.67 - Last mean reward per episode: -157.51
Saving new best model at 79891 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 341           |
|    ep_rew_mean          | -151          |
| time/                   |               |
|    fps                  | 113           |
|    iterations           | 40            |
|    time_elapsed         | 724           |
|    total_timesteps      | 81920         |
| train/                  |               |
|    approx_kl            | 0.00038135672 |
|    clip_fraction        | 0.00557       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0314       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 8.22          |
|    n_updates            | 390           |
|    policy_gradient_loss | -0.000271     |
|    value_loss           | 37            |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 354           |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 113           |
|    iterations           | 41            |
|    time_elapsed         | 742           |
|    total_timesteps      | 83968         |
| train/                  |               |
|    approx_kl            | 0.00012319014 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0293       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 31.3          |
|    n_updates            | 400           |
|    policy_gradient_loss | -2.8e-05      |
|    value_loss           | 97.8          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 353          |
|    ep_rew_mean          | -157         |
| time/                   |              |
|    fps                  | 112          |
|    iterations           | 42           |
|    time_elapsed         | 761          |
|    total_timesteps      | 86016        |
| train/                  |              |
|    approx_kl            | 6.125189e-05 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0314      |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 8.48         |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.000122    |
|    value_loss           | 36.6         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 362           |
|    ep_rew_mean          | -162          |
| time/                   |               |
|    fps                  | 112           |
|    iterations           | 43            |
|    time_elapsed         | 779           |
|    total_timesteps      | 88064         |
| train/                  |               |
|    approx_kl            | 0.00013715937 |
|    clip_fraction        | 0.00166       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0342       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 7.52          |
|    n_updates            | 420           |
|    policy_gradient_loss | 8.77e-05      |
|    value_loss           | 45.1          |
-------------------------------------------
Num timesteps: 90000
Best mean reward: -157.51 - Last mean reward per episode: -151.74
Saving new best model at 89941 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 342           |
|    ep_rew_mean          | -152          |
| time/                   |               |
|    fps                  | 113           |
|    iterations           | 44            |
|    time_elapsed         | 796           |
|    total_timesteps      | 90112         |
| train/                  |               |
|    approx_kl            | 0.00018903022 |
|    clip_fraction        | 0.004         |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0323       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 11.5          |
|    n_updates            | 430           |
|    policy_gradient_loss | -4.51e-05     |
|    value_loss           | 51            |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 343           |
|    ep_rew_mean          | -152          |
| time/                   |               |
|    fps                  | 113           |
|    iterations           | 45            |
|    time_elapsed         | 814           |
|    total_timesteps      | 92160         |
| train/                  |               |
|    approx_kl            | 0.00020643524 |
|    clip_fraction        | 0.00327       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0252       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 42            |
|    n_updates            | 440           |
|    policy_gradient_loss | -0.000274     |
|    value_loss           | 135           |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 355            |
|    ep_rew_mean          | -159           |
| time/                   |                |
|    fps                  | 112            |
|    iterations           | 46             |
|    time_elapsed         | 834            |
|    total_timesteps      | 94208          |
| train/                  |                |
|    approx_kl            | 0.000112264854 |
|    clip_fraction        | 0.00337        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0195        |
|    explained_variance   | 0              |
|    learning_rate        | 0.0003         |
|    loss                 | 34.9           |
|    n_updates            | 450            |
|    policy_gradient_loss | -0.000237      |
|    value_loss           | 81.9           |
--------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 373           |
|    ep_rew_mean          | -168          |
| time/                   |               |
|    fps                  | 112           |
|    iterations           | 47            |
|    time_elapsed         | 856           |
|    total_timesteps      | 96256         |
| train/                  |               |
|    approx_kl            | 4.1762803e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0177       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 4.92          |
|    n_updates            | 460           |
|    policy_gradient_loss | -2.73e-05     |
|    value_loss           | 42.6          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 366           |
|    ep_rew_mean          | -164          |
| time/                   |               |
|    fps                  | 112           |
|    iterations           | 48            |
|    time_elapsed         | 876           |
|    total_timesteps      | 98304         |
| train/                  |               |
|    approx_kl            | 3.2389566e-05 |
|    clip_fraction        | 0.000732      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0154       |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 8.49          |
|    n_updates            | 470           |
|    policy_gradient_loss | -3.25e-05     |
|    value_loss           | 11.5          |
-------------------------------------------
Num timesteps: 100000
Best mean reward: -151.74 - Last mean reward per episode: -168.03
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 374           |
|    ep_rew_mean          | -168          |
| time/                   |               |
|    fps                  | 112           |
|    iterations           | 49            |
|    time_elapsed         | 895           |
|    total_timesteps      | 100352        |
| train/                  |               |
|    approx_kl            | 2.8642331e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0133       |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 52.6          |
|    n_updates            | 480           |
|    policy_gradient_loss | -1.68e-05     |
|    value_loss           | 93.6          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 381           |
|    ep_rew_mean          | -172          |
| time/                   |               |
|    fps                  | 112           |
|    iterations           | 50            |
|    time_elapsed         | 913           |
|    total_timesteps      | 102400        |
| train/                  |               |
|    approx_kl            | 7.0842216e-05 |
|    clip_fraction        | 0.000928      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0103       |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 73.2          |
|    n_updates            | 490           |
|    policy_gradient_loss | -7.97e-05     |
|    value_loss           | 100           |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 373          |
|    ep_rew_mean          | -168         |
| time/                   |              |
|    fps                  | 112          |
|    iterations           | 51           |
|    time_elapsed         | 930          |
|    total_timesteps      | 104448       |
| train/                  |              |
|    approx_kl            | 5.024197e-05 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00796     |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 0.564        |
|    n_updates            | 500          |
|    policy_gradient_loss | -6.56e-05    |
|    value_loss           | 29.4         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 390           |
|    ep_rew_mean          | -178          |
| time/                   |               |
|    fps                  | 112           |
|    iterations           | 52            |
|    time_elapsed         | 948           |
|    total_timesteps      | 106496        |
| train/                  |               |
|    approx_kl            | 5.4918695e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00638      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 54.2          |
|    n_updates            | 510           |
|    policy_gradient_loss | -3.04e-05     |
|    value_loss           | 87.2          |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 375       |
|    ep_rew_mean          | -170      |
| time/                   |           |
|    fps                  | 112       |
|    iterations           | 53        |
|    time_elapsed         | 968       |
|    total_timesteps      | 108544    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00625  |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0003    |
|    loss                 | 1.32      |
|    n_updates            | 520       |
|    policy_gradient_loss | -1.82e-09 |
|    value_loss           | 25        |
---------------------------------------
Num timesteps: 110000
Best mean reward: -151.74 - Last mean reward per episode: -169.19
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 374          |
|    ep_rew_mean          | -169         |
| time/                   |              |
|    fps                  | 112          |
|    iterations           | 54           |
|    time_elapsed         | 986          |
|    total_timesteps      | 110592       |
| train/                  |              |
|    approx_kl            | 7.380263e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00521     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 72.6         |
|    n_updates            | 530          |
|    policy_gradient_loss | -5.13e-05    |
|    value_loss           | 91.2         |
------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 354       |
|    ep_rew_mean          | -159      |
| time/                   |           |
|    fps                  | 112       |
|    iterations           | 55        |
|    time_elapsed         | 1005      |
|    total_timesteps      | 112640    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00448  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 87        |
|    n_updates            | 540       |
|    policy_gradient_loss | -8.98e-10 |
|    value_loss           | 107       |
---------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 352          |
|    ep_rew_mean          | -158         |
| time/                   |              |
|    fps                  | 111          |
|    iterations           | 56           |
|    time_elapsed         | 1024         |
|    total_timesteps      | 114688       |
| train/                  |              |
|    approx_kl            | 6.068556e-05 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0072      |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 49.7         |
|    n_updates            | 550          |
|    policy_gradient_loss | 2.16e-05     |
|    value_loss           | 99.9         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 362          |
|    ep_rew_mean          | -163         |
| time/                   |              |
|    fps                  | 111          |
|    iterations           | 57           |
|    time_elapsed         | 1043         |
|    total_timesteps      | 116736       |
| train/                  |              |
|    approx_kl            | 4.932162e-05 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00436     |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 52.3         |
|    n_updates            | 560          |
|    policy_gradient_loss | -8.03e-05    |
|    value_loss           | 54.3         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 376           |
|    ep_rew_mean          | -171          |
| time/                   |               |
|    fps                  | 111           |
|    iterations           | 58            |
|    time_elapsed         | 1063          |
|    total_timesteps      | 118784        |
| train/                  |               |
|    approx_kl            | 6.7918445e-06 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00393      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 23            |
|    n_updates            | 570           |
|    policy_gradient_loss | 7.98e-06      |
|    value_loss           | 29            |
-------------------------------------------
Num timesteps: 120000
Best mean reward: -151.74 - Last mean reward per episode: -173.51
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 372          |
|    ep_rew_mean          | -168         |
| time/                   |              |
|    fps                  | 111          |
|    iterations           | 59           |
|    time_elapsed         | 1082         |
|    total_timesteps      | 120832       |
| train/                  |              |
|    approx_kl            | 7.617037e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00231     |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 14.2         |
|    n_updates            | 580          |
|    policy_gradient_loss | -2.96e-05    |
|    value_loss           | 62.8         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 374           |
|    ep_rew_mean          | -170          |
| time/                   |               |
|    fps                  | 111           |
|    iterations           | 60            |
|    time_elapsed         | 1101          |
|    total_timesteps      | 122880        |
| train/                  |               |
|    approx_kl            | 1.9389816e-05 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00191      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 5.3           |
|    n_updates            | 590           |
|    policy_gradient_loss | -1.93e-05     |
|    value_loss           | 58            |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 388           |
|    ep_rew_mean          | -177          |
| time/                   |               |
|    fps                  | 111           |
|    iterations           | 61            |
|    time_elapsed         | 1121          |
|    total_timesteps      | 124928        |
| train/                  |               |
|    approx_kl            | 7.4579526e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00149      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 0.342         |
|    n_updates            | 600           |
|    policy_gradient_loss | -1.62e-05     |
|    value_loss           | 12.1          |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 377       |
|    ep_rew_mean          | -171      |
| time/                   |           |
|    fps                  | 111       |
|    iterations           | 62        |
|    time_elapsed         | 1142      |
|    total_timesteps      | 126976    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00143  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 38.4      |
|    n_updates            | 610       |
|    policy_gradient_loss | 3.42e-08  |
|    value_loss           | 23.1      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 388      |
|    ep_rew_mean          | -177     |
| time/                   |          |
|    fps                  | 111      |
|    iterations           | 63       |
|    time_elapsed         | 1157     |
|    total_timesteps      | 129024   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00143 |
|    explained_variance   | 5.96e-08 |
|    learning_rate        | 0.0003   |
|    loss                 | 22.7     |
|    n_updates            | 620      |
|    policy_gradient_loss | 7.65e-10 |
|    value_loss           | 87.1     |
--------------------------------------
Num timesteps: 130000
Best mean reward: -151.74 - Last mean reward per episode: -181.34
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 404       |
|    ep_rew_mean          | -185      |
| time/                   |           |
|    fps                  | 111       |
|    iterations           | 64        |
|    time_elapsed         | 1174      |
|    total_timesteps      | 131072    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00143  |
|    explained_variance   | 1.79e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 1.04      |
|    n_updates            | 630       |
|    policy_gradient_loss | -2.34e-09 |
|    value_loss           | 48.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 414       |
|    ep_rew_mean          | -190      |
| time/                   |           |
|    fps                  | 111       |
|    iterations           | 65        |
|    time_elapsed         | 1196      |
|    total_timesteps      | 133120    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00143  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 0.778     |
|    n_updates            | 640       |
|    policy_gradient_loss | -1.85e-08 |
|    value_loss           | 13.3      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 423           |
|    ep_rew_mean          | -195          |
| time/                   |               |
|    fps                  | 111           |
|    iterations           | 66            |
|    time_elapsed         | 1216          |
|    total_timesteps      | 135168        |
| train/                  |               |
|    approx_kl            | 1.7736776e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00117      |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 2.49          |
|    n_updates            | 650           |
|    policy_gradient_loss | -2.24e-05     |
|    value_loss           | 54            |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 410       |
|    ep_rew_mean          | -188      |
| time/                   |           |
|    fps                  | 111       |
|    iterations           | 67        |
|    time_elapsed         | 1234      |
|    total_timesteps      | 137216    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00113  |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0003    |
|    loss                 | 62.3      |
|    n_updates            | 660       |
|    policy_gradient_loss | -2.12e-09 |
|    value_loss           | 102       |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 410           |
|    ep_rew_mean          | -188          |
| time/                   |               |
|    fps                  | 111           |
|    iterations           | 68            |
|    time_elapsed         | 1253          |
|    total_timesteps      | 139264        |
| train/                  |               |
|    approx_kl            | 0.00016531465 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000667     |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 30.4          |
|    n_updates            | 670           |
|    policy_gradient_loss | -2.21e-05     |
|    value_loss           | 48.7          |
-------------------------------------------
Num timesteps: 140000
Best mean reward: -151.74 - Last mean reward per episode: -185.40
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 392           |
|    ep_rew_mean          | -178          |
| time/                   |               |
|    fps                  | 111           |
|    iterations           | 69            |
|    time_elapsed         | 1272          |
|    total_timesteps      | 141312        |
| train/                  |               |
|    approx_kl            | 0.00018324165 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000251     |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 33.6          |
|    n_updates            | 680           |
|    policy_gradient_loss | -3.01e-05     |
|    value_loss           | 103           |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 395       |
|    ep_rew_mean          | -180      |
| time/                   |           |
|    fps                  | 110       |
|    iterations           | 70        |
|    time_elapsed         | 1292      |
|    total_timesteps      | 143360    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 2.38e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 46.1      |
|    n_updates            | 690       |
|    policy_gradient_loss | 2.98e-10  |
|    value_loss           | 131       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 398       |
|    ep_rew_mean          | -182      |
| time/                   |           |
|    fps                  | 110       |
|    iterations           | 71        |
|    time_elapsed         | 1312      |
|    total_timesteps      | 145408    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 5.27      |
|    n_updates            | 700       |
|    policy_gradient_loss | -1.67e-10 |
|    value_loss           | 48.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 405       |
|    ep_rew_mean          | -185      |
| time/                   |           |
|    fps                  | 110       |
|    iterations           | 72        |
|    time_elapsed         | 1332      |
|    total_timesteps      | 147456    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 50.4      |
|    n_updates            | 710       |
|    policy_gradient_loss | 1.8e-10   |
|    value_loss           | 49        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 409       |
|    ep_rew_mean          | -187      |
| time/                   |           |
|    fps                  | 110       |
|    iterations           | 73        |
|    time_elapsed         | 1351      |
|    total_timesteps      | 149504    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 27.7      |
|    n_updates            | 720       |
|    policy_gradient_loss | -1.52e-10 |
|    value_loss           | 62.3      |
---------------------------------------
Num timesteps: 150000
Best mean reward: -151.74 - Last mean reward per episode: -186.84
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 417       |
|    ep_rew_mean          | -192      |
| time/                   |           |
|    fps                  | 110       |
|    iterations           | 74        |
|    time_elapsed         | 1370      |
|    total_timesteps      | 151552    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 10.5      |
|    n_updates            | 730       |
|    policy_gradient_loss | 3.22e-09  |
|    value_loss           | 45.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 429       |
|    ep_rew_mean          | -198      |
| time/                   |           |
|    fps                  | 110       |
|    iterations           | 75        |
|    time_elapsed         | 1391      |
|    total_timesteps      | 153600    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 18        |
|    n_updates            | 740       |
|    policy_gradient_loss | -4.15e-10 |
|    value_loss           | 25.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 432       |
|    ep_rew_mean          | -199      |
| time/                   |           |
|    fps                  | 110       |
|    iterations           | 76        |
|    time_elapsed         | 1410      |
|    total_timesteps      | 155648    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 64.5      |
|    n_updates            | 750       |
|    policy_gradient_loss | 1.08e-09  |
|    value_loss           | 66.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 434       |
|    ep_rew_mean          | -200      |
| time/                   |           |
|    fps                  | 109       |
|    iterations           | 77        |
|    time_elapsed         | 1438      |
|    total_timesteps      | 157696    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 51.9      |
|    n_updates            | 760       |
|    policy_gradient_loss | 7.13e-10  |
|    value_loss           | 70.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 421       |
|    ep_rew_mean          | -194      |
| time/                   |           |
|    fps                  | 109       |
|    iterations           | 78        |
|    time_elapsed         | 1465      |
|    total_timesteps      | 159744    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 38.6      |
|    n_updates            | 770       |
|    policy_gradient_loss | 2.71e-10  |
|    value_loss           | 104       |
---------------------------------------
Num timesteps: 160000
Best mean reward: -151.74 - Last mean reward per episode: -193.81
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 410       |
|    ep_rew_mean          | -188      |
| time/                   |           |
|    fps                  | 108       |
|    iterations           | 79        |
|    time_elapsed         | 1487      |
|    total_timesteps      | 161792    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0003    |
|    loss                 | 10.1      |
|    n_updates            | 780       |
|    policy_gradient_loss | 2.11e-09  |
|    value_loss           | 55.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 410       |
|    ep_rew_mean          | -188      |
| time/                   |           |
|    fps                  | 108       |
|    iterations           | 80        |
|    time_elapsed         | 1511      |
|    total_timesteps      | 163840    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 40.3      |
|    n_updates            | 790       |
|    policy_gradient_loss | 4.86e-10  |
|    value_loss           | 86.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 407       |
|    ep_rew_mean          | -186      |
| time/                   |           |
|    fps                  | 107       |
|    iterations           | 81        |
|    time_elapsed         | 1537      |
|    total_timesteps      | 165888    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 10.5      |
|    n_updates            | 800       |
|    policy_gradient_loss | -3.74e-09 |
|    value_loss           | 37.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 400       |
|    ep_rew_mean          | -182      |
| time/                   |           |
|    fps                  | 107       |
|    iterations           | 82        |
|    time_elapsed         | 1562      |
|    total_timesteps      | 167936    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 27.1      |
|    n_updates            | 810       |
|    policy_gradient_loss | 1.89e-10  |
|    value_loss           | 82.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 394       |
|    ep_rew_mean          | -179      |
| time/                   |           |
|    fps                  | 106       |
|    iterations           | 83        |
|    time_elapsed         | 1589      |
|    total_timesteps      | 169984    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 1.89      |
|    n_updates            | 820       |
|    policy_gradient_loss | 6.72e-09  |
|    value_loss           | 45.6      |
---------------------------------------
Num timesteps: 170000
Best mean reward: -151.74 - Last mean reward per episode: -179.24
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 398       |
|    ep_rew_mean          | -182      |
| time/                   |           |
|    fps                  | 106       |
|    iterations           | 84        |
|    time_elapsed         | 1612      |
|    total_timesteps      | 172032    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 15.1      |
|    n_updates            | 830       |
|    policy_gradient_loss | 1.02e-09  |
|    value_loss           | 45.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 403       |
|    ep_rew_mean          | -185      |
| time/                   |           |
|    fps                  | 106       |
|    iterations           | 85        |
|    time_elapsed         | 1641      |
|    total_timesteps      | 174080    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 8.46      |
|    n_updates            | 840       |
|    policy_gradient_loss | 2.18e-08  |
|    value_loss           | 12.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 409       |
|    ep_rew_mean          | -188      |
| time/                   |           |
|    fps                  | 105       |
|    iterations           | 86        |
|    time_elapsed         | 1664      |
|    total_timesteps      | 176128    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 11.9      |
|    n_updates            | 850       |
|    policy_gradient_loss | -3.43e-09 |
|    value_loss           | 27.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 424       |
|    ep_rew_mean          | -195      |
| time/                   |           |
|    fps                  | 105       |
|    iterations           | 87        |
|    time_elapsed         | 1688      |
|    total_timesteps      | 178176    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 7.65      |
|    n_updates            | 860       |
|    policy_gradient_loss | 1.66e-09  |
|    value_loss           | 69.5      |
---------------------------------------
Num timesteps: 180000
Best mean reward: -151.74 - Last mean reward per episode: -193.06
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 419       |
|    ep_rew_mean          | -193      |
| time/                   |           |
|    fps                  | 105       |
|    iterations           | 88        |
|    time_elapsed         | 1713      |
|    total_timesteps      | 180224    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 3.22      |
|    n_updates            | 870       |
|    policy_gradient_loss | 7.76e-10  |
|    value_loss           | 28        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 429       |
|    ep_rew_mean          | -199      |
| time/                   |           |
|    fps                  | 105       |
|    iterations           | 89        |
|    time_elapsed         | 1735      |
|    total_timesteps      | 182272    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 2.76      |
|    n_updates            | 880       |
|    policy_gradient_loss | 4.44e-09  |
|    value_loss           | 34.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 426       |
|    ep_rew_mean          | -198      |
| time/                   |           |
|    fps                  | 104       |
|    iterations           | 90        |
|    time_elapsed         | 1759      |
|    total_timesteps      | 184320    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 52.5      |
|    n_updates            | 890       |
|    policy_gradient_loss | 7.83e-10  |
|    value_loss           | 58        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 423       |
|    ep_rew_mean          | -196      |
| time/                   |           |
|    fps                  | 104       |
|    iterations           | 91        |
|    time_elapsed         | 1780      |
|    total_timesteps      | 186368    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 41.6      |
|    n_updates            | 900       |
|    policy_gradient_loss | -3.52e-10 |
|    value_loss           | 160       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 407       |
|    ep_rew_mean          | -188      |
| time/                   |           |
|    fps                  | 104       |
|    iterations           | 92        |
|    time_elapsed         | 1803      |
|    total_timesteps      | 188416    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 43.7      |
|    n_updates            | 910       |
|    policy_gradient_loss | 7.03e-09  |
|    value_loss           | 37.7      |
---------------------------------------
Num timesteps: 190000
Best mean reward: -151.74 - Last mean reward per episode: -175.71
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 385       |
|    ep_rew_mean          | -176      |
| time/                   |           |
|    fps                  | 104       |
|    iterations           | 93        |
|    time_elapsed         | 1826      |
|    total_timesteps      | 190464    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0003    |
|    loss                 | 62.5      |
|    n_updates            | 920       |
|    policy_gradient_loss | 1.42e-09  |
|    value_loss           | 121       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 373       |
|    ep_rew_mean          | -169      |
| time/                   |           |
|    fps                  | 104       |
|    iterations           | 94        |
|    time_elapsed         | 1847      |
|    total_timesteps      | 192512    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 42.2      |
|    n_updates            | 930       |
|    policy_gradient_loss | -1.66e-09 |
|    value_loss           | 82.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 377       |
|    ep_rew_mean          | -171      |
| time/                   |           |
|    fps                  | 103       |
|    iterations           | 95        |
|    time_elapsed         | 1873      |
|    total_timesteps      | 194560    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 34        |
|    n_updates            | 940       |
|    policy_gradient_loss | 7.04e-10  |
|    value_loss           | 101       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 376       |
|    ep_rew_mean          | -171      |
| time/                   |           |
|    fps                  | 95        |
|    iterations           | 96        |
|    time_elapsed         | 2056      |
|    total_timesteps      | 196608    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 14.7      |
|    n_updates            | 950       |
|    policy_gradient_loss | 2.11e-10  |
|    value_loss           | 46.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 388       |
|    ep_rew_mean          | -177      |
| time/                   |           |
|    fps                  | 95        |
|    iterations           | 97        |
|    time_elapsed         | 2085      |
|    total_timesteps      | 198656    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0003    |
|    loss                 | 27.1      |
|    n_updates            | 960       |
|    policy_gradient_loss | 1.73e-09  |
|    value_loss           | 87.6      |
---------------------------------------
Num timesteps: 200000
Best mean reward: -151.74 - Last mean reward per episode: -176.88
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 394       |
|    ep_rew_mean          | -180      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 98        |
|    time_elapsed         | 2116      |
|    total_timesteps      | 200704    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 4.78      |
|    n_updates            | 970       |
|    policy_gradient_loss | 4.69e-09  |
|    value_loss           | 35.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 413       |
|    ep_rew_mean          | -190      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 99        |
|    time_elapsed         | 2146      |
|    total_timesteps      | 202752    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 5.19      |
|    n_updates            | 980       |
|    policy_gradient_loss | -4.99e-09 |
|    value_loss           | 11.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 406       |
|    ep_rew_mean          | -187      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 100       |
|    time_elapsed         | 2171      |
|    total_timesteps      | 204800    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0003    |
|    loss                 | 8.51      |
|    n_updates            | 990       |
|    policy_gradient_loss | 4.69e-09  |
|    value_loss           | 31.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 413       |
|    ep_rew_mean          | -190      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 101       |
|    time_elapsed         | 2197      |
|    total_timesteps      | 206848    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 46.4      |
|    n_updates            | 1000      |
|    policy_gradient_loss | 1.24e-09  |
|    value_loss           | 66.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 416       |
|    ep_rew_mean          | -191      |
| time/                   |           |
|    fps                  | 93        |
|    iterations           | 102       |
|    time_elapsed         | 2229      |
|    total_timesteps      | 208896    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 52.6      |
|    n_updates            | 1010      |
|    policy_gradient_loss | 1.83e-09  |
|    value_loss           | 78.6      |
---------------------------------------
Num timesteps: 210000
Best mean reward: -151.74 - Last mean reward per episode: -189.81
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 403       |
|    ep_rew_mean          | -185      |
| time/                   |           |
|    fps                  | 93        |
|    iterations           | 103       |
|    time_elapsed         | 2256      |
|    total_timesteps      | 210944    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 83.4      |
|    n_updates            | 1020      |
|    policy_gradient_loss | -9.47e-10 |
|    value_loss           | 69.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 402       |
|    ep_rew_mean          | -184      |
| time/                   |           |
|    fps                  | 93        |
|    iterations           | 104       |
|    time_elapsed         | 2281      |
|    total_timesteps      | 212992    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 49.7      |
|    n_updates            | 1030      |
|    policy_gradient_loss | -2.43e-09 |
|    value_loss           | 59.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 398       |
|    ep_rew_mean          | -182      |
| time/                   |           |
|    fps                  | 93        |
|    iterations           | 105       |
|    time_elapsed         | 2302      |
|    total_timesteps      | 215040    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 34.6      |
|    n_updates            | 1040      |
|    policy_gradient_loss | 5.61e-08  |
|    value_loss           | 20.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 407       |
|    ep_rew_mean          | -186      |
| time/                   |           |
|    fps                  | 93        |
|    iterations           | 106       |
|    time_elapsed         | 2318      |
|    total_timesteps      | 217088    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0003    |
|    loss                 | 27.7      |
|    n_updates            | 1050      |
|    policy_gradient_loss | 1.36e-09  |
|    value_loss           | 54.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 402       |
|    ep_rew_mean          | -184      |
| time/                   |           |
|    fps                  | 93        |
|    iterations           | 107       |
|    time_elapsed         | 2337      |
|    total_timesteps      | 219136    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 22.9      |
|    n_updates            | 1060      |
|    policy_gradient_loss | -1.99e-10 |
|    value_loss           | 40.8      |
---------------------------------------
Num timesteps: 220000
Best mean reward: -151.74 - Last mean reward per episode: -184.50
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 398       |
|    ep_rew_mean          | -181      |
| time/                   |           |
|    fps                  | 93        |
|    iterations           | 108       |
|    time_elapsed         | 2356      |
|    total_timesteps      | 221184    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 8.34      |
|    n_updates            | 1070      |
|    policy_gradient_loss | -6.9e-10  |
|    value_loss           | 50.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 399       |
|    ep_rew_mean          | -181      |
| time/                   |           |
|    fps                  | 93        |
|    iterations           | 109       |
|    time_elapsed         | 2375      |
|    total_timesteps      | 223232    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 39        |
|    n_updates            | 1080      |
|    policy_gradient_loss | 8.07e-10  |
|    value_loss           | 69        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 413       |
|    ep_rew_mean          | -189      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 110       |
|    time_elapsed         | 2393      |
|    total_timesteps      | 225280    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 19.9      |
|    n_updates            | 1090      |
|    policy_gradient_loss | 2.56e-09  |
|    value_loss           | 41.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 408       |
|    ep_rew_mean          | -186      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 111       |
|    time_elapsed         | 2413      |
|    total_timesteps      | 227328    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 40.8      |
|    n_updates            | 1100      |
|    policy_gradient_loss | -2.91e-09 |
|    value_loss           | 94.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 413       |
|    ep_rew_mean          | -189      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 112       |
|    time_elapsed         | 2432      |
|    total_timesteps      | 229376    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0003    |
|    loss                 | 75.3      |
|    n_updates            | 1110      |
|    policy_gradient_loss | 1.35e-09  |
|    value_loss           | 117       |
---------------------------------------
Num timesteps: 230000
Best mean reward: -151.74 - Last mean reward per episode: -188.52
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 422       |
|    ep_rew_mean          | -193      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 113       |
|    time_elapsed         | 2451      |
|    total_timesteps      | 231424    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 43.1      |
|    n_updates            | 1120      |
|    policy_gradient_loss | -1.89e-10 |
|    value_loss           | 79.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 427       |
|    ep_rew_mean          | -196      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 114       |
|    time_elapsed         | 2471      |
|    total_timesteps      | 233472    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 32.7      |
|    n_updates            | 1130      |
|    policy_gradient_loss | -2.08e-09 |
|    value_loss           | 49.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 421       |
|    ep_rew_mean          | -192      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 115       |
|    time_elapsed         | 2489      |
|    total_timesteps      | 235520    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 53.5      |
|    n_updates            | 1140      |
|    policy_gradient_loss | -8.29e-11 |
|    value_loss           | 112       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 415       |
|    ep_rew_mean          | -190      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 116       |
|    time_elapsed         | 2508      |
|    total_timesteps      | 237568    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000231 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 10.7      |
|    n_updates            | 1150      |
|    policy_gradient_loss | 1.8e-09   |
|    value_loss           | 86.6      |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 411         |
|    ep_rew_mean          | -188        |
| time/                   |             |
|    fps                  | 94          |
|    iterations           | 117         |
|    time_elapsed         | 2526        |
|    total_timesteps      | 239616      |
| train/                  |             |
|    approx_kl            | 0.000165921 |
|    clip_fraction        | 0.000439    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000119   |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 28          |
|    n_updates            | 1160        |
|    policy_gradient_loss | -3e-05      |
|    value_loss           | 85.5        |
-----------------------------------------
Num timesteps: 240000
Best mean reward: -151.74 - Last mean reward per episode: -187.81
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 407       |
|    ep_rew_mean          | -186      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 118       |
|    time_elapsed         | 2547      |
|    total_timesteps      | 241664    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000112 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 24.6      |
|    n_updates            | 1170      |
|    policy_gradient_loss | 3.35e-09  |
|    value_loss           | 51.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 406       |
|    ep_rew_mean          | -185      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 119       |
|    time_elapsed         | 2569      |
|    total_timesteps      | 243712    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000112 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 4.92      |
|    n_updates            | 1180      |
|    policy_gradient_loss | 2.35e-08  |
|    value_loss           | 18.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 410       |
|    ep_rew_mean          | -187      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 120       |
|    time_elapsed         | 2592      |
|    total_timesteps      | 245760    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000112 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0003    |
|    loss                 | 2.06      |
|    n_updates            | 1190      |
|    policy_gradient_loss | -2.22e-10 |
|    value_loss           | 25.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 421       |
|    ep_rew_mean          | -193      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 121       |
|    time_elapsed         | 2616      |
|    total_timesteps      | 247808    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000112 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 26.5      |
|    n_updates            | 1200      |
|    policy_gradient_loss | -7.58e-10 |
|    value_loss           | 65.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 389       |
|    ep_rew_mean          | -176      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 122       |
|    time_elapsed         | 2646      |
|    total_timesteps      | 249856    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000112 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 32.5      |
|    n_updates            | 1210      |
|    policy_gradient_loss | 3.58e-09  |
|    value_loss           | 57.5      |
---------------------------------------
Num timesteps: 250000
Best mean reward: -151.74 - Last mean reward per episode: -167.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 372       |
|    ep_rew_mean          | -167      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 123       |
|    time_elapsed         | 2672      |
|    total_timesteps      | 251904    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000112 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 98.7      |
|    n_updates            | 1220      |
|    policy_gradient_loss | -7.76e-10 |
|    value_loss           | 163       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 374       |
|    ep_rew_mean          | -169      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 124       |
|    time_elapsed         | 2697      |
|    total_timesteps      | 253952    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000112 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 15.2      |
|    n_updates            | 1230      |
|    policy_gradient_loss | -3.17e-09 |
|    value_loss           | 45.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 376       |
|    ep_rew_mean          | -169      |
| time/                   |           |
|    fps                  | 93        |
|    iterations           | 125       |
|    time_elapsed         | 2724      |
|    total_timesteps      | 256000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000112 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 8.73      |
|    n_updates            | 1240      |
|    policy_gradient_loss | 1.48e-09  |
|    value_loss           | 48.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 371       |
|    ep_rew_mean          | -167      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 126       |
|    time_elapsed         | 2744      |
|    total_timesteps      | 258048    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000112 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 13.8      |
|    n_updates            | 1250      |
|    policy_gradient_loss | -1.45e-09 |
|    value_loss           | 37.4      |
---------------------------------------
Num timesteps: 260000
Best mean reward: -151.74 - Last mean reward per episode: -162.54
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 363       |
|    ep_rew_mean          | -163      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 127       |
|    time_elapsed         | 2762      |
|    total_timesteps      | 260096    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000112 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 9.92      |
|    n_updates            | 1260      |
|    policy_gradient_loss | -1.76e-09 |
|    value_loss           | 56.4      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 360       |
|    ep_rew_mean          | -161      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 128       |
|    time_elapsed         | 2784      |
|    total_timesteps      | 262144    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000112 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 22.6      |
|    n_updates            | 1270      |
|    policy_gradient_loss | 2.02e-09  |
|    value_loss           | 100       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 361       |
|    ep_rew_mean          | -162      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 129       |
|    time_elapsed         | 2803      |
|    total_timesteps      | 264192    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000112 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 91        |
|    n_updates            | 1280      |
|    policy_gradient_loss | 6.94e-10  |
|    value_loss           | 80.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 372       |
|    ep_rew_mean          | -168      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 130       |
|    time_elapsed         | 2821      |
|    total_timesteps      | 266240    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000112 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 22.6      |
|    n_updates            | 1290      |
|    policy_gradient_loss | -1.62e-10 |
|    value_loss           | 72.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 357       |
|    ep_rew_mean          | -160      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 131       |
|    time_elapsed         | 2838      |
|    total_timesteps      | 268288    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000112 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 0.39      |
|    n_updates            | 1300      |
|    policy_gradient_loss | -4.85e-09 |
|    value_loss           | 19.1      |
---------------------------------------
Num timesteps: 270000
Best mean reward: -151.74 - Last mean reward per episode: -163.12
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 363       |
|    ep_rew_mean          | -163      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 132       |
|    time_elapsed         | 2855      |
|    total_timesteps      | 270336    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000112 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 32.9      |
|    n_updates            | 1310      |
|    policy_gradient_loss | -2.91e-12 |
|    value_loss           | 145       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 378       |
|    ep_rew_mean          | -171      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 133       |
|    time_elapsed         | 2874      |
|    total_timesteps      | 272384    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000112 |
|    explained_variance   | 1.79e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 9.48      |
|    n_updates            | 1320      |
|    policy_gradient_loss | 1.29e-09  |
|    value_loss           | 93.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 386       |
|    ep_rew_mean          | -175      |
| time/                   |           |
|    fps                  | 94        |
|    iterations           | 134       |
|    time_elapsed         | 2893      |
|    total_timesteps      | 274432    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000112 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 26.2      |
|    n_updates            | 1330      |
|    policy_gradient_loss | 1.24e-08  |
|    value_loss           | 22.8      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 371           |
|    ep_rew_mean          | -167          |
| time/                   |               |
|    fps                  | 94            |
|    iterations           | 135           |
|    time_elapsed         | 2911          |
|    total_timesteps      | 276480        |
| train/                  |               |
|    approx_kl            | 0.00039754086 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000241     |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 16.5          |
|    n_updates            | 1340          |
|    policy_gradient_loss | -0.000331     |
|    value_loss           | 44.1          |
-------------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 361      |
|    ep_rew_mean          | -162     |
| time/                   |          |
|    fps                  | 95       |
|    iterations           | 136      |
|    time_elapsed         | 2930     |
|    total_timesteps      | 278528   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00025 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 23.1     |
|    n_updates            | 1350     |
|    policy_gradient_loss | 1.39e-09 |
|    value_loss           | 108      |
--------------------------------------
Num timesteps: 280000
Best mean reward: -151.74 - Last mean reward per episode: -152.74
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 352      |
|    ep_rew_mean          | -157     |
| time/                   |          |
|    fps                  | 95       |
|    iterations           | 137      |
|    time_elapsed         | 2949     |
|    total_timesteps      | 280576   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00025 |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.0003   |
|    loss                 | 32.2     |
|    n_updates            | 1360     |
|    policy_gradient_loss | 0        |
|    value_loss           | 59.8     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 349       |
|    ep_rew_mean          | -156      |
| time/                   |           |
|    fps                  | 95        |
|    iterations           | 138       |
|    time_elapsed         | 2968      |
|    total_timesteps      | 282624    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00025  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 59.4      |
|    n_updates            | 1370      |
|    policy_gradient_loss | -2.98e-11 |
|    value_loss           | 83.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 359       |
|    ep_rew_mean          | -161      |
| time/                   |           |
|    fps                  | 95        |
|    iterations           | 139       |
|    time_elapsed         | 2988      |
|    total_timesteps      | 284672    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00025  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 2.35      |
|    n_updates            | 1380      |
|    policy_gradient_loss | -1.6e-09  |
|    value_loss           | 43.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 374       |
|    ep_rew_mean          | -169      |
| time/                   |           |
|    fps                  | 95        |
|    iterations           | 140       |
|    time_elapsed         | 3009      |
|    total_timesteps      | 286720    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00025  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 34.8      |
|    n_updates            | 1390      |
|    policy_gradient_loss | -1.3e-09  |
|    value_loss           | 51.9      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 389      |
|    ep_rew_mean          | -177     |
| time/                   |          |
|    fps                  | 95       |
|    iterations           | 141      |
|    time_elapsed         | 3030     |
|    total_timesteps      | 288768   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00025 |
|    explained_variance   | 1.79e-07 |
|    learning_rate        | 0.0003   |
|    loss                 | 11.3     |
|    n_updates            | 1400     |
|    policy_gradient_loss | 1.73e-08 |
|    value_loss           | 11.9     |
--------------------------------------
Num timesteps: 290000
Best mean reward: -151.74 - Last mean reward per episode: -180.19
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 404       |
|    ep_rew_mean          | -184      |
| time/                   |           |
|    fps                  | 95        |
|    iterations           | 142       |
|    time_elapsed         | 3050      |
|    total_timesteps      | 290816    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00025  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 12.1      |
|    n_updates            | 1410      |
|    policy_gradient_loss | -4.49e-09 |
|    value_loss           | 26.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 396       |
|    ep_rew_mean          | -181      |
| time/                   |           |
|    fps                  | 95        |
|    iterations           | 143       |
|    time_elapsed         | 3071      |
|    total_timesteps      | 292864    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00025  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 99.8      |
|    n_updates            | 1420      |
|    policy_gradient_loss | -1.36e-09 |
|    value_loss           | 77.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 378       |
|    ep_rew_mean          | -171      |
| time/                   |           |
|    fps                  | 95        |
|    iterations           | 144       |
|    time_elapsed         | 3091      |
|    total_timesteps      | 294912    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00025  |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 19.4      |
|    n_updates            | 1430      |
|    policy_gradient_loss | -5.06e-08 |
|    value_loss           | 20.9      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 371      |
|    ep_rew_mean          | -168     |
| time/                   |          |
|    fps                  | 95       |
|    iterations           | 145      |
|    time_elapsed         | 3110     |
|    total_timesteps      | 296960   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00025 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 26.7     |
|    n_updates            | 1440     |
|    policy_gradient_loss | 1.05e-09 |
|    value_loss           | 102      |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 380       |
|    ep_rew_mean          | -172      |
| time/                   |           |
|    fps                  | 95        |
|    iterations           | 146       |
|    time_elapsed         | 3126      |
|    total_timesteps      | 299008    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00025  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 34.6      |
|    n_updates            | 1450      |
|    policy_gradient_loss | -9.14e-10 |
|    value_loss           | 126       |
---------------------------------------
Num timesteps: 300000
Best mean reward: -151.74 - Last mean reward per episode: -174.53
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 393      |
|    ep_rew_mean          | -179     |
| time/                   |          |
|    fps                  | 95       |
|    iterations           | 147      |
|    time_elapsed         | 3147     |
|    total_timesteps      | 301056   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00025 |
|    explained_variance   | 5.96e-08 |
|    learning_rate        | 0.0003   |
|    loss                 | 1.82     |
|    n_updates            | 1460     |
|    policy_gradient_loss | 1.82e-10 |
|    value_loss           | 24       |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 377       |
|    ep_rew_mean          | -170      |
| time/                   |           |
|    fps                  | 95        |
|    iterations           | 148       |
|    time_elapsed         | 3162      |
|    total_timesteps      | 303104    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00025  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 64.8      |
|    n_updates            | 1470      |
|    policy_gradient_loss | 1.16e-09  |
|    value_loss           | 63.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 379       |
|    ep_rew_mean          | -171      |
| time/                   |           |
|    fps                  | 95        |
|    iterations           | 149       |
|    time_elapsed         | 3180      |
|    total_timesteps      | 305152    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00025  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 29        |
|    n_updates            | 1480      |
|    policy_gradient_loss | -2.47e-11 |
|    value_loss           | 98        |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 397      |
|    ep_rew_mean          | -181     |
| time/                   |          |
|    fps                  | 96       |
|    iterations           | 150      |
|    time_elapsed         | 3198     |
|    total_timesteps      | 307200   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00025 |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.0003   |
|    loss                 | 20.3     |
|    n_updates            | 1490     |
|    policy_gradient_loss | 6.6e-10  |
|    value_loss           | 61.4     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 403      |
|    ep_rew_mean          | -184     |
| time/                   |          |
|    fps                  | 96       |
|    iterations           | 151      |
|    time_elapsed         | 3217     |
|    total_timesteps      | 309248   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00025 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 19       |
|    n_updates            | 1500     |
|    policy_gradient_loss | 1.53e-09 |
|    value_loss           | 37.4     |
--------------------------------------
Num timesteps: 310000
Best mean reward: -151.74 - Last mean reward per episode: -189.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 413          |
|    ep_rew_mean          | -189         |
| time/                   |              |
|    fps                  | 96           |
|    iterations           | 152          |
|    time_elapsed         | 3234         |
|    total_timesteps      | 311296       |
| train/                  |              |
|    approx_kl            | 7.090121e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000169    |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 28.3         |
|    n_updates            | 1510         |
|    policy_gradient_loss | -2.46e-05    |
|    value_loss           | 46.4         |
------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 423       |
|    ep_rew_mean          | -195      |
| time/                   |           |
|    fps                  | 96        |
|    iterations           | 153       |
|    time_elapsed         | 3254      |
|    total_timesteps      | 313344    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 17.1      |
|    n_updates            | 1520      |
|    policy_gradient_loss | 1.24e-08  |
|    value_loss           | 12.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 420       |
|    ep_rew_mean          | -193      |
| time/                   |           |
|    fps                  | 96        |
|    iterations           | 154       |
|    time_elapsed         | 3270      |
|    total_timesteps      | 315392    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 16.6      |
|    n_updates            | 1530      |
|    policy_gradient_loss | -1.22e-09 |
|    value_loss           | 68.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 427       |
|    ep_rew_mean          | -197      |
| time/                   |           |
|    fps                  | 96        |
|    iterations           | 155       |
|    time_elapsed         | 3289      |
|    total_timesteps      | 317440    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 27.6      |
|    n_updates            | 1540      |
|    policy_gradient_loss | 3.97e-10  |
|    value_loss           | 54.6      |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 439           |
|    ep_rew_mean          | -204          |
| time/                   |               |
|    fps                  | 96            |
|    iterations           | 156           |
|    time_elapsed         | 3310          |
|    total_timesteps      | 319488        |
| train/                  |               |
|    approx_kl            | 0.00014331151 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.08e-05     |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 23            |
|    n_updates            | 1550          |
|    policy_gradient_loss | -2.17e-05     |
|    value_loss           | 46.3          |
-------------------------------------------
Num timesteps: 320000
Best mean reward: -151.74 - Last mean reward per episode: -203.53
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 423       |
|    ep_rew_mean          | -195      |
| time/                   |           |
|    fps                  | 96        |
|    iterations           | 157       |
|    time_elapsed         | 3330      |
|    total_timesteps      | 321536    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.32e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 45.8      |
|    n_updates            | 1560      |
|    policy_gradient_loss | -6.13e-10 |
|    value_loss           | 49.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 432       |
|    ep_rew_mean          | -200      |
| time/                   |           |
|    fps                  | 96        |
|    iterations           | 158       |
|    time_elapsed         | 3347      |
|    total_timesteps      | 323584    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.32e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 84.6      |
|    n_updates            | 1570      |
|    policy_gradient_loss | 6.13e-10  |
|    value_loss           | 144       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 419       |
|    ep_rew_mean          | -193      |
| time/                   |           |
|    fps                  | 96        |
|    iterations           | 159       |
|    time_elapsed         | 3366      |
|    total_timesteps      | 325632    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.32e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 8.86      |
|    n_updates            | 1580      |
|    policy_gradient_loss | -2.53e-09 |
|    value_loss           | 32.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 405       |
|    ep_rew_mean          | -186      |
| time/                   |           |
|    fps                  | 96        |
|    iterations           | 160       |
|    time_elapsed         | 3386      |
|    total_timesteps      | 327680    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.32e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 44.7      |
|    n_updates            | 1590      |
|    policy_gradient_loss | -5.82e-11 |
|    value_loss           | 77.1      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 384       |
|    ep_rew_mean          | -175      |
| time/                   |           |
|    fps                  | 96        |
|    iterations           | 161       |
|    time_elapsed         | 3405      |
|    total_timesteps      | 329728    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.32e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 23.9      |
|    n_updates            | 1600      |
|    policy_gradient_loss | -2.75e-09 |
|    value_loss           | 58.6      |
---------------------------------------
Num timesteps: 330000
Best mean reward: -151.74 - Last mean reward per episode: -179.16
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 384       |
|    ep_rew_mean          | -175      |
| time/                   |           |
|    fps                  | 96        |
|    iterations           | 162       |
|    time_elapsed         | 3422      |
|    total_timesteps      | 331776    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.32e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 50.3      |
|    n_updates            | 1610      |
|    policy_gradient_loss | 4.06e-10  |
|    value_loss           | 99.8      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 392       |
|    ep_rew_mean          | -179      |
| time/                   |           |
|    fps                  | 97        |
|    iterations           | 163       |
|    time_elapsed         | 3441      |
|    total_timesteps      | 333824    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.32e-05 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0003    |
|    loss                 | 4.92      |
|    n_updates            | 1620      |
|    policy_gradient_loss | -7.29e-10 |
|    value_loss           | 47.5      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 404       |
|    ep_rew_mean          | -185      |
| time/                   |           |
|    fps                  | 97        |
|    iterations           | 164       |
|    time_elapsed         | 3460      |
|    total_timesteps      | 335872    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.32e-05 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0003    |
|    loss                 | 18.2      |
|    n_updates            | 1630      |
|    policy_gradient_loss | 1.06e-09  |
|    value_loss           | 53.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 407       |
|    ep_rew_mean          | -186      |
| time/                   |           |
|    fps                  | 97        |
|    iterations           | 165       |
|    time_elapsed         | 3478      |
|    total_timesteps      | 337920    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.32e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 36.4      |
|    n_updates            | 1640      |
|    policy_gradient_loss | 5.09e-11  |
|    value_loss           | 38.6      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 402       |
|    ep_rew_mean          | -184      |
| time/                   |           |
|    fps                  | 97        |
|    iterations           | 166       |
|    time_elapsed         | 3495      |
|    total_timesteps      | 339968    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.32e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 64.2      |
|    n_updates            | 1650      |
|    policy_gradient_loss | 6.55e-10  |
|    value_loss           | 109       |
---------------------------------------
Num timesteps: 340000
Best mean reward: -151.74 - Last mean reward per episode: -184.21
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 406       |
|    ep_rew_mean          | -186      |
| time/                   |           |
|    fps                  | 97        |
|    iterations           | 167       |
|    time_elapsed         | 3514      |
|    total_timesteps      | 342016    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.32e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 42.7      |
|    n_updates            | 1660      |
|    policy_gradient_loss | -1.1e-09  |
|    value_loss           | 50.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 406       |
|    ep_rew_mean          | -186      |
| time/                   |           |
|    fps                  | 97        |
|    iterations           | 168       |
|    time_elapsed         | 3531      |
|    total_timesteps      | 344064    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.32e-05 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 21.6      |
|    n_updates            | 1670      |
|    policy_gradient_loss | -1.4e-10  |
|    value_loss           | 38.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 414       |
|    ep_rew_mean          | -190      |
| time/                   |           |
|    fps                  | 97        |
|    iterations           | 169       |
|    time_elapsed         | 3548      |
|    total_timesteps      | 346112    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.32e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 18.6      |
|    n_updates            | 1680      |
|    policy_gradient_loss | 1.47e-09  |
|    value_loss           | 94.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 405       |
|    ep_rew_mean          | -185      |
| time/                   |           |
|    fps                  | 97        |
|    iterations           | 170       |
|    time_elapsed         | 3564      |
|    total_timesteps      | 348160    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.32e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 9.33      |
|    n_updates            | 1690      |
|    policy_gradient_loss | 3.15e-09  |
|    value_loss           | 67.8      |
---------------------------------------
Num timesteps: 350000
Best mean reward: -151.74 - Last mean reward per episode: -185.93
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 415       |
|    ep_rew_mean          | -191      |
| time/                   |           |
|    fps                  | 97        |
|    iterations           | 171       |
|    time_elapsed         | 3584      |
|    total_timesteps      | 350208    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.32e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 25        |
|    n_updates            | 1700      |
|    policy_gradient_loss | -3.26e-10 |
|    value_loss           | 42.9      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 405       |
|    ep_rew_mean          | -185      |
| time/                   |           |
|    fps                  | 97        |
|    iterations           | 172       |
|    time_elapsed         | 3603      |
|    total_timesteps      | 352256    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.32e-05 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0003    |
|    loss                 | 39.8      |
|    n_updates            | 1710      |
|    policy_gradient_loss | 3.67e-10  |
|    value_loss           | 52        |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 413       |
|    ep_rew_mean          | -189      |
| time/                   |           |
|    fps                  | 97        |
|    iterations           | 173       |
|    time_elapsed         | 3622      |
|    total_timesteps      | 354304    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.32e-05 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0003    |
|    loss                 | 31.5      |
|    n_updates            | 1720      |
|    policy_gradient_loss | 4.02e-10  |
|    value_loss           | 52.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 415       |
|    ep_rew_mean          | -190      |
| time/                   |           |
|    fps                  | 97        |
|    iterations           | 174       |
|    time_elapsed         | 3641      |
|    total_timesteps      | 356352    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.32e-05 |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 10.8      |
|    n_updates            | 1730      |
|    policy_gradient_loss | 8.25e-09  |
|    value_loss           | 20.7      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 408       |
|    ep_rew_mean          | -186      |
| time/                   |           |
|    fps                  | 97        |
|    iterations           | 175       |
|    time_elapsed         | 3658      |
|    total_timesteps      | 358400    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.32e-05 |
|    explained_variance   | 1.79e-07  |
|    learning_rate        | 0.0003    |
|    loss                 | 6.55      |
|    n_updates            | 1740      |
|    policy_gradient_loss | -2.3e-10  |
|    value_loss           | 41.8      |
---------------------------------------
Num timesteps: 360000
Best mean reward: -151.74 - Last mean reward per episode: -179.01
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 393       |
|    ep_rew_mean          | -178      |
| time/                   |           |
|    fps                  | 98        |
|    iterations           | 176       |
|    time_elapsed         | 3675      |
|    total_timesteps      | 360448    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.32e-05 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 73.6      |
|    n_updates            | 1750      |
|    policy_gradient_loss | -1.34e-09 |
|    value_loss           | 119       |
---------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 391          |
|    ep_rew_mean          | -177         |
| time/                   |              |
|    fps                  | 98           |
|    iterations           | 177          |
|    time_elapsed         | 3692         |
|    total_timesteps      | 362496       |
| train/                  |              |
|    approx_kl            | 0.0001370807 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.92e-05    |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 98.3         |
|    n_updates            | 1760         |
|    policy_gradient_loss | -3.57e-05    |
|    value_loss           | 105          |
------------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 392      |
|    ep_rew_mean          | -178     |
| time/                   |          |
|    fps                  | 98       |
|    iterations           | 178      |
|    time_elapsed         | 3711     |
|    total_timesteps      | 364544   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -4.6e-05 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 36.7     |
|    n_updates            | 1770     |
|    policy_gradient_loss | -1.2e-09 |
|    value_loss           | 91.3     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 413       |
|    ep_rew_mean          | -189      |
| time/                   |           |
|    fps                  | 98        |
|    iterations           | 179       |
|    time_elapsed         | 3732      |
|    total_timesteps      | 366592    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.6e-05  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 37.6      |
|    n_updates            | 1780      |
|    policy_gradient_loss | -5.29e-10 |
|    value_loss           | 36.2      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 403      |
|    ep_rew_mean          | -184     |
| time/                   |          |
|    fps                  | 98       |
|    iterations           | 180      |
|    time_elapsed         | 3751     |
|    total_timesteps      | 368640   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -4.6e-05 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 8.85     |
|    n_updates            | 1790     |
|    policy_gradient_loss | 9.61e-09 |
|    value_loss           | 32.5     |
--------------------------------------
Num timesteps: 370000
Best mean reward: -151.74 - Last mean reward per episode: -182.83
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 389       |
|    ep_rew_mean          | -176      |
| time/                   |           |
|    fps                  | 98        |
|    iterations           | 181       |
|    time_elapsed         | 3769      |
|    total_timesteps      | 370688    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.6e-05  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0003    |
|    loss                 | 47.4      |
|    n_updates            | 1800      |
|    policy_gradient_loss | -2.08e-09 |
|    value_loss           | 97.4      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 389      |
|    ep_rew_mean          | -176     |
| time/                   |          |
|    fps                  | 98       |
|    iterations           | 182      |
|    time_elapsed         | 3789     |
|    total_timesteps      | 372736   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -4.6e-05 |
|    explained_variance   | 0        |
|    learning_rate        | 0.0003   |
|    loss                 | 41.3     |
|    n_updates            | 1810     |
|    policy_gradient_loss | 2.62e-09 |
|    value_loss           | 101      |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 385       |
|    ep_rew_mean          | -175      |
| time/                   |           |
|    fps                  | 98        |
|    iterations           | 183       |
|    time_elapsed         | 3807      |
|    total_timesteps      | 374784    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.6e-05  |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 6.7       |
|    n_updates            | 1820      |
|    policy_gradient_loss | -1.04e-09 |
|    value_loss           | 64.2      |
---------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 291, in <module>
    model.learn(total_timesteps=1000000, callback=[WandbCallback(), save_callback])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 313, in learn
    self.train()
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 217, in train
    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 730, in evaluate_actions
    features = self.extract_features(obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/policy.py", line 43, in forward
    pyg_data = self.encoder.encode(observations)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/encoder/GraphEncoder.py", line 572, in encode
    graph.add_node(i, type=self.obj_type_id, features=node_features[i].tolist())
KeyboardInterrupt