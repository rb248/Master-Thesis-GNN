
Using cpu device
-----------------------------
| time/              |      |
|    fps             | 161  |
|    iterations      | 1    |
|    time_elapsed    | 12   |
|    total_timesteps | 2048 |
-----------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 68          |
|    iterations           | 2           |
|    time_elapsed         | 59          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.010487776 |
|    clip_fraction        | 0.0321      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | -0.0271     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0135     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00125    |
|    value_loss           | 0.000722    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 4.1e+03      |
|    ep_rew_mean          | -4.73        |
| time/                   |              |
|    fps                  | 57           |
|    iterations           | 3            |
|    time_elapsed         | 106          |
|    total_timesteps      | 6144         |
| train/                  |              |
|    approx_kl            | 0.0063770115 |
|    clip_fraction        | 0.0551       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.07        |
|    explained_variance   | 0.00483      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00946     |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00189     |
|    value_loss           | 0.000262     |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.41e+03    |
|    ep_rew_mean          | -3.87       |
| time/                   |             |
|    fps                  | 50          |
|    iterations           | 4           |
|    time_elapsed         | 163         |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.014638903 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 2.13e-05    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0265      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.007      |
|    value_loss           | 0.000341    |
-----------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -3.87
Saving new best model at 6829 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.41e+03    |
|    ep_rew_mean          | -3.87       |
| time/                   |             |
|    fps                  | 48          |
|    iterations           | 5           |
|    time_elapsed         | 212         |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.015287446 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.985      |
|    explained_variance   | 0.00227     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.011       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00361    |
|    value_loss           | 0.00122     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.41e+03    |
|    ep_rew_mean          | -3.95       |
| time/                   |             |
|    fps                  | 46          |
|    iterations           | 6           |
|    time_elapsed         | 265         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.013395026 |
|    clip_fraction        | 0.0242      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.902      |
|    explained_variance   | 0.00442     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0109      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00127    |
|    value_loss           | 0.000133    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.52e+03    |
|    ep_rew_mean          | 70.9        |
| time/                   |             |
|    fps                  | 45          |
|    iterations           | 7           |
|    time_elapsed         | 313         |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.004717868 |
|    clip_fraction        | 0.0118      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.926      |
|    explained_variance   | -0.0147     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00744    |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.000591    |
|    value_loss           | 0.000148    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.52e+03    |
|    ep_rew_mean          | 70.9        |
| time/                   |             |
|    fps                  | 45          |
|    iterations           | 8           |
|    time_elapsed         | 357         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.013225904 |
|    clip_fraction        | 0.0536      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.894      |
|    explained_variance   | 4.17e-06    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.46        |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00198    |
|    value_loss           | 101         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.28e+03    |
|    ep_rew_mean          | 136         |
| time/                   |             |
|    fps                  | 45          |
|    iterations           | 9           |
|    time_elapsed         | 404         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.009179793 |
|    clip_fraction        | 0.0696      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.756      |
|    explained_variance   | 2.03e-06    |
|    learning_rate        | 0.0003      |
|    loss                 | 49.1        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00303    |
|    value_loss           | 159         |
-----------------------------------------
Num timesteps: 20000
Best mean reward: -3.87 - Last mean reward per episode: 196.21
Saving new best model at 19555 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.26e+03     |
|    ep_rew_mean          | 196          |
| time/                   |              |
|    fps                  | 44           |
|    iterations           | 10           |
|    time_elapsed         | 457          |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.0028267186 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.733       |
|    explained_variance   | -3.58e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 36.5         |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.000668    |
|    value_loss           | 159          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.26e+03     |
|    ep_rew_mean          | 196          |
| time/                   |              |
|    fps                  | 44           |
|    iterations           | 11           |
|    time_elapsed         | 508          |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0034298275 |
|    clip_fraction        | 0.0561       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.723       |
|    explained_variance   | 1.73e-06     |
|    learning_rate        | 0.0003       |
|    loss                 | 72.7         |
|    n_updates            | 100          |
|    policy_gradient_loss | -3.55e-05    |
|    value_loss           | 160          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.22e+03     |
|    ep_rew_mean          | 311          |
| time/                   |              |
|    fps                  | 43           |
|    iterations           | 12           |
|    time_elapsed         | 564          |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0018056063 |
|    clip_fraction        | 0.00713      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.669       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 134          |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.000134    |
|    value_loss           | 267          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.23e+03     |
|    ep_rew_mean          | 384          |
| time/                   |              |
|    fps                  | 43           |
|    iterations           | 13           |
|    time_elapsed         | 615          |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.0053865965 |
|    clip_fraction        | 0.0221       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.606       |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 103          |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.0012      |
|    value_loss           | 195          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.23e+03     |
|    ep_rew_mean          | 384          |
| time/                   |              |
|    fps                  | 43           |
|    iterations           | 14           |
|    time_elapsed         | 666          |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0014829205 |
|    clip_fraction        | 0.022        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.53        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 125          |
|    n_updates            | 130          |
|    policy_gradient_loss | 0.000519     |
|    value_loss           | 196          |
------------------------------------------
Num timesteps: 30000
Best mean reward: 196.21 - Last mean reward per episode: 407.32
Saving new best model at 28673 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.19e+03     |
|    ep_rew_mean          | 407          |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 15           |
|    time_elapsed         | 716          |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.0016282473 |
|    clip_fraction        | 0.0194       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.559       |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 60.1         |
|    n_updates            | 140          |
|    policy_gradient_loss | 0.000503     |
|    value_loss           | 195          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.21e+03     |
|    ep_rew_mean          | 506          |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 16           |
|    time_elapsed         | 769          |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0024367531 |
|    clip_fraction        | 0.029        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.472       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 80.5         |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00135     |
|    value_loss           | 299          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.21e+03     |
|    ep_rew_mean          | 506          |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 17           |
|    time_elapsed         | 823          |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0013286588 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.464       |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 166          |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.000428    |
|    value_loss           | 300          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.17e+03      |
|    ep_rew_mean          | 505           |
| time/                   |               |
|    fps                  | 42            |
|    iterations           | 18            |
|    time_elapsed         | 875           |
|    total_timesteps      | 36864         |
| train/                  |               |
|    approx_kl            | 0.00038306433 |
|    clip_fraction        | 0.031         |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.419        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 2.66          |
|    n_updates            | 170           |
|    policy_gradient_loss | -7e-05        |
|    value_loss           | 121           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.14e+03      |
|    ep_rew_mean          | 546           |
| time/                   |               |
|    fps                  | 41            |
|    iterations           | 19            |
|    time_elapsed         | 930           |
|    total_timesteps      | 38912         |
| train/                  |               |
|    approx_kl            | 0.00080119394 |
|    clip_fraction        | 0.0187        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.421        |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 72.8          |
|    n_updates            | 180           |
|    policy_gradient_loss | -0.000831     |
|    value_loss           | 264           |
-------------------------------------------
Num timesteps: 40000
Best mean reward: 407.32 - Last mean reward per episode: 546.23
Saving new best model at 37629 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 3.11e+03      |
|    ep_rew_mean          | 550           |
| time/                   |               |
|    fps                  | 41            |
|    iterations           | 20            |
|    time_elapsed         | 985           |
|    total_timesteps      | 40960         |
| train/                  |               |
|    approx_kl            | 0.00086378714 |
|    clip_fraction        | 0.00776       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.41         |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 92.4          |
|    n_updates            | 190           |
|    policy_gradient_loss | 0.000207      |
|    value_loss           | 231           |
-------------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 290, in <module>
    model.learn(total_timesteps=1000000, callback=[WandbCallback(), save_callback])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 300, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 179, in collect_rollouts
    actions, values, log_probs = self.policy(obs_tensor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 645, in forward
    features = self.extract_features(obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/policy.py", line 45, in forward
    obj_emb = self.model(pyg_data.x_dict, pyg_data.edge_index_dict, pyg_data.batch_dict)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_gnn.py", line 103, in forward
    self.layer(x_dict, edge_index_dict)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/model/hetero_gnn.py", line 84, in layer
    obj_emb = self.obj_update(obj_emb)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/models/mlp.py", line 245, in forward
    x = self.lins[-1](x)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch_geometric/nn/dense/linear.py", line 147, in forward
    return F.linear(x, self.weight, self.bias)
KeyboardInterrupt