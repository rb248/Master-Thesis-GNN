diff --git a/games/encoder/__pycache__/GraphEncoder.cpython-310.pyc b/games/encoder/__pycache__/GraphEncoder.cpython-310.pyc
index ed8debb..32692bf 100644
Binary files a/games/encoder/__pycache__/GraphEncoder.cpython-310.pyc and b/games/encoder/__pycache__/GraphEncoder.cpython-310.pyc differ
diff --git a/games/freeway/__pycache__/run_supervised_gnn.cpython-310.pyc b/games/freeway/__pycache__/run_supervised_gnn.cpython-310.pyc
index 5d5a053..1447f50 100644
Binary files a/games/freeway/__pycache__/run_supervised_gnn.cpython-310.pyc and b/games/freeway/__pycache__/run_supervised_gnn.cpython-310.pyc differ
diff --git a/games/freeway/freeway_envs/__pycache__/freeway_env.cpython-310.pyc b/games/freeway/freeway_envs/__pycache__/freeway_env.cpython-310.pyc
index d739a48..cfaef85 100644
Binary files a/games/freeway/freeway_envs/__pycache__/freeway_env.cpython-310.pyc and b/games/freeway/freeway_envs/__pycache__/freeway_env.cpython-310.pyc differ
diff --git a/games/freeway/freeway_envs/freeway_env.py b/games/freeway/freeway_envs/freeway_env.py
index c2c7f50..1111a38 100644
--- a/games/freeway/freeway_envs/freeway_env.py
+++ b/games/freeway/freeway_envs/freeway_env.py
@@ -8,6 +8,7 @@ import networkx as nx
 from torch_geometric.data import HeteroData, Batch
 from collections import defaultdict
 from itertools import combinations
+from stable_baselines3 import PPO
 
 class FreewayEnv(gym.Env):
     metadata = {'render_modes': ['human', 'rgb_array']}
@@ -93,6 +94,7 @@ class FreewayEnv(gym.Env):
         hit = any(self.player_rect.colliderect(pygame.Rect(car['x'], car['lane'], self.car_width, self.car_height)) for car in self.cars)
         if hit:
             self.player_rect.y = self.window_height - self.player_height - 10
+            reward = -10
 
         current_time = pygame.time.get_ticks()
         
@@ -164,16 +166,30 @@ class FreewayEnv(gym.Env):
     def close(self):
         pygame.quit()
 
-if __name__ == "__main__":
-    env = FreewayEnv(render_mode='human', observation_type='pixel')
-    env.reset()
+if __name__=="__main__":
+    env = FreewayEnv(render_mode='human', observation_type='graph')
 
+    #model = PPO.load("ppo_freeway_pixel")
+    model = PPO.load("logs/Freeway-GNN-training/best_model.zip")
+    #model = PPO.load("ppo_custom_heterognn")
+    obs,_ = env.reset()
     done = False
-    try:
+    total_reward = 0
+    n_episodes = 10
+    for _ in range(n_episodes):
+        obs,_ = env.reset()
+        done = False
         while not done:
-            action = env.action_space.sample()
-            _, _, done, _,_ = env.step(action)
+            action, _ = model.predict(obs)
+            # if action == 2 or action == 0:
+            #     print(f"Player speed: {env.player_speed}")
+            #action = env.action_space.sample()
+            obs, reward, done, _,_ = env.step(action)
+            total_reward += reward
+            pygame.time.delay(50)
             env.render()
-            pygame.time.wait(10)
-    finally:
-        env.close()
+
+        print(f"Total reward: {total_reward}")
+        total_reward = 0
+
+    env.close()
diff --git a/games/freeway/run_supervised_gnn.py b/games/freeway/run_supervised_gnn.py
index c4d023e..65d4e07 100644
--- a/games/freeway/run_supervised_gnn.py
+++ b/games/freeway/run_supervised_gnn.py
@@ -1,12 +1,250 @@
+# # import io
+# import os
+# import numpy as np
+# from stable_baselines3 import PPO
+# from stable_baselines3.common.env_util import make_vec_env
+# from stable_baselines3.common.callbacks import BaseCallback
+# from wandb.integration.sb3 import WandbCallback
+# from games.freeway.freeway_envs.freeway_env import FreewayEnv, FreewayEnvConstant
+# from games.model.policy import CustomHeteroGNN
+# from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
+# from stable_baselines3.common.monitor import Monitor
+# from stable_baselines3.common.callbacks import EvalCallback
+# from stable_baselines3.common.monitor import load_results
+# from stable_baselines3.common.results_plotter import ts2xy
+# import torch
+# import wandb
+
+# log_dir = "./logs/Freeway-GNN-training/"
+# os.makedirs(log_dir, exist_ok=True)
+
+# class SaveOnBestTrainingRewardCallback(BaseCallback):
+#     def __init__(self, check_freq, log_dir, verbose=1):
+#         super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)
+#         self.check_freq = check_freq
+#         self.log_dir = log_dir
+#         self.save_path = os.path.join(log_dir, "best_model")
+#         self.best_mean_reward = -np.inf
+
+#     def _init_callback(self) -> None:
+#         if self.save_path is not None:
+#             os.makedirs(self.save_path, exist_ok=True)
+
+#     def _on_step(self) -> bool:
+#         if self.n_calls % self.check_freq == 0:
+#             x, y = ts2xy(load_results(self.log_dir), "timesteps")
+#             if len(x) > 0:
+#                 mean_reward = np.mean(y[-100:])
+#                 if self.verbose > 0:
+#                     print(f"Num timesteps: {self.num_timesteps}")
+#                     print(f"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}")
+#                 if mean_reward > self.best_mean_reward:
+#                     self.best_mean_reward = mean_reward
+#                     if self.verbose > 0:
+#                         print(f"Saving new best model at {x[-1]} timesteps")
+#                         print(f"Saving new best model to {self.save_path}.zip")
+#                     self.model.save(self.save_path)
+#                 #wandb.log({"mean_reward": mean_reward, "timesteps": self.num_timesteps})
+#             else:
+#                 device = "cpu"
+#                 if self.verbose > 0:
+#                     print("No data available for logging.")
+#                 #wandb.log({"timesteps": self.num_timesteps})
+#         return True
+
+# def make_env(lanes, max_cars, car_speed, seed=0, rank=None):
+#     def _init():
+#         env = FreewayEnvConstant( render_mode='human', observation_type='graph')
+#         monitor_path = os.path.join(log_dir, f"monitor_{rank}.csv")
+#         os.makedirs(log_dir, exist_ok=True)  # Create log directory if it doesn't exist
+#         env = Monitor(env, filename=monitor_path, allow_early_resets=True)
+#         env.seed(seed + rank)
+#         return env
+#     return _init
+
+
+
+# def main():
+#     # Initialize wandb
+#     wandb.init(
+#         project="gnn_atari_freeway_random",  # Replace with your project name
+#         sync_tensorboard=True,        # Automatically sync SB3 logs with wandb
+#         monitor_gym=True,             # Automatically log gym environments
+#         save_code=True                # Save the code used for this run
+#     )
+
+#     # Define environment configurations
+#     #envs = SubprocVecEnv([make_env([50, 80, 120], 20, 2, rank=i) for i in range(2)]) 
+#     envs = DummyVecEnv([make_env([50, 80, 120], 10, 2, rank=i) for i in range(1)])
+#     #env = FreewayEnv(lanes=[50, 80, 120], max_cars=20, car_speed=2, render_mode='human', observation_type='graph')
+#     policy_kwargs = dict(
+#         features_extractor_class=CustomHeteroGNN,
+#         features_extractor_kwargs=dict(
+#             features_dim=64,
+#             hidden_size=128,
+#             num_layer=2,
+#             obj_type_id='obj',
+#             arity_dict={'ChickenOnLane': 2, 'CarOnLane': 2, 'LaneNextToLane': 2},
+#             game='freeway'
+#         ),
+#     )
+#     # check if mps is available
+#     # if torch.backends.mps.is_available():
+#     #     device = "mps"
+#     # else:
+#     #     device = "cpu" 
+#     device = "cuda" if torch.cuda.is_available() else "cpu"
+#     # Create the PPO model with the custom feature extractor
+#     model = PPO('MlpPolicy', envs, policy_kwargs=policy_kwargs, verbose=2, device=device, ent_coef=0.01)
+#     #model = PPO('MlpPolicy', env, policy_kwargs= policy_kwargs, verbose=2,)
+#     #model.set_env(envs)
+#     # Set up the evaluation environment and callback
+#     # eval_env = FreewayEnv(lanes=[50, 80, 120], max_cars=20, car_speed=2, render_mode='human', observation_type='graph')
+#     # eval_env = Monitor(eval_env)  # Apply Monitor wrapper here
+#     eval_callback = EvalCallback(envs, best_model_save_path='./logs/freeway-GNN-eval',
+#                                  log_path='./logs/freeway-GNN-eval', eval_freq=5000,
+#                                  deterministic=True, render=False, n_eval_episodes=5)
+
+#     #Create and configure the custom callback
+#     save_best_callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)
+
+#     # Train the model with WandbCallback and the custom callback
+#     model.learn(total_timesteps=1000000, callback=[WandbCallback(), save_best_callback])
+#     #model.learn(total_timesteps=1000000)
+
+#     model.save("ppo_custom_heterognn_freeway")
+
+# if __name__ == "__main__":
+#     main() 
+
+# # import wandb
+# # from stable_baselines3 import PPO
+# # from stable_baselines3.common.env_util import make_vec_env
+# # from wandb.integration.sb3 import WandbCallback
+# # #from games.model.policy import CustomActorCriticPolicy
+# # from games.freeway.freeway_envs.freeway_env import FreewayEnv
+# # from games.model.policy import CustomCNN, CustomHeteroGNN
+# # import pygame 
+# # from stable_baselines3.common.vec_env import SubprocVecEnv
+# # import os
+# # from stable_baselines3.common.monitor import Monitor
+# # # #Initialize wandb
+# # wandb.init(
+# #     project="gnn_atari_freeway",  # Replace with your project name
+# #     sync_tensorboard=True,        # Automatically sync SB3 logs with wandb
+# #     monitor_gym=True,             # Automatically log gym environments
+# #     save_code=True                # Save the code used for this run
+# # )
+
+# # # wandb.init(
+# # #     project="cnn_g",  # Replace with your project name
+# # #     sync_tensorboard=True,        # Automatically sync SB3 logs with wandb
+# # #     monitor_gym=True,             # Automatically log gym environments
+# # #     save_code=True                # Save the code used for this run
+# # # )
+# # log_dir = "./logs/Freeway-GNN-training/"
+# # # Wrap the environment 
+# # def make_env(lanes, max_cars, car_speed, seed=0, rank=None):
+# #     def _init():
+# #         env = FreewayEnv(lanes=lanes, max_cars=max_cars, car_speed=car_speed, render_mode='human', observation_type='graph')
+# #         monitor_path = os.path.join(log_dir, f"monitor_{rank}.csv")
+# #         os.makedirs(log_dir, exist_ok=True)  # Create log directory if it doesn't exist
+# #         env = Monitor(env, filename=monitor_path, allow_early_resets=True)
+# #         env.seed(seed + rank)
+# #         return env
+# #     return _init 
+
+
+# # #env = FreewayEnv(render_mode='human', observation_type='graph')
+# # #env = make_vec_env(lambda: env, n_envs=8, vec_env_cls=SubprocVecEnv)
+# # 
+# # # policy_kwargs = dict(
+# # #     features_extractor_class=CustomCNN,
+# # #     features_extractor_kwargs=dict(features_dim=128),
+# # # )
+
+# # policy_kwargs = dict(
+# #     features_extractor_class=CustomHeteroGNN,
+# #     features_extractor_kwargs=dict(
+# #         features_dim=64,
+# #         hidden_size=64,
+# #         num_layer=2,
+# #         obj_type_id='obj',
+# #         arity_dict={'ChickenOnLane':2, 'CarOnLane':2, 'LaneNextToLane':2},
+# #         game = 'freeway'
+# #     ),
+# # )
+
+# # # # Create the PPO model with the custom feature extractor
+# # model = PPO('MlpPolicy', env, policy_kwargs=policy_kwargs, verbose=2, n_steps= 128)
+# # # # Train the model with WandbCallback
+# # model.learn(total_timesteps=1000000, callback=WandbCallback() )
+# # # # Save the model
+# # model.save("ppo_custom_heterognn")
 import wandb
 from stable_baselines3 import PPO
 from stable_baselines3.common.env_util import make_vec_env
 from wandb.integration.sb3 import WandbCallback
 #from games.model.policy import CustomActorCriticPolicy
 from games.freeway.freeway_envs.freeway_env import FreewayEnv
-from games.model.policy import CustomCNN, CustomHeteroGNN
 import pygame
+from stable_baselines3.common.callbacks import BaseCallback
+import os
+import numpy as np
+from stable_baselines3.common.vec_env import DummyVecEnv
+from stable_baselines3.common.monitor import Monitor
+from stable_baselines3.common.callbacks import EvalCallback
+from stable_baselines3.common.monitor import load_results
+from stable_baselines3.common.results_plotter import ts2xy
+from games.model.policy import CustomHeteroGNN
 #Initialize wandb
+class SaveOnBestTrainingRewardCallback(BaseCallback):
+    def __init__(self, check_freq, log_dir, verbose=1):
+        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)
+        self.check_freq = check_freq
+        self.log_dir = log_dir
+        self.save_path = os.path.join(log_dir, "best_model")
+        self.best_mean_reward = -np.inf
+
+    def _init_callback(self) -> None:
+        if self.save_path is not None:
+            os.makedirs(self.save_path, exist_ok=True)
+
+    def _on_step(self) -> bool:
+        if self.n_calls % self.check_freq == 0:
+            x, y = ts2xy(load_results(self.log_dir), "timesteps")
+            if len(x) > 0:
+                mean_reward = np.mean(y[-100:])
+                if self.verbose > 0:
+                    print(f"Num timesteps: {self.num_timesteps}")
+                    print(f"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}")
+                if mean_reward > self.best_mean_reward:
+                    self.best_mean_reward = mean_reward
+                    if self.verbose > 0:
+                        print(f"Saving new best model at {x[-1]} timesteps")
+                        print(f"Saving new best model to {self.save_path}.zip")
+                    self.model.save(self.save_path)
+                #wandb.log({"mean_reward": mean_reward, "timesteps": self.num_timesteps})
+            else:
+                device = "cpu"
+                if self.verbose > 0:
+                    print("No data available for logging.")
+                #wandb.log({"timesteps": self.num_timesteps})
+        return True 
+log_dir = "./logs/Freeway-GNN-training/"
+
+def make_env(lanes, max_cars, car_speed, seed=0, rank=None):
+    def _init():
+        env = FreewayEnv( render_mode='human', observation_type='graph')
+        monitor_path = os.path.join(log_dir, f"monitor_{rank}.csv")
+        os.makedirs(log_dir, exist_ok=True)  # Create log directory if it doesn't exist
+        env = Monitor(env, filename=monitor_path, allow_early_resets=True)
+        env.seed(seed + rank)
+        return env
+    return _init 
+envs = DummyVecEnv([make_env([50, 80, 120], 10, 2, rank=i) for i in range(1)])
+
+
 wandb.init(
     project="gnn_atari_freeway",  # Replace with your project name
     sync_tensorboard=True,        # Automatically sync SB3 logs with wandb
@@ -23,18 +261,20 @@ wandb.init(
 
 # Wrap the environment 
 
-env = FreewayEnv(render_mode='human', observation_type='graph')
+#env = FreewayEnv(render_mode='human', observation_type='graph')
 # policy_kwargs = dict(
 #     features_extractor_class=CustomCNN,
 #     features_extractor_kwargs=dict(features_dim=128),
-# )
+# ) 
+
+save_callback = SaveOnBestTrainingRewardCallback(check_freq=10000, log_dir=log_dir)
 
 policy_kwargs = dict(
     features_extractor_class=CustomHeteroGNN,
     features_extractor_kwargs=dict(
         features_dim=64,
         hidden_size=64,
-        num_layer=2,
+        num_layer=10,
         obj_type_id='obj',
         arity_dict={'ChickenOnLane':2, 'CarOnLane':2, 'LaneNextToLane':2},
         game = 'freeway'
@@ -42,9 +282,11 @@ policy_kwargs = dict(
 )
 
 # # Create the PPO model with the custom feature extractor
-model = PPO('MlpPolicy', env, policy_kwargs=policy_kwargs, verbose=1)
+model = PPO('MlpPolicy', envs, policy_kwargs=policy_kwargs, verbose=2)
+#model = PPO.load("logs/Freeway-GNN-training/best-threelanes-constant-speed.zip") 
+#model.set_env(envs)
+
 # # Train the model with WandbCallback
-model.learn(total_timesteps=1000000, callback=WandbCallback())
+model.learn(total_timesteps=1000000, callback=[WandbCallback(), save_callback]) 
 # # Save the model
-model.save("ppo_custom_heterognn")
-
+model.save("ppo_custom_heterognn")
\ No newline at end of file
diff --git a/games/model/__pycache__/hetero_gnn.cpython-310.pyc b/games/model/__pycache__/hetero_gnn.cpython-310.pyc
index 1a1deec..5030808 100644
Binary files a/games/model/__pycache__/hetero_gnn.cpython-310.pyc and b/games/model/__pycache__/hetero_gnn.cpython-310.pyc differ
diff --git a/games/model/__pycache__/hetero_message_passing.cpython-310.pyc b/games/model/__pycache__/hetero_message_passing.cpython-310.pyc
index 243e3b4..d03f7d3 100644
Binary files a/games/model/__pycache__/hetero_message_passing.cpython-310.pyc and b/games/model/__pycache__/hetero_message_passing.cpython-310.pyc differ
diff --git a/games/model/__pycache__/policy.cpython-310.pyc b/games/model/__pycache__/policy.cpython-310.pyc
index 18a2c62..e375957 100644
Binary files a/games/model/__pycache__/policy.cpython-310.pyc and b/games/model/__pycache__/policy.cpython-310.pyc differ
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index 3716d80..10f67ce 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20240623_124336-r6aa08rh/logs/debug-internal.log
\ No newline at end of file
+run-20240804_005910-1xpx5xyy/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index 8205bd4..cba46bf 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20240623_124336-r6aa08rh/logs/debug.log
\ No newline at end of file
+run-20240804_005910-1xpx5xyy/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 3d11859..7e70603 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20240623_124336-r6aa08rh
\ No newline at end of file
+run-20240804_005910-1xpx5xyy
\ No newline at end of file
