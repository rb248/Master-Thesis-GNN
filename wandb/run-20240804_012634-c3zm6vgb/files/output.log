
Using cpu device
-----------------------------
| time/              |      |
|    fps             | 136  |
|    iterations      | 1    |
|    time_elapsed    | 14   |
|    total_timesteps | 2048 |
-----------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -1.02e+03   |
| time/                   |             |
|    fps                  | 50          |
|    iterations           | 2           |
|    time_elapsed         | 81          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.010823517 |
|    clip_fraction        | 0.0819      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.000827    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.38        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0019     |
|    value_loss           | 3.85        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.05e+03     |
|    ep_rew_mean          | -1.02e+03    |
| time/                   |              |
|    fps                  | 42           |
|    iterations           | 3            |
|    time_elapsed         | 145          |
|    total_timesteps      | 6144         |
| train/                  |              |
|    approx_kl            | 0.0029491296 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 6.68e-06     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0204       |
|    n_updates            | 20           |
|    policy_gradient_loss | -8.5e-05     |
|    value_loss           | 14.6         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.05e+03    |
|    ep_rew_mean          | -1.02e+03   |
| time/                   |             |
|    fps                  | 39          |
|    iterations           | 4           |
|    time_elapsed         | 209         |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.010214428 |
|    clip_fraction        | 0.0249      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 8.31e-05    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.155       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.000507   |
|    value_loss           | 10.6        |
-----------------------------------------
Num timesteps: 10000
Best mean reward: -inf - Last mean reward per episode: -1031.12
Saving new best model at 8251 timesteps
Saving new best model to ./logs/Freeway-GNN-training/best_model.zip
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.06e+03     |
|    ep_rew_mean          | -1.03e+03    |
| time/                   |              |
|    fps                  | 38           |
|    iterations           | 5            |
|    time_elapsed         | 268          |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.0034402257 |
|    clip_fraction        | 0.00425      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.03        |
|    explained_variance   | 9.54e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000382    |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.000314    |
|    value_loss           | 6.64         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.14e+03    |
|    ep_rew_mean          | -1.07e+03   |
| time/                   |             |
|    fps                  | 37          |
|    iterations           | 6           |
|    time_elapsed         | 325         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.009305321 |
|    clip_fraction        | 0.019       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | -0.000473   |
|    learning_rate        | 0.0003      |
|    loss                 | 0.376       |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.000585   |
|    value_loss           | 6.71        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.21e+03    |
|    ep_rew_mean          | -1.1e+03    |
| time/                   |             |
|    fps                  | 37          |
|    iterations           | 7           |
|    time_elapsed         | 383         |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.014084557 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 0.064       |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00408    |
|    value_loss           | 6.16        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.19e+03    |
|    ep_rew_mean          | -1.1e+03    |
| time/                   |             |
|    fps                  | 37          |
|    iterations           | 8           |
|    time_elapsed         | 442         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.013863618 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.998      |
|    explained_variance   | -0.0227     |
|    learning_rate        | 0.0003      |
|    loss                 | 9.47        |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00238    |
|    value_loss           | 5.81        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.19e+03    |
|    ep_rew_mean          | -1.1e+03    |
| time/                   |             |
|    fps                  | 36          |
|    iterations           | 9           |
|    time_elapsed         | 501         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.006013021 |
|    clip_fraction        | 0.0396      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.981      |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.31        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.000901   |
|    value_loss           | 6.37        |
-----------------------------------------
Traceback (most recent call last):
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/rishubbhatia/Documents/master thesis/Master-Thesis-GNN/games/freeway/run_supervised_gnn.py", line 290, in <module>
    model.learn(total_timesteps=1000000, callback=[WandbCallback(), save_callback])
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    return super().learn(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 313, in learn
    self.train()
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 279, in train
    loss.backward()
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/Users/rishubbhatia/miniforge3/envs/games/lib/python3.10/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt